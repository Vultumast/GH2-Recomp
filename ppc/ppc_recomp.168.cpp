#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_8265E4E8"))) PPC_WEAK_FUNC(sub_8265E4E8);
PPC_FUNC_IMPL(__imp__sub_8265E4E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, ctx.r9.u32);
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, ctx.r9.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,48(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// lhz r10,14(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 14);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// li r10,8
	ctx.r10.s64 = 8;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// lwz r10,-52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// li r10,8
	ctx.r10.s64 = 8;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, r11.u32);
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, ctx.r9.u32);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265e684
	if (!cr6.gt) goto loc_8265E684;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265E684:
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265e69c
	if (!cr6.gt) goto loc_8265E69C;
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
loc_8265E69C:
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x8265e6ac
	if (cr6.gt) goto loc_8265E6AC;
	// b 0x8265f3a0
	goto loc_8265F3A0;
loc_8265E6AC:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265e6e8
	if (cr6.eq) goto loc_8265E6E8;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,76(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265e6e8
	if (!cr6.eq) goto loc_8265E6E8;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,92(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 92);
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
	// b 0x8265e714
	goto loc_8265E714;
loc_8265E6E8:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
loc_8265E714:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265e748
	if (cr6.eq) goto loc_8265E748;
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
	// b 0x8265e750
	goto loc_8265E750;
loc_8265E748:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
loc_8265E750:
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265e860
	if (!cr6.lt) goto loc_8265E860;
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, ctx.r9.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// b 0x8265e7c0
	goto loc_8265E7C0;
loc_8265E7B4:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
loc_8265E7C0:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265e848
	if (!cr6.lt) goto loc_8265E848;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265e7fc
	goto loc_8265E7FC;
loc_8265E7F0:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265E7FC:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265e834
	if (!cr6.lt) goto loc_8265E834;
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r9,-20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stbx r9,r11,r10
	PPC_STORE_U8(r11.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
	// b 0x8265e7f0
	goto loc_8265E7F0;
loc_8265E834:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// b 0x8265e7b4
	goto loc_8265E7B4;
loc_8265E848:
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
loc_8265E860:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// b 0x8265e880
	goto loc_8265E880;
loc_8265E874:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
loc_8265E880:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// lwz r10,-24(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265e974
	if (!cr6.lt) goto loc_8265E974;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265e8ec
	goto loc_8265E8EC;
loc_8265E8E0:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265E8EC:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265e950
	if (!cr6.lt) goto loc_8265E950;
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,-80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r9,-32(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265e8e0
	goto loc_8265E8E0;
loc_8265E950:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// b 0x8265e874
	goto loc_8265E874;
loc_8265E974:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265eb0c
	if (!cr6.lt) goto loc_8265EB0C;
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// b 0x8265e99c
	goto loc_8265E99C;
loc_8265E990:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
loc_8265E99C:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265eb0c
	if (!cr6.lt) goto loc_8265EB0C;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ea98
	if (!cr6.lt) goto loc_8265EA98;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265ea20
	goto loc_8265EA20;
loc_8265EA14:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265EA20:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265ea84
	if (!cr6.lt) goto loc_8265EA84;
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,-80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r9,-32(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265ea14
	goto loc_8265EA14;
loc_8265EA84:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// b 0x8265eaf8
	goto loc_8265EAF8;
loc_8265EA98:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265eab0
	goto loc_8265EAB0;
loc_8265EAA4:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265EAB0:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265eae8
	if (!cr6.lt) goto loc_8265EAE8;
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stbx r9,r11,r10
	PPC_STORE_U8(r11.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265eaa4
	goto loc_8265EAA4;
loc_8265EAE8:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
loc_8265EAF8:
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// b 0x8265e990
	goto loc_8265E990;
loc_8265EB0C:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265eb48
	if (cr6.eq) goto loc_8265EB48;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,76(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265eb48
	if (!cr6.eq) goto loc_8265EB48;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,84(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,96(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 96);
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
	// b 0x8265eb98
	goto loc_8265EB98;
loc_8265EB48:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// lwz r9,32(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,-28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
loc_8265EB98:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lwz r9,-88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265ebec
	if (cr6.eq) goto loc_8265EBEC;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
	// b 0x8265ebf4
	goto loc_8265EBF4;
loc_8265EBEC:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
loc_8265EBF4:
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265ed38
	if (!cr6.lt) goto loc_8265ED38;
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, ctx.r9.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// b 0x8265ec8c
	goto loc_8265EC8C;
loc_8265EC80:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
loc_8265EC8C:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265ed20
	if (!cr6.lt) goto loc_8265ED20;
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265ecd8
	goto loc_8265ECD8;
loc_8265ECCC:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265ECD8:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ed0c
	if (!cr6.lt) goto loc_8265ED0C;
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stbx r9,r11,r10
	PPC_STORE_U8(r11.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265eccc
	goto loc_8265ECCC;
loc_8265ED0C:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// b 0x8265ec80
	goto loc_8265EC80;
loc_8265ED20:
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
loc_8265ED38:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// b 0x8265ed6c
	goto loc_8265ED6C;
loc_8265ED60:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
loc_8265ED6C:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265ee5c
	if (!cr6.lt) goto loc_8265EE5C;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265eddc
	goto loc_8265EDDC;
loc_8265EDD0:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265EDDC:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ee38
	if (!cr6.lt) goto loc_8265EE38;
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r9,-28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r9,-32(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265edd0
	goto loc_8265EDD0;
loc_8265EE38:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// b 0x8265ed60
	goto loc_8265ED60;
loc_8265EE5C:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ef40
	if (!cr6.lt) goto loc_8265EF40;
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// b 0x8265ee9c
	goto loc_8265EE9C;
loc_8265EE90:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
loc_8265EE9C:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ef40
	if (!cr6.lt) goto loc_8265EF40;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265eee8
	goto loc_8265EEE8;
loc_8265EEDC:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265EEE8:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ef1c
	if (!cr6.lt) goto loc_8265EF1C;
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stbx r9,r11,r10
	PPC_STORE_U8(r11.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265eedc
	goto loc_8265EEDC;
loc_8265EF1C:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// b 0x8265ee90
	goto loc_8265EE90;
loc_8265EF40:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265ef7c
	if (cr6.eq) goto loc_8265EF7C;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,76(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265ef7c
	if (!cr6.eq) goto loc_8265EF7C;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,88(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 88);
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,100(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 100);
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
	// b 0x8265efe8
	goto loc_8265EFE8;
loc_8265EF7C:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// lwz r9,32(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-44(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r9,-28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,-28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
loc_8265EFE8:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lwz r9,-88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r9,-68(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265f058
	if (cr6.eq) goto loc_8265F058;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r11.u32);
	// b 0x8265f060
	goto loc_8265F060;
loc_8265F058:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r11.u32);
loc_8265F060:
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265f198
	if (!cr6.lt) goto loc_8265F198;
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, ctx.r9.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// b 0x8265f0ec
	goto loc_8265F0EC;
loc_8265F0E0:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
loc_8265F0EC:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265f180
	if (!cr6.lt) goto loc_8265F180;
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265f138
	goto loc_8265F138;
loc_8265F12C:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265F138:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265f16c
	if (!cr6.lt) goto loc_8265F16C;
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stbx r9,r11,r10
	PPC_STORE_U8(r11.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265f12c
	goto loc_8265F12C;
loc_8265F16C:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// b 0x8265f0e0
	goto loc_8265F0E0;
loc_8265F180:
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
loc_8265F198:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// b 0x8265f1cc
	goto loc_8265F1CC;
loc_8265F1C0:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
loc_8265F1CC:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265f2bc
	if (!cr6.lt) goto loc_8265F2BC;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265f23c
	goto loc_8265F23C;
loc_8265F230:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265F23C:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265f298
	if (!cr6.lt) goto loc_8265F298;
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r9,-28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r9,-32(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265f230
	goto loc_8265F230;
loc_8265F298:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// b 0x8265f1c0
	goto loc_8265F1C0;
loc_8265F2BC:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265f3a0
	if (!cr6.lt) goto loc_8265F3A0;
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// b 0x8265f2fc
	goto loc_8265F2FC;
loc_8265F2F0:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
loc_8265F2FC:
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265f3a0
	if (!cr6.lt) goto loc_8265F3A0;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265f348
	goto loc_8265F348;
loc_8265F33C:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265F348:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265f37c
	if (!cr6.lt) goto loc_8265F37C;
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stbx r9,r11,r10
	PPC_STORE_U8(r11.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265f33c
	goto loc_8265F33C;
loc_8265F37C:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// b 0x8265f2f0
	goto loc_8265F2F0;
loc_8265F3A0:
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265F3A4"))) PPC_WEAK_FUNC(sub_8265F3A4);
PPC_FUNC_IMPL(__imp__sub_8265F3A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265F3A8"))) PPC_WEAK_FUNC(sub_8265F3A8);
PPC_FUNC_IMPL(__imp__sub_8265F3A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r3,420(r1)
	PPC_STORE_U32(ctx.r1.u32 + 420, ctx.r3.u32);
	// stw r4,428(r1)
	PPC_STORE_U32(ctx.r1.u32 + 428, ctx.r4.u32);
	// stw r5,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, ctx.r5.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r11.u32);
	// lwz r11,204(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r9.u32);
	// lwz r11,272(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// rlwinm r11,r11,7,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, ctx.r9.u32);
	// lwz r11,204(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,204(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, ctx.r9.u32);
	// lwz r11,272(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,272(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r9.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,48(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// lhz r10,14(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 14);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// li r10,8
	ctx.r10.s64 = 8;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// lwz r10,272(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// li r10,8
	ctx.r10.s64 = 8;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,280(r1)
	PPC_STORE_U32(ctx.r1.u32 + 280, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,296(r1)
	PPC_STORE_U32(ctx.r1.u32 + 296, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, r11.u32);
	// lwz r11,272(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,272(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r9.u32);
	// lwz r11,240(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,276(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,240(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, ctx.r9.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r9,272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,428(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, r11.u32);
	// lwz r11,252(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// stw r11,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,252(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, r11.u32);
	// lwz r11,192(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r11.u32);
	// lwz r11,248(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// stw r11,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, r11.u32);
	// lwz r11,264(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x8265f6a4
	if (!cr6.eq) goto loc_8265F6A4;
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// stw r11,360(r1)
	PPC_STORE_U32(ctx.r1.u32 + 360, r11.u32);
	// lwz r11,360(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// beq cr6,0x8265f674
	if (cr6.eq) goto loc_8265F674;
	// lwz r11,360(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	// cmpwi cr6,r11,128
	cr6.compare<int32_t>(r11.s32, 128, xer);
	// beq cr6,0x8265f680
	if (cr6.eq) goto loc_8265F680;
	// lwz r11,360(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	// cmpwi cr6,r11,192
	cr6.compare<int32_t>(r11.s32, 192, xer);
	// beq cr6,0x8265f68c
	if (cr6.eq) goto loc_8265F68C;
	// b 0x8265f698
	goto loc_8265F698;
loc_8265F674:
	// li r11,64
	r11.s64 = 64;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// b 0x8265f6a0
	goto loc_8265F6A0;
loc_8265F680:
	// li r11,128
	r11.s64 = 128;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// b 0x8265f6a0
	goto loc_8265F6A0;
loc_8265F68C:
	// li r11,192
	r11.s64 = 192;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// b 0x8265f6a0
	goto loc_8265F6A0;
loc_8265F698:
	// li r11,0
	r11.s64 = 0;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
loc_8265F6A0:
	// b 0x8265f6ac
	goto loc_8265F6AC;
loc_8265F6A4:
	// li r11,0
	r11.s64 = 0;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
loc_8265F6AC:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265f6d8
	if (cr6.eq) goto loc_8265F6D8;
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,92(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 92);
	// stw r11,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r11.u32);
	// b 0x8265f704
	goto loc_8265F704;
loc_8265F6D8:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,428(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r11.u32);
loc_8265F704:
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r3,32(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// stw r3,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r3.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// stw r3,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r3.u32);
	// lwz r11,264(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265f744
	if (!cr6.lt) goto loc_8265F744;
	// li r11,0
	r11.s64 = 0;
	// stw r11,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, r11.u32);
loc_8265F744:
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x8265f754
	if (cr6.gt) goto loc_8265F754;
	// b 0x82661808
	goto loc_82661808;
loc_8265F754:
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r10,436(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265f76c
	if (!cr6.gt) goto loc_8265F76C;
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// stw r11,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, r11.u32);
loc_8265F76C:
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r10,212(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265f784
	if (!cr6.gt) goto loc_8265F784;
	// lwz r11,212(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// stw r11,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, r11.u32);
loc_8265F784:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265f7ac
	if (cr6.eq) goto loc_8265F7AC;
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, r11.u32);
	// b 0x8265f7b4
	goto loc_8265F7B4;
loc_8265F7AC:
	// li r11,0
	r11.s64 = 0;
	// stw r11,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, r11.u32);
loc_8265F7B4:
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x8265f7f4
	goto loc_8265F7F4;
loc_8265F7E8:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_8265F7F4:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,216(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265f848
	if (!cr6.lt) goto loc_8265F848;
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,144(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stwx r11,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, r11.u32);
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// lwz r10,264(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// b 0x8265f7e8
	goto loc_8265F7E8;
loc_8265F848:
	// b 0x8265f858
	goto loc_8265F858;
loc_8265F84C:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_8265F858:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265f898
	if (!cr6.lt) goto loc_8265F898;
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,144(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stwx r11,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, r11.u32);
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// lwz r10,264(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// b 0x8265f84c
	goto loc_8265F84C;
loc_8265F898:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265fa7c
	if (!cr6.lt) goto loc_8265FA7C;
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// stw r11,304(r1)
	PPC_STORE_U32(ctx.r1.u32 + 304, r11.u32);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r9.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x8265f8f0
	goto loc_8265F8F0;
loc_8265F8E4:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_8265F8F0:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,216(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265f978
	if (!cr6.lt) goto loc_8265F978;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,308(r1)
	PPC_STORE_U32(ctx.r1.u32 + 308, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stw r11,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r11.u32);
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// lwz r11,152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r10,308(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,308(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,152(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// b 0x8265f8e4
	goto loc_8265F8E4;
loc_8265F978:
	// b 0x8265f988
	goto loc_8265F988;
loc_8265F97C:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_8265F988:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265f9c4
	if (!cr6.lt) goto loc_8265F9C4;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r9,152(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r9,r11
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stbx r11,r10,r8
	PPC_STORE_U8(ctx.r10.u32 + ctx.r8.u32, r11.u8);
	// b 0x8265f97c
	goto loc_8265F97C;
loc_8265F9C4:
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// b 0x8265f9f0
	goto loc_8265F9F0;
loc_8265F9E4:
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
loc_8265F9F0:
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,188(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265fa64
	if (!cr6.lt) goto loc_8265FA64;
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x8265fa20
	goto loc_8265FA20;
loc_8265FA14:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_8265FA20:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265fa50
	if (!cr6.lt) goto loc_8265FA50;
	// lwz r11,304(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r9,244(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stbx r11,r9,r8
	PPC_STORE_U8(ctx.r9.u32 + ctx.r8.u32, r11.u8);
	// b 0x8265fa14
	goto loc_8265FA14;
loc_8265FA50:
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// b 0x8265f9e4
	goto loc_8265F9E4;
loc_8265FA64:
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
loc_8265FA7C:
	// lwz r11,164(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// stw r11,368(r1)
	PPC_STORE_U32(ctx.r1.u32 + 368, r11.u32);
	// lwz r11,368(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// beq cr6,0x8265faac
	if (cr6.eq) goto loc_8265FAAC;
	// lwz r11,368(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// cmpwi cr6,r11,128
	cr6.compare<int32_t>(r11.s32, 128, xer);
	// beq cr6,0x8265fbbc
	if (cr6.eq) goto loc_8265FBBC;
	// lwz r11,368(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// cmpwi cr6,r11,192
	cr6.compare<int32_t>(r11.s32, 192, xer);
	// beq cr6,0x8265fccc
	if (cr6.eq) goto loc_8265FCCC;
	// b 0x8265fddc
	goto loc_8265FDDC;
loc_8265FAAC:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265fb38
	if (!cr6.eq) goto loc_8265FB38;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r9,272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r7,32(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r6,216(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82675b30
	sub_82675B30(ctx, base);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// b 0x8265fbb8
	goto loc_8265FBB8;
loc_8265FB38:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r9,272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r7,32(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r6,216(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x826767b8
	sub_826767B8(ctx, base);
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
loc_8265FBB8:
	// b 0x8265fe94
	goto loc_8265FE94;
loc_8265FBBC:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265fc48
	if (!cr6.eq) goto loc_8265FC48;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r9,272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r7,32(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r6,216(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82675238
	sub_82675238(ctx, base);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// b 0x8265fcc8
	goto loc_8265FCC8;
loc_8265FC48:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r9,272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r7,32(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r6,216(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82675610
	sub_82675610(ctx, base);
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
loc_8265FCC8:
	// b 0x8265fe94
	goto loc_8265FE94;
loc_8265FCCC:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265fd58
	if (!cr6.eq) goto loc_8265FD58;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r9,272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r7,32(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r6,216(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82677d08
	sub_82677D08(ctx, base);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// b 0x8265fdd8
	goto loc_8265FDD8;
loc_8265FD58:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r9,272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r7,32(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r6,216(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82678580
	sub_82678580(ctx, base);
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
loc_8265FDD8:
	// b 0x8265fe94
	goto loc_8265FE94;
loc_8265FDDC:
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// lwz r11,264(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r9,272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r7,32(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r6,216(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82678e88
	sub_82678E88(ctx, base);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
loc_8265FE94:
	// li r11,0
	r11.s64 = 0;
	// stw r11,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r11.u32);
	// lwz r11,192(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r11.u32);
	// lwz r11,248(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// stw r11,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, r11.u32);
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r10,436(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660380
	if (!cr6.lt) goto loc_82660380;
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// b 0x8265fedc
	goto loc_8265FEDC;
loc_8265FED0:
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
loc_8265FEDC:
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r10,436(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660380
	if (!cr6.lt) goto loc_82660380;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, r11.u32);
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,272(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, r11.u32);
	// lwz r11,168(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r11.u32);
	// lwz r11,152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r10,172(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r11.u32);
	// lwz r11,152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r10,172(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,272(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r11.u32);
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,204(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660268
	if (!cr6.lt) goto loc_82660268;
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r10,180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82660070
	if (!cr6.eq) goto loc_82660070;
	// lwz r11,232(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// stw r11,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r11.u32);
	// lwz r11,284(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r11.u32);
	// lwz r11,312(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// stw r11,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x8265ff98
	goto loc_8265FF98;
loc_8265FF8C:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_8265FF98:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,216(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660020
	if (!cr6.lt) goto loc_82660020;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stw r11,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r11.u32);
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// lwz r11,220(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lwz r10,316(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,316(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,220(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// b 0x8265ff8c
	goto loc_8265FF8C;
loc_82660020:
	// b 0x82660030
	goto loc_82660030;
loc_82660024:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82660030:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8266006c
	if (!cr6.lt) goto loc_8266006C;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r9,220(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r9,r11
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stbx r11,r10,r8
	PPC_STORE_U8(ctx.r10.u32 + ctx.r8.u32, r11.u8);
	// b 0x82660024
	goto loc_82660024;
loc_8266006C:
	// b 0x826601e4
	goto loc_826601E4;
loc_82660070:
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82660084
	if (!cr6.eq) goto loc_82660084;
	// b 0x826601e4
	goto loc_826601E4;
loc_82660084:
	// lwz r11,192(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r11.u32);
	// lwz r11,248(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// stw r11,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x826600ac
	goto loc_826600AC;
loc_826600A0:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_826600AC:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,216(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660178
	if (!cr6.lt) goto loc_82660178;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,320(r1)
	PPC_STORE_U32(ctx.r1.u32 + 320, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stw r11,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r11.u32);
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r10,320(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,320(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,232(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,220(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lwz r10,320(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,320(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,220(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// b 0x826600a0
	goto loc_826600A0;
loc_82660178:
	// b 0x82660188
	goto loc_82660188;
loc_8266017C:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82660188:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826601e4
	if (!cr6.lt) goto loc_826601E4;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, r11.u32);
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r10,324(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// lwz r9,232(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stbx r11,r9,r8
	PPC_STORE_U8(ctx.r9.u32 + ctx.r8.u32, r11.u8);
	// lwz r11,220(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lwz r10,324(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// lwz r9,284(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stbx r11,r9,r8
	PPC_STORE_U8(ctx.r9.u32 + ctx.r8.u32, r11.u8);
	// b 0x8266017c
	goto loc_8266017C;
loc_826601E4:
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// stw r11,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r11.u32);
	// lwz r11,220(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// stw r11,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x8266020c
	goto loc_8266020C;
loc_82660200:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_8266020C:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x82660264
	if (!cr6.lt) goto loc_82660264;
	// lwz r11,232(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,208(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,168(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// b 0x82660200
	goto loc_82660200;
loc_82660264:
	// b 0x8266035c
	goto loc_8266035C;
loc_82660268:
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x82660280
	goto loc_82660280;
loc_82660274:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82660280:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,216(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660308
	if (!cr6.lt) goto loc_82660308;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,328(r1)
	PPC_STORE_U32(ctx.r1.u32 + 328, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stw r11,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r11.u32);
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r10,328(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,328(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// b 0x82660274
	goto loc_82660274;
loc_82660308:
	// b 0x82660318
	goto loc_82660318;
loc_8266030C:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82660318:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8266035c
	if (!cr6.lt) goto loc_8266035C;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, r11.u32);
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r10,332(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// lwz r9,244(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stbx r11,r9,r8
	PPC_STORE_U8(ctx.r9.u32 + ctx.r8.u32, r11.u8);
	// b 0x8266030c
	goto loc_8266030C;
loc_8266035C:
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// b 0x8265fed0
	goto loc_8265FED0;
loc_82660380:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826603ac
	if (cr6.eq) goto loc_826603AC;
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,84(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,96(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 96);
	// stw r11,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r11.u32);
	// b 0x826603fc
	goto loc_826603FC;
loc_826603AC:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,420(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// lwz r9,32(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,428(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,296(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,296(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// stw r11,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r11.u32);
loc_826603FC:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lwz r9,204(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,204(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// lwz r9,272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,428(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r11.u32);
	// lwz r11,252(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// stw r11,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r11.u32);
	// lwz r11,252(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r10,296(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, r11.u32);
	// lwz r11,192(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r11.u32);
	// lwz r11,248(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// stw r11,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826604b8
	if (cr6.eq) goto loc_826604B8;
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, r11.u32);
	// b 0x826604c0
	goto loc_826604C0;
loc_826604B8:
	// li r11,0
	r11.s64 = 0;
	// stw r11,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, r11.u32);
loc_826604C0:
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x82660514
	goto loc_82660514;
loc_82660508:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82660514:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660568
	if (!cr6.lt) goto loc_82660568;
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,144(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stwx r11,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, r11.u32);
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// lwz r10,264(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// b 0x82660508
	goto loc_82660508;
loc_82660568:
	// b 0x82660578
	goto loc_82660578;
loc_8266056C:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82660578:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,296(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826605b4
	if (!cr6.lt) goto loc_826605B4;
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,144(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stwx r11,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, r11.u32);
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// lwz r10,264(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// b 0x8266056c
	goto loc_8266056C;
loc_826605B4:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x826607b8
	if (!cr6.lt) goto loc_826607B8;
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// stw r11,336(r1)
	PPC_STORE_U32(ctx.r1.u32 + 336, r11.u32);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r9.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x8266060c
	goto loc_8266060C;
loc_82660600:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_8266060C:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660694
	if (!cr6.lt) goto loc_82660694;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stw r11,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r11.u32);
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// lwz r11,152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r10,340(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,340(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,152(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// b 0x82660600
	goto loc_82660600;
loc_82660694:
	// b 0x826606a4
	goto loc_826606A4;
loc_82660698:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_826606A4:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,296(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826606dc
	if (!cr6.lt) goto loc_826606DC;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r9,152(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r9,r11
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stbx r11,r10,r8
	PPC_STORE_U8(ctx.r10.u32 + ctx.r8.u32, r11.u8);
	// b 0x82660698
	goto loc_82660698;
loc_826606DC:
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// b 0x8266071c
	goto loc_8266071C;
loc_82660710:
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
loc_8266071C:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,188(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826607a0
	if (!cr6.lt) goto loc_826607A0;
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x82660760
	goto loc_82660760;
loc_82660754:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82660760:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,296(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8266078c
	if (!cr6.lt) goto loc_8266078C;
	// lwz r11,336(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r9,244(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stbx r11,r9,r8
	PPC_STORE_U8(ctx.r9.u32 + ctx.r8.u32, r11.u8);
	// b 0x82660754
	goto loc_82660754;
loc_8266078C:
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// b 0x82660710
	goto loc_82660710;
loc_826607A0:
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
loc_826607B8:
	// lwz r11,164(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// stw r11,376(r1)
	PPC_STORE_U32(ctx.r1.u32 + 376, r11.u32);
	// lwz r11,376(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// beq cr6,0x826607e8
	if (cr6.eq) goto loc_826607E8;
	// lwz r11,376(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	// cmpwi cr6,r11,128
	cr6.compare<int32_t>(r11.s32, 128, xer);
	// beq cr6,0x82660928
	if (cr6.eq) goto loc_82660928;
	// lwz r11,376(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	// cmpwi cr6,r11,192
	cr6.compare<int32_t>(r11.s32, 192, xer);
	// beq cr6,0x82660a68
	if (cr6.eq) goto loc_82660A68;
	// b 0x82660ba8
	goto loc_82660BA8;
loc_826607E8:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8266088c
	if (!cr6.eq) goto loc_8266088C;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82675b30
	sub_82675B30(ctx, base);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// b 0x82660924
	goto loc_82660924;
loc_8266088C:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x826767b8
	sub_826767B8(ctx, base);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
loc_82660924:
	// b 0x82660c78
	goto loc_82660C78;
loc_82660928:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826609cc
	if (!cr6.eq) goto loc_826609CC;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82675238
	sub_82675238(ctx, base);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// b 0x82660a64
	goto loc_82660A64;
loc_826609CC:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82675610
	sub_82675610(ctx, base);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
loc_82660A64:
	// b 0x82660c78
	goto loc_82660C78;
loc_82660A68:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82660b0c
	if (!cr6.eq) goto loc_82660B0C;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82677d08
	sub_82677D08(ctx, base);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// b 0x82660ba4
	goto loc_82660BA4;
loc_82660B0C:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82678580
	sub_82678580(ctx, base);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
loc_82660BA4:
	// b 0x82660c78
	goto loc_82660C78;
loc_82660BA8:
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// lwz r11,264(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82678e88
	sub_82678E88(ctx, base);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
loc_82660C78:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,212(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660df8
	if (!cr6.lt) goto loc_82660DF8;
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// b 0x82660cb8
	goto loc_82660CB8;
loc_82660CAC:
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
loc_82660CB8:
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r10,212(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660df8
	if (!cr6.lt) goto loc_82660DF8;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,260(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, r11.u32);
	// lwz r11,152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r10,172(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x82660d04
	goto loc_82660D04;
loc_82660CF8:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82660D04:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660d8c
	if (!cr6.lt) goto loc_82660D8C;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,344(r1)
	PPC_STORE_U32(ctx.r1.u32 + 344, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stw r11,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r11.u32);
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r10,344(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,344(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// b 0x82660cf8
	goto loc_82660CF8;
loc_82660D8C:
	// b 0x82660d9c
	goto loc_82660D9C;
loc_82660D90:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82660D9C:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,296(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82660dd4
	if (!cr6.lt) goto loc_82660DD4;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r9,196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r9,r11
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stbx r11,r10,r8
	PPC_STORE_U8(ctx.r10.u32 + ctx.r8.u32, r11.u8);
	// b 0x82660d90
	goto loc_82660D90;
loc_82660DD4:
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// b 0x82660cac
	goto loc_82660CAC;
loc_82660DF8:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82660e24
	if (cr6.eq) goto loc_82660E24;
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,88(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 88);
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,100(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 100);
	// stw r11,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r11.u32);
	// b 0x82660e90
	goto loc_82660E90;
loc_82660E24:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,420(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// lwz r9,32(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,276(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r9,296(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,428(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,296(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,296(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// stw r11,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r11.u32);
loc_82660E90:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lwz r9,204(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,296(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r9,240(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,204(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// lwz r9,272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,240(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r9,420(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,44(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,428(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r11.u32);
	// lwz r11,252(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// stw r11,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r11.u32);
	// lwz r11,252(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r10,296(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, r11.u32);
	// lwz r11,192(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r11.u32);
	// lwz r11,248(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// stw r11,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82660f84
	if (cr6.eq) goto loc_82660F84;
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, r11.u32);
	// b 0x82660f8c
	goto loc_82660F8C;
loc_82660F84:
	// li r11,0
	r11.s64 = 0;
	// stw r11,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, r11.u32);
loc_82660F8C:
	// lwz r11,380(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x826611b8
	if (!cr6.lt) goto loc_826611B8;
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// stw r11,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, r11.u32);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r9.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x8266100c
	goto loc_8266100C;
loc_82661000:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_8266100C:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82661094
	if (!cr6.lt) goto loc_82661094;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stw r11,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r11.u32);
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// lwz r11,152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r10,352(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,352(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,152(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// b 0x82661000
	goto loc_82661000;
loc_82661094:
	// b 0x826610a4
	goto loc_826610A4;
loc_82661098:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_826610A4:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,296(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826610dc
	if (!cr6.lt) goto loc_826610DC;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r9,152(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r9,r11
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stbx r11,r10,r8
	PPC_STORE_U8(ctx.r10.u32 + ctx.r8.u32, r11.u8);
	// b 0x82661098
	goto loc_82661098;
loc_826610DC:
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// b 0x8266111c
	goto loc_8266111C;
loc_82661110:
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
loc_8266111C:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,188(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826611a0
	if (!cr6.lt) goto loc_826611A0;
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x82661160
	goto loc_82661160;
loc_82661154:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82661160:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,296(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8266118c
	if (!cr6.lt) goto loc_8266118C;
	// lwz r11,348(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r9,244(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stbx r11,r9,r8
	PPC_STORE_U8(ctx.r9.u32 + ctx.r8.u32, r11.u8);
	// b 0x82661154
	goto loc_82661154;
loc_8266118C:
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// b 0x82661110
	goto loc_82661110;
loc_826611A0:
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
loc_826611B8:
	// lwz r11,164(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// stw r11,384(r1)
	PPC_STORE_U32(ctx.r1.u32 + 384, r11.u32);
	// lwz r11,384(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// beq cr6,0x826611e8
	if (cr6.eq) goto loc_826611E8;
	// lwz r11,384(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	// cmpwi cr6,r11,128
	cr6.compare<int32_t>(r11.s32, 128, xer);
	// beq cr6,0x82661328
	if (cr6.eq) goto loc_82661328;
	// lwz r11,384(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	// cmpwi cr6,r11,192
	cr6.compare<int32_t>(r11.s32, 192, xer);
	// beq cr6,0x82661468
	if (cr6.eq) goto loc_82661468;
	// b 0x826615a8
	goto loc_826615A8;
loc_826611E8:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8266128c
	if (!cr6.eq) goto loc_8266128C;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82675b30
	sub_82675B30(ctx, base);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// b 0x82661324
	goto loc_82661324;
loc_8266128C:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x826767b8
	sub_826767B8(ctx, base);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
loc_82661324:
	// b 0x82661678
	goto loc_82661678;
loc_82661328:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826613cc
	if (!cr6.eq) goto loc_826613CC;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82675238
	sub_82675238(ctx, base);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// b 0x82661464
	goto loc_82661464;
loc_826613CC:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82675610
	sub_82675610(ctx, base);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
loc_82661464:
	// b 0x82661678
	goto loc_82661678;
loc_82661468:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8266150c
	if (!cr6.eq) goto loc_8266150C;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82677d08
	sub_82677D08(ctx, base);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// b 0x826615a4
	goto loc_826615A4;
loc_8266150C:
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82678580
	sub_82678580(ctx, base);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
loc_826615A4:
	// b 0x82661678
	goto loc_82661678;
loc_826615A8:
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// lwz r11,264(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,428(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r8,288(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r7,296(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r5,252(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,244(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// bl 0x82678e88
	sub_82678E88(ctx, base);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
loc_82661678:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r9,212(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826617f8
	if (!cr6.lt) goto loc_826617F8;
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// b 0x826616b8
	goto loc_826616B8;
loc_826616AC:
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
loc_826616B8:
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r10,212(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826617f8
	if (!cr6.lt) goto loc_826617F8;
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,260(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, r11.u32);
	// lwz r11,152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r10,172(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// b 0x82661704
	goto loc_82661704;
loc_826616F8:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_82661704:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8266178c
	if (!cr6.lt) goto loc_8266178C;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, r11.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stw r11,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r11.u32);
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r10,356(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,356(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r9,292(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// b 0x826616f8
	goto loc_826616F8;
loc_8266178C:
	// b 0x8266179c
	goto loc_8266179C;
loc_82661790:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
loc_8266179C:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,296(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826617d4
	if (!cr6.lt) goto loc_826617D4;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r9,196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lbzx r11,r9,r11
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stbx r11,r10,r8
	PPC_STORE_U8(ctx.r10.u32 + ctx.r8.u32, r11.u8);
	// b 0x82661790
	goto loc_82661790;
loc_826617D4:
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r10,288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
	// lwz r11,236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// b 0x826616ac
	goto loc_826616AC;
loc_826617F8:
	// lwz r3,148(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// bl 0x82604090
	sub_82604090(ctx, base);
	// lwz r3,144(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_82661808:
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82661818"))) PPC_WEAK_FUNC(sub_82661818);
PPC_FUNC_IMPL(__imp__sub_82661818) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// stw r4,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r4.u32);
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// lwz r5,132(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r4,124(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266185C"))) PPC_WEAK_FUNC(sub_8266185C);
PPC_FUNC_IMPL(__imp__sub_8266185C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82661860"))) PPC_WEAK_FUNC(sub_82661860);
PPC_FUNC_IMPL(__imp__sub_82661860) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// stw r4,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r4.u32);
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// lwz r5,132(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r4,124(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826618A4"))) PPC_WEAK_FUNC(sub_826618A4);
PPC_FUNC_IMPL(__imp__sub_826618A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826618A8"))) PPC_WEAK_FUNC(sub_826618A8);
PPC_FUNC_IMPL(__imp__sub_826618A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// stw r4,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r4.u32);
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// stw r6,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r6.u32);
	// stw r7,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r7.u32);
	// stw r8,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r8.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// stw r10,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r10.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,140(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// stw r10,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r10.u32);
	// lwz r11,156(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82661908
	if (!cr6.lt) goto loc_82661908;
	// lwz r11,156(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
loc_82661908:
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8266191c
	if (!cr6.lt) goto loc_8266191C;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82661dfc
	goto loc_82661DFC;
loc_8266191C:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82661940
	if (cr6.eq) goto loc_82661940;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82661940
	if (cr6.eq) goto loc_82661940;
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82661948
	if (!cr6.eq) goto loc_82661948;
loc_82661940:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82661dfc
	goto loc_82661DFC;
loc_82661948:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// ori r10,r10,22857
	ctx.r10.u64 = ctx.r10.u64 | 22857;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x826619d0
	if (cr6.eq) goto loc_826619D0;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lis r10,12338
	ctx.r10.s64 = 808583168;
	// ori r10,r10,13385
	ctx.r10.u64 = ctx.r10.u64 | 13385;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x826619d0
	if (cr6.eq) goto loc_826619D0;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lis r10,12593
	ctx.r10.s64 = 825294848;
	// ori r10,r10,13392
	ctx.r10.u64 = ctx.r10.u64 | 13392;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x826619d0
	if (cr6.eq) goto loc_826619D0;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lhz r10,14(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 14);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// li r10,8
	ctx.r10.s64 = 8;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// li r10,4
	ctx.r10.s64 = 4;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// b 0x826619fc
	goto loc_826619FC;
loc_826619D0:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lhz r10,14(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 14);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// li r10,8
	ctx.r10.s64 = 8;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_826619FC:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// beq cr6,0x82661a18
	if (cr6.eq) goto loc_82661A18;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82661dfc
	goto loc_82661DFC;
loc_82661A18:
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82661a40
	if (cr6.eq) goto loc_82661A40;
	// lwz r11,156(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82661a40
	if (cr6.eq) goto loc_82661A40;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,60(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82661a48
	if (!cr6.eq) goto loc_82661A48;
loc_82661A40:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82661dfc
	goto loc_82661DFC;
loc_82661A48:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x82661a70
	if (!cr6.eq) goto loc_82661A70;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// beq cr6,0x82661a78
	if (cr6.eq) goto loc_82661A78;
loc_82661A70:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82661dfc
	goto loc_82661DFC;
loc_82661A78:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r10,24(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x82661aa8
	if (!cr6.eq) goto loc_82661AA8;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r10,28(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// beq cr6,0x82661ab0
	if (cr6.eq) goto loc_82661AB0;
loc_82661AA8:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82661dfc
	goto loc_82661DFC;
loc_82661AB0:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,60(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// stw r11,64(r10)
	PPC_STORE_U32(ctx.r10.u32 + 64, r11.u32);
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82661ba0
	if (cr6.eq) goto loc_82661BA0;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r10,12889
	ctx.r10.s64 = 844693504;
	// ori r10,r10,21849
	ctx.r10.u64 = ctx.r10.u64 | 21849;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x82661b44
	if (!cr6.eq) goto loc_82661B44;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82661b20
	if (cr6.eq) goto loc_82661B20;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lis r10,-32154
	ctx.r10.s64 = -2107244544;
	// addi r10,r10,-14232
	ctx.r10.s64 = ctx.r10.s64 + -14232;
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lis r10,-32154
	ctx.r10.s64 = -2107244544;
	// addi r10,r10,-12640
	ctx.r10.s64 = ctx.r10.s64 + -12640;
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// b 0x82661b40
	goto loc_82661B40;
loc_82661B20:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lis r10,-32154
	ctx.r10.s64 = -2107244544;
	// addi r10,r10,-17264
	ctx.r10.s64 = ctx.r10.s64 + -17264;
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lis r10,-32154
	ctx.r10.s64 = -2107244544;
	// addi r10,r10,-15520
	ctx.r10.s64 = ctx.r10.s64 + -15520;
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
loc_82661B40:
	// b 0x82661ba0
	goto loc_82661BA0;
loc_82661B44:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x82661b54
	if (!cr6.eq) goto loc_82661B54;
	// b 0x82661ba0
	goto loc_82661BA0;
loc_82661B54:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r10,12850
	ctx.r10.s64 = 842137600;
	// ori r10,r10,13392
	ctx.r10.u64 = ctx.r10.u64 | 13392;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82661b80
	if (!cr6.eq) goto loc_82661B80;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// li r10,2
	ctx.r10.s64 = 2;
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
loc_82661B80:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lis r10,-32154
	ctx.r10.s64 = -2107244544;
	// addi r10,r10,-9360
	ctx.r10.s64 = ctx.r10.s64 + -9360;
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lis r10,-32154
	ctx.r10.s64 = -2107244544;
	// addi r10,r10,-6936
	ctx.r10.s64 = ctx.r10.s64 + -6936;
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
loc_82661BA0:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,28(r10)
	PPC_STORE_U32(ctx.r10.u32 + 28, r11.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,24(r10)
	PPC_STORE_U32(ctx.r10.u32 + 24, r11.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r10,24(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x82661c28
	if (!cr6.eq) goto loc_82661C28;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// lwz r10,28(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x82661c28
	if (!cr6.eq) goto loc_82661C28;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r5,20(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r3,140(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// b 0x82661df8
	goto loc_82661DF8;
loc_82661C28:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// lwz r10,28(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x82661cac
	if (!cr6.eq) goto loc_82661CAC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82661c6c
	if (!cr6.eq) goto loc_82661C6C;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// cmpwi cr6,r11,24
	cr6.compare<int32_t>(r11.s32, 24, xer);
	// bge cr6,0x82661c94
	if (!cr6.lt) goto loc_82661C94;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// beq cr6,0x82661c94
	if (cr6.eq) goto loc_82661C94;
loc_82661C6C:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// ori r10,r10,22857
	ctx.r10.u64 = ctx.r10.u64 | 22857;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// beq cr6,0x82661c94
	if (cr6.eq) goto loc_82661C94;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r10,12338
	ctx.r10.s64 = 808583168;
	// ori r10,r10,13385
	ctx.r10.u64 = ctx.r10.u64 | 13385;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x82661cac
	if (!cr6.eq) goto loc_82661CAC;
loc_82661C94:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,140(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// stw r10,64(r11)
	PPC_STORE_U32(r11.u32 + 64, ctx.r10.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
loc_82661CAC:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r10,24(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x82661d68
	if (!cr6.eq) goto loc_82661D68;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82661cf0
	if (!cr6.eq) goto loc_82661CF0;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// cmpwi cr6,r11,24
	cr6.compare<int32_t>(r11.s32, 24, xer);
	// bge cr6,0x82661d18
	if (!cr6.lt) goto loc_82661D18;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// beq cr6,0x82661d18
	if (cr6.eq) goto loc_82661D18;
loc_82661CF0:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// ori r10,r10,22857
	ctx.r10.u64 = ctx.r10.u64 | 22857;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// beq cr6,0x82661d18
	if (cr6.eq) goto loc_82661D18;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r10,12338
	ctx.r10.s64 = 808583168;
	// ori r10,r10,13385
	ctx.r10.u64 = ctx.r10.u64 | 13385;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x82661d68
	if (!cr6.eq) goto loc_82661D68;
loc_82661D18:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// stw r10,64(r11)
	PPC_STORE_U32(r11.u32 + 64, ctx.r10.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82661d64
	if (cr6.eq) goto loc_82661D64;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x82661d64
	if (!cr6.eq) goto loc_82661D64;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r5,36(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x82661818
	sub_82661818(ctx, base);
loc_82661D64:
	// b 0x82661df8
	goto loc_82661DF8;
loc_82661D68:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x82661df8
	if (!cr6.eq) goto loc_82661DF8;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,68(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 68);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82661db0
	if (!cr6.eq) goto loc_82661DB0;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82661db0
	if (cr6.eq) goto loc_82661DB0;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r5,36(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x8265f3a8
	sub_8265F3A8(ctx, base);
	// b 0x82661df8
	goto loc_82661DF8;
loc_82661DB0:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,68(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 68);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82661dd4
	if (!cr6.eq) goto loc_82661DD4;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r5,28(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x82661860
	sub_82661860(ctx, base);
loc_82661DD4:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82661df8
	if (cr6.eq) goto loc_82661DF8;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r5,36(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x82661818
	sub_82661818(ctx, base);
loc_82661DF8:
	// li r3,1
	ctx.r3.s64 = 1;
loc_82661DFC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82661E0C"))) PPC_WEAK_FUNC(sub_82661E0C);
PPC_FUNC_IMPL(__imp__sub_82661E0C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82661E10"))) PPC_WEAK_FUNC(sub_82661E10);
PPC_FUNC_IMPL(__imp__sub_82661E10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	// mflr r12
	// bl 0x8239bcc0
	// lwz r23,104(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 104);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r21,108(r3)
	r21.u64 = PPC_LOAD_U32(ctx.r3.u32 + 108);
	// lwz r9,112(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	// lwz r27,116(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	// lwz r28,120(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	// lwz r11,80(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lwz r10,92(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 92);
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// srawi r22,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r22.s64 = r11.s32 >> 1;
	// stw r23,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, r23.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r21,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r21.u32);
	// stw r9,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r9.u32);
	// stw r4,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r4.u32);
	// stw r27,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r27.u32);
	// stw r28,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r28.u32);
	// ble cr6,0x82663914
	if (!cr6.gt) goto loc_82663914;
	// lis r6,-32249
	ctx.r6.s64 = -2113470464;
	// fsub f8,f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f8.f64 = ctx.f2.f64 - ctx.f1.f64;
	// lis r7,-32248
	ctx.r7.s64 = -2113404928;
	// lis r8,-32249
	ctx.r8.s64 = -2113470464;
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f9,-30992(r6)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r6.u32 + -30992);
	// li r24,16
	r24.s64 = 16;
	// lfd f11,-26736(r7)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r7.u32 + -26736);
	// li r25,128
	r25.s64 = 128;
	// lfd f6,-31368(r8)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r8.u32 + -31368);
	// lfd f10,-28592(r10)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r10.u32 + -28592);
	// lfd f7,-31360(r11)
	ctx.f7.u64 = PPC_LOAD_U64(r11.u32 + -31360);
loc_82661E94:
	// extsw r11,r4
	r11.s64 = ctx.r4.s32;
	// lwz r10,96(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 96);
	// li r8,1
	ctx.r8.s64 = 1;
	// fmr f0,f8
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f8.f64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// std r11,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, r11.u64);
	// stw r8,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, ctx.r8.u32);
	// lfd f13,-208(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmadd f12,f13,f3,f4
	ctx.f12.f64 = ctx.f13.f64 * ctx.f3.f64 + ctx.f4.f64;
	// beq cr6,0x82661ecc
	if (cr6.eq) goto loc_82661ECC;
	// fsub f13,f3,f7
	ctx.f13.f64 = ctx.f3.f64 - ctx.f7.f64;
	// fmul f13,f13,f10
	ctx.f13.f64 = ctx.f13.f64 * ctx.f10.f64;
	// b 0x82661ed0
	goto loc_82661ED0;
loc_82661ECC:
	// fmr f13,f6
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f6.f64;
loc_82661ED0:
	// fadd f13,f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f13.f64 + ctx.f12.f64;
	// addi r11,r1,-268
	r11.s64 = ctx.r1.s64 + -268;
	// addi r6,r1,-304
	ctx.r6.s64 = ctx.r1.s64 + -304;
	// lwz r10,80(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r5,r1,-288
	ctx.r5.s64 = ctx.r1.s64 + -288;
	// lwz r7,100(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 100);
	// addi r31,r1,-252
	r31.s64 = ctx.r1.s64 + -252;
	// fctiwz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f12,0,r11
	PPC_STORE_U32(r11.u32, ctx.f12.u32);
	// lwz r11,-268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// fmul f12,f13,f10
	ctx.f12.f64 = ctx.f13.f64 * ctx.f10.f64;
	// rlwinm r30,r11,8,0,23
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// extsw r30,r30
	r30.s64 = r30.s32;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// std r30,-224(r1)
	PPC_STORE_U64(ctx.r1.u32 + -224, r30.u64);
	// stw r10,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r10.u32);
	// lfd f5,-224(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -224);
	// fcfid f5,f5
	ctx.f5.f64 = double(ctx.f5.s64);
	// fmsub f5,f13,f11,f5
	ctx.f5.f64 = ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64;
	// fctiwz f5,f5
	ctx.f5.s64 = (ctx.f5.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f5.f64));
	// stfiwx f5,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f5.u32);
	// stfiwx f12,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f12.u32);
	// lwz r26,-288(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// lwz r5,-304(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// rlwinm r7,r26,8,0,23
	ctx.r7.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 8) & 0xFFFFFF00;
	// mullw r6,r5,r5
	ctx.r6.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r5.s32);
	// extsw r30,r7
	r30.s64 = ctx.r7.s32;
	// srawi r7,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 8;
	// std r30,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, r30.u64);
	// stw r7,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r7.u32);
	// mullw r7,r7,r5
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r5.s32);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// stw r7,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, ctx.r7.u32);
	// lfd f12,-176(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// fmsub f13,f13,f9,f12
	ctx.f13.f64 = ctx.f13.f64 * ctx.f9.f64 - ctx.f12.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r31
	PPC_STORE_U32(r31.u32, ctx.f13.u32);
	// lwz r7,-252(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -252);
	// mullw r6,r7,r7
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r7.s32);
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// mullw r7,r6,r7
	ctx.r7.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// stw r6,-324(r1)
	PPC_STORE_U32(ctx.r1.u32 + -324, ctx.r6.u32);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// stw r7,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, ctx.r7.u32);
	// ble cr6,0x82662ff8
	if (!cr6.gt) goto loc_82662FF8;
	// lwz r7,84(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x82662ff8
	if (!cr6.lt) goto loc_82662FF8;
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r7,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, ctx.r7.u32);
	// ble cr6,0x826631ec
	if (!cr6.gt) goto loc_826631EC;
loc_82661FB8:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r11,r1,-260
	r11.s64 = ctx.r1.s64 + -260;
	// addi r7,r1,-292
	ctx.r7.s64 = ctx.r1.s64 + -292;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,-260(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// rlwinm r6,r11,8,0,23
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// std r6,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r6.u64);
	// lfd f13,-192(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f11,f13
	ctx.f13.f64 = f0.f64 * ctx.f11.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// ble cr6,0x82662ee0
	if (!cr6.gt) goto loc_82662EE0;
	// lwz r7,80(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x82662ee0
	if (!cr6.lt) goto loc_82662EE0;
	// lwz r28,-292(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// mullw r11,r28,r28
	r11.s64 = int64_t(r28.s32) * int64_t(r28.s32);
	// lbz r10,-1(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + -1);
	// lbz r5,2(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r25,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r25.s64 = r11.s32 >> 8;
	// add r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// subf r26,r8,r9
	r26.s64 = ctx.r9.s64 - ctx.r8.s64;
	// lbz r8,1(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// add r24,r7,r9
	r24.u64 = ctx.r7.u64 + ctx.r9.u64;
	// mullw r9,r25,r28
	ctx.r9.s64 = int64_t(r25.s32) * int64_t(r28.s32);
	// lbz r31,-1(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + -1);
	// lbz r30,1(r26)
	r30.u64 = PPC_LOAD_U8(r26.u32 + 1);
	// lbz r4,0(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 0);
	// lbz r20,1(r24)
	r20.u64 = PPC_LOAD_U8(r24.u32 + 1);
	// lbz r7,0(r6)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lbz r29,1(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lbz r21,2(r6)
	r21.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// srawi r23,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	r23.s64 = ctx.r9.s32 >> 8;
	// lbz r6,-1(r26)
	ctx.r6.u64 = PPC_LOAD_U8(r26.u32 + -1);
	// lbz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// rotlwi r3,r11,1
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r19,r10,r9
	r19.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lbz r27,-1(r24)
	r27.u64 = PPC_LOAD_U8(r24.u32 + -1);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r26,2(r26)
	r26.u64 = PPC_LOAD_U8(r26.u32 + 2);
	// rlwinm r19,r19,1,0,30
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r24,2(r24)
	r24.u64 = PPC_LOAD_U8(r24.u32 + 2);
	// add r18,r3,r30
	r18.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r19,r4
	ctx.r3.s64 = ctx.r4.s64 - r19.s64;
	// rlwinm r19,r18,1,0,30
	r19.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// add r18,r3,r5
	r18.u64 = ctx.r3.u64 + ctx.r5.u64;
	// subf r3,r20,r19
	ctx.r3.s64 = r19.s64 - r20.s64;
	// subf r19,r29,r7
	r19.s64 = ctx.r7.s64 - r29.s64;
	// subf r17,r21,r3
	r17.s64 = ctx.r3.s64 - r21.s64;
	// subf r3,r5,r19
	ctx.r3.s64 = r19.s64 - ctx.r5.s64;
	// subf r19,r30,r9
	r19.s64 = ctx.r9.s64 - r30.s64;
	// add r15,r3,r10
	r15.u64 = ctx.r3.u64 + ctx.r10.u64;
	// add r3,r29,r6
	ctx.r3.u64 = r29.u64 + ctx.r6.u64;
	// rlwinm r19,r19,1,0,30
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r19,r4,r19
	r19.s64 = r19.s64 - ctx.r4.s64;
	// subf r3,r27,r3
	ctx.r3.s64 = ctx.r3.s64 - r27.s64;
	// subf r16,r31,r21
	r16.s64 = r21.s64 - r31.s64;
	// subf r14,r26,r3
	r14.s64 = ctx.r3.s64 - r26.s64;
	// subf r3,r6,r19
	ctx.r3.s64 = r19.s64 - ctx.r6.s64;
	// rlwinm r19,r14,1,0,30
	r19.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r3,r20
	ctx.r3.u64 = ctx.r3.u64 + r20.u64;
	// add r19,r19,r24
	r19.u64 = r19.u64 + r24.u64;
	// add r3,r3,r26
	ctx.r3.u64 = ctx.r3.u64 + r26.u64;
	// rlwinm r19,r19,1,0,30
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r19,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, r19.u32);
	// add r19,r7,r8
	r19.u64 = ctx.r7.u64 + ctx.r8.u64;
	// mulli r14,r19,13
	r14.s64 = r19.s64 * 13;
	// subf r19,r24,r3
	r19.s64 = ctx.r3.s64 - r24.s64;
	// rotlwi r3,r11,2
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 2);
	// add r19,r19,r27
	r19.u64 = r19.u64 + r27.u64;
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// stw r3,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, ctx.r3.u32);
	// rlwinm r3,r19,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r19,r7,r29
	r19.s64 = r29.s64 - ctx.r7.s64;
	// stw r3,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, ctx.r3.u32);
	// rlwinm r3,r18,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r3,r18,r3
	ctx.r3.s64 = ctx.r3.s64 - r18.s64;
	// rlwinm r18,r17,2,0,29
	r18.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r18,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r18.u32);
	// lwz r18,-360(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// add r3,r3,r18
	ctx.r3.u64 = ctx.r3.u64 + r18.u64;
	// lwz r18,-316(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// add r18,r17,r18
	r18.u64 = r17.u64 + r18.u64;
	// add r3,r3,r18
	ctx.r3.u64 = ctx.r3.u64 + r18.u64;
	// rlwinm r18,r15,3,0,28
	r18.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r17,r14,r3
	r17.s64 = ctx.r3.s64 - r14.s64;
	// subf r3,r15,r18
	ctx.r3.s64 = r18.s64 - r15.s64;
	// srawi r18,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	r18.s64 = r17.s32 >> 1;
	// lwz r17,-344(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// add r3,r17,r3
	ctx.r3.u64 = r17.u64 + ctx.r3.u64;
	// stw r3,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, ctx.r3.u32);
	// mullw r3,r18,r25
	ctx.r3.s64 = int64_t(r18.s32) * int64_t(r25.s32);
	// subf r18,r4,r8
	r18.s64 = ctx.r8.s64 - ctx.r4.s64;
	// stw r3,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r3.u32);
	// subf r15,r11,r9
	r15.s64 = ctx.r9.s64 - r11.s64;
	// subf r17,r29,r18
	r17.s64 = r18.s64 - r29.s64;
	// subf r15,r6,r15
	r15.s64 = r15.s64 - ctx.r6.s64;
	// add r17,r17,r9
	r17.u64 = r17.u64 + ctx.r9.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r18,r30,r20
	r18.s64 = r20.s64 - r30.s64;
	// subf r15,r4,r15
	r15.s64 = r15.s64 - ctx.r4.s64;
	// subf r20,r20,r4
	r20.s64 = ctx.r4.s64 - r20.s64;
	// add r14,r15,r27
	r14.u64 = r15.u64 + r27.u64;
	// stw r17,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, r17.u32);
	// subf r17,r8,r19
	r17.s64 = r19.s64 - ctx.r8.s64;
	// add r14,r14,r7
	r14.u64 = r14.u64 + ctx.r7.u64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// rlwinm r14,r14,1,0,30
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r8,r29
	ctx.r3.s64 = r29.s64 - ctx.r8.s64;
	// stw r17,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, r17.u32);
	// subf r17,r31,r10
	r17.s64 = ctx.r10.s64 - r31.s64;
	// stw r14,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r14.u32);
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r17,r5,r17
	r17.s64 = r17.s64 - ctx.r5.s64;
	// subf r17,r6,r17
	r17.s64 = r17.s64 - ctx.r6.s64;
	// add r15,r17,r27
	r15.u64 = r17.u64 + r27.u64;
	// rlwinm r17,r16,2,0,29
	r17.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 2) & 0xFFFFFFFC;
	// add r15,r15,r21
	r15.u64 = r15.u64 + r21.u64;
	// add r17,r16,r17
	r17.u64 = r16.u64 + r17.u64;
	// subf r16,r11,r8
	r16.s64 = ctx.r8.s64 - r11.s64;
	// rlwinm r14,r15,1,0,30
	r14.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r21,r21,r20
	r21.s64 = r20.s64 - r21.s64;
	// lwz r15,-344(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// stw r16,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, r16.u32);
	// add r16,r15,r17
	r16.u64 = r15.u64 + r17.u64;
	// rotlwi r17,r7,1
	r17.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// add r17,r17,r9
	r17.u64 = r17.u64 + ctx.r9.u64;
	// lwz r15,-344(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// stw r17,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, r17.u32);
	// subf r17,r24,r14
	r17.s64 = r14.s64 - r24.s64;
	// mulli r15,r15,11
	r15.s64 = r15.s64 * 11;
	// add r17,r17,r26
	r17.u64 = r17.u64 + r26.u64;
	// add r16,r16,r15
	r16.u64 = r16.u64 + r15.u64;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r20,r16,1
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x1) != 0);
	r20.s64 = r16.s32 >> 1;
	// lwz r16,-320(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// lwz r14,-344(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// stw r17,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, r17.u32);
	// rlwinm r15,r14,1,0,30
	r15.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r17,-316(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// subf r14,r10,r21
	r14.s64 = r21.s64 - ctx.r10.s64;
	// mullw r21,r20,r23
	r21.s64 = int64_t(r20.s32) * int64_t(r23.s32);
	// rlwinm r20,r3,1,0,30
	r20.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r16,r16,r15
	r16.s64 = r15.s64 - r16.s64;
	// add r20,r3,r20
	r20.u64 = ctx.r3.u64 + r20.u64;
	// rotlwi r3,r31,2
	ctx.r3.u64 = __builtin_rotateleft32(r31.u32, 2);
	// add r20,r17,r20
	r20.u64 = r17.u64 + r20.u64;
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// rotlwi r17,r10,3
	r17.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// subf r3,r3,r20
	ctx.r3.s64 = r20.s64 - ctx.r3.s64;
	// subf r20,r10,r17
	r20.s64 = r17.s64 - ctx.r10.s64;
	// lwz r17,-360(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// subf r15,r11,r7
	r15.s64 = ctx.r7.s64 - r11.s64;
	// add r20,r3,r20
	r20.u64 = ctx.r3.u64 + r20.u64;
	// lwz r3,-360(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// rlwinm r17,r17,3,0,28
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r20,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r20.s64 = r20.s32 >> 1;
	// subf r3,r3,r17
	ctx.r3.s64 = r17.s64 - ctx.r3.s64;
	// subf r17,r4,r16
	r17.s64 = r16.s64 - ctx.r4.s64;
	// mullw r20,r20,r28
	r20.s64 = int64_t(r20.s32) * int64_t(r28.s32);
	// lwz r16,-344(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// stw r17,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, r17.u32);
	// add r17,r16,r3
	r17.u64 = r16.u64 + ctx.r3.u64;
	// mulli r3,r15,11
	ctx.r3.s64 = r15.s64 * 11;
	// stw r3,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, ctx.r3.u32);
	// subf r3,r9,r14
	ctx.r3.s64 = r14.s64 - ctx.r9.s64;
	// lwz r14,-272(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// rlwinm r16,r18,2,0,29
	r16.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// add r21,r14,r21
	r21.u64 = r14.u64 + r21.u64;
	// lwz r15,-344(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// srawi r15,r15,1
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x1) != 0);
	r15.s64 = r15.s32 >> 1;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lwz r14,-316(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// subf r31,r10,r31
	r31.s64 = r31.s64 - ctx.r10.s64;
	// add r18,r18,r16
	r18.u64 = r18.u64 + r16.u64;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r16,r9,r30
	r16.s64 = r30.s64 - ctx.r9.s64;
	// subf r31,r27,r31
	r31.s64 = r31.s64 - r27.s64;
	// add r17,r17,r14
	r17.u64 = r17.u64 + r14.u64;
	// rlwinm r14,r16,1,0,30
	r14.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r16,-364(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// add r18,r17,r18
	r18.u64 = r17.u64 + r18.u64;
	// subf r17,r11,r10
	r17.s64 = ctx.r10.s64 - r11.s64;
	// stw r31,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, r31.u32);
	// add r21,r21,r20
	r21.u64 = r21.u64 + r20.u64;
	// rlwinm r20,r15,8,0,23
	r20.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 8) & 0xFFFFFF00;
	// subf r17,r6,r17
	r17.s64 = r17.s64 - ctx.r6.s64;
	// add r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 + ctx.r5.u64;
	// add r21,r21,r20
	r21.u64 = r21.u64 + r20.u64;
	// srawi r20,r18,1
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x1) != 0);
	r20.s64 = r18.s32 >> 1;
	// lwz r18,-276(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// rlwinm r15,r17,1,0,30
	r15.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// rlwinm r17,r16,1,0,30
	r17.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r31,r5,r15
	r31.s64 = r15.s64 - ctx.r5.s64;
	// mullw r21,r21,r18
	r21.s64 = int64_t(r21.s32) * int64_t(r18.s32);
	// subf r15,r29,r14
	r15.s64 = r14.s64 - r29.s64;
	// add r17,r16,r17
	r17.u64 = r16.u64 + r17.u64;
	// rlwinm r18,r3,1,0,30
	r18.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r31,r8
	r31.u64 = r31.u64 + ctx.r8.u64;
	// add r18,r18,r17
	r18.u64 = r18.u64 + r17.u64;
	// subf r16,r8,r15
	r16.s64 = r15.s64 - ctx.r8.s64;
	// add r17,r31,r26
	r17.u64 = r31.u64 + r26.u64;
	// rlwinm r31,r19,1,0,30
	r31.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r27,r27,r18
	r27.s64 = r18.s64 - r27.s64;
	// subf r16,r10,r16
	r16.s64 = r16.s64 - ctx.r10.s64;
	// add r19,r19,r31
	r19.u64 = r19.u64 + r31.u64;
	// subf r27,r26,r27
	r27.s64 = r27.s64 - r26.s64;
	// subf r31,r26,r16
	r31.s64 = r16.s64 - r26.s64;
	// rotlwi r26,r8,1
	r26.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// rlwinm r18,r17,1,0,30
	r18.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// add r26,r26,r10
	r26.u64 = r26.u64 + ctx.r10.u64;
	// add r19,r18,r19
	r19.u64 = r18.u64 + r19.u64;
	// rlwinm r17,r26,1,0,30
	r17.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// rotlwi r26,r30,2
	r26.u64 = __builtin_rotateleft32(r30.u32, 2);
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// add r30,r30,r26
	r30.u64 = r30.u64 + r26.u64;
	// rotlwi r26,r9,3
	r26.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r27,r27,r24
	r27.u64 = r27.u64 + r24.u64;
	// subf r30,r30,r19
	r30.s64 = r19.s64 - r30.s64;
	// add r31,r31,r5
	r31.u64 = r31.u64 + ctx.r5.u64;
	// subf r26,r9,r26
	r26.s64 = r26.s64 - ctx.r9.s64;
	// add r24,r27,r6
	r24.u64 = r27.u64 + ctx.r6.u64;
	// subf r3,r8,r11
	ctx.r3.s64 = r11.s64 - ctx.r8.s64;
	// add r26,r30,r26
	r26.u64 = r30.u64 + r26.u64;
	// subf r18,r10,r11
	r18.s64 = r11.s64 - ctx.r10.s64;
	// rlwinm r30,r3,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r14,-364(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// mullw r20,r20,r25
	r20.s64 = int64_t(r20.s32) * int64_t(r25.s32);
	// subf r29,r29,r14
	r29.s64 = r14.s64 - r29.s64;
	// srawi r26,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	r26.s64 = r26.s32 >> 1;
	// subf r29,r7,r29
	r29.s64 = r29.s64 - ctx.r7.s64;
	// subf r29,r9,r29
	r29.s64 = r29.s64 - ctx.r9.s64;
	// add r29,r29,r4
	r29.u64 = r29.u64 + ctx.r4.u64;
	// add r29,r29,r8
	r29.u64 = r29.u64 + ctx.r8.u64;
	// add r27,r29,r11
	r27.u64 = r29.u64 + r11.u64;
	// add r29,r31,r11
	r29.u64 = r31.u64 + r11.u64;
	// mullw r31,r24,r23
	r31.s64 = int64_t(r24.s32) * int64_t(r23.s32);
	// add r24,r27,r6
	r24.u64 = r27.u64 + ctx.r6.u64;
	// subf r27,r9,r18
	r27.s64 = r18.s64 - ctx.r9.s64;
	// add r18,r3,r30
	r18.u64 = ctx.r3.u64 + r30.u64;
	// lwz r3,-320(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// add r19,r29,r6
	r19.u64 = r29.u64 + ctx.r6.u64;
	// add r30,r20,r31
	r30.u64 = r20.u64 + r31.u64;
	// subf r17,r3,r17
	r17.s64 = r17.s64 - ctx.r3.s64;
	// mullw r29,r24,r28
	r29.s64 = int64_t(r24.s32) * int64_t(r28.s32);
	// mullw r3,r26,r25
	ctx.r3.s64 = int64_t(r26.s32) * int64_t(r25.s32);
	// add r27,r27,r6
	r27.u64 = r27.u64 + ctx.r6.u64;
	// mullw r31,r19,r23
	r31.s64 = int64_t(r19.s32) * int64_t(r23.s32);
	// subf r6,r10,r18
	ctx.r6.s64 = r18.s64 - ctx.r10.s64;
	// subf r26,r5,r17
	r26.s64 = r17.s64 - ctx.r5.s64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lwz r29,-256(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -256);
	// add r5,r6,r5
	ctx.r5.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mullw r31,r27,r28
	r31.s64 = int64_t(r27.s32) * int64_t(r28.s32);
	// srawi r27,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	r27.s64 = r26.s32 >> 1;
	// srawi r26,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	r26.s64 = ctx.r5.s32 >> 1;
	// lwz r5,-304(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// add r31,r3,r31
	r31.u64 = ctx.r3.u64 + r31.u64;
	// mullw r6,r30,r29
	ctx.r6.s64 = int64_t(r30.s32) * int64_t(r29.s32);
	// add r3,r21,r6
	ctx.r3.u64 = r21.u64 + ctx.r6.u64;
	// mullw r30,r31,r5
	r30.s64 = int64_t(r31.s32) * int64_t(ctx.r5.s32);
	// mullw r6,r27,r25
	ctx.r6.s64 = int64_t(r27.s32) * int64_t(r25.s32);
	// mullw r31,r26,r23
	r31.s64 = int64_t(r26.s32) * int64_t(r23.s32);
	// add r31,r6,r31
	r31.u64 = ctx.r6.u64 + r31.u64;
	// subf r6,r7,r11
	ctx.r6.s64 = r11.s64 - ctx.r7.s64;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// rotlwi r10,r11,8
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 8);
	// rlwinm r11,r6,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r9,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 1;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// mullw r9,r9,r28
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r28.s32);
	// srawi r8,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 1;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// mullw r9,r8,r5
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82662474
	if (!cr6.gt) goto loc_82662474;
	// li r11,255
	r11.s64 = 255;
	// b 0x82662480
	goto loc_82662480;
loc_82662474:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82662480
	if (!cr6.lt) goto loc_82662480;
	// li r11,0
	r11.s64 = 0;
loc_82662480:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,-352(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,-356(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -356);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// stw r9,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r9.u32);
	// beq cr6,0x82662e94
	if (cr6.eq) goto loc_82662E94;
	// fmul f13,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = f0.f64 * ctx.f10.f64;
	// addi r11,r1,-260
	r11.s64 = ctx.r1.s64 + -260;
	// lwz r10,-288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// addi r9,r1,-292
	ctx.r9.s64 = ctx.r1.s64 + -292;
	// rlwinm r23,r22,1,0,30
	r23.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r10,r10,r22
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r22.s32);
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,-260(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// addi r8,r22,-1
	ctx.r8.s64 = r22.s64 + -1;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// addi r7,r22,2
	ctx.r7.s64 = r22.s64 + 2;
	// addi r3,r23,1
	ctx.r3.s64 = r23.s64 + 1;
	// stw r10,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, ctx.r10.u32);
	// addi r26,r23,-1
	r26.s64 = r23.s64 + -1;
	// addi r24,r23,2
	r24.s64 = r23.s64 + 2;
	// addi r5,r22,1
	ctx.r5.s64 = r22.s64 + 1;
	// std r11,-216(r1)
	PPC_STORE_U64(ctx.r1.u32 + -216, r11.u64);
	// lwz r11,-300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// stw r3,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, ctx.r3.u32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r26,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, r26.u32);
	// stw r24,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r24.u32);
	// subf r29,r22,r10
	r29.s64 = ctx.r10.s64 - r22.s64;
	// lbzx r28,r8,r10
	r28.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// lbzx r20,r7,r10
	r20.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r10.u32);
	// lbzx r21,r3,r10
	r21.u64 = PPC_LOAD_U8(ctx.r3.u32 + ctx.r10.u32);
	// lbzx r27,r5,r10
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r10.u32);
	// lbzx r25,r26,r10
	r25.u64 = PPC_LOAD_U8(r26.u32 + ctx.r10.u32);
	// lbzx r19,r24,r10
	r19.u64 = PPC_LOAD_U8(r24.u32 + ctx.r10.u32);
	// lbzx r31,r23,r10
	r31.u64 = PPC_LOAD_U8(r23.u32 + ctx.r10.u32);
	// lbz r30,2(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbzx r7,r10,r22
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + r22.u32);
	// lbz r8,1(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r26,1(r29)
	r26.u64 = PPC_LOAD_U8(r29.u32 + 1);
	// lbz r5,-1(r29)
	ctx.r5.u64 = PPC_LOAD_U8(r29.u32 + -1);
	// lbz r24,2(r29)
	r24.u64 = PPC_LOAD_U8(r29.u32 + 2);
	// lfd f13,-216(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -216);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f9,f13
	ctx.f13.f64 = f0.f64 * ctx.f9.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r6,-292(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// lbz r9,-1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// mullw r11,r6,r6
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r6.s32);
	// srawi r4,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r4.s64 = r11.s32 >> 8;
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// mullw r18,r4,r6
	r18.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r6.s32);
	// srawi r29,r18,8
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0xFF) != 0);
	r29.s64 = r18.s32 >> 8;
	// rotlwi r3,r11,1
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r18,r7,r8
	r18.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// mulli r15,r18,13
	r15.s64 = r18.s64 * 13;
	// add r18,r9,r10
	r18.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r3,r3,r26
	ctx.r3.u64 = ctx.r3.u64 + r26.u64;
	// rlwinm r18,r18,1,0,30
	r18.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r17,r3,1,0,30
	r17.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r18,r31
	ctx.r3.s64 = r31.s64 - r18.s64;
	// subf r18,r21,r17
	r18.s64 = r17.s64 - r21.s64;
	// add r17,r27,r5
	r17.u64 = r27.u64 + ctx.r5.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// rlwinm r16,r17,1,0,30
	r16.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// rotlwi r17,r11,2
	r17.u64 = __builtin_rotateleft32(r11.u32, 2);
	// subf r16,r25,r16
	r16.s64 = r16.s64 - r25.s64;
	// add r17,r11,r17
	r17.u64 = r11.u64 + r17.u64;
	// subf r18,r20,r18
	r18.s64 = r18.s64 - r20.s64;
	// stw r17,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, r17.u32);
	// subf r17,r24,r16
	r17.s64 = r16.s64 - r24.s64;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// add r17,r17,r19
	r17.u64 = r17.u64 + r19.u64;
	// rlwinm r16,r17,1,0,30
	r16.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r17,r3,3,0,28
	r17.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r3,r3,r17
	ctx.r3.s64 = r17.s64 - ctx.r3.s64;
	// rlwinm r17,r18,2,0,29
	r17.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r3,r16
	ctx.r3.u64 = ctx.r3.u64 + r16.u64;
	// add r18,r18,r17
	r18.u64 = r18.u64 + r17.u64;
	// add r3,r3,r18
	ctx.r3.u64 = ctx.r3.u64 + r18.u64;
	// subf r3,r15,r3
	ctx.r3.s64 = ctx.r3.s64 - r15.s64;
	// subf r18,r27,r7
	r18.s64 = ctx.r7.s64 - r27.s64;
	// subf r16,r26,r10
	r16.s64 = ctx.r10.s64 - r26.s64;
	// subf r17,r30,r18
	r17.s64 = r18.s64 - r30.s64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// add r17,r17,r9
	r17.u64 = r17.u64 + ctx.r9.u64;
	// srawi r15,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	r15.s64 = ctx.r3.s32 >> 1;
	// subf r3,r28,r20
	ctx.r3.s64 = r20.s64 - r28.s64;
	// subf r18,r8,r27
	r18.s64 = r27.s64 - ctx.r8.s64;
	// stw r17,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, r17.u32);
	// subf r17,r31,r8
	r17.s64 = ctx.r8.s64 - r31.s64;
	// subf r17,r27,r17
	r17.s64 = r17.s64 - r27.s64;
	// add r17,r17,r10
	r17.u64 = r17.u64 + ctx.r10.u64;
	// stw r17,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, r17.u32);
	// subf r17,r31,r16
	r17.s64 = r16.s64 - r31.s64;
	// subf r16,r11,r10
	r16.s64 = ctx.r10.s64 - r11.s64;
	// subf r17,r5,r17
	r17.s64 = r17.s64 - ctx.r5.s64;
	// subf r16,r5,r16
	r16.s64 = r16.s64 - ctx.r5.s64;
	// add r17,r17,r21
	r17.u64 = r17.u64 + r21.u64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// add r14,r17,r24
	r14.u64 = r17.u64 + r24.u64;
	// subf r17,r31,r16
	r17.s64 = r16.s64 - r31.s64;
	// rlwinm r16,r14,1,0,30
	r16.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r14,r28,r9
	r14.s64 = ctx.r9.s64 - r28.s64;
	// subf r16,r19,r16
	r16.s64 = r16.s64 - r19.s64;
	// rlwinm r14,r14,1,0,30
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r16,r16,r25
	r16.u64 = r16.u64 + r25.u64;
	// subf r14,r30,r14
	r14.s64 = r14.s64 - r30.s64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r14,r5,r14
	r14.s64 = r14.s64 - ctx.r5.s64;
	// add r17,r17,r25
	r17.u64 = r17.u64 + r25.u64;
	// add r17,r17,r7
	r17.u64 = r17.u64 + ctx.r7.u64;
	// stw r16,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, r16.u32);
	// subf r16,r11,r8
	r16.s64 = ctx.r8.s64 - r11.s64;
	// stw r14,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r14.u32);
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r16,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, r16.u32);
	// rotlwi r16,r7,1
	r16.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// stw r17,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, r17.u32);
	// add r16,r16,r10
	r16.u64 = r16.u64 + ctx.r10.u64;
	// rlwinm r14,r16,1,0,30
	r14.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r16,r15,r4
	r16.s64 = int64_t(r15.s32) * int64_t(ctx.r4.s32);
	// stw r16,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, r16.u32);
	// rlwinm r16,r3,2,0,29
	r16.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,-360(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// lwz r17,-364(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// stw r16,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, r16.u32);
	// lwz r16,-360(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// mulli r17,r17,11
	r17.s64 = r17.s64 * 11;
	// rlwinm r16,r16,3,0,28
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r16,r15,r16
	r16.s64 = r16.s64 - r15.s64;
	// lwz r15,-364(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// add r15,r3,r15
	r15.u64 = ctx.r3.u64 + r15.u64;
	// lwz r3,-272(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// add r3,r3,r16
	ctx.r3.u64 = ctx.r3.u64 + r16.u64;
	// rlwinm r16,r18,1,0,30
	r16.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r3,r15
	ctx.r3.u64 = ctx.r3.u64 + r15.u64;
	// lwz r15,-344(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// add r16,r18,r16
	r16.u64 = r18.u64 + r16.u64;
	// add r3,r3,r17
	ctx.r3.u64 = ctx.r3.u64 + r17.u64;
	// rotlwi r18,r28,2
	r18.u64 = __builtin_rotateleft32(r28.u32, 2);
	// add r16,r15,r16
	r16.u64 = r15.u64 + r16.u64;
	// add r18,r28,r18
	r18.u64 = r28.u64 + r18.u64;
	// rotlwi r15,r9,3
	r15.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// stw r3,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, ctx.r3.u32);
	// subf r18,r18,r16
	r18.s64 = r16.s64 - r18.s64;
	// lwz r16,-316(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// subf r17,r9,r15
	r17.s64 = r15.s64 - ctx.r9.s64;
	// add r3,r16,r25
	ctx.r3.u64 = r16.u64 + r25.u64;
	// add r18,r18,r17
	r18.u64 = r18.u64 + r17.u64;
	// lwz r17,-320(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// add r15,r3,r20
	r15.u64 = ctx.r3.u64 + r20.u64;
	// subf r17,r17,r14
	r17.s64 = r14.s64 - r17.s64;
	// subf r17,r31,r17
	r17.s64 = r17.s64 - r31.s64;
	// lwz r16,-364(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// srawi r16,r16,1
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x1) != 0);
	r16.s64 = r16.s32 >> 1;
	// srawi r18,r18,1
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x1) != 0);
	r18.s64 = r18.s32 >> 1;
	// mullw r3,r16,r29
	ctx.r3.s64 = int64_t(r16.s32) * int64_t(r29.s32);
	// rlwinm r16,r15,1,0,30
	r16.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r15,-352(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// mullw r18,r18,r6
	r18.s64 = int64_t(r18.s32) * int64_t(ctx.r6.s32);
	// add r3,r15,r3
	ctx.r3.u64 = r15.u64 + ctx.r3.u64;
	// srawi r15,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	r15.s64 = r17.s32 >> 1;
	// subf r17,r19,r16
	r17.s64 = r16.s64 - r19.s64;
	// add r3,r3,r18
	ctx.r3.u64 = ctx.r3.u64 + r18.u64;
	// add r16,r17,r24
	r16.u64 = r17.u64 + r24.u64;
	// lwz r17,-340(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	// rlwinm r18,r15,8,0,23
	r18.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r15,r17,3,0,28
	r15.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// add r14,r3,r18
	r14.u64 = ctx.r3.u64 + r18.u64;
	// subf r18,r17,r15
	r18.s64 = r15.s64 - r17.s64;
	// lwz r17,-324(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// rlwinm r3,r16,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r16,r14,r17
	r16.s64 = int64_t(r14.s32) * int64_t(r17.s32);
	// add r3,r3,r18
	ctx.r3.u64 = ctx.r3.u64 + r18.u64;
	// subf r18,r26,r21
	r18.s64 = r21.s64 - r26.s64;
	// subf r21,r21,r31
	r21.s64 = r31.s64 - r21.s64;
	// subf r14,r9,r28
	r14.s64 = r28.s64 - ctx.r9.s64;
	// subf r21,r20,r21
	r21.s64 = r21.s64 - r20.s64;
	// rlwinm r20,r14,1,0,30
	r20.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r21,r9,r21
	r21.s64 = r21.s64 - ctx.r9.s64;
	// stw r3,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, ctx.r3.u32);
	// subf r20,r25,r20
	r20.s64 = r20.s64 - r25.s64;
	// subf r21,r10,r21
	r21.s64 = r21.s64 - ctx.r10.s64;
	// subf r20,r27,r20
	r20.s64 = r20.s64 - r27.s64;
	// add r28,r21,r28
	r28.u64 = r21.u64 + r28.u64;
	// subf r14,r11,r9
	r14.s64 = ctx.r9.s64 - r11.s64;
	// add r28,r28,r30
	r28.u64 = r28.u64 + r30.u64;
	// subf r21,r5,r14
	r21.s64 = r14.s64 - ctx.r5.s64;
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// stw r20,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, r20.u32);
	// subf r14,r10,r26
	r14.s64 = r26.s64 - ctx.r10.s64;
	// rlwinm r20,r28,1,0,30
	r20.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r28,r21,1,0,30
	r28.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r15,r11,r7
	r15.s64 = ctx.r7.s64 - r11.s64;
	// rlwinm r14,r14,1,0,30
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r7,r27
	ctx.r3.s64 = r27.s64 - ctx.r7.s64;
	// subf r14,r27,r14
	r14.s64 = r14.s64 - r27.s64;
	// subf r17,r8,r3
	r17.s64 = ctx.r3.s64 - ctx.r8.s64;
	// subf r14,r8,r14
	r14.s64 = r14.s64 - ctx.r8.s64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// subf r14,r9,r14
	r14.s64 = r14.s64 - ctx.r9.s64;
	// lwz r21,-340(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	// subf r21,r7,r21
	r21.s64 = r21.s64 - ctx.r7.s64;
	// stw r21,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, r21.u32);
	// subf r21,r30,r28
	r21.s64 = r28.s64 - r30.s64;
	// mulli r28,r15,11
	r28.s64 = r15.s64 * 11;
	// add r21,r21,r8
	r21.u64 = r21.u64 + ctx.r8.u64;
	// add r21,r21,r24
	r21.u64 = r21.u64 + r24.u64;
	// rlwinm r21,r21,1,0,30
	r21.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r15,-340(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	// subf r27,r10,r15
	r27.s64 = r15.s64 - ctx.r10.s64;
	// lwz r15,-364(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// add r28,r15,r28
	r28.u64 = r15.u64 + r28.u64;
	// rlwinm r15,r3,1,0,30
	r15.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r15,r3,r15
	r15.u64 = ctx.r3.u64 + r15.u64;
	// add r27,r27,r8
	r27.u64 = r27.u64 + ctx.r8.u64;
	// add r15,r21,r15
	r15.u64 = r21.u64 + r15.u64;
	// rlwinm r21,r17,1,0,30
	r21.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r24,r14
	ctx.r3.s64 = r14.s64 - r24.s64;
	// add r17,r17,r21
	r17.u64 = r17.u64 + r21.u64;
	// rlwinm r21,r18,2,0,29
	r21.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// add r20,r20,r17
	r20.u64 = r20.u64 + r17.u64;
	// add r21,r18,r21
	r21.u64 = r18.u64 + r21.u64;
	// subf r25,r25,r20
	r25.s64 = r20.s64 - r25.s64;
	// add r21,r28,r21
	r21.u64 = r28.u64 + r21.u64;
	// subf r28,r24,r25
	r28.s64 = r25.s64 - r24.s64;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// add r25,r28,r19
	r25.u64 = r28.u64 + r19.u64;
	// rotlwi r28,r26,2
	r28.u64 = __builtin_rotateleft32(r26.u32, 2);
	// srawi r24,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	r24.s64 = r21.s32 >> 1;
	// add r25,r25,r5
	r25.u64 = r25.u64 + ctx.r5.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// add r3,r3,r7
	ctx.r3.u64 = ctx.r3.u64 + ctx.r7.u64;
	// add r27,r27,r5
	r27.u64 = r27.u64 + ctx.r5.u64;
	// rotlwi r21,r10,3
	r21.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// mullw r28,r24,r4
	r28.s64 = int64_t(r24.s32) * int64_t(ctx.r4.s32);
	// mullw r24,r25,r29
	r24.s64 = int64_t(r25.s32) * int64_t(r29.s32);
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// mullw r27,r27,r6
	r27.s64 = int64_t(r27.s32) * int64_t(ctx.r6.s32);
	// subf r26,r26,r15
	r26.s64 = r15.s64 - r26.s64;
	// subf r25,r10,r21
	r25.s64 = r21.s64 - ctx.r10.s64;
	// add r28,r28,r24
	r28.u64 = r28.u64 + r24.u64;
	// lwz r18,-264(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// lwz r19,-252(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -252);
	// add r26,r26,r25
	r26.u64 = r26.u64 + r25.u64;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// add r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 + ctx.r5.u64;
	// srawi r27,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	r27.s64 = r26.s32 >> 1;
	// subf r25,r9,r11
	r25.s64 = r11.s64 - ctx.r9.s64;
	// mullw r26,r3,r29
	r26.s64 = int64_t(ctx.r3.s32) * int64_t(r29.s32);
	// mullw r27,r27,r4
	r27.s64 = int64_t(r27.s32) * int64_t(ctx.r4.s32);
	// subf r3,r10,r25
	ctx.r3.s64 = r25.s64 - ctx.r10.s64;
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// rotlwi r26,r8,1
	r26.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// add r25,r3,r5
	r25.u64 = ctx.r3.u64 + ctx.r5.u64;
	// subf r3,r8,r11
	ctx.r3.s64 = r11.s64 - ctx.r8.s64;
	// subf r24,r9,r8
	r24.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r8,r26,r9
	ctx.r8.u64 = r26.u64 + ctx.r9.u64;
	// subf r26,r10,r7
	r26.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r8,r25,r6
	ctx.r8.s64 = int64_t(r25.s32) * int64_t(ctx.r6.s32);
	// lwz r25,-320(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// mullw r28,r28,r18
	r28.s64 = int64_t(r28.s32) * int64_t(r18.s32);
	// add r27,r27,r8
	r27.u64 = r27.u64 + ctx.r8.u64;
	// rotlwi r8,r11,8
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 8);
	// add r28,r16,r28
	r28.u64 = r16.u64 + r28.u64;
	// mullw r11,r27,r19
	r11.s64 = int64_t(r27.s32) * int64_t(r19.s32);
	// add r11,r28,r11
	r11.u64 = r28.u64 + r11.u64;
	// rlwinm r28,r5,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r7,r25,r7
	ctx.r7.s64 = ctx.r7.s64 - r25.s64;
	// add r28,r5,r28
	r28.u64 = ctx.r5.u64 + r28.u64;
	// rlwinm r5,r3,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r10,r10,r28
	ctx.r10.s64 = r28.s64 - ctx.r10.s64;
	// add r5,r3,r5
	ctx.r5.u64 = ctx.r3.u64 + ctx.r5.u64;
	// subf r7,r30,r7
	ctx.r7.s64 = ctx.r7.s64 - r30.s64;
	// add r3,r10,r31
	ctx.r3.u64 = ctx.r10.u64 + r31.u64;
	// subf r10,r9,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r9.s64;
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// srawi r9,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r3.s32 >> 1;
	// add r5,r10,r30
	ctx.r5.u64 = ctx.r10.u64 + r30.u64;
	// mullw r10,r9,r18
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(r18.s32);
	// mullw r7,r7,r4
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r4.s32);
	// srawi r9,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r5.s32 >> 1;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// mullw r9,r9,r29
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r29.s32);
	// srawi r5,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	ctx.r5.s64 = r24.s32 >> 1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r5,r6
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r6.s32);
	// srawi r7,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	ctx.r7.s64 = r26.s32 >> 1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r7,r19
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(r19.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82662960
	if (!cr6.gt) goto loc_82662960;
	// li r11,255
	r11.s64 = 255;
	// b 0x8266296c
	goto loc_8266296C;
loc_82662960:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8266296c
	if (!cr6.lt) goto loc_8266296C;
	// li r11,0
	r11.s64 = 0;
loc_8266296C:
	// lwz r10,-312(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// addi r8,r22,-1
	ctx.r8.s64 = r22.s64 + -1;
	// lwz r9,-296(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lwz r11,-336(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// subf r3,r22,r10
	ctx.r3.s64 = ctx.r10.s64 - r22.s64;
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbzx r28,r8,r10
	r28.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// addi r8,r22,1
	ctx.r8.s64 = r22.s64 + 1;
	// lbzx r31,r23,r10
	r31.u64 = PPC_LOAD_U8(r23.u32 + ctx.r10.u32);
	// lbz r26,1(r3)
	r26.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// lbzx r7,r10,r22
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + r22.u32);
	// stw r11,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r11.u32);
	// lbzx r27,r8,r10
	r27.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// lwz r8,-328(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf r17,r27,r7
	r17.s64 = ctx.r7.s64 - r27.s64;
	// lbz r9,-1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// rotlwi r24,r11,1
	r24.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lbz r30,2(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r5,-1(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + -1);
	// lbzx r23,r8,r10
	r23.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// addi r8,r22,2
	ctx.r8.s64 = r22.s64 + 2;
	// add r16,r24,r28
	r16.u64 = r24.u64 + r28.u64;
	// lbz r24,2(r3)
	r24.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// subf r15,r30,r17
	r15.s64 = r17.s64 - r30.s64;
	// add r16,r16,r26
	r16.u64 = r16.u64 + r26.u64;
	// add r15,r15,r9
	r15.u64 = r15.u64 + ctx.r9.u64;
	// lbzx r21,r8,r10
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,-348(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// subf r17,r28,r21
	r17.s64 = r21.s64 - r28.s64;
	// stw r15,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, r15.u32);
	// lbzx r25,r8,r10
	r25.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// lwz r8,-368(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// stw r16,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r16.u32);
	// lbzx r20,r8,r10
	r20.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// lbz r8,1(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// subf r3,r7,r27
	ctx.r3.s64 = r27.s64 - ctx.r7.s64;
	// subf r14,r11,r8
	r14.s64 = ctx.r8.s64 - r11.s64;
	// add r15,r7,r8
	r15.u64 = ctx.r7.u64 + ctx.r8.u64;
	// mulli r14,r14,11
	r14.s64 = r14.s64 * 11;
	// stw r14,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r14.u32);
	// add r14,r9,r10
	r14.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mulli r15,r15,13
	r15.s64 = r15.s64 * 13;
	// stw r15,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, r15.u32);
	// rlwinm r14,r14,1,0,30
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r16,r14,r31
	r16.s64 = r31.s64 - r14.s64;
	// add r16,r16,r30
	r16.u64 = r16.u64 + r30.u64;
	// stw r16,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r16.u32);
	// lwz r14,-368(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// subf r14,r23,r14
	r14.s64 = r14.s64 - r23.s64;
	// subf r16,r21,r14
	r16.s64 = r14.s64 - r21.s64;
	// add r14,r27,r5
	r14.u64 = r27.u64 + ctx.r5.u64;
	// rlwinm r14,r14,1,0,30
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r16,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, r16.u32);
	// subf r16,r26,r10
	r16.s64 = ctx.r10.s64 - r26.s64;
	// subf r14,r25,r14
	r14.s64 = r14.s64 - r25.s64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r14,r24,r14
	r14.s64 = r14.s64 - r24.s64;
	// subf r16,r31,r16
	r16.s64 = r16.s64 - r31.s64;
	// subf r16,r5,r16
	r16.s64 = r16.s64 - ctx.r5.s64;
	// add r15,r16,r23
	r15.u64 = r16.u64 + r23.u64;
	// rlwinm r16,r14,1,0,30
	r16.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r15,r15,r24
	r15.u64 = r15.u64 + r24.u64;
	// add r16,r16,r20
	r16.u64 = r16.u64 + r20.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r16,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, r16.u32);
	// subf r16,r20,r15
	r16.s64 = r15.s64 - r20.s64;
	// lwz r15,-360(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// add r16,r16,r25
	r16.u64 = r16.u64 + r25.u64;
	// rlwinm r15,r15,3,0,28
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r16,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r16.u32);
	// lwz r16,-360(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// subf r16,r16,r15
	r16.s64 = r15.s64 - r16.s64;
	// rlwinm r15,r17,2,0,29
	r15.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r15,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, r15.u32);
	// lwz r15,-368(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// add r15,r15,r16
	r15.u64 = r15.u64 + r16.u64;
	// lwz r16,-348(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// add r17,r17,r16
	r17.u64 = r17.u64 + r16.u64;
	// rotlwi r16,r11,2
	r16.u64 = __builtin_rotateleft32(r11.u32, 2);
	// add r17,r15,r17
	r17.u64 = r15.u64 + r17.u64;
	// add r16,r11,r16
	r16.u64 = r11.u64 + r16.u64;
	// stw r16,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, r16.u32);
	// lwz r16,-328(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// add r14,r17,r16
	r14.u64 = r17.u64 + r16.u64;
	// lwz r17,-312(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// rlwinm r16,r17,3,0,28
	r16.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r17,r17,r16
	r17.s64 = r16.s64 - r17.s64;
	// lwz r16,-352(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// rlwinm r16,r16,2,0,29
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r16,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r16.u32);
	// lwz r16,-340(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	// add r17,r17,r16
	r17.u64 = r17.u64 + r16.u64;
	// stw r17,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, r17.u32);
	// lwz r17,-352(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// lwz r16,-368(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// add r17,r17,r16
	r17.u64 = r17.u64 + r16.u64;
	// lwz r16,-348(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// add r17,r16,r17
	r17.u64 = r16.u64 + r17.u64;
	// lwz r16,-364(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// subf r17,r16,r17
	r17.s64 = r17.s64 - r16.s64;
	// subf r16,r31,r8
	r16.s64 = ctx.r8.s64 - r31.s64;
	// srawi r17,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	r17.s64 = r17.s32 >> 1;
	// subf r15,r27,r16
	r15.s64 = r16.s64 - r27.s64;
	// subf r16,r8,r3
	r16.s64 = ctx.r3.s64 - ctx.r8.s64;
	// add r15,r15,r10
	r15.u64 = r15.u64 + ctx.r10.u64;
	// add r16,r16,r11
	r16.u64 = r16.u64 + r11.u64;
	// mullw r17,r17,r4
	r17.s64 = int64_t(r17.s32) * int64_t(ctx.r4.s32);
	// stw r15,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, r15.u32);
	// stw r16,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r16.u32);
	// stw r17,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r17.u32);
	// subf r16,r26,r23
	r16.s64 = r23.s64 - r26.s64;
	// subf r23,r23,r31
	r23.s64 = r31.s64 - r23.s64;
	// subf r15,r28,r9
	r15.s64 = ctx.r9.s64 - r28.s64;
	// subf r17,r8,r27
	r17.s64 = r27.s64 - ctx.r8.s64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r16,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, r16.u32);
	// subf r16,r11,r10
	r16.s64 = ctx.r10.s64 - r11.s64;
	// stw r23,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r23.u32);
	// subf r15,r30,r15
	r15.s64 = r15.s64 - r30.s64;
	// subf r16,r5,r16
	r16.s64 = r16.s64 - ctx.r5.s64;
	// subf r15,r5,r15
	r15.s64 = r15.s64 - ctx.r5.s64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// add r23,r15,r25
	r23.u64 = r15.u64 + r25.u64;
	// subf r16,r31,r16
	r16.s64 = r16.s64 - r31.s64;
	// add r23,r23,r21
	r23.u64 = r23.u64 + r21.u64;
	// add r16,r16,r25
	r16.u64 = r16.u64 + r25.u64;
	// rlwinm r23,r23,1,0,30
	r23.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// add r16,r16,r7
	r16.u64 = r16.u64 + ctx.r7.u64;
	// lwz r15,-368(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// stw r23,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r23.u32);
	// subf r15,r21,r15
	r15.s64 = r15.s64 - r21.s64;
	// rlwinm r21,r16,1,0,30
	r21.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r15,r9,r15
	r15.s64 = r15.s64 - ctx.r9.s64;
	// rotlwi r16,r7,1
	r16.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// subf r23,r10,r15
	r23.s64 = r15.s64 - ctx.r10.s64;
	// add r15,r16,r10
	r15.u64 = r16.u64 + ctx.r10.u64;
	// add r23,r23,r28
	r23.u64 = r23.u64 + r28.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// lwz r16,-368(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// stw r23,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r23.u32);
	// srawi r23,r14,1
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x1) != 0);
	r23.s64 = r14.s32 >> 1;
	// subf r16,r20,r16
	r16.s64 = r16.s64 - r20.s64;
	// lwz r14,-320(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// mullw r23,r23,r29
	r23.s64 = int64_t(r23.s32) * int64_t(r29.s32);
	// add r16,r16,r24
	r16.u64 = r16.u64 + r24.u64;
	// subf r15,r14,r15
	r15.s64 = r15.s64 - r14.s64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r15,r31,r15
	r15.s64 = r15.s64 - r31.s64;
	// stw r16,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, r16.u32);
	// subf r16,r11,r7
	r16.s64 = ctx.r7.s64 - r11.s64;
	// lwz r14,-368(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// stw r23,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r23.u32);
	// add r14,r14,r26
	r14.u64 = r14.u64 + r26.u64;
	// rlwinm r23,r17,1,0,30
	r23.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// add r17,r17,r23
	r17.u64 = r17.u64 + r23.u64;
	// rotlwi r23,r28,2
	r23.u64 = __builtin_rotateleft32(r28.u32, 2);
	// add r21,r21,r17
	r21.u64 = r21.u64 + r17.u64;
	// add r23,r28,r23
	r23.u64 = r28.u64 + r23.u64;
	// rotlwi r17,r9,3
	r17.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// subf r23,r23,r21
	r23.s64 = r21.s64 - r23.s64;
	// subf r21,r9,r17
	r21.s64 = r17.s64 - ctx.r9.s64;
	// lwz r17,-360(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// subf r28,r9,r28
	r28.s64 = r28.s64 - ctx.r9.s64;
	// add r21,r23,r21
	r21.u64 = r23.u64 + r21.u64;
	// lwz r23,-360(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// rlwinm r17,r17,3,0,28
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r21,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	r21.s64 = r21.s32 >> 1;
	// subf r23,r23,r17
	r23.s64 = r17.s64 - r23.s64;
	// lwz r17,-348(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// srawi r15,r15,1
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x1) != 0);
	r15.s64 = r15.s32 >> 1;
	// add r23,r17,r23
	r23.u64 = r17.u64 + r23.u64;
	// rlwinm r28,r28,1,0,30
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r21,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, r21.u32);
	// rlwinm r21,r14,1,0,30
	r21.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r23,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, r23.u32);
	// mulli r23,r16,11
	r23.s64 = r16.s64 * 11;
	// stw r23,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, r23.u32);
	// lwz r23,-352(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// rlwinm r16,r23,2,0,29
	r16.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r23,-328(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// rlwinm r17,r23,1,0,30
	r17.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// add r23,r23,r17
	r23.u64 = r23.u64 + r17.u64;
	// lwz r17,-368(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// stw r23,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r23.u32);
	// lwz r23,-312(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// add r23,r23,r17
	r23.u64 = r23.u64 + r17.u64;
	// lwz r14,-348(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// mullw r17,r14,r6
	r17.s64 = int64_t(r14.s32) * int64_t(ctx.r6.s32);
	// stw r17,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r17.u32);
	// lwz r14,-364(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// lwz r17,-340(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	// add r17,r17,r14
	r17.u64 = r17.u64 + r14.u64;
	// lwz r14,-352(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// add r16,r14,r16
	r16.u64 = r14.u64 + r16.u64;
	// add r17,r17,r16
	r17.u64 = r17.u64 + r16.u64;
	// lwz r14,-328(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// add r14,r21,r14
	r14.u64 = r21.u64 + r14.u64;
	// subf r16,r25,r14
	r16.s64 = r14.s64 - r25.s64;
	// subf r14,r10,r26
	r14.s64 = r26.s64 - ctx.r10.s64;
	// subf r25,r25,r28
	r25.s64 = r28.s64 - r25.s64;
	// rlwinm r14,r14,1,0,30
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r21,-368(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// add r23,r23,r21
	r23.u64 = r23.u64 + r21.u64;
	// rlwinm r21,r15,8,0,23
	r21.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r15,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	r15.s64 = r17.s32 >> 1;
	// lwz r17,-324(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// add r21,r23,r21
	r21.u64 = r23.u64 + r21.u64;
	// subf r23,r24,r16
	r23.s64 = r16.s64 - r24.s64;
	// mullw r16,r15,r4
	r16.s64 = int64_t(r15.s32) * int64_t(ctx.r4.s32);
	// subf r15,r11,r9
	r15.s64 = ctx.r9.s64 - r11.s64;
	// mullw r17,r21,r17
	r17.s64 = int64_t(r21.s32) * int64_t(r17.s32);
	// subf r15,r5,r15
	r15.s64 = r15.s64 - ctx.r5.s64;
	// add r20,r23,r20
	r20.u64 = r23.u64 + r20.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r23,r7,r11
	r23.s64 = r11.s64 - ctx.r7.s64;
	// subf r28,r30,r15
	r28.s64 = r15.s64 - r30.s64;
	// subf r15,r27,r14
	r15.s64 = r14.s64 - r27.s64;
	// subf r27,r27,r25
	r27.s64 = r25.s64 - r27.s64;
	// add r28,r28,r8
	r28.u64 = r28.u64 + ctx.r8.u64;
	// subf r27,r7,r27
	r27.s64 = r27.s64 - ctx.r7.s64;
	// subf r15,r8,r15
	r15.s64 = r15.s64 - ctx.r8.s64;
	// add r14,r28,r24
	r14.u64 = r28.u64 + r24.u64;
	// rlwinm r25,r3,1,0,30
	r25.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r28,r10,r27
	r28.s64 = r27.s64 - ctx.r10.s64;
	// subf r15,r9,r15
	r15.s64 = r15.s64 - ctx.r9.s64;
	// add r25,r3,r25
	r25.u64 = ctx.r3.u64 + r25.u64;
	// rlwinm r27,r14,1,0,30
	r27.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r24,r15
	ctx.r3.s64 = r15.s64 - r24.s64;
	// add r28,r28,r31
	r28.u64 = r28.u64 + r31.u64;
	// add r25,r27,r25
	r25.u64 = r27.u64 + r25.u64;
	// subf r21,r8,r11
	r21.s64 = r11.s64 - ctx.r8.s64;
	// add r3,r3,r7
	ctx.r3.u64 = ctx.r3.u64 + ctx.r7.u64;
	// rotlwi r27,r8,1
	r27.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// add r28,r28,r8
	r28.u64 = r28.u64 + ctx.r8.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// add r24,r27,r9
	r24.u64 = r27.u64 + ctx.r9.u64;
	// add r27,r28,r11
	r27.u64 = r28.u64 + r11.u64;
	// add r28,r3,r11
	r28.u64 = ctx.r3.u64 + r11.u64;
	// add r20,r20,r5
	r20.u64 = r20.u64 + ctx.r5.u64;
	// add r14,r28,r5
	r14.u64 = r28.u64 + ctx.r5.u64;
	// rotlwi r28,r26,2
	r28.u64 = __builtin_rotateleft32(r26.u32, 2);
	// mullw r3,r20,r29
	ctx.r3.s64 = int64_t(r20.s32) * int64_t(r29.s32);
	// add r20,r27,r5
	r20.u64 = r27.u64 + ctx.r5.u64;
	// add r28,r26,r28
	r28.u64 = r26.u64 + r28.u64;
	// rotlwi r27,r10,3
	r27.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// subf r28,r28,r25
	r28.s64 = r25.s64 - r28.s64;
	// subf r27,r10,r27
	r27.s64 = r27.s64 - ctx.r10.s64;
	// rlwinm r24,r24,1,0,30
	r24.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// add r27,r28,r27
	r27.u64 = r28.u64 + r27.u64;
	// rlwinm r28,r23,1,0,30
	r28.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r27,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	r27.s64 = r27.s32 >> 1;
	// add r23,r23,r28
	r23.u64 = r23.u64 + r28.u64;
	// lwz r28,-320(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// mullw r26,r20,r6
	r26.s64 = int64_t(r20.s32) * int64_t(ctx.r6.s32);
	// stw r27,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r27.u32);
	// add r27,r16,r3
	r27.u64 = r16.u64 + ctx.r3.u64;
	// subf r24,r28,r24
	r24.s64 = r24.s64 - r28.s64;
	// subf r15,r9,r11
	r15.s64 = r11.s64 - ctx.r9.s64;
	// mullw r28,r14,r29
	r28.s64 = int64_t(r14.s32) * int64_t(r29.s32);
	// subf r25,r10,r15
	r25.s64 = r15.s64 - ctx.r10.s64;
	// subf r24,r30,r24
	r24.s64 = r24.s64 - r30.s64;
	// add r25,r25,r5
	r25.u64 = r25.u64 + ctx.r5.u64;
	// subf r5,r10,r23
	ctx.r5.s64 = r23.s64 - ctx.r10.s64;
	// add r26,r27,r26
	r26.u64 = r27.u64 + r26.u64;
	// add r31,r5,r31
	r31.u64 = ctx.r5.u64 + r31.u64;
	// mullw r27,r25,r6
	r27.s64 = int64_t(r25.s32) * int64_t(ctx.r6.s32);
	// srawi r25,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	r25.s64 = r24.s32 >> 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// subf r8,r9,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r9.s64;
	// mullw r31,r31,r18
	r31.s64 = int64_t(r31.s32) * int64_t(r18.s32);
	// subf r7,r10,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r10.s64;
	// rotlwi r11,r11,8
	r11.u64 = __builtin_rotateleft32(r11.u32, 8);
	// lwz r20,-368(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// mullw r5,r26,r18
	ctx.r5.s64 = int64_t(r26.s32) * int64_t(r18.s32);
	// mullw r3,r20,r4
	ctx.r3.s64 = int64_t(r20.s32) * int64_t(ctx.r4.s32);
	// add r28,r3,r28
	r28.u64 = ctx.r3.u64 + r28.u64;
	// rlwinm r3,r21,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r4,r25,r4
	ctx.r4.s64 = int64_t(r25.s32) * int64_t(ctx.r4.s32);
	// add r3,r21,r3
	ctx.r3.u64 = r21.u64 + ctx.r3.u64;
	// add r4,r4,r31
	ctx.r4.u64 = ctx.r4.u64 + r31.u64;
	// subf r3,r9,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r9.s64;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// add r5,r17,r5
	ctx.r5.u64 = r17.u64 + ctx.r5.u64;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// mullw r9,r3,r29
	ctx.r9.s64 = int64_t(ctx.r3.s32) * int64_t(r29.s32);
	// add r10,r4,r9
	ctx.r10.u64 = ctx.r4.u64 + ctx.r9.u64;
	// mullw r9,r8,r6
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r7,r19
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(r19.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r28,r28,r19
	r28.s64 = int64_t(r28.s32) * int64_t(r19.s32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// add r5,r5,r28
	ctx.r5.u64 = ctx.r5.u64 + r28.u64;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82662e4c
	if (!cr6.gt) goto loc_82662E4C;
	// li r11,255
	r11.s64 = 255;
	// b 0x82662e58
	goto loc_82662E58;
loc_82662E4C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82662e58
	if (!cr6.lt) goto loc_82662E58;
	// li r11,0
	r11.s64 = 0;
loc_82662E58:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,-332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// lwz r9,-356(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -356);
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
	// lwz r26,-288(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// lwz r5,-304(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// li r25,128
	r25.s64 = 128;
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// li r24,16
	r24.s64 = 16;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// lwz r10,-284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// lwz r27,-336(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// stw r28,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r28.u32);
	// b 0x82662eb4
	goto loc_82662EB4;
loc_82662E94:
	// lwz r10,-284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// li r25,128
	r25.s64 = 128;
	// lwz r27,-336(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// li r24,16
	r24.s64 = 16;
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r26,-288(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// lwz r28,-332(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
loc_82662EB0:
	// li r8,1
	ctx.r8.s64 = 1;
loc_82662EB4:
	// lwz r11,-308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -308);
	// lwz r7,88(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r21,-296(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// lwz r23,-300(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// stw r8,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, ctx.r8.u32);
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// stw r11,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, r11.u32);
	// blt cr6,0x82661fb8
	if (cr6.lt) goto loc_82661FB8;
	// lwz r4,-280(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// b 0x826631ec
	goto loc_826631EC;
loc_82662EE0:
	// lwz r7,80(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x82662fc4
	if (!cr6.lt) goto loc_82662FC4;
	// addi r6,r7,-1
	ctx.r6.s64 = ctx.r7.s64 + -1;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// bge cr6,0x82662f5c
	if (!cr6.lt) goto loc_82662F5C;
	// rotlwi r6,r7,0
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// lbzx r7,r11,r10
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// lbz r4,1(r4)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// lbz r31,0(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lbz r29,1(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lwz r6,-292(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// mullw r30,r4,r6
	r30.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r6.s32);
	// subf r4,r4,r29
	ctx.r4.s64 = r29.s64 - ctx.r4.s64;
	// subf r29,r31,r4
	r29.s64 = ctx.r4.s64 - r31.s64;
	// mullw r4,r31,r5
	ctx.r4.s64 = int64_t(r31.s32) * int64_t(ctx.r5.s32);
	// add r31,r29,r7
	r31.u64 = r29.u64 + ctx.r7.u64;
	// mullw r31,r31,r6
	r31.s64 = int64_t(r31.s32) * int64_t(ctx.r6.s32);
	// mullw r31,r31,r5
	r31.s64 = int64_t(r31.s32) * int64_t(ctx.r5.s32);
	// srawi r31,r31,8
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFF) != 0);
	r31.s64 = r31.s32 >> 8;
	// subfic r6,r6,256
	xer.ca = ctx.r6.u32 <= 256;
	ctx.r6.s64 = 256 - ctx.r6.s64;
	// subf r6,r5,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r5.s64;
	// mullw r7,r6,r7
	ctx.r7.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// add r7,r31,r7
	ctx.r7.u64 = r31.u64 + ctx.r7.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// b 0x82662f7c
	goto loc_82662F7C;
loc_82662F5C:
	// add r6,r7,r11
	ctx.r6.u64 = ctx.r7.u64 + r11.u64;
	// lbzx r7,r11,r10
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// subfic r4,r5,256
	xer.ca = ctx.r5.u32 <= 256;
	ctx.r4.s64 = 256 - ctx.r5.s64;
	// mullw r7,r7,r4
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r4.s32);
	// lbzx r6,r6,r10
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// mullw r6,r6,r5
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r5.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// rlwinm r7,r7,24,8,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 24) & 0xFFFFFF;
loc_82662F7C:
	// stb r7,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r7.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r9,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r9.u32);
	// beq cr6,0x82662eb0
	if (cr6.eq) goto loc_82662EB0;
	// mullw r7,r26,r22
	ctx.r7.s64 = int64_t(r26.s32) * int64_t(r22.s32);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// li r8,0
	ctx.r8.s64 = 0;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// lbzx r7,r11,r23
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + r23.u32);
	// stb r7,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r7.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// lbzx r11,r11,r21
	r11.u64 = PPC_LOAD_U8(r11.u32 + r21.u32);
	// stw r27,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r27.u32);
	// stb r11,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r11.u8);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// stw r28,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r28.u32);
	// b 0x82662eb4
	goto loc_82662EB4;
loc_82662FC4:
	// stb r24,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r24.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r9,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r9.u32);
	// beq cr6,0x82662eb0
	if (cr6.eq) goto loc_82662EB0;
	// stb r25,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r25.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// stb r25,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r25.u8);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r27,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r27.u32);
	// stw r28,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r28.u32);
	// b 0x82662eb4
	goto loc_82662EB4;
loc_82662FF8:
	// lwz r7,84(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x82663194
	if (!cr6.lt) goto loc_82663194;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// bge cr6,0x826630e4
	if (!cr6.lt) goto loc_826630E4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826631ec
	if (!cr6.gt) goto loc_826631EC;
loc_82663020:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r11,r1,-244
	r11.s64 = ctx.r1.s64 + -244;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,-244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -244);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x826630a4
	if (cr6.lt) goto loc_826630A4;
	// lwz r7,80(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x826630a4
	if (!cr6.lt) goto loc_826630A4;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// lbzx r31,r11,r10
	r31.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// subfic r30,r5,256
	xer.ca = ctx.r5.u32 <= 256;
	r30.s64 = 256 - ctx.r5.s64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// mullw r8,r31,r30
	ctx.r8.s64 = int64_t(r31.s32) * int64_t(r30.s32);
	// lbzx r7,r7,r10
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r10.u32);
	// mullw r7,r7,r5
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r5.s32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r8,r8,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 24) & 0xFFFFFF;
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// beq cr6,0x826630cc
	if (cr6.eq) goto loc_826630CC;
	// mullw r7,r26,r22
	ctx.r7.s64 = int64_t(r26.s32) * int64_t(r22.s32);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// li r8,0
	ctx.r8.s64 = 0;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// lbzx r7,r11,r23
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + r23.u32);
	// stb r7,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r7.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// lbzx r11,r11,r21
	r11.u64 = PPC_LOAD_U8(r11.u32 + r21.u32);
	// stb r11,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r11.u8);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// b 0x826630d0
	goto loc_826630D0;
loc_826630A4:
	// stb r24,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r24.u8);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// beq cr6,0x826630cc
	if (cr6.eq) goto loc_826630CC;
	// stb r25,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r25.u8);
	// li r8,0
	ctx.r8.s64 = 0;
	// stb r25,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r25.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// b 0x826630d0
	goto loc_826630D0;
loc_826630CC:
	// li r8,1
	ctx.r8.s64 = 1;
loc_826630D0:
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// blt cr6,0x82663020
	if (cr6.lt) goto loc_82663020;
	// b 0x826631e0
	goto loc_826631E0;
loc_826630E4:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826631ec
	if (!cr6.gt) goto loc_826631EC;
loc_826630EC:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r11,r1,-240
	r11.s64 = ctx.r1.s64 + -240;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r7,-240(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -240);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// blt cr6,0x82663154
	if (cr6.lt) goto loc_82663154;
	// lwz r11,80(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// bge cr6,0x82663154
	if (!cr6.lt) goto loc_82663154;
	// lbzx r11,r7,r10
	r11.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r10.u32);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// beq cr6,0x8266317c
	if (cr6.eq) goto loc_8266317C;
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// mullw r11,r26,r22
	r11.s64 = int64_t(r26.s32) * int64_t(r22.s32);
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// lbzx r7,r11,r23
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + r23.u32);
	// stb r7,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r7.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// lbzx r11,r11,r21
	r11.u64 = PPC_LOAD_U8(r11.u32 + r21.u32);
	// stb r11,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r11.u8);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// b 0x82663180
	goto loc_82663180;
loc_82663154:
	// stb r24,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r24.u8);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// beq cr6,0x8266317c
	if (cr6.eq) goto loc_8266317C;
	// stb r25,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r25.u8);
	// li r8,0
	ctx.r8.s64 = 0;
	// stb r25,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r25.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// b 0x82663180
	goto loc_82663180;
loc_8266317C:
	// li r8,1
	ctx.r8.s64 = 1;
loc_82663180:
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// blt cr6,0x826630ec
	if (cr6.lt) goto loc_826630EC;
	// b 0x826631e0
	goto loc_826631E0;
loc_82663194:
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826631ec
	if (!cr6.gt) goto loc_826631EC;
loc_826631A4:
	// stb r24,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r24.u8);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// beq cr6,0x826631cc
	if (cr6.eq) goto loc_826631CC;
	// stb r25,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r25.u8);
	// li r8,0
	ctx.r8.s64 = 0;
	// stb r25,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r25.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// b 0x826631d0
	goto loc_826631D0;
loc_826631CC:
	// li r8,1
	ctx.r8.s64 = 1;
loc_826631D0:
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x826631a4
	if (cr6.lt) goto loc_826631A4;
loc_826631E0:
	// stw r28,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r28.u32);
	// stw r27,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r27.u32);
	// stw r9,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r9.u32);
loc_826631EC:
	// addi r6,r4,1
	ctx.r6.s64 = ctx.r4.s64 + 1;
	// lwz r4,80(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r11,r1,-268
	r11.s64 = ctx.r1.s64 + -268;
	// lwz r7,100(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 100);
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// fmr f0,f8
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f8.f64;
	// lis r8,-32126
	ctx.r8.s64 = -2105409536;
	// addi r5,r1,-304
	ctx.r5.s64 = ctx.r1.s64 + -304;
	// stw r6,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r6.u32);
	// std r10,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r10.u64);
	// lfd f13,-200(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmadd f13,f13,f3,f4
	ctx.f13.f64 = ctx.f13.f64 * ctx.f3.f64 + ctx.f4.f64;
	// fctiwz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f12,0,r11
	PPC_STORE_U32(r11.u32, ctx.f12.u32);
	// lwz r11,-268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// rlwinm r10,r11,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// stw r10,-15412(r8)
	PPC_STORE_U32(ctx.r8.u32 + -15412, ctx.r10.u32);
	// extsw r8,r10
	ctx.r8.s64 = ctx.r10.s32;
	// mullw r10,r4,r11
	ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// std r8,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r8.u64);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stw r10,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r10.u32);
	// lfd f12,-184(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// fmsub f13,f13,f11,f12
	ctx.f13.f64 = ctx.f13.f64 * ctx.f11.f64 - ctx.f12.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f13.u32);
	// lwz r19,-304(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// mullw r8,r19,r19
	ctx.r8.s64 = int64_t(r19.s32) * int64_t(r19.s32);
	// srawi r18,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	r18.s64 = ctx.r8.s32 >> 8;
	// mullw r8,r18,r19
	ctx.r8.s64 = int64_t(r18.s32) * int64_t(r19.s32);
	// srawi r17,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	r17.s64 = ctx.r8.s32 >> 8;
	// ble cr6,0x826637e4
	if (!cr6.gt) goto loc_826637E4;
	// lwz r8,84(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x826637e4
	if (!cr6.lt) goto loc_826637E4;
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r7,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, ctx.r7.u32);
	// ble cr6,0x82663900
	if (!cr6.gt) goto loc_82663900;
loc_8266329C:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r11,r1,-232
	r11.s64 = ctx.r1.s64 + -232;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,-232(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -232);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// ble cr6,0x8266379c
	if (!cr6.gt) goto loc_8266379C;
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x8266379c
	if (!cr6.lt) goto loc_8266379C;
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r4,r1,-248
	ctx.r4.s64 = ctx.r1.s64 + -248;
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r26,r8,r9
	r26.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lbz r5,2(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// subf r7,r8,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r8.s64;
	// lbz r8,1(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// stw r11,-15412(r10)
	PPC_STORE_U32(ctx.r10.u32 + -15412, r11.u32);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lbz r10,-1(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + -1);
	// add r25,r6,r9
	r25.u64 = ctx.r6.u64 + ctx.r9.u64;
	// lbz r31,-1(r26)
	r31.u64 = PPC_LOAD_U8(r26.u32 + -1);
	// lbz r6,-1(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + -1);
	// std r11,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, r11.u64);
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r27,2(r7)
	r27.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// lbz r30,1(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// rotlwi r3,r11,1
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lbz r29,1(r26)
	r29.u64 = PPC_LOAD_U8(r26.u32 + 1);
	// lbz r24,2(r26)
	r24.u64 = PPC_LOAD_U8(r26.u32 + 2);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r23,1(r25)
	r23.u64 = PPC_LOAD_U8(r25.u32 + 1);
	// add r16,r3,r30
	r16.u64 = ctx.r3.u64 + r30.u64;
	// lbz r28,-1(r25)
	r28.u64 = PPC_LOAD_U8(r25.u32 + -1);
	// lfd f13,-168(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f11,f13
	ctx.f13.f64 = f0.f64 * ctx.f11.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f13.u32);
	// lwz r21,-248(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -248);
	// lbz r4,0(r25)
	ctx.r4.u64 = PPC_LOAD_U8(r25.u32 + 0);
	// mullw r9,r21,r21
	ctx.r9.s64 = int64_t(r21.s32) * int64_t(r21.s32);
	// lbz r25,2(r25)
	r25.u64 = PPC_LOAD_U8(r25.u32 + 2);
	// srawi r20,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	r20.s64 = ctx.r9.s32 >> 8;
	// lbz r9,0(r7)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbz r7,0(r26)
	ctx.r7.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// add r26,r10,r9
	r26.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r26,r26,1,0,30
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r26,r4
	ctx.r3.s64 = ctx.r4.s64 - r26.s64;
	// rlwinm r26,r16,1,0,30
	r26.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// add r16,r29,r6
	r16.u64 = r29.u64 + ctx.r6.u64;
	// subf r26,r23,r26
	r26.s64 = r26.s64 - r23.s64;
	// add r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 + ctx.r5.u64;
	// subf r26,r24,r26
	r26.s64 = r26.s64 - r24.s64;
	// stw r26,-324(r1)
	PPC_STORE_U32(ctx.r1.u32 + -324, r26.u32);
	// rlwinm r26,r16,1,0,30
	r26.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r16,r31,r10
	r16.s64 = ctx.r10.s64 - r31.s64;
	// subf r26,r28,r26
	r26.s64 = r26.s64 - r28.s64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r26,r27,r26
	r26.s64 = r26.s64 - r27.s64;
	// subf r15,r5,r16
	r15.s64 = r16.s64 - ctx.r5.s64;
	// rlwinm r16,r26,1,0,30
	r16.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r26,r6,r15
	r26.s64 = r15.s64 - ctx.r6.s64;
	// add r16,r16,r25
	r16.u64 = r16.u64 + r25.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// add r26,r26,r24
	r26.u64 = r26.u64 + r24.u64;
	// rlwinm r26,r26,1,0,30
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r16,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r16.u32);
	// add r16,r7,r8
	r16.u64 = ctx.r7.u64 + ctx.r8.u64;
	// subf r15,r25,r26
	r15.s64 = r26.s64 - r25.s64;
	// mulli r14,r16,13
	r14.s64 = r16.s64 * 13;
	// rotlwi r16,r11,2
	r16.u64 = __builtin_rotateleft32(r11.u32, 2);
	// add r15,r15,r27
	r15.u64 = r15.u64 + r27.u64;
	// add r16,r11,r16
	r16.u64 = r11.u64 + r16.u64;
	// subf r26,r7,r29
	r26.s64 = r29.s64 - ctx.r7.s64;
	// stw r16,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, r16.u32);
	// rlwinm r16,r15,1,0,30
	r16.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r16,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r16.u32);
	// rlwinm r16,r3,3,0,28
	r16.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r3,r3,r16
	ctx.r3.s64 = r16.s64 - ctx.r3.s64;
	// lwz r16,-324(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// rlwinm r16,r16,2,0,29
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,-368(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// add r3,r3,r15
	ctx.r3.u64 = ctx.r3.u64 + r15.u64;
	// lwz r15,-324(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// add r16,r15,r16
	r16.u64 = r15.u64 + r16.u64;
	// add r3,r3,r16
	ctx.r3.u64 = ctx.r3.u64 + r16.u64;
	// subf r3,r14,r3
	ctx.r3.s64 = ctx.r3.s64 - r14.s64;
	// subf r16,r29,r7
	r16.s64 = ctx.r7.s64 - r29.s64;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// subf r15,r30,r23
	r15.s64 = r23.s64 - r30.s64;
	// mullw r3,r3,r18
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r18.s32);
	// stw r15,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r15.u32);
	// stw r3,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, ctx.r3.u32);
	// subf r3,r4,r8
	ctx.r3.s64 = ctx.r8.s64 - ctx.r4.s64;
	// subf r15,r11,r7
	r15.s64 = ctx.r7.s64 - r11.s64;
	// subf r3,r29,r3
	ctx.r3.s64 = ctx.r3.s64 - r29.s64;
	// mulli r15,r15,11
	r15.s64 = r15.s64 * 11;
	// add r3,r3,r9
	ctx.r3.u64 = ctx.r3.u64 + ctx.r9.u64;
	// stw r3,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, ctx.r3.u32);
	// subf r3,r5,r16
	ctx.r3.s64 = r16.s64 - ctx.r5.s64;
	// subf r16,r30,r9
	r16.s64 = ctx.r9.s64 - r30.s64;
	// add r3,r3,r10
	ctx.r3.u64 = ctx.r3.u64 + ctx.r10.u64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r16,r4,r16
	r16.s64 = r16.s64 - ctx.r4.s64;
	// subf r16,r6,r16
	r16.s64 = r16.s64 - ctx.r6.s64;
	// stw r3,-324(r1)
	PPC_STORE_U32(ctx.r1.u32 + -324, ctx.r3.u32);
	// subf r3,r31,r24
	ctx.r3.s64 = r24.s64 - r31.s64;
	// add r16,r16,r23
	r16.u64 = r16.u64 + r23.u64;
	// subf r23,r23,r4
	r23.s64 = ctx.r4.s64 - r23.s64;
	// add r16,r16,r27
	r16.u64 = r16.u64 + r27.u64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r3,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, ctx.r3.u32);
	// subf r3,r8,r26
	ctx.r3.s64 = r26.s64 - ctx.r8.s64;
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// stw r16,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r16.u32);
	// stw r3,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, ctx.r3.u32);
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// subf r3,r6,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r6.s64;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r5,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r5.s64;
	// add r3,r3,r8
	ctx.r3.u64 = ctx.r3.u64 + ctx.r8.u64;
	// add r14,r3,r27
	r14.u64 = ctx.r3.u64 + r27.u64;
	// rlwinm r3,r26,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r16,r14,1,0,30
	r16.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r26,r26,r3
	r26.u64 = r26.u64 + ctx.r3.u64;
	// lwz r14,-368(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// subf r3,r25,r14
	ctx.r3.s64 = r14.s64 - r25.s64;
	// add r14,r16,r26
	r14.u64 = r16.u64 + r26.u64;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// subf r26,r24,r23
	r26.s64 = r23.s64 - r24.s64;
	// lwz r23,-328(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r24,-312(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// stw r3,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, ctx.r3.u32);
	// subf r3,r10,r26
	ctx.r3.s64 = r26.s64 - ctx.r10.s64;
	// subf r26,r11,r8
	r26.s64 = ctx.r8.s64 - r11.s64;
	// subf r3,r9,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r9.s64;
	// mulli r16,r26,11
	r16.s64 = r26.s64 * 11;
	// stw r3,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, ctx.r3.u32);
	// lwz r3,-348(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// rlwinm r26,r3,3,0,28
	r26.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r3,r3,r26
	ctx.r3.s64 = r26.s64 - ctx.r3.s64;
	// rlwinm r26,r23,2,0,29
	r26.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r24,r3
	ctx.r3.u64 = r24.u64 + ctx.r3.u64;
	// add r26,r23,r26
	r26.u64 = r23.u64 + r26.u64;
	// add r3,r3,r15
	ctx.r3.u64 = ctx.r3.u64 + r15.u64;
	// lwz r15,-324(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// rotlwi r24,r30,2
	r24.u64 = __builtin_rotateleft32(r30.u32, 2);
	// add r3,r3,r26
	ctx.r3.u64 = ctx.r3.u64 + r26.u64;
	// lwz r26,-324(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// add r24,r30,r24
	r24.u64 = r30.u64 + r24.u64;
	// rotlwi r23,r9,3
	r23.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// rlwinm r15,r15,3,0,28
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r24,r24,r14
	r24.s64 = r14.s64 - r24.s64;
	// stw r3,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, ctx.r3.u32);
	// subf r23,r9,r23
	r23.s64 = r23.s64 - ctx.r9.s64;
	// subf r3,r26,r15
	ctx.r3.s64 = r15.s64 - r26.s64;
	// lwz r15,-264(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// add r23,r24,r23
	r23.u64 = r24.u64 + r23.u64;
	// rlwinm r26,r15,2,0,29
	r26.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r24,-340(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	// add r3,r24,r3
	ctx.r3.u64 = r24.u64 + ctx.r3.u64;
	// add r24,r15,r26
	r24.u64 = r15.u64 + r26.u64;
	// lwz r14,-368(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// srawi r14,r14,1
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x1) != 0);
	r14.s64 = r14.s32 >> 1;
	// srawi r15,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r15.s64 = r23.s32 >> 1;
	// mullw r26,r14,r17
	r26.s64 = int64_t(r14.s32) * int64_t(r17.s32);
	// add r23,r3,r24
	r23.u64 = ctx.r3.u64 + r24.u64;
	// lwz r3,-256(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -256);
	// lwz r24,-364(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// add r23,r23,r16
	r23.u64 = r23.u64 + r16.u64;
	// subf r16,r11,r9
	r16.s64 = ctx.r9.s64 - r11.s64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// subf r16,r6,r16
	r16.s64 = r16.s64 - ctx.r6.s64;
	// add r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 + ctx.r5.u64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r16,r4,r16
	r16.s64 = r16.s64 - ctx.r4.s64;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r30,r9,r30
	r30.s64 = r30.s64 - ctx.r9.s64;
	// add r26,r24,r26
	r26.u64 = r24.u64 + r26.u64;
	// mullw r24,r15,r19
	r24.s64 = int64_t(r15.s32) * int64_t(r19.s32);
	// stw r16,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r16.u32);
	// stw r3,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, ctx.r3.u32);
	// lwz r16,-360(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// rlwinm r14,r30,1,0,30
	r14.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r15,r10,r31
	r15.s64 = r31.s64 - ctx.r10.s64;
	// lwz r30,-360(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// rlwinm r30,r30,1,0,30
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// add r16,r16,r30
	r16.u64 = r16.u64 + r30.u64;
	// subf r15,r28,r15
	r15.s64 = r15.s64 - r28.s64;
	// subf r3,r8,r29
	ctx.r3.s64 = r29.s64 - ctx.r8.s64;
	// subf r14,r29,r14
	r14.s64 = r14.s64 - r29.s64;
	// subf r29,r29,r15
	r29.s64 = r15.s64 - r29.s64;
	// subf r14,r8,r14
	r14.s64 = r14.s64 - ctx.r8.s64;
	// add r26,r26,r24
	r26.u64 = r26.u64 + r24.u64;
	// srawi r23,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r23.s64 = r23.s32 >> 1;
	// mullw r24,r26,r20
	r24.s64 = int64_t(r26.s32) * int64_t(r20.s32);
	// subf r26,r8,r11
	r26.s64 = r11.s64 - ctx.r8.s64;
	// mullw r23,r23,r18
	r23.s64 = int64_t(r23.s32) * int64_t(r18.s32);
	// lwz r30,-368(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// lwz r15,-348(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// add r16,r15,r16
	r16.u64 = r15.u64 + r16.u64;
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// subf r15,r10,r14
	r15.s64 = r14.s64 - ctx.r10.s64;
	// subf r14,r7,r29
	r14.s64 = r29.s64 - ctx.r7.s64;
	// subf r28,r28,r16
	r28.s64 = r16.s64 - r28.s64;
	// subf r29,r27,r15
	r29.s64 = r15.s64 - r27.s64;
	// rlwinm r16,r30,1,0,30
	r16.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r30,r9,r14
	r30.s64 = r14.s64 - ctx.r9.s64;
	// subf r28,r27,r28
	r28.s64 = r28.s64 - r27.s64;
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// add r30,r30,r4
	r30.u64 = r30.u64 + ctx.r4.u64;
	// add r28,r28,r25
	r28.u64 = r28.u64 + r25.u64;
	// add r29,r29,r5
	r29.u64 = r29.u64 + ctx.r5.u64;
	// add r30,r30,r8
	r30.u64 = r30.u64 + ctx.r8.u64;
	// add r27,r28,r6
	r27.u64 = r28.u64 + ctx.r6.u64;
	// add r28,r29,r11
	r28.u64 = r29.u64 + r11.u64;
	// add r29,r30,r11
	r29.u64 = r30.u64 + r11.u64;
	// mullw r30,r27,r17
	r30.s64 = int64_t(r27.s32) * int64_t(r17.s32);
	// rlwinm r27,r3,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r26,1,0,30
	r25.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// add r27,r3,r27
	r27.u64 = ctx.r3.u64 + r27.u64;
	// rotlwi r3,r31,2
	ctx.r3.u64 = __builtin_rotateleft32(r31.u32, 2);
	// add r26,r26,r25
	r26.u64 = r26.u64 + r25.u64;
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// add r25,r16,r27
	r25.u64 = r16.u64 + r27.u64;
	// rotlwi r31,r10,3
	r31.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// subf r27,r10,r26
	r27.s64 = r26.s64 - ctx.r10.s64;
	// subf r3,r3,r25
	ctx.r3.s64 = r25.s64 - ctx.r3.s64;
	// subf r31,r10,r31
	r31.s64 = r31.s64 - ctx.r10.s64;
	// add r27,r27,r5
	r27.u64 = r27.u64 + ctx.r5.u64;
	// mullw r15,r20,r21
	r15.s64 = int64_t(r20.s32) * int64_t(r21.s32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// subf r14,r10,r11
	r14.s64 = r11.s64 - ctx.r10.s64;
	// srawi r27,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	r27.s64 = r27.s32 >> 1;
	// add r25,r28,r6
	r25.u64 = r28.u64 + ctx.r6.u64;
	// srawi r26,r15,8
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0xFF) != 0);
	r26.s64 = r15.s32 >> 8;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// add r29,r29,r6
	r29.u64 = r29.u64 + ctx.r6.u64;
	// subf r28,r9,r14
	r28.s64 = r14.s64 - ctx.r9.s64;
	// add r31,r23,r30
	r31.u64 = r23.u64 + r30.u64;
	// mullw r30,r25,r19
	r30.s64 = int64_t(r25.s32) * int64_t(r19.s32);
	// mullw r3,r3,r18
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r18.s32);
	// mullw r29,r29,r17
	r29.s64 = int64_t(r29.s32) * int64_t(r17.s32);
	// add r28,r28,r6
	r28.u64 = r28.u64 + ctx.r6.u64;
	// subf r25,r10,r8
	r25.s64 = ctx.r8.s64 - ctx.r10.s64;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r6,r3,r29
	ctx.r6.u64 = ctx.r3.u64 + r29.u64;
	// rlwinm r30,r27,8,0,23
	r30.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r29,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	r29.s64 = r25.s32 >> 1;
	// mullw r3,r28,r19
	ctx.r3.s64 = int64_t(r28.s32) * int64_t(r19.s32);
	// add r30,r31,r30
	r30.u64 = r31.u64 + r30.u64;
	// add r3,r6,r3
	ctx.r3.u64 = ctx.r6.u64 + ctx.r3.u64;
	// rlwinm r31,r29,8,0,23
	r31.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 8) & 0xFFFFFF00;
	// rotlwi r8,r8,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// rotlwi r31,r7,1
	r31.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r31,r31,r9
	r31.u64 = r31.u64 + ctx.r9.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r31,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r5,r5,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r5.s64;
	// lwz r10,-276(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// subf r8,r4,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r4.s64;
	// mullw r6,r30,r26
	ctx.r6.s64 = int64_t(r30.s32) * int64_t(r26.s32);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r10,r10,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r10.s64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// srawi r5,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r10.s32 >> 1;
	// add r6,r24,r6
	ctx.r6.u64 = r24.u64 + ctx.r6.u64;
	// mullw r3,r3,r21
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r21.s32);
	// mullw r10,r8,r18
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(r18.s32);
	// mullw r8,r5,r20
	ctx.r8.s64 = int64_t(ctx.r5.s32) * int64_t(r20.s32);
	// add r3,r6,r3
	ctx.r3.u64 = ctx.r6.u64 + ctx.r3.u64;
	// subf r6,r7,r11
	ctx.r6.s64 = r11.s64 - ctx.r7.s64;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rotlwi r10,r11,8
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 8);
	// rlwinm r11,r6,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r9,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 1;
	// mullw r11,r11,r17
	r11.s64 = int64_t(r11.s32) * int64_t(r17.s32);
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// mullw r9,r9,r19
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r19.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82663760
	if (!cr6.gt) goto loc_82663760;
	// li r11,255
	r11.s64 = 255;
	// b 0x8266376c
	goto loc_8266376C;
loc_82663760:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8266376c
	if (!cr6.lt) goto loc_8266376C;
	// li r11,0
	r11.s64 = 0;
loc_8266376C:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,-356(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -356);
	// lwz r28,-332(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// li r24,16
	r24.s64 = 16;
	// lwz r7,-308(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -308);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// li r25,128
	r25.s64 = 128;
	// lwz r27,-336(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// lwz r10,-284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// b 0x826637bc
	goto loc_826637BC;
loc_8266379C:
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x826637b4
	if (!cr6.lt) goto loc_826637B4;
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// b 0x826637b8
	goto loc_826637B8;
loc_826637B4:
	// stb r24,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r24.u8);
loc_826637B8:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
loc_826637BC:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// stw r9,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r9.u32);
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// stw r7,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, ctx.r7.u32);
	// blt cr6,0x8266329c
	if (cr6.lt) goto loc_8266329C;
	// lwz r6,-280(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// lwz r21,-296(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// lwz r23,-300(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// b 0x82663900
	goto loc_82663900;
loc_826637E4:
	// lwz r8,84(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x826638d4
	if (!cr6.lt) goto loc_826638D4;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// bge cr6,0x82663878
	if (!cr6.lt) goto loc_82663878;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82663900
	if (!cr6.gt) goto loc_82663900;
loc_8266380C:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r11,r1,-236
	r11.s64 = ctx.r1.s64 + -236;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,-236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x8266385c
	if (cr6.lt) goto loc_8266385C;
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x8266385c
	if (!cr6.lt) goto loc_8266385C;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// subfic r5,r19,256
	xer.ca = r19.u32 <= 256;
	ctx.r5.s64 = 256 - r19.s64;
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// lbzx r8,r8,r10
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// mullw r8,r8,r19
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r19.s32);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,24,8,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFFFFFF;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// b 0x82663860
	goto loc_82663860;
loc_8266385C:
	// stb r24,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r24.u8);
loc_82663860:
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// blt cr6,0x8266380c
	if (cr6.lt) goto loc_8266380C;
	// b 0x826638fc
	goto loc_826638FC;
loc_82663878:
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82663900
	if (!cr6.gt) goto loc_82663900;
loc_82663884:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r11,r1,-228
	r11.s64 = ctx.r1.s64 + -228;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,-228(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -228);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x826638b8
	if (cr6.lt) goto loc_826638B8;
	// lwz r7,80(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x826638b8
	if (!cr6.lt) goto loc_826638B8;
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// b 0x826638bc
	goto loc_826638BC;
loc_826638B8:
	// stb r24,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r24.u8);
loc_826638BC:
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// blt cr6,0x82663884
	if (cr6.lt) goto loc_82663884;
	// b 0x826638fc
	goto loc_826638FC;
loc_826638D4:
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82663900
	if (!cr6.gt) goto loc_82663900;
loc_826638E4:
	// stb r24,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r24.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x826638e4
	if (cr6.lt) goto loc_826638E4;
loc_826638FC:
	// stw r9,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r9.u32);
loc_82663900:
	// addi r4,r6,1
	ctx.r4.s64 = ctx.r6.s64 + 1;
	// lwz r11,92(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 92);
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// stw r4,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r4.u32);
	// blt cr6,0x82661e94
	if (cr6.lt) goto loc_82661E94;
loc_82663914:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82663918"))) PPC_WEAK_FUNC(sub_82663918);
PPC_FUNC_IMPL(__imp__sub_82663918) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	// mflr r12
	// bl 0x8239bcc0
	// li r26,0
	r26.s64 = 0;
	// lwz r11,112(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	// lwz r10,92(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 92);
	// mr r27,r26
	r27.u64 = r26.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r11.u32);
	// stw r27,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, r27.u32);
	// ble cr6,0x8266471c
	if (!cr6.gt) goto loc_8266471C;
	// lis r7,-32248
	ctx.r7.s64 = -2113404928;
	// fsub f8,f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f8.f64 = ctx.f2.f64 - ctx.f1.f64;
	// lis r8,-32249
	ctx.r8.s64 = -2113470464;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// lfd f12,-26736(r7)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r7.u32 + -26736);
	// lfd f7,-31368(r8)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r8.u32 + -31368);
	// lfd f9,-28592(r9)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r9.u32 + -28592);
	// lfd f10,-31360(r10)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r10.u32 + -31360);
loc_82663964:
	// extsw r10,r27
	ctx.r10.s64 = r27.s32;
	// lwz r9,96(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 96);
	// fmr f0,f8
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f8.f64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// std r10,-216(r1)
	PPC_STORE_U64(ctx.r1.u32 + -216, ctx.r10.u64);
	// lfd f13,-216(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -216);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmadd f11,f13,f3,f4
	ctx.f11.f64 = ctx.f13.f64 * ctx.f3.f64 + ctx.f4.f64;
	// beq cr6,0x82663994
	if (cr6.eq) goto loc_82663994;
	// fsub f13,f3,f10
	ctx.f13.f64 = ctx.f3.f64 - ctx.f10.f64;
	// fmul f13,f13,f9
	ctx.f13.f64 = ctx.f13.f64 * ctx.f9.f64;
	// b 0x82663998
	goto loc_82663998;
loc_82663994:
	// fmr f13,f7
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f7.f64;
loc_82663998:
	// fadd f13,f13,f11
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f13.f64 + ctx.f11.f64;
	// addi r10,r1,-256
	ctx.r10.s64 = ctx.r1.s64 + -256;
	// lis r7,-32126
	ctx.r7.s64 = -2105409536;
	// lwz r5,80(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r6,r1,-228
	ctx.r6.s64 = ctx.r1.s64 + -228;
	// lwz r8,100(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 100);
	// fctiwz f11,f13
	ctx.f11.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f11,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f11.u32);
	// lwz r10,-256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -256);
	// rlwinm r9,r10,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r9,-15412(r7)
	PPC_STORE_U32(ctx.r7.u32 + -15412, ctx.r9.u32);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// mullw r7,r5,r10
	ctx.r7.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// std r9,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r9.u64);
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r9,r8
	r31.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r31,-220(r1)
	PPC_STORE_U32(ctx.r1.u32 + -220, r31.u32);
	// lfd f11,-200(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fmsub f13,f13,f12,f11
	ctx.f13.f64 = ctx.f13.f64 * ctx.f12.f64 - ctx.f11.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f13.u32);
	// lwz r25,-228(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -228);
	// mullw r9,r25,r25
	ctx.r9.s64 = int64_t(r25.s32) * int64_t(r25.s32);
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stw r9,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r9.u32);
	// mullw r9,r9,r25
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r25.s32);
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stw r9,-248(r1)
	PPC_STORE_U32(ctx.r1.u32 + -248, ctx.r9.u32);
	// ble cr6,0x82664260
	if (!cr6.gt) goto loc_82664260;
	// lwz r9,84(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x8266425c
	if (!cr6.lt) goto loc_8266425C;
	// mr r28,r26
	r28.u64 = r26.u64;
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r28,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r28.u32);
	// ble cr6,0x82664708
	if (!cr6.gt) goto loc_82664708;
loc_82663A38:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r10,r1,-264
	ctx.r10.s64 = ctx.r1.s64 + -264;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r10,-264(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82663f90
	if (!cr6.gt) goto loc_82663F90;
	// lwz r9,80(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x82663f8c
	if (!cr6.lt) goto loc_82663F8C;
	// rlwinm r11,r10,8,0,23
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r8,r1,-236
	ctx.r8.s64 = ctx.r1.s64 + -236;
	// stw r11,-15412(r9)
	PPC_STORE_U32(ctx.r9.u32 + -15412, r11.u32);
	// extsw r9,r11
	ctx.r9.s64 = r11.s32;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,4
	ctx.r10.s64 = 4;
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
	// std r9,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r9.u64);
	// stw r10,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r10.u32);
	// lfd f13,-168(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f12,f13
	ctx.f13.f64 = f0.f64 * ctx.f12.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r21,-236(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -236);
	// mullw r11,r21,r21
	r11.s64 = int64_t(r21.s32) * int64_t(r21.s32);
	// srawi r20,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r20.s64 = r11.s32 >> 8;
	// mullw r11,r20,r21
	r11.s64 = int64_t(r20.s32) * int64_t(r21.s32);
	// srawi r18,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r18.s64 = r11.s32 >> 8;
loc_82663AB4:
	// lwz r9,80(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r10,-4(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + -4);
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 8);
	// subf r26,r6,r31
	r26.s64 = r31.s64 - ctx.r6.s64;
	// lbz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 4);
	// add r6,r7,r31
	ctx.r6.u64 = ctx.r7.u64 + r31.u64;
	// rlwinm r7,r9,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// rotlwi r17,r11,2
	r17.u64 = __builtin_rotateleft32(r11.u32, 2);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r29,4(r26)
	r29.u64 = PPC_LOAD_U8(r26.u32 + 4);
	// add r17,r11,r17
	r17.u64 = r11.u64 + r17.u64;
	// lbz r30,-4(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + -4);
	// add r24,r7,r31
	r24.u64 = ctx.r7.u64 + r31.u64;
	// lbz r28,4(r6)
	r28.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// rotlwi r19,r11,1
	r19.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lbz r7,0(r6)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lbz r6,-4(r26)
	ctx.r6.u64 = PPC_LOAD_U8(r26.u32 + -4);
	// lbzx r23,r9,r31
	r23.u64 = PPC_LOAD_U8(ctx.r9.u32 + r31.u32);
	// add r19,r19,r30
	r19.u64 = r19.u64 + r30.u64;
	// lbz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// subf r15,r28,r7
	r15.s64 = ctx.r7.s64 - r28.s64;
	// stw r17,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, r17.u32);
	// add r16,r19,r29
	r16.u64 = r19.u64 + r29.u64;
	// add r17,r10,r9
	r17.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lbz r4,0(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 0);
	// lbz r22,4(r24)
	r22.u64 = PPC_LOAD_U8(r24.u32 + 4);
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r27,-4(r24)
	r27.u64 = PPC_LOAD_U8(r24.u32 + -4);
	// lbz r26,8(r26)
	r26.u64 = PPC_LOAD_U8(r26.u32 + 8);
	// subf r19,r17,r4
	r19.s64 = ctx.r4.s64 - r17.s64;
	// lbz r24,8(r24)
	r24.u64 = PPC_LOAD_U8(r24.u32 + 8);
	// add r17,r19,r5
	r17.u64 = r19.u64 + ctx.r5.u64;
	// subf r19,r22,r16
	r19.s64 = r16.s64 - r22.s64;
	// subf r16,r23,r19
	r16.s64 = r19.s64 - r23.s64;
	// subf r19,r5,r15
	r19.s64 = r15.s64 - ctx.r5.s64;
	// subf r15,r29,r9
	r15.s64 = ctx.r9.s64 - r29.s64;
	// add r19,r19,r10
	r19.u64 = r19.u64 + ctx.r10.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r15,r4,r15
	r15.s64 = r15.s64 - ctx.r4.s64;
	// stw r19,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r19.u32);
	// add r19,r28,r6
	r19.u64 = r28.u64 + ctx.r6.u64;
	// rlwinm r19,r19,1,0,30
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r19,r27,r19
	r19.s64 = r19.s64 - r27.s64;
	// subf r14,r26,r19
	r14.s64 = r19.s64 - r26.s64;
	// subf r19,r6,r15
	r19.s64 = r15.s64 - ctx.r6.s64;
	// rlwinm r15,r14,1,0,30
	r15.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r19,r19,r22
	r19.u64 = r19.u64 + r22.u64;
	// add r15,r15,r24
	r15.u64 = r15.u64 + r24.u64;
	// add r19,r19,r26
	r19.u64 = r19.u64 + r26.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r19,r19,1,0,30
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r15,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, r15.u32);
	// add r15,r7,r8
	r15.u64 = ctx.r7.u64 + ctx.r8.u64;
	// mulli r14,r15,13
	r14.s64 = r15.s64 * 13;
	// subf r15,r24,r19
	r15.s64 = r19.s64 - r24.s64;
	// subf r19,r7,r28
	r19.s64 = r28.s64 - ctx.r7.s64;
	// add r15,r15,r27
	r15.u64 = r15.u64 + r27.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r15,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, r15.u32);
	// rlwinm r15,r17,3,0,28
	r15.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r17,r17,r15
	r17.s64 = r15.s64 - r17.s64;
	// rlwinm r15,r16,2,0,29
	r15.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r15,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, r15.u32);
	// lwz r15,-300(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// add r17,r17,r15
	r17.u64 = r17.u64 + r15.u64;
	// lwz r15,-304(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// add r16,r16,r15
	r16.u64 = r16.u64 + r15.u64;
	// add r16,r17,r16
	r16.u64 = r17.u64 + r16.u64;
	// lwz r17,-312(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// rlwinm r15,r17,3,0,28
	r15.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r14,r14,r16
	r14.s64 = r16.s64 - r14.s64;
	// subf r16,r17,r15
	r16.s64 = r15.s64 - r17.s64;
	// srawi r14,r14,1
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x1) != 0);
	r14.s64 = r14.s32 >> 1;
	// subf r17,r30,r23
	r17.s64 = r23.s64 - r30.s64;
	// mullw r14,r14,r20
	r14.s64 = int64_t(r14.s32) * int64_t(r20.s32);
	// rlwinm r15,r17,2,0,29
	r15.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 2) & 0xFFFFFFFC;
	// add r17,r17,r15
	r17.u64 = r17.u64 + r15.u64;
	// stw r14,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r14.u32);
	// lwz r14,-320(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// subf r15,r29,r22
	r15.s64 = r22.s64 - r29.s64;
	// subf r22,r22,r4
	r22.s64 = ctx.r4.s64 - r22.s64;
	// add r16,r14,r16
	r16.u64 = r14.u64 + r16.u64;
	// subf r14,r11,r8
	r14.s64 = ctx.r8.s64 - r11.s64;
	// stw r17,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, r17.u32);
	// subf r17,r4,r8
	r17.s64 = ctx.r8.s64 - ctx.r4.s64;
	// stw r15,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, r15.u32);
	// subf r15,r11,r9
	r15.s64 = ctx.r9.s64 - r11.s64;
	// subf r17,r28,r17
	r17.s64 = r17.s64 - r28.s64;
	// subf r15,r6,r15
	r15.s64 = r15.s64 - ctx.r6.s64;
	// add r17,r17,r9
	r17.u64 = r17.u64 + ctx.r9.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r15,r4,r15
	r15.s64 = r15.s64 - ctx.r4.s64;
	// stw r17,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, r17.u32);
	// subf r17,r8,r19
	r17.s64 = r19.s64 - ctx.r8.s64;
	// add r15,r15,r27
	r15.u64 = r15.u64 + r27.u64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// add r15,r15,r7
	r15.u64 = r15.u64 + ctx.r7.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r17,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r17.u32);
	// subf r17,r30,r10
	r17.s64 = ctx.r10.s64 - r30.s64;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r15,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r15.u32);
	// subf r17,r5,r17
	r17.s64 = r17.s64 - ctx.r5.s64;
	// subf r17,r6,r17
	r17.s64 = r17.s64 - ctx.r6.s64;
	// add r17,r17,r27
	r17.u64 = r17.u64 + r27.u64;
	// add r17,r17,r23
	r17.u64 = r17.u64 + r23.u64;
	// subf r23,r23,r22
	r23.s64 = r22.s64 - r23.s64;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r15,-320(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// stw r17,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, r17.u32);
	// rotlwi r17,r7,1
	r17.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// add r16,r16,r15
	r16.u64 = r16.u64 + r15.u64;
	// mulli r15,r14,11
	r15.s64 = r14.s64 * 11;
	// add r14,r17,r9
	r14.u64 = r17.u64 + ctx.r9.u64;
	// add r16,r16,r15
	r16.u64 = r16.u64 + r15.u64;
	// rlwinm r15,r14,1,0,30
	r15.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r22,r16,1
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x1) != 0);
	r22.s64 = r16.s32 >> 1;
	// lwz r16,-276(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// subf r16,r16,r15
	r16.s64 = r15.s64 - r16.s64;
	// subf r15,r10,r23
	r15.s64 = r23.s64 - ctx.r10.s64;
	// mullw r23,r22,r18
	r23.s64 = int64_t(r22.s32) * int64_t(r18.s32);
	// lwz r14,-312(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// stw r23,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, r23.u32);
	// subf r23,r8,r28
	r23.s64 = r28.s64 - ctx.r8.s64;
	// rlwinm r22,r23,1,0,30
	r22.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// add r22,r23,r22
	r22.u64 = r23.u64 + r22.u64;
	// rotlwi r23,r30,2
	r23.u64 = __builtin_rotateleft32(r30.u32, 2);
	// add r22,r14,r22
	r22.u64 = r14.u64 + r22.u64;
	// add r23,r30,r23
	r23.u64 = r30.u64 + r23.u64;
	// rotlwi r14,r10,3
	r14.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// subf r23,r23,r22
	r23.s64 = r22.s64 - r23.s64;
	// subf r22,r10,r14
	r22.s64 = r14.s64 - ctx.r10.s64;
	// lwz r14,-304(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// add r22,r23,r22
	r22.u64 = r23.u64 + r22.u64;
	// lwz r23,-304(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// rlwinm r14,r14,3,0,28
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r23,r23,r14
	r23.s64 = r14.s64 - r23.s64;
	// srawi r14,r22,1
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x1) != 0);
	r14.s64 = r22.s32 >> 1;
	// subf r22,r4,r16
	r22.s64 = r16.s64 - ctx.r4.s64;
	// srawi r22,r22,1
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x1) != 0);
	r22.s64 = r22.s32 >> 1;
	// lwz r17,-320(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// subf r17,r24,r17
	r17.s64 = r17.s64 - r24.s64;
	// add r17,r17,r26
	r17.u64 = r17.u64 + r26.u64;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r17,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, r17.u32);
	// subf r17,r11,r7
	r17.s64 = ctx.r7.s64 - r11.s64;
	// lwz r16,-320(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// add r16,r16,r23
	r16.u64 = r16.u64 + r23.u64;
	// mulli r23,r17,11
	r23.s64 = r17.s64 * 11;
	// stw r23,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, r23.u32);
	// lwz r23,-300(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// subf r17,r9,r15
	r17.s64 = r15.s64 - ctx.r9.s64;
	// lwz r15,-308(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -308);
	// rlwinm r23,r23,2,0,29
	r23.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r22,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, r22.u32);
	// stw r17,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r17.u32);
	// lwz r17,-296(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// add r22,r17,r15
	r22.u64 = r17.u64 + r15.u64;
	// lwz r15,-320(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// mullw r17,r14,r21
	r17.s64 = int64_t(r14.s32) * int64_t(r21.s32);
	// lwz r14,-308(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -308);
	// add r16,r16,r15
	r16.u64 = r16.u64 + r15.u64;
	// lwz r15,-300(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// add r22,r22,r17
	r22.u64 = r22.u64 + r17.u64;
	// add r15,r15,r23
	r15.u64 = r15.u64 + r23.u64;
	// lwz r23,-312(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// rlwinm r17,r14,8,0,23
	r17.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 8) & 0xFFFFFF00;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// lwz r14,-272(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// subf r30,r10,r30
	r30.s64 = r30.s64 - ctx.r10.s64;
	// add r23,r23,r5
	r23.u64 = r23.u64 + ctx.r5.u64;
	// rlwinm r30,r30,1,0,30
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r17,r22,r17
	r17.u64 = r22.u64 + r17.u64;
	// lwz r22,-292(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// add r16,r16,r15
	r16.u64 = r16.u64 + r15.u64;
	// subf r30,r27,r30
	r30.s64 = r30.s64 - r27.s64;
	// add r15,r23,r29
	r15.u64 = r23.u64 + r29.u64;
	// rlwinm r23,r22,1,0,30
	r23.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r16,r16,1
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x1) != 0);
	r16.s64 = r16.s32 >> 1;
	// add r23,r22,r23
	r23.u64 = r22.u64 + r23.u64;
	// mullw r16,r16,r20
	r16.s64 = int64_t(r16.s32) * int64_t(r20.s32);
	// stw r30,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r30.u32);
	// stw r23,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, r23.u32);
	// stw r16,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r16.u32);
	// rlwinm r16,r15,1,0,30
	r16.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r15,r11,r10
	r15.s64 = ctx.r10.s64 - r11.s64;
	// mullw r17,r17,r14
	r17.s64 = int64_t(r17.s32) * int64_t(r14.s32);
	// subf r15,r6,r15
	r15.s64 = r15.s64 - ctx.r6.s64;
	// subf r14,r9,r29
	r14.s64 = r29.s64 - ctx.r9.s64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r14,r14,1,0,30
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r30,r5,r15
	r30.s64 = r15.s64 - ctx.r5.s64;
	// subf r15,r28,r14
	r15.s64 = r14.s64 - r28.s64;
	// add r30,r30,r8
	r30.u64 = r30.u64 + ctx.r8.u64;
	// subf r15,r8,r15
	r15.s64 = r15.s64 - ctx.r8.s64;
	// subf r23,r8,r11
	r23.s64 = r11.s64 - ctx.r8.s64;
	// subf r15,r10,r15
	r15.s64 = r15.s64 - ctx.r10.s64;
	// subf r22,r7,r11
	r22.s64 = r11.s64 - ctx.r7.s64;
	// lwz r14,-292(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// subf r28,r28,r14
	r28.s64 = r14.s64 - r28.s64;
	// lwz r14,-308(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -308);
	// subf r28,r7,r28
	r28.s64 = r28.s64 - ctx.r7.s64;
	// add r16,r16,r14
	r16.u64 = r16.u64 + r14.u64;
	// add r14,r30,r26
	r14.u64 = r30.u64 + r26.u64;
	// rlwinm r30,r19,1,0,30
	r30.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r27,r27,r16
	r27.s64 = r16.s64 - r27.s64;
	// add r19,r19,r30
	r19.u64 = r19.u64 + r30.u64;
	// subf r27,r26,r27
	r27.s64 = r27.s64 - r26.s64;
	// subf r30,r26,r15
	r30.s64 = r15.s64 - r26.s64;
	// rotlwi r26,r8,1
	r26.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// rlwinm r16,r14,1,0,30
	r16.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r26,r26,r10
	r26.u64 = r26.u64 + ctx.r10.u64;
	// subf r28,r9,r28
	r28.s64 = r28.s64 - ctx.r9.s64;
	// rlwinm r15,r26,1,0,30
	r15.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// rotlwi r26,r29,2
	r26.u64 = __builtin_rotateleft32(r29.u32, 2);
	// add r19,r16,r19
	r19.u64 = r16.u64 + r19.u64;
	// add r29,r29,r26
	r29.u64 = r29.u64 + r26.u64;
	// rotlwi r26,r9,3
	r26.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r28,r28,r4
	r28.u64 = r28.u64 + ctx.r4.u64;
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// add r27,r27,r24
	r27.u64 = r27.u64 + r24.u64;
	// subf r29,r29,r19
	r29.s64 = r19.s64 - r29.s64;
	// subf r26,r9,r26
	r26.s64 = r26.s64 - ctx.r9.s64;
	// add r28,r28,r8
	r28.u64 = r28.u64 + ctx.r8.u64;
	// add r30,r30,r5
	r30.u64 = r30.u64 + ctx.r5.u64;
	// add r24,r27,r6
	r24.u64 = r27.u64 + ctx.r6.u64;
	// add r26,r29,r26
	r26.u64 = r29.u64 + r26.u64;
	// add r27,r28,r11
	r27.u64 = r28.u64 + r11.u64;
	// rlwinm r29,r23,1,0,30
	r29.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// add r28,r30,r11
	r28.u64 = r30.u64 + r11.u64;
	// subf r16,r10,r11
	r16.s64 = r11.s64 - ctx.r10.s64;
	// mullw r30,r24,r18
	r30.s64 = int64_t(r24.s32) * int64_t(r18.s32);
	// srawi r24,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	r24.s64 = r26.s32 >> 1;
	// add r23,r23,r29
	r23.u64 = r23.u64 + r29.u64;
	// lwz r29,-276(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// add r27,r27,r6
	r27.u64 = r27.u64 + ctx.r6.u64;
	// add r19,r28,r6
	r19.u64 = r28.u64 + ctx.r6.u64;
	// subf r26,r9,r16
	r26.s64 = r16.s64 - ctx.r9.s64;
	// subf r16,r29,r15
	r16.s64 = r15.s64 - r29.s64;
	// lwz r29,-296(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// mullw r27,r27,r21
	r27.s64 = int64_t(r27.s32) * int64_t(r21.s32);
	// add r28,r29,r30
	r28.u64 = r29.u64 + r30.u64;
	// mullw r30,r24,r20
	r30.s64 = int64_t(r24.s32) * int64_t(r20.s32);
	// mullw r29,r19,r18
	r29.s64 = int64_t(r19.s32) * int64_t(r18.s32);
	// add r26,r26,r6
	r26.u64 = r26.u64 + ctx.r6.u64;
	// subf r6,r10,r23
	ctx.r6.s64 = r23.s64 - ctx.r10.s64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// add r27,r28,r27
	r27.u64 = r28.u64 + r27.u64;
	// lwz r28,-248(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -248);
	// subf r24,r5,r16
	r24.s64 = r16.s64 - ctx.r5.s64;
	// mullw r29,r26,r21
	r29.s64 = int64_t(r26.s32) * int64_t(r21.s32);
	// add r5,r6,r5
	ctx.r5.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// srawi r26,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	r26.s64 = r24.s32 >> 1;
	// mullw r6,r27,r28
	ctx.r6.s64 = int64_t(r27.s32) * int64_t(r28.s32);
	// srawi r27,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	r27.s64 = ctx.r5.s32 >> 1;
	// mullw r29,r30,r25
	r29.s64 = int64_t(r30.s32) * int64_t(r25.s32);
	// mullw r30,r27,r18
	r30.s64 = int64_t(r27.s32) * int64_t(r18.s32);
	// mullw r5,r26,r20
	ctx.r5.s64 = int64_t(r26.s32) * int64_t(r20.s32);
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// rlwinm r30,r22,1,0,30
	r30.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// add r10,r22,r30
	ctx.r10.u64 = r22.u64 + r30.u64;
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// subf r9,r9,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r9.s64;
	// rotlwi r10,r11,8
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 8);
	// add r11,r9,r4
	r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r6,r17,r6
	ctx.r6.u64 = r17.u64 + ctx.r6.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r9,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 1;
	// mullw r11,r11,r28
	r11.s64 = int64_t(r11.s32) * int64_t(r28.s32);
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// mullw r9,r9,r21
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r21.s32);
	// srawi r8,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 1;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// mullw r9,r8,r25
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(r25.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r6,r6,r29
	ctx.r6.u64 = ctx.r6.u64 + r29.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82663f40
	if (!cr6.gt) goto loc_82663F40;
	// li r11,255
	r11.s64 = 255;
	// b 0x82663f4c
	goto loc_82663F4C;
loc_82663F40:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82663f4c
	if (!cr6.lt) goto loc_82663F4C;
	// li r11,0
	r11.s64 = 0;
loc_82663F4C:
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lwz r11,-316(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// lwz r10,-284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r10,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r10.u32);
	// stw r11,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r11.u32);
	// bne cr6,0x82663ab4
	if (!cr6.eq) goto loc_82663AB4;
	// lwz r27,-280(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// li r26,0
	r26.s64 = 0;
	// lwz r28,-288(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// lwz r31,-220(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -220);
	// b 0x82664244
	goto loc_82664244;
loc_82663F8C:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
loc_82663F90:
	// blt cr6,0x82664138
	if (cr6.lt) goto loc_82664138;
	// lwz r9,80(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x82664138
	if (!cr6.lt) goto loc_82664138;
	// rlwinm r9,r10,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r7,80(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r4,r1,-240
	ctx.r4.s64 = ctx.r1.s64 + -240;
	// stw r9,-15412(r10)
	PPC_STORE_U32(ctx.r10.u32 + -15412, ctx.r9.u32);
	// add r10,r8,r31
	ctx.r10.u64 = ctx.r8.u64 + r31.u64;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lbz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// std r8,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r8.u64);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lfd f13,-184(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f12,f13
	ctx.f13.f64 = f0.f64 * ctx.f12.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f13.u32);
	// lbz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbz r7,4(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lwz r9,-240(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -240);
	// subf r7,r4,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r4.s64;
	// subfic r8,r9,256
	xer.ca = ctx.r9.u32 <= 256;
	ctx.r8.s64 = 256 - ctx.r9.s64;
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// mullw r29,r6,r9
	r29.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r9.s32);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// mullw r4,r4,r25
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r25.s32);
	// mullw r7,r7,r9
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// mullw r7,r7,r25
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r25.s32);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// subf r8,r25,r8
	ctx.r8.s64 = ctx.r8.s64 - r25.s64;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// mullw r30,r8,r5
	r30.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// lwz r6,80(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r7,5(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r6,1(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// mullw r30,r7,r9
	r30.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lbz r4,1(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// lbz r29,5(r5)
	r29.u64 = PPC_LOAD_U8(ctx.r5.u32 + 5);
	// mullw r5,r8,r6
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// subf r29,r4,r29
	r29.s64 = r29.s64 - ctx.r4.s64;
	// mullw r4,r4,r25
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r25.s32);
	// subf r7,r7,r29
	ctx.r7.s64 = r29.s64 - ctx.r7.s64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r7,r7,r9
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// mullw r7,r7,r25
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r25.s32);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// stb r7,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r7.u8);
	// lwz r6,80(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r7,6(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r6,2(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// mullw r30,r7,r9
	r30.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lbz r4,2(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + 2);
	// lbz r29,6(r5)
	r29.u64 = PPC_LOAD_U8(ctx.r5.u32 + 6);
	// mullw r5,r8,r6
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// subf r29,r4,r29
	r29.s64 = r29.s64 - ctx.r4.s64;
	// mullw r4,r4,r25
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r25.s32);
	// subf r7,r7,r29
	ctx.r7.s64 = r29.s64 - ctx.r7.s64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r7,r7,r9
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// mullw r7,r7,r25
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r25.s32);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// stb r7,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r7.u8);
	// lwz r6,80(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r7,7(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r6,3(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// mullw r4,r7,r9
	ctx.r4.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// mullw r8,r8,r6
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// lbz r5,3(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r10,7(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// subf r10,r5,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r5.s64;
	// mullw r5,r5,r25
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r25.s32);
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// mullw r10,r10,r25
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r25.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stb r10,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r10.u8);
	// b 0x8266423c
	goto loc_8266423C;
loc_82664138:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82664238
	if (!cr6.gt) goto loc_82664238;
	// lwz r7,80(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// bge cr6,0x82664238
	if (!cr6.lt) goto loc_82664238;
	// rlwinm r9,r10,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lis r8,-32126
	ctx.r8.s64 = -2105409536;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// stw r9,-15412(r8)
	PPC_STORE_U32(ctx.r8.u32 + -15412, ctx.r9.u32);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// addi r8,r1,-224
	ctx.r8.s64 = ctx.r1.s64 + -224;
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// std r9,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r9.u64);
	// lbzx r9,r6,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// mullw r6,r9,r25
	ctx.r6.s64 = int64_t(ctx.r9.s32) * int64_t(r25.s32);
	// lfd f13,-208(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f12,f13
	ctx.f13.f64 = f0.f64 * ctx.f12.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r9,-224(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -224);
	// subfic r8,r9,256
	xer.ca = ctx.r9.u32 <= 256;
	ctx.r8.s64 = 256 - ctx.r9.s64;
	// subf r8,r25,r8
	ctx.r8.s64 = ctx.r8.s64 - r25.s64;
	// add r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r4,r8,r9
	ctx.r4.u64 = ctx.r8.u64 + ctx.r9.u64;
	// mullw r7,r5,r7
	ctx.r7.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r7.s32);
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 + ctx.r9.u64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// add r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r9,1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r9,r4,r9
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lbz r8,1(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// mullw r8,r8,r25
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r25.s32);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stb r9,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r9.u8);
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r9,r6,r9
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r9.s32);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lbz r8,2(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 2);
	// mullw r8,r8,r25
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r25.s32);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stb r9,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r9.u8);
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r9,3(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r9,r5,r9
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r9.s32);
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lbz r10,3(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// mullw r10,r10,r25
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r25.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stb r10,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r10.u8);
	// b 0x8266423c
	goto loc_8266423C;
loc_82664238:
	// stb r26,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r26.u8);
loc_8266423C:
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r11,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r11.u32);
loc_82664244:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// stw r28,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r28.u32);
	// blt cr6,0x82663a38
	if (cr6.lt) goto loc_82663A38;
	// b 0x82664708
	goto loc_82664708;
loc_8266425C:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
loc_82664260:
	// blt cr6,0x82664560
	if (cr6.lt) goto loc_82664560;
	// lwz r9,84(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x82664560
	if (!cr6.lt) goto loc_82664560;
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// mr r28,r26
	r28.u64 = r26.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82664708
	if (!cr6.gt) goto loc_82664708;
loc_82664284:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r10,r1,-232
	ctx.r10.s64 = ctx.r1.s64 + -232;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r10,-232(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -232);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82664544
	if (cr6.lt) goto loc_82664544;
	// lwz r9,80(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x82664444
	if (!cr6.lt) goto loc_82664444;
	// rlwinm r9,r10,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r7,80(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r4,r1,-244
	ctx.r4.s64 = ctx.r1.s64 + -244;
	// stw r9,-15412(r10)
	PPC_STORE_U32(ctx.r10.u32 + -15412, ctx.r9.u32);
	// add r10,r8,r31
	ctx.r10.u64 = ctx.r8.u64 + r31.u64;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lbz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// std r8,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r8.u64);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lfd f13,-192(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f12,f13
	ctx.f13.f64 = f0.f64 * ctx.f12.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f13.u32);
	// lbz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbz r7,4(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lwz r9,-244(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -244);
	// subf r7,r4,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r4.s64;
	// subfic r8,r9,256
	xer.ca = ctx.r9.u32 <= 256;
	ctx.r8.s64 = 256 - ctx.r9.s64;
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// mullw r29,r6,r9
	r29.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r9.s32);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// subf r8,r25,r8
	ctx.r8.s64 = ctx.r8.s64 - r25.s64;
	// mullw r7,r7,r9
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// mullw r7,r7,r25
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r25.s32);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// mullw r30,r8,r5
	r30.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// mullw r4,r4,r25
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r25.s32);
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// lwz r6,80(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r7,5(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r6,1(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// mullw r29,r7,r9
	r29.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// mullw r30,r8,r6
	r30.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// lbz r4,1(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// lbz r5,5(r5)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + 5);
	// subf r24,r4,r5
	r24.s64 = ctx.r5.s64 - ctx.r4.s64;
	// mullw r5,r4,r25
	ctx.r5.s64 = int64_t(ctx.r4.s32) * int64_t(r25.s32);
	// subf r7,r7,r24
	ctx.r7.s64 = r24.s64 - ctx.r7.s64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r7,r7,r9
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// mullw r7,r7,r25
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r25.s32);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// stb r7,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r7.u8);
	// lwz r6,80(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r7,6(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r6,2(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// mullw r29,r7,r9
	r29.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// mullw r30,r8,r6
	r30.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// lbz r4,2(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + 2);
	// lbz r5,6(r5)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + 6);
	// subf r24,r4,r5
	r24.s64 = ctx.r5.s64 - ctx.r4.s64;
	// mullw r5,r4,r25
	ctx.r5.s64 = int64_t(ctx.r4.s32) * int64_t(r25.s32);
	// subf r7,r7,r24
	ctx.r7.s64 = r24.s64 - ctx.r7.s64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r7,r7,r9
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// mullw r7,r7,r25
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r25.s32);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// stb r7,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r7.u8);
	// lwz r6,80(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r7,7(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r6,3(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// mullw r30,r7,r9
	r30.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// mullw r4,r8,r6
	ctx.r4.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// lbz r5,3(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r10,7(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// subf r10,r5,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r5.s64;
	// mullw r8,r5,r25
	ctx.r8.s64 = int64_t(ctx.r5.s32) * int64_t(r25.s32);
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// mullw r10,r10,r25
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r25.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stb r10,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r10.u8);
	// b 0x82664548
	goto loc_82664548;
loc_82664444:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82664544
	if (!cr6.gt) goto loc_82664544;
	// lwz r7,80(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// bge cr6,0x82664544
	if (!cr6.lt) goto loc_82664544;
	// rlwinm r9,r10,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lis r8,-32126
	ctx.r8.s64 = -2105409536;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// stw r9,-15412(r8)
	PPC_STORE_U32(ctx.r8.u32 + -15412, ctx.r9.u32);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// addi r8,r1,-268
	ctx.r8.s64 = ctx.r1.s64 + -268;
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// std r9,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r9.u64);
	// lbzx r9,r6,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// mullw r6,r9,r25
	ctx.r6.s64 = int64_t(ctx.r9.s32) * int64_t(r25.s32);
	// lfd f13,-176(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f12,f13
	ctx.f13.f64 = f0.f64 * ctx.f12.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r9,-268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// subfic r8,r9,256
	xer.ca = ctx.r9.u32 <= 256;
	ctx.r8.s64 = 256 - ctx.r9.s64;
	// subf r8,r25,r8
	ctx.r8.s64 = ctx.r8.s64 - r25.s64;
	// add r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r4,r8,r9
	ctx.r4.u64 = ctx.r8.u64 + ctx.r9.u64;
	// mullw r7,r5,r7
	ctx.r7.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r7.s32);
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 + ctx.r9.u64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// add r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r9,1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r9,r4,r9
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lbz r8,1(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// mullw r8,r8,r25
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r25.s32);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stb r9,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r9.u8);
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r9,r6,r9
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r9.s32);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lbz r8,2(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 2);
	// mullw r8,r8,r25
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r25.s32);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stb r9,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r9.u8);
	// lwz r8,80(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r9,3(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r9,r5,r9
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r9.s32);
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lbz r10,3(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// mullw r10,r10,r25
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r25.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stb r10,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r10.u8);
	// b 0x82664548
	goto loc_82664548;
loc_82664544:
	// stb r26,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r26.u8);
loc_82664548:
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// blt cr6,0x82664284
	if (cr6.lt) goto loc_82664284;
	// b 0x82664704
	goto loc_82664704;
loc_82664560:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826646dc
	if (!cr6.gt) goto loc_826646DC;
	// lwz r9,84(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x826646dc
	if (!cr6.lt) goto loc_826646DC;
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82664708
	if (!cr6.gt) goto loc_82664708;
loc_82664584:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r10,r1,-260
	ctx.r10.s64 = ctx.r1.s64 + -260;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r9,-260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt cr6,0x826646c0
	if (cr6.lt) goto loc_826646C0;
	// lwz r10,80(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bge cr6,0x82664674
	if (!cr6.lt) goto loc_82664674;
	// rlwinm r10,r9,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// lis r8,-32126
	ctx.r8.s64 = -2105409536;
	// stw r10,-15412(r8)
	PPC_STORE_U32(ctx.r8.u32 + -15412, ctx.r10.u32);
	// extsw r8,r10
	ctx.r8.s64 = ctx.r10.s32;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,-252
	ctx.r9.s64 = ctx.r1.s64 + -252;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// std r8,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r8.u64);
	// lbz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lfd f13,-160(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f12,f13
	ctx.f13.f64 = f0.f64 * ctx.f12.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r9,-252(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -252);
	// subfic r4,r9,256
	xer.ca = ctx.r9.u32 <= 256;
	ctx.r4.s64 = 256 - ctx.r9.s64;
	// mullw r6,r8,r9
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// subf r8,r25,r4
	ctx.r8.s64 = ctx.r4.s64 - r25.s64;
	// add r4,r8,r25
	ctx.r4.u64 = ctx.r8.u64 + r25.u64;
	// add r30,r8,r25
	r30.u64 = ctx.r8.u64 + r25.u64;
	// mullw r7,r4,r7
	ctx.r7.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r7.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// add r4,r8,r25
	ctx.r4.u64 = ctx.r8.u64 + r25.u64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// add r6,r8,r25
	ctx.r6.u64 = ctx.r8.u64 + r25.u64;
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// lbz r8,1(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r7,5(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// mullw r8,r30,r8
	ctx.r8.s64 = int64_t(r30.s32) * int64_t(ctx.r8.s32);
	// mullw r7,r7,r9
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// srawi r8,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 8;
	// stb r8,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r8.u8);
	// lbz r8,2(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r7,6(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// mullw r8,r6,r8
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r8.s32);
	// mullw r7,r7,r9
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// srawi r8,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 8;
	// stb r8,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r8.u8);
	// lbz r8,3(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r10,7(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// mullw r9,r10,r9
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// mullw r10,r4,r8
	ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stb r10,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r10.u8);
	// b 0x826646c4
	goto loc_826646C4;
loc_82664674:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x826646c0
	if (!cr6.gt) goto loc_826646C0;
	// lwz r10,80(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bge cr6,0x826646c0
	if (!cr6.lt) goto loc_826646C0;
	// rlwinm r10,r9,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// stw r10,-15412(r9)
	PPC_STORE_U32(ctx.r9.u32 + -15412, ctx.r10.u32);
	// add r10,r8,r31
	ctx.r10.u64 = ctx.r8.u64 + r31.u64;
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// lbz r9,1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// stb r9,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r9.u8);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// stb r9,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r9.u8);
	// lbz r10,3(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// stb r10,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r10.u8);
	// b 0x826646c4
	goto loc_826646C4;
loc_826646C0:
	// stb r26,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r26.u8);
loc_826646C4:
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r5,r10
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r10.s32, xer);
	// blt cr6,0x82664584
	if (cr6.lt) goto loc_82664584;
	// b 0x82664704
	goto loc_82664704;
loc_826646DC:
	// lwz r9,88(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82664708
	if (!cr6.gt) goto loc_82664708;
loc_826646EC:
	// stb r26,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r26.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,88(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x826646ec
	if (cr6.lt) goto loc_826646EC;
loc_82664704:
	// stw r11,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r11.u32);
loc_82664708:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// lwz r10,92(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 92);
	// cmpw cr6,r27,r10
	cr6.compare<int32_t>(r27.s32, ctx.r10.s32, xer);
	// stw r27,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, r27.u32);
	// blt cr6,0x82663964
	if (cr6.lt) goto loc_82663964;
loc_8266471C:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82664720"))) PPC_WEAK_FUNC(sub_82664720);
PPC_FUNC_IMPL(__imp__sub_82664720) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc0
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x82664750
	if (cr6.eq) goto loc_82664750;
	// lwz r7,112(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	// lwz r11,100(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 100);
	// addi r8,r7,1
	ctx.r8.s64 = ctx.r7.s64 + 1;
	// addi r9,r7,3
	ctx.r9.s64 = ctx.r7.s64 + 3;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// addi r27,r11,3
	r27.s64 = r11.s64 + 3;
	// b 0x8266476c
	goto loc_8266476C;
loc_82664750:
	// lwz r30,100(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 100);
	// lwz r8,112(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// addi r7,r8,1
	ctx.r7.s64 = ctx.r8.s64 + 1;
	// addi r9,r8,2
	ctx.r9.s64 = ctx.r8.s64 + 2;
	// addi r27,r30,2
	r27.s64 = r30.s64 + 2;
	// stw r11,100(r3)
	PPC_STORE_U32(ctx.r3.u32 + 100, r11.u32);
loc_8266476C:
	// lwz r11,92(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 92);
	// li r25,0
	r25.s64 = 0;
	// lis r5,-32248
	ctx.r5.s64 = -2113404928;
	// stw r27,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, r27.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r30,-232(r1)
	PPC_STORE_U32(ctx.r1.u32 + -232, r30.u32);
	// lis r6,-32249
	ctx.r6.s64 = -2113470464;
	// stw r9,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, ctx.r9.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stw r8,-324(r1)
	PPC_STORE_U32(ctx.r1.u32 + -324, ctx.r8.u32);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// stw r7,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, ctx.r7.u32);
	// lfd f8,-26736(r5)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r5.u32 + -26736);
	// stw r25,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r25.u32);
	// lfd f5,-31368(r6)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r6.u32 + -31368);
	// lfd f7,-28592(r10)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r10.u32 + -28592);
	// lfd f6,-31360(r11)
	ctx.f6.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// ble cr6,0x82665158
	if (!cr6.gt) goto loc_82665158;
	// fsub f11,f2,f1
	ctx.f11.f64 = ctx.f2.f64 - ctx.f1.f64;
	// li r26,16
	r26.s64 = 16;
loc_826647BC:
	// extsw r11,r25
	r11.s64 = r25.s32;
	// lwz r10,96(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 96);
	// fmr f0,f11
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f11.f64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// std r11,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, r11.u64);
	// lfd f13,-208(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmadd f12,f13,f3,f4
	ctx.f12.f64 = ctx.f13.f64 * ctx.f3.f64 + ctx.f4.f64;
	// beq cr6,0x826647ec
	if (cr6.eq) goto loc_826647EC;
	// fsub f13,f3,f6
	ctx.f13.f64 = ctx.f3.f64 - ctx.f6.f64;
	// fmul f13,f13,f7
	ctx.f13.f64 = ctx.f13.f64 * ctx.f7.f64;
	// b 0x826647f0
	goto loc_826647F0;
loc_826647EC:
	// fmr f13,f5
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f5.f64;
loc_826647F0:
	// fadd f13,f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f13.f64 + ctx.f12.f64;
	// addi r11,r1,-264
	r11.s64 = ctx.r1.s64 + -264;
	// lis r6,-32126
	ctx.r6.s64 = -2105409536;
	// lwz r31,80(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r4,r1,-268
	ctx.r4.s64 = ctx.r1.s64 + -268;
	// lwz r5,100(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 100);
	// fctiwz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f12,0,r11
	PPC_STORE_U32(r11.u32, ctx.f12.u32);
	// lwz r11,-264(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// rlwinm r10,r11,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,-15412(r6)
	PPC_STORE_U32(ctx.r6.u32 + -15412, ctx.r10.u32);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// mullw r6,r31,r11
	ctx.r6.s64 = int64_t(r31.s32) * int64_t(r11.s32);
	// std r10,-224(r1)
	PPC_STORE_U64(ctx.r1.u32 + -224, ctx.r10.u64);
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r10,r5
	ctx.r6.u64 = ctx.r10.u64 + ctx.r5.u64;
	// stw r6,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, ctx.r6.u32);
	// lfd f12,-224(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -224);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// fmsub f13,f13,f8,f12
	ctx.f13.f64 = ctx.f13.f64 * ctx.f8.f64 - ctx.f12.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f13.u32);
	// lwz r19,-268(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// mullw r10,r19,r19
	ctx.r10.s64 = int64_t(r19.s32) * int64_t(r19.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stw r10,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, ctx.r10.u32);
	// mullw r10,r10,r19
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r19.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stw r10,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r10.u32);
	// ble cr6,0x82664ebc
	if (!cr6.gt) goto loc_82664EBC;
	// lwz r10,84(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82664eb8
	if (!cr6.lt) goto loc_82664EB8;
	// li r28,0
	r28.s64 = 0;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r28,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, r28.u32);
	// ble cr6,0x82665144
	if (!cr6.gt) goto loc_82665144;
loc_82664890:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r11,r1,-248
	r11.s64 = ctx.r1.s64 + -248;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r10,-248(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -248);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82664d84
	if (!cr6.gt) goto loc_82664D84;
	// lwz r11,80(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r5,r11,-2
	ctx.r5.s64 = r11.s64 + -2;
	// cmpw cr6,r10,r5
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, xer);
	// bge cr6,0x82664d80
	if (!cr6.lt) goto loc_82664D80;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lis r8,-32126
	ctx.r8.s64 = -2105409536;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,-15412(r8)
	PPC_STORE_U32(ctx.r8.u32 + -15412, ctx.r10.u32);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r4,r1,-252
	ctx.r4.s64 = ctx.r1.s64 + -252;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r21,r5,r9
	r21.u64 = ctx.r5.u64 + ctx.r9.u64;
	// lbz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// std r10,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r10.u64);
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// lbz r10,-2(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + -2);
	// add r24,r6,r9
	r24.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rotlwi r3,r11,1
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lbzx r23,r8,r9
	r23.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r9.u32);
	// lbz r8,2(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lbz r31,-2(r21)
	r31.u64 = PPC_LOAD_U8(r21.u32 + -2);
	// lbz r30,2(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r22,2(r24)
	r22.u64 = PPC_LOAD_U8(r24.u32 + 2);
	// lbz r29,2(r21)
	r29.u64 = PPC_LOAD_U8(r21.u32 + 2);
	// add r18,r3,r30
	r18.u64 = ctx.r3.u64 + r30.u64;
	// lbz r6,-2(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + -2);
	// lbz r27,-2(r24)
	r27.u64 = PPC_LOAD_U8(r24.u32 + -2);
	// add r16,r29,r6
	r16.u64 = r29.u64 + ctx.r6.u64;
	// lbz r26,4(r7)
	r26.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lfd f13,-176(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f8,f13
	ctx.f13.f64 = f0.f64 * ctx.f8.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f13.u32);
	// lwz r28,-252(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -252);
	// lbz r4,0(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 0);
	// mullw r9,r28,r28
	ctx.r9.s64 = int64_t(r28.s32) * int64_t(r28.s32);
	// lbz r24,4(r24)
	r24.u64 = PPC_LOAD_U8(r24.u32 + 4);
	// srawi r25,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	r25.s64 = ctx.r9.s32 >> 8;
	// lbz r9,0(r7)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbz r7,0(r21)
	ctx.r7.u64 = PPC_LOAD_U8(r21.u32 + 0);
	// add r20,r10,r9
	r20.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r21,r25,r28
	r21.s64 = int64_t(r25.s32) * int64_t(r28.s32);
	// rlwinm r20,r20,1,0,30
	r20.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r21,r21,8
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0xFF) != 0);
	r21.s64 = r21.s32 >> 8;
	// subf r3,r20,r4
	ctx.r3.s64 = ctx.r4.s64 - r20.s64;
	// rlwinm r20,r18,1,0,30
	r20.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 + ctx.r5.u64;
	// subf r20,r22,r20
	r20.s64 = r20.s64 - r22.s64;
	// subf r17,r30,r9
	r17.s64 = ctx.r9.s64 - r30.s64;
	// subf r18,r23,r20
	r18.s64 = r20.s64 - r23.s64;
	// rlwinm r20,r16,1,0,30
	r20.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// add r16,r7,r8
	r16.u64 = ctx.r7.u64 + ctx.r8.u64;
	// subf r20,r27,r20
	r20.s64 = r20.s64 - r27.s64;
	// mulli r14,r16,13
	r14.s64 = r16.s64 * 13;
	// subf r20,r26,r20
	r20.s64 = r20.s64 - r26.s64;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r20,r20,1,0,30
	r20.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r17,r4,r17
	r17.s64 = r17.s64 - ctx.r4.s64;
	// add r16,r20,r24
	r16.u64 = r20.u64 + r24.u64;
	// rotlwi r20,r11,2
	r20.u64 = __builtin_rotateleft32(r11.u32, 2);
	// rlwinm r15,r16,1,0,30
	r15.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r16,r3,3,0,28
	r16.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r20,r11,r20
	r20.u64 = r11.u64 + r20.u64;
	// subf r3,r3,r16
	ctx.r3.s64 = r16.s64 - ctx.r3.s64;
	// rlwinm r16,r18,2,0,29
	r16.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r3,r15
	ctx.r3.u64 = ctx.r3.u64 + r15.u64;
	// add r18,r18,r16
	r18.u64 = r18.u64 + r16.u64;
	// subf r17,r6,r17
	r17.s64 = r17.s64 - ctx.r6.s64;
	// add r3,r3,r18
	ctx.r3.u64 = ctx.r3.u64 + r18.u64;
	// stw r20,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, r20.u32);
	// subf r20,r7,r29
	r20.s64 = r29.s64 - ctx.r7.s64;
	// subf r3,r14,r3
	ctx.r3.s64 = ctx.r3.s64 - r14.s64;
	// subf r18,r29,r7
	r18.s64 = ctx.r7.s64 - r29.s64;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// mullw r3,r3,r25
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r25.s32);
	// stw r3,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r3.u32);
	// subf r3,r31,r23
	ctx.r3.s64 = r23.s64 - r31.s64;
	// subf r16,r5,r18
	r16.s64 = r18.s64 - ctx.r5.s64;
	// add r17,r17,r22
	r17.u64 = r17.u64 + r22.u64;
	// add r16,r16,r10
	r16.u64 = r16.u64 + ctx.r10.u64;
	// add r17,r17,r26
	r17.u64 = r17.u64 + r26.u64;
	// subf r14,r11,r8
	r14.s64 = ctx.r8.s64 - r11.s64;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r18,r8,r29
	r18.s64 = r29.s64 - ctx.r8.s64;
	// stw r16,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r16.u32);
	// subf r16,r4,r8
	r16.s64 = ctx.r8.s64 - ctx.r4.s64;
	// subf r16,r29,r16
	r16.s64 = r16.s64 - r29.s64;
	// add r16,r16,r9
	r16.u64 = r16.u64 + ctx.r9.u64;
	// stw r16,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r16.u32);
	// subf r16,r11,r9
	r16.s64 = ctx.r9.s64 - r11.s64;
	// subf r16,r6,r16
	r16.s64 = r16.s64 - ctx.r6.s64;
	// rlwinm r15,r16,1,0,30
	r15.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r16,r24,r17
	r16.s64 = r17.s64 - r24.s64;
	// subf r17,r4,r15
	r17.s64 = r15.s64 - ctx.r4.s64;
	// subf r15,r31,r10
	r15.s64 = ctx.r10.s64 - r31.s64;
	// add r17,r17,r27
	r17.u64 = r17.u64 + r27.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// add r17,r17,r7
	r17.u64 = r17.u64 + ctx.r7.u64;
	// subf r15,r5,r15
	r15.s64 = r15.s64 - ctx.r5.s64;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// add r16,r16,r27
	r16.u64 = r16.u64 + r27.u64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r17,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, r17.u32);
	// subf r17,r6,r15
	r17.s64 = r15.s64 - ctx.r6.s64;
	// mulli r15,r14,11
	r15.s64 = r14.s64 * 11;
	// lwz r14,-300(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// stw r16,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, r16.u32);
	// stw r15,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, r15.u32);
	// add r17,r17,r27
	r17.u64 = r17.u64 + r27.u64;
	// subf r15,r11,r7
	r15.s64 = ctx.r7.s64 - r11.s64;
	// add r17,r17,r23
	r17.u64 = r17.u64 + r23.u64;
	// rotlwi r16,r7,1
	r16.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// add r16,r16,r9
	r16.u64 = r16.u64 + ctx.r9.u64;
	// subf r17,r24,r17
	r17.s64 = r17.s64 - r24.s64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r16,r14,r16
	r16.s64 = r16.s64 - r14.s64;
	// stw r17,-244(r1)
	PPC_STORE_U32(ctx.r1.u32 + -244, r17.u32);
	// mulli r17,r15,11
	r17.s64 = r15.s64 * 11;
	// stw r17,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, r17.u32);
	// lwz r17,-312(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// subf r14,r4,r16
	r14.s64 = r16.s64 - ctx.r4.s64;
	// rlwinm r16,r17,3,0,28
	r16.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r16,r17,r16
	r16.s64 = r16.s64 - r17.s64;
	// rlwinm r17,r3,2,0,29
	r17.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r17,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r17.u32);
	// rlwinm r17,r18,1,0,30
	r17.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// add r17,r18,r17
	r17.u64 = r18.u64 + r17.u64;
	// rotlwi r18,r31,2
	r18.u64 = __builtin_rotateleft32(r31.u32, 2);
	// add r18,r31,r18
	r18.u64 = r31.u64 + r18.u64;
	// lwz r15,-280(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// add r16,r15,r16
	r16.u64 = r15.u64 + r16.u64;
	// lwz r15,-312(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// add r3,r3,r15
	ctx.r3.u64 = ctx.r3.u64 + r15.u64;
	// lwz r15,-276(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// add r3,r16,r3
	ctx.r3.u64 = r16.u64 + ctx.r3.u64;
	// lwz r16,-284(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// add r17,r16,r17
	r17.u64 = r16.u64 + r17.u64;
	// rotlwi r16,r10,3
	r16.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r15,r3,r15
	r15.u64 = ctx.r3.u64 + r15.u64;
	// subf r3,r18,r17
	ctx.r3.s64 = r17.s64 - r18.s64;
	// subf r18,r10,r16
	r18.s64 = r16.s64 - ctx.r10.s64;
	// srawi r17,r15,1
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x1) != 0);
	r17.s64 = r15.s32 >> 1;
	// lwz r15,-292(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// add r18,r3,r18
	r18.u64 = ctx.r3.u64 + r18.u64;
	// mullw r3,r17,r21
	ctx.r3.s64 = int64_t(r17.s32) * int64_t(r21.s32);
	// lwz r17,-272(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// add r3,r17,r3
	ctx.r3.u64 = r17.u64 + ctx.r3.u64;
	// lwz r17,-244(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -244);
	// srawi r18,r18,1
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x1) != 0);
	r18.s64 = r18.s32 >> 1;
	// srawi r16,r14,1
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x1) != 0);
	r16.s64 = r14.s32 >> 1;
	// add r14,r17,r26
	r14.u64 = r17.u64 + r26.u64;
	// rlwinm r17,r15,3,0,28
	r17.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 3) & 0xFFFFFFF8;
	// mullw r18,r18,r28
	r18.s64 = int64_t(r18.s32) * int64_t(r28.s32);
	// stw r17,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r17.u32);
	// add r17,r3,r18
	r17.u64 = ctx.r3.u64 + r18.u64;
	// rlwinm r16,r16,8,0,23
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r3,r14,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r14,-292(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// add r17,r17,r16
	r17.u64 = r17.u64 + r16.u64;
	// subf r18,r15,r14
	r18.s64 = r14.s64 - r15.s64;
	// subf r15,r11,r10
	r15.s64 = ctx.r10.s64 - r11.s64;
	// add r3,r3,r18
	ctx.r3.u64 = ctx.r3.u64 + r18.u64;
	// lwz r18,-240(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -240);
	// subf r15,r6,r15
	r15.s64 = r15.s64 - ctx.r6.s64;
	// mullw r17,r17,r18
	r17.s64 = int64_t(r17.s32) * int64_t(r18.s32);
	// lwz r18,-260(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// add r16,r3,r18
	r16.u64 = ctx.r3.u64 + r18.u64;
	// subf r18,r30,r22
	r18.s64 = r22.s64 - r30.s64;
	// subf r22,r22,r4
	r22.s64 = ctx.r4.s64 - r22.s64;
	// subf r14,r9,r30
	r14.s64 = r30.s64 - ctx.r9.s64;
	// subf r23,r23,r22
	r23.s64 = r22.s64 - r23.s64;
	// subf r22,r10,r31
	r22.s64 = r31.s64 - ctx.r10.s64;
	// subf r23,r10,r23
	r23.s64 = r23.s64 - ctx.r10.s64;
	// subf r3,r8,r20
	ctx.r3.s64 = r20.s64 - ctx.r8.s64;
	// subf r23,r9,r23
	r23.s64 = r23.s64 - ctx.r9.s64;
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// add r31,r23,r31
	r31.u64 = r23.u64 + r31.u64;
	// rlwinm r23,r22,1,0,30
	r23.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r31,r5
	r31.u64 = r31.u64 + ctx.r5.u64;
	// rlwinm r22,r15,1,0,30
	r22.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r15,r14,1,0,30
	r15.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r14,r31,r30
	r14.u64 = r31.u64 + r30.u64;
	// subf r31,r5,r22
	r31.s64 = r22.s64 - ctx.r5.s64;
	// subf r23,r27,r23
	r23.s64 = r23.s64 - r27.s64;
	// subf r15,r29,r15
	r15.s64 = r15.s64 - r29.s64;
	// add r31,r31,r8
	r31.u64 = r31.u64 + ctx.r8.u64;
	// subf r29,r29,r23
	r29.s64 = r23.s64 - r29.s64;
	// subf r23,r8,r15
	r23.s64 = r15.s64 - ctx.r8.s64;
	// add r15,r31,r26
	r15.u64 = r31.u64 + r26.u64;
	// rlwinm r31,r20,1,0,30
	r31.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r23,r10,r23
	r23.s64 = r23.s64 - ctx.r10.s64;
	// add r20,r20,r31
	r20.u64 = r20.u64 + r31.u64;
	// subf r29,r7,r29
	r29.s64 = r29.s64 - ctx.r7.s64;
	// subf r31,r26,r23
	r31.s64 = r23.s64 - r26.s64;
	// rlwinm r23,r18,2,0,29
	r23.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r9,r29
	r29.s64 = r29.s64 - ctx.r9.s64;
	// add r23,r18,r23
	r23.u64 = r18.u64 + r23.u64;
	// add r29,r29,r4
	r29.u64 = r29.u64 + ctx.r4.u64;
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// add r23,r16,r23
	r23.u64 = r16.u64 + r23.u64;
	// add r29,r29,r8
	r29.u64 = r29.u64 + ctx.r8.u64;
	// add r18,r15,r20
	r18.u64 = r15.u64 + r20.u64;
	// add r31,r31,r5
	r31.u64 = r31.u64 + ctx.r5.u64;
	// srawi r20,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r20.s64 = r23.s32 >> 1;
	// add r23,r29,r11
	r23.u64 = r29.u64 + r11.u64;
	// add r29,r31,r11
	r29.u64 = r31.u64 + r11.u64;
	// mullw r31,r20,r25
	r31.s64 = int64_t(r20.s32) * int64_t(r25.s32);
	// rlwinm r20,r3,1,0,30
	r20.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r22,r14,1,0,30
	r22.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r20,r3,r20
	r20.u64 = ctx.r3.u64 + r20.u64;
	// rotlwi r3,r30,2
	ctx.r3.u64 = __builtin_rotateleft32(r30.u32, 2);
	// add r22,r22,r20
	r22.u64 = r22.u64 + r20.u64;
	// add r30,r30,r3
	r30.u64 = r30.u64 + ctx.r3.u64;
	// subf r3,r27,r22
	ctx.r3.s64 = r22.s64 - r27.s64;
	// subf r16,r10,r11
	r16.s64 = r11.s64 - ctx.r10.s64;
	// subf r3,r26,r3
	ctx.r3.s64 = ctx.r3.s64 - r26.s64;
	// rotlwi r26,r9,3
	r26.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r27,r3,r24
	r27.u64 = ctx.r3.u64 + r24.u64;
	// subf r3,r30,r18
	ctx.r3.s64 = r18.s64 - r30.s64;
	// subf r30,r9,r26
	r30.s64 = r26.s64 - ctx.r9.s64;
	// add r27,r27,r6
	r27.u64 = r27.u64 + ctx.r6.u64;
	// add r30,r3,r30
	r30.u64 = ctx.r3.u64 + r30.u64;
	// add r26,r23,r6
	r26.u64 = r23.u64 + ctx.r6.u64;
	// mullw r3,r27,r21
	ctx.r3.s64 = int64_t(r27.s32) * int64_t(r21.s32);
	// add r23,r29,r6
	r23.u64 = r29.u64 + ctx.r6.u64;
	// srawi r24,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r24.s64 = r30.s32 >> 1;
	// add r30,r31,r3
	r30.u64 = r31.u64 + ctx.r3.u64;
	// mullw r29,r26,r28
	r29.s64 = int64_t(r26.s32) * int64_t(r28.s32);
	// subf r27,r9,r16
	r27.s64 = r16.s64 - ctx.r9.s64;
	// mullw r31,r23,r21
	r31.s64 = int64_t(r23.s32) * int64_t(r21.s32);
	// mullw r3,r24,r25
	ctx.r3.s64 = int64_t(r24.s32) * int64_t(r25.s32);
	// add r6,r27,r6
	ctx.r6.u64 = r27.u64 + ctx.r6.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lwz r29,-288(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// mullw r31,r6,r28
	r31.s64 = int64_t(ctx.r6.s32) * int64_t(r28.s32);
	// mullw r6,r30,r29
	ctx.r6.s64 = int64_t(r30.s32) * int64_t(r29.s32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// rotlwi r30,r8,1
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// add r6,r17,r6
	ctx.r6.u64 = r17.u64 + ctx.r6.u64;
	// mullw r3,r3,r19
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r19.s32);
	// add r30,r30,r10
	r30.u64 = r30.u64 + ctx.r10.u64;
	// add r31,r6,r3
	r31.u64 = ctx.r6.u64 + ctx.r3.u64;
	// subf r6,r8,r11
	ctx.r6.s64 = r11.s64 - ctx.r8.s64;
	// subf r27,r10,r8
	r27.s64 = ctx.r8.s64 - ctx.r10.s64;
	// rlwinm r8,r30,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r7,r11
	ctx.r3.s64 = r11.s64 - ctx.r7.s64;
	// subf r30,r9,r7
	r30.s64 = ctx.r7.s64 - ctx.r9.s64;
	// lwz r7,-300(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// rotlwi r8,r11,8
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 8);
	// subf r11,r5,r7
	r11.s64 = ctx.r7.s64 - ctx.r5.s64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r7,r3,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// add r7,r3,r7
	ctx.r7.u64 = ctx.r3.u64 + ctx.r7.u64;
	// add r6,r10,r5
	ctx.r6.u64 = ctx.r10.u64 + ctx.r5.u64;
	// subf r10,r9,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r9.s64;
	// srawi r9,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r6.s32 >> 1;
	// add r7,r10,r4
	ctx.r7.u64 = ctx.r10.u64 + ctx.r4.u64;
	// mullw r11,r11,r25
	r11.s64 = int64_t(r11.s32) * int64_t(r25.s32);
	// mullw r10,r9,r21
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(r21.s32);
	// srawi r9,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mullw r10,r9,r29
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(r29.s32);
	// srawi r7,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r7.s64 = r27.s32 >> 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mullw r10,r7,r28
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(r28.s32);
	// srawi r9,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r9.s64 = r30.s32 >> 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mullw r10,r9,r19
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(r19.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82664d3c
	if (!cr6.gt) goto loc_82664D3C;
	// li r11,255
	r11.s64 = 255;
	// b 0x82664d48
	goto loc_82664D48;
loc_82664D3C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82664d48
	if (!cr6.lt) goto loc_82664D48;
	// li r11,0
	r11.s64 = 0;
loc_82664D48:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,-328(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// lwz r8,-324(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// li r26,16
	r26.s64 = 16;
	// lwz r30,-232(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -232);
	// addi r7,r11,2
	ctx.r7.s64 = r11.s64 + 2;
	// lwz r27,-256(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -256);
	// lwz r9,-320(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r28,-308(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -308);
	// lwz r25,-296(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// lwz r6,-304(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// b 0x82664e9c
	goto loc_82664E9C;
loc_82664D80:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
loc_82664D84:
	// beq cr6,0x82664e04
	if (cr6.eq) goto loc_82664E04;
	// lwz r4,80(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r11,r4,-1
	r11.s64 = ctx.r4.s64 + -1;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x82664e04
	if (cr6.lt) goto loc_82664E04;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82664dfc
	if (!cr6.gt) goto loc_82664DFC;
	// cmpw cr6,r10,r4
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r4.s32, xer);
	// bge cr6,0x82664dfc
	if (!cr6.lt) goto loc_82664DFC;
	// rlwinm r11,r10,8,0,23
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r5,r10,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r31,r1,-260
	r31.s64 = ctx.r1.s64 + -260;
	// stw r11,-15412(r10)
	PPC_STORE_U32(ctx.r10.u32 + -15412, r11.u32);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// add r10,r5,r6
	ctx.r10.u64 = ctx.r5.u64 + ctx.r6.u64;
	// rlwinm r5,r4,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r4,r19,256
	xer.ca = r19.u32 <= 256;
	ctx.r4.s64 = 256 - r19.s64;
	// std r11,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, r11.u64);
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbzx r10,r5,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r10.u32);
	// mullw r11,r4,r11
	r11.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// mullw r10,r10,r19
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r19.s32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lfd f13,-192(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f8,f13
	ctx.f13.f64 = f0.f64 * ctx.f8.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r31
	PPC_STORE_U32(r31.u32, ctx.f13.u32);
	// b 0x82664e90
	goto loc_82664E90;
loc_82664DFC:
	// stb r26,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r26.u8);
	// b 0x82664e98
	goto loc_82664E98;
loc_82664E04:
	// rlwinm r5,r10,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,80(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// stw r10,-15412(r11)
	PPC_STORE_U32(r11.u32 + -15412, ctx.r10.u32);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// add r11,r5,r6
	r11.u64 = ctx.r5.u64 + ctx.r6.u64;
	// rlwinm r5,r4,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r4,r1,-316
	ctx.r4.s64 = ctx.r1.s64 + -316;
	// std r10,-216(r1)
	PPC_STORE_U64(ctx.r1.u32 + -216, ctx.r10.u64);
	// add r10,r5,r11
	ctx.r10.u64 = ctx.r5.u64 + r11.u64;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lfd f13,-216(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -216);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f8,f13
	ctx.f13.f64 = f0.f64 * ctx.f8.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f13.u32);
	// lbz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lwz r31,-316(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// subf r24,r4,r10
	r24.s64 = ctx.r10.s64 - ctx.r4.s64;
	// mullw r29,r5,r31
	r29.s64 = int64_t(ctx.r5.s32) * int64_t(r31.s32);
	// subf r5,r5,r24
	ctx.r5.s64 = r24.s64 - ctx.r5.s64;
	// mullw r10,r4,r19
	ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(r19.s32);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// mullw r5,r5,r31
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r31.s32);
	// mullw r5,r5,r19
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r19.s32);
	// srawi r5,r5,8
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 8;
	// subfic r4,r31,256
	xer.ca = r31.u32 <= 256;
	ctx.r4.s64 = 256 - r31.s64;
	// subf r4,r19,r4
	ctx.r4.s64 = ctx.r4.s64 - r19.s64;
	// mullw r11,r4,r11
	r11.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_82664E90:
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r11.u8);
loc_82664E98:
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
loc_82664E9C:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// stw r7,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, ctx.r7.u32);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// stw r28,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, r28.u32);
	// blt cr6,0x82664890
	if (cr6.lt) goto loc_82664890;
	// b 0x82665144
	goto loc_82665144;
loc_82664EB8:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
loc_82664EBC:
	// blt cr6,0x8266502c
	if (cr6.lt) goto loc_8266502C;
	// lwz r10,84(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8266502c
	if (!cr6.lt) goto loc_8266502C;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82665144
	if (!cr6.gt) goto loc_82665144;
loc_82664EE0:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r11,r1,-332
	r11.s64 = ctx.r1.s64 + -332;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r10,-332(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82665010
	if (cr6.lt) goto loc_82665010;
	// lwz r5,80(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r11,r5,-1
	r11.s64 = ctx.r5.s64 + -1;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x82664fa0
	if (!cr6.lt) goto loc_82664FA0;
	// rlwinm r4,r5,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lis r5,-32126
	ctx.r5.s64 = -2105409536;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// addi r31,r1,-336
	r31.s64 = ctx.r1.s64 + -336;
	// stw r10,-15412(r5)
	PPC_STORE_U32(ctx.r5.u32 + -15412, ctx.r10.u32);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// std r10,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r10.u64);
	// add r10,r4,r11
	ctx.r10.u64 = ctx.r4.u64 + r11.u64;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// subf r24,r4,r10
	r24.s64 = ctx.r10.s64 - ctx.r4.s64;
	// mullw r10,r4,r19
	ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(r19.s32);
	// lfd f13,-200(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f8,f13
	ctx.f13.f64 = f0.f64 * ctx.f8.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r31
	PPC_STORE_U32(r31.u32, ctx.f13.u32);
	// lwz r31,-336(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// mullw r29,r5,r31
	r29.s64 = int64_t(ctx.r5.s32) * int64_t(r31.s32);
	// subf r5,r5,r24
	ctx.r5.s64 = r24.s64 - ctx.r5.s64;
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// mullw r5,r5,r31
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r31.s32);
	// mullw r5,r5,r19
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r19.s32);
	// srawi r5,r5,8
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 8;
	// subfic r4,r31,256
	xer.ca = r31.u32 <= 256;
	ctx.r4.s64 = 256 - r31.s64;
	// subf r4,r19,r4
	ctx.r4.s64 = ctx.r4.s64 - r19.s64;
	// mullw r11,r4,r11
	r11.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r11.u8);
	// b 0x82665014
	goto loc_82665014;
loc_82664FA0:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82665010
	if (!cr6.gt) goto loc_82665010;
	// lwz r4,80(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r10,r4
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r4.s32, xer);
	// bge cr6,0x82665010
	if (!cr6.lt) goto loc_82665010;
	// rlwinm r11,r10,8,0,23
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r5,r10,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r31,r1,-304
	r31.s64 = ctx.r1.s64 + -304;
	// stw r11,-15412(r10)
	PPC_STORE_U32(ctx.r10.u32 + -15412, r11.u32);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// add r10,r5,r6
	ctx.r10.u64 = ctx.r5.u64 + ctx.r6.u64;
	// rlwinm r5,r4,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r4,r19,256
	xer.ca = r19.u32 <= 256;
	ctx.r4.s64 = 256 - r19.s64;
	// std r11,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, r11.u64);
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbzx r10,r5,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r10.u32);
	// mullw r11,r4,r11
	r11.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// mullw r10,r10,r19
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r19.s32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r11.u8);
	// lfd f13,-184(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f8,f13
	ctx.f13.f64 = f0.f64 * ctx.f8.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r31
	PPC_STORE_U32(r31.u32, ctx.f13.u32);
	// b 0x82665014
	goto loc_82665014;
loc_82665010:
	// stb r26,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r26.u8);
loc_82665014:
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// blt cr6,0x82664ee0
	if (cr6.lt) goto loc_82664EE0;
	// b 0x82665140
	goto loc_82665140;
loc_8266502C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82665118
	if (!cr6.gt) goto loc_82665118;
	// lwz r10,84(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82665118
	if (!cr6.lt) goto loc_82665118;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// li r4,0
	ctx.r4.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82665144
	if (!cr6.gt) goto loc_82665144;
loc_82665050:
	// fadd f0,f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f0.f64 + ctx.f1.f64;
	// addi r11,r1,-236
	r11.s64 = ctx.r1.s64 + -236;
	// fctiwz f13,f0
	ctx.f13.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,-236(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x826650fc
	if (cr6.lt) goto loc_826650FC;
	// lwz r10,80(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826650d8
	if (!cr6.lt) goto loc_826650D8;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// stw r11,-15412(r10)
	PPC_STORE_U32(ctx.r10.u32 + -15412, r11.u32);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// add r10,r5,r6
	ctx.r10.u64 = ctx.r5.u64 + ctx.r6.u64;
	// addi r5,r1,-228
	ctx.r5.s64 = ctx.r1.s64 + -228;
	// std r11,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, r11.u64);
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r31,2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lfd f13,-168(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f8,f13
	ctx.f13.f64 = f0.f64 * ctx.f8.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f13.u32);
	// lwz r10,-228(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -228);
	// subfic r5,r10,256
	xer.ca = ctx.r10.u32 <= 256;
	ctx.r5.s64 = 256 - ctx.r10.s64;
	// mullw r10,r31,r10
	ctx.r10.s64 = int64_t(r31.s32) * int64_t(ctx.r10.s32);
	// mullw r11,r5,r11
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r11.u8);
	// b 0x82665100
	goto loc_82665100;
loc_826650D8:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826650fc
	if (!cr6.gt) goto loc_826650FC;
	// lwz r10,80(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826650fc
	if (!cr6.lt) goto loc_826650FC;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r11,r11,r6
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// stb r11,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r11.u8);
	// b 0x82665100
	goto loc_82665100;
loc_826650FC:
	// stb r26,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r26.u8);
loc_82665100:
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// blt cr6,0x82665050
	if (cr6.lt) goto loc_82665050;
	// b 0x82665140
	goto loc_82665140;
loc_82665118:
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82665144
	if (!cr6.gt) goto loc_82665144;
loc_82665128:
	// stb r26,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r26.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82665128
	if (cr6.lt) goto loc_82665128;
loc_82665140:
	// stw r7,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, ctx.r7.u32);
loc_82665144:
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// lwz r11,92(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 92);
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// stw r25,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r25.u32);
	// blt cr6,0x826647bc
	if (cr6.lt) goto loc_826647BC;
loc_82665158:
	// li r25,0
	r25.s64 = 0;
	// lwz r11,92(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r25,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r25.u32);
	// ble cr6,0x82666230
	if (!cr6.gt) goto loc_82666230;
	// lis r11,-32251
	r11.s64 = -2113601536;
	// li r26,128
	r26.s64 = 128;
	// lfd f0,264(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 264);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// fmul f10,f1,f0
	ctx.f10.f64 = ctx.f1.f64 * f0.f64;
	// lfd f11,-30992(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + -30992);
	// fsub f9,f2,f10
	ctx.f9.f64 = ctx.f2.f64 - ctx.f10.f64;
loc_82665188:
	// extsw r11,r25
	r11.s64 = r25.s32;
	// lwz r10,96(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 96);
	// fmr f0,f9
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f9.f64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// std r11,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, r11.u64);
	// lfd f13,-168(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmadd f12,f13,f3,f4
	ctx.f12.f64 = ctx.f13.f64 * ctx.f3.f64 + ctx.f4.f64;
	// beq cr6,0x826651b8
	if (cr6.eq) goto loc_826651B8;
	// fsub f13,f3,f6
	ctx.f13.f64 = ctx.f3.f64 - ctx.f6.f64;
	// fmul f13,f13,f7
	ctx.f13.f64 = ctx.f13.f64 * ctx.f7.f64;
	// b 0x826651bc
	goto loc_826651BC;
loc_826651B8:
	// fmr f13,f5
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f5.f64;
loc_826651BC:
	// fadd f13,f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f13.f64 + ctx.f12.f64;
	// addi r11,r1,-268
	r11.s64 = ctx.r1.s64 + -268;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r6,r1,-228
	ctx.r6.s64 = ctx.r1.s64 + -228;
	// fctiwz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f12,0,r11
	PPC_STORE_U32(r11.u32, ctx.f12.u32);
	// lwz r7,-268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// rlwinm r11,r7,8,0,23
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 8) & 0xFFFFFF00;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r11,-15412(r10)
	PPC_STORE_U32(ctx.r10.u32 + -15412, r11.u32);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, r11.u64);
	// lfd f12,-184(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// fmsub f13,f13,f8,f12
	ctx.f13.f64 = ctx.f13.f64 * ctx.f8.f64 - ctx.f12.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f13.u32);
	// lwz r24,-228(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -228);
	// mullw r11,r24,r24
	r11.s64 = int64_t(r24.s32) * int64_t(r24.s32);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stw r11,-252(r1)
	PPC_STORE_U32(ctx.r1.u32 + -252, r11.u32);
	// mullw r11,r11,r24
	r11.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stw r11,-248(r1)
	PPC_STORE_U32(ctx.r1.u32 + -248, r11.u32);
	// ble cr6,0x82665e30
	if (!cr6.gt) goto loc_82665E30;
	// lwz r11,84(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// bge cr6,0x82665e2c
	if (!cr6.lt) goto loc_82665E2C;
	// li r28,0
	r28.s64 = 0;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r28,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, r28.u32);
	// ble cr6,0x8266621c
	if (!cr6.gt) goto loc_8266621C;
loc_82665244:
	// fadd f0,f10,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f10.f64 + f0.f64;
	// addi r11,r1,-236
	r11.s64 = ctx.r1.s64 + -236;
	// fmul f13,f0,f7
	ctx.f13.f64 = f0.f64 * ctx.f7.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r10,-236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -236);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82665c14
	if (!cr6.gt) goto loc_82665C14;
	// lwz r11,80(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// srawi r6,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r6.s64 = r11.s32 >> 1;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// addi r6,r6,-2
	ctx.r6.s64 = ctx.r6.s64 + -2;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// bge cr6,0x82665c10
	if (!cr6.lt) goto loc_82665C10;
	// mullw r8,r11,r7
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r6,r1,-272
	ctx.r6.s64 = ctx.r1.s64 + -272;
	// addi r5,r11,-2
	ctx.r5.s64 = r11.s64 + -2;
	// addi r4,r11,4
	ctx.r4.s64 = r11.s64 + 4;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r29,r4,1,0,30
	r29.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,-15412(r9)
	PPC_STORE_U32(ctx.r9.u32 + -15412, ctx.r10.u32);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// add r9,r8,r7
	ctx.r9.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// std r10,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r10.u64);
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// rlwinm r26,r11,1,0,30
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r10,2,0,29
	r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r9,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r9.u32);
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// rlwinm r30,r10,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r4,r7,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r3,r8,r9
	ctx.r3.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbzx r28,r5,r9
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r9.u32);
	// rotlwi r5,r11,1
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lbzx r22,r29,r9
	r22.u64 = PPC_LOAD_U8(r29.u32 + ctx.r9.u32);
	// lbz r20,4(r3)
	r20.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
	// add r29,r5,r28
	r29.u64 = ctx.r5.u64 + r28.u64;
	// lbzx r27,r30,r9
	r27.u64 = PPC_LOAD_U8(r30.u32 + ctx.r9.u32);
	// lbzx r21,r25,r9
	r21.u64 = PPC_LOAD_U8(r25.u32 + ctx.r9.u32);
	// lbz r10,-4(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + -4);
	// lbz r31,8(r9)
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + 8);
	// lbz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// lbz r5,-4(r4)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r4.u32 + -4);
	// lbz r25,-4(r3)
	r25.u64 = PPC_LOAD_U8(ctx.r3.u32 + -4);
	// lbz r30,0(r3)
	r30.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r23,8(r4)
	r23.u64 = PPC_LOAD_U8(ctx.r4.u32 + 8);
	// lfd f13,-200(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f11,f13
	ctx.f13.f64 = f0.f64 * ctx.f11.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f13.u32);
	// lbzx r6,r26,r9
	ctx.r6.u64 = PPC_LOAD_U8(r26.u32 + ctx.r9.u32);
	// lbz r26,4(r4)
	r26.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// add r29,r29,r26
	r29.u64 = r29.u64 + r26.u64;
	// rlwinm r29,r29,1,0,30
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r29,r20,r29
	r29.s64 = r29.s64 - r20.s64;
	// subf r19,r22,r29
	r19.s64 = r29.s64 - r22.s64;
	// add r29,r27,r5
	r29.u64 = r27.u64 + ctx.r5.u64;
	// rlwinm r18,r29,1,0,30
	r18.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// rotlwi r29,r11,2
	r29.u64 = __builtin_rotateleft32(r11.u32, 2);
	// subf r18,r25,r18
	r18.s64 = r18.s64 - r25.s64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// stw r29,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r29.u32);
	// subf r29,r23,r18
	r29.s64 = r18.s64 - r23.s64;
	// rlwinm r29,r29,1,0,30
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// add r29,r29,r21
	r29.u64 = r29.u64 + r21.u64;
	// rlwinm r17,r29,1,0,30
	r17.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,-272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addze r7,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r7.s64 = temp.s64;
	// lbz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r4,r7,r7
	ctx.r4.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r7.s32);
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r4,r4,8
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 8;
	// subf r3,r3,r30
	ctx.r3.s64 = r30.s64 - ctx.r3.s64;
	// mullw r29,r4,r7
	r29.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r7.s32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// srawi r29,r29,8
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xFF) != 0);
	r29.s64 = r29.s32 >> 8;
	// rlwinm r18,r3,3,0,28
	r18.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r18,r3,r18
	r18.s64 = r18.s64 - ctx.r3.s64;
	// rlwinm r3,r19,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 2) & 0xFFFFFFFC;
	// add r18,r18,r17
	r18.u64 = r18.u64 + r17.u64;
	// add r3,r19,r3
	ctx.r3.u64 = r19.u64 + ctx.r3.u64;
	// subf r19,r27,r6
	r19.s64 = ctx.r6.s64 - r27.s64;
	// stw r18,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r18.u32);
	// stw r3,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, ctx.r3.u32);
	// subf r3,r28,r22
	ctx.r3.s64 = r22.s64 - r28.s64;
	// subf r18,r31,r19
	r18.s64 = r19.s64 - r31.s64;
	// subf r17,r11,r9
	r17.s64 = ctx.r9.s64 - r11.s64;
	// add r18,r18,r10
	r18.u64 = r18.u64 + ctx.r10.u64;
	// subf r17,r5,r17
	r17.s64 = r17.s64 - ctx.r5.s64;
	// subf r14,r11,r8
	r14.s64 = ctx.r8.s64 - r11.s64;
	// rlwinm r16,r17,1,0,30
	r16.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// add r15,r6,r8
	r15.u64 = ctx.r6.u64 + ctx.r8.u64;
	// stw r18,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r18.u32);
	// subf r18,r26,r9
	r18.s64 = ctx.r9.s64 - r26.s64;
	// mulli r15,r15,13
	r15.s64 = r15.s64 * 13;
	// rlwinm r18,r18,1,0,30
	r18.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r19,r8,r27
	r19.s64 = r27.s64 - ctx.r8.s64;
	// subf r18,r30,r18
	r18.s64 = r18.s64 - r30.s64;
	// subf r17,r5,r18
	r17.s64 = r18.s64 - ctx.r5.s64;
	// subf r18,r30,r16
	r18.s64 = r16.s64 - r30.s64;
	// add r17,r17,r20
	r17.u64 = r17.u64 + r20.u64;
	// add r18,r18,r25
	r18.u64 = r18.u64 + r25.u64;
	// add r17,r17,r23
	r17.u64 = r17.u64 + r23.u64;
	// add r16,r18,r6
	r16.u64 = r18.u64 + ctx.r6.u64;
	// rlwinm r18,r17,1,0,30
	r18.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r17,r28,r10
	r17.s64 = ctx.r10.s64 - r28.s64;
	// subf r18,r21,r18
	r18.s64 = r18.s64 - r21.s64;
	// add r18,r18,r25
	r18.u64 = r18.u64 + r25.u64;
	// rlwinm r18,r18,1,0,30
	r18.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r18,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r18.u32);
	// rlwinm r18,r16,1,0,30
	r18.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r16,r17,1,0,30
	r16.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r17,-332(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// subf r16,r31,r16
	r16.s64 = r16.s64 - r31.s64;
	// stw r18,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, r18.u32);
	// lwz r18,-336(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// add r18,r18,r17
	r18.u64 = r18.u64 + r17.u64;
	// rotlwi r17,r6,1
	r17.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// add r17,r17,r9
	r17.u64 = r17.u64 + ctx.r9.u64;
	// stw r18,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r18.u32);
	// mulli r18,r14,11
	r18.s64 = r14.s64 * 11;
	// lwz r14,-336(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// subf r15,r15,r14
	r15.s64 = r14.s64 - r15.s64;
	// rlwinm r14,r17,1,0,30
	r14.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r17,r5,r16
	r17.s64 = r16.s64 - ctx.r5.s64;
	// lwz r16,-312(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// srawi r15,r15,1
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x1) != 0);
	r15.s64 = r15.s32 >> 1;
	// subf r14,r30,r14
	r14.s64 = r14.s64 - r30.s64;
	// stw r17,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r17.u32);
	// rlwinm r17,r3,2,0,29
	r17.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r17,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r17.u32);
	// lwz r17,-312(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// rlwinm r17,r17,3,0,28
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r17,r16,r17
	r17.s64 = r17.s64 - r16.s64;
	// lwz r16,-336(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// add r16,r3,r16
	r16.u64 = ctx.r3.u64 + r16.u64;
	// lwz r3,-316(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// add r3,r3,r17
	ctx.r3.u64 = ctx.r3.u64 + r17.u64;
	// rlwinm r17,r19,1,0,30
	r17.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r3,r16
	ctx.r3.u64 = ctx.r3.u64 + r16.u64;
	// lwz r16,-304(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// add r17,r19,r17
	r17.u64 = r19.u64 + r17.u64;
	// rotlwi r19,r28,2
	r19.u64 = __builtin_rotateleft32(r28.u32, 2);
	// add r17,r16,r17
	r17.u64 = r16.u64 + r17.u64;
	// add r19,r28,r19
	r19.u64 = r28.u64 + r19.u64;
	// rotlwi r16,r10,3
	r16.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r18,r3,r18
	r18.u64 = ctx.r3.u64 + r18.u64;
	// subf r3,r19,r17
	ctx.r3.s64 = r17.s64 - r19.s64;
	// subf r19,r10,r16
	r19.s64 = r16.s64 - ctx.r10.s64;
	// lwz r16,-328(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// srawi r17,r18,1
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x1) != 0);
	r17.s64 = r18.s32 >> 1;
	// add r3,r3,r19
	ctx.r3.u64 = ctx.r3.u64 + r19.u64;
	// lwz r19,-332(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// add r18,r19,r25
	r18.u64 = r19.u64 + r25.u64;
	// mullw r19,r17,r29
	r19.s64 = int64_t(r17.s32) * int64_t(r29.s32);
	// stw r3,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, ctx.r3.u32);
	// mullw r3,r15,r4
	ctx.r3.s64 = int64_t(r15.s32) * int64_t(ctx.r4.s32);
	// lwz r15,-336(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// add r3,r3,r19
	ctx.r3.u64 = ctx.r3.u64 + r19.u64;
	// mullw r19,r15,r7
	r19.s64 = int64_t(r15.s32) * int64_t(ctx.r7.s32);
	// subf r17,r16,r14
	r17.s64 = r14.s64 - r16.s64;
	// add r18,r18,r22
	r18.u64 = r18.u64 + r22.u64;
	// add r19,r3,r19
	r19.u64 = ctx.r3.u64 + r19.u64;
	// srawi r17,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	r17.s64 = r17.s32 >> 1;
	// rlwinm r15,r18,1,0,30
	r15.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r18,r17,8,0,23
	r18.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 8) & 0xFFFFFF00;
	// subf r3,r21,r15
	ctx.r3.s64 = r15.s64 - r21.s64;
	// add r19,r19,r18
	r19.u64 = r19.u64 + r18.u64;
	// add r18,r3,r23
	r18.u64 = ctx.r3.u64 + r23.u64;
	// subf r17,r30,r8
	r17.s64 = ctx.r8.s64 - r30.s64;
	// rlwinm r15,r18,1,0,30
	r15.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r18,r26,r20
	r18.s64 = r20.s64 - r26.s64;
	// subf r20,r20,r30
	r20.s64 = r30.s64 - r20.s64;
	// subf r14,r10,r28
	r14.s64 = r28.s64 - ctx.r10.s64;
	// subf r22,r22,r20
	r22.s64 = r20.s64 - r22.s64;
	// subf r3,r27,r17
	ctx.r3.s64 = r17.s64 - r27.s64;
	// lwz r17,-252(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -252);
	// subf r22,r10,r22
	r22.s64 = r22.s64 - ctx.r10.s64;
	// mullw r19,r19,r17
	r19.s64 = int64_t(r19.s32) * int64_t(r17.s32);
	// stw r19,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r19.u32);
	// subf r22,r9,r22
	r22.s64 = r22.s64 - ctx.r9.s64;
	// add r19,r3,r9
	r19.u64 = ctx.r3.u64 + ctx.r9.u64;
	// add r28,r22,r28
	r28.u64 = r22.u64 + r28.u64;
	// rlwinm r22,r19,3,0,28
	r22.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 3) & 0xFFFFFFF8;
	// add r28,r28,r31
	r28.u64 = r28.u64 + r31.u64;
	// rlwinm r20,r14,1,0,30
	r20.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// subf r14,r11,r10
	r14.s64 = ctx.r10.s64 - r11.s64;
	// rlwinm r28,r28,1,0,30
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r20,r25,r20
	r20.s64 = r20.s64 - r25.s64;
	// subf r14,r5,r14
	r14.s64 = r14.s64 - ctx.r5.s64;
	// subf r20,r27,r20
	r20.s64 = r20.s64 - r27.s64;
	// rlwinm r14,r14,1,0,30
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r20,r6,r20
	r20.s64 = r20.s64 - ctx.r6.s64;
	// stw r28,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r28.u32);
	// subf r28,r19,r22
	r28.s64 = r22.s64 - r19.s64;
	// subf r19,r9,r26
	r19.s64 = r26.s64 - ctx.r9.s64;
	// subf r22,r11,r6
	r22.s64 = ctx.r6.s64 - r11.s64;
	// add r28,r15,r28
	r28.u64 = r15.u64 + r28.u64;
	// subf r20,r9,r20
	r20.s64 = r20.s64 - ctx.r9.s64;
	// subf r3,r6,r27
	ctx.r3.s64 = r27.s64 - ctx.r6.s64;
	// add r20,r20,r30
	r20.u64 = r20.u64 + r30.u64;
	// stw r19,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r19.u32);
	// mulli r19,r22,11
	r19.s64 = r22.s64 * 11;
	// subf r22,r31,r14
	r22.s64 = r14.s64 - r31.s64;
	// lwz r14,-336(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// add r28,r28,r19
	r28.u64 = r28.u64 + r19.u64;
	// rlwinm r15,r14,1,0,30
	r15.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r22,r22,r8
	r22.u64 = r22.u64 + ctx.r8.u64;
	// subf r19,r27,r15
	r19.s64 = r15.s64 - r27.s64;
	// add r27,r20,r8
	r27.u64 = r20.u64 + ctx.r8.u64;
	// add r22,r22,r23
	r22.u64 = r22.u64 + r23.u64;
	// subf r17,r8,r11
	r17.s64 = r11.s64 - ctx.r8.s64;
	// subf r20,r8,r19
	r20.s64 = r19.s64 - ctx.r8.s64;
	// rlwinm r15,r22,1,0,30
	r15.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// add r17,r17,r3
	r17.u64 = r17.u64 + ctx.r3.u64;
	// subf r22,r10,r20
	r22.s64 = r20.s64 - ctx.r10.s64;
	// rlwinm r20,r18,2,0,29
	r20.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r19,r17,1,0,30
	r19.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// add r20,r18,r20
	r20.u64 = r18.u64 + r20.u64;
	// lwz r18,-332(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// add r19,r17,r19
	r19.u64 = r17.u64 + r19.u64;
	// add r20,r28,r20
	r20.u64 = r28.u64 + r20.u64;
	// add r19,r18,r19
	r19.u64 = r18.u64 + r19.u64;
	// subf r22,r23,r22
	r22.s64 = r22.s64 - r23.s64;
	// subf r28,r25,r19
	r28.s64 = r19.s64 - r25.s64;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// subf r25,r23,r28
	r25.s64 = r28.s64 - r23.s64;
	// rotlwi r28,r26,2
	r28.u64 = __builtin_rotateleft32(r26.u32, 2);
	// add r25,r25,r21
	r25.u64 = r25.u64 + r21.u64;
	// srawi r23,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r23.s64 = r20.s32 >> 1;
	// add r25,r25,r5
	r25.u64 = r25.u64 + ctx.r5.u64;
	// add r28,r26,r28
	r28.u64 = r26.u64 + r28.u64;
	// mullw r26,r23,r4
	r26.s64 = int64_t(r23.s32) * int64_t(ctx.r4.s32);
	// mullw r25,r25,r29
	r25.s64 = int64_t(r25.s32) * int64_t(r29.s32);
	// add r14,r27,r5
	r14.u64 = r27.u64 + ctx.r5.u64;
	// rotlwi r21,r9,3
	r21.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// rlwinm r27,r3,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r26,r26,r25
	r26.u64 = r26.u64 + r25.u64;
	// subf r28,r28,r15
	r28.s64 = r15.s64 - r28.s64;
	// subf r23,r9,r21
	r23.s64 = r21.s64 - ctx.r9.s64;
	// mullw r25,r14,r7
	r25.s64 = int64_t(r14.s32) * int64_t(ctx.r7.s32);
	// add r27,r3,r27
	r27.u64 = ctx.r3.u64 + r27.u64;
	// add r28,r28,r23
	r28.u64 = r28.u64 + r23.u64;
	// add r3,r22,r6
	ctx.r3.u64 = r22.u64 + ctx.r6.u64;
	// add r26,r26,r25
	r26.u64 = r26.u64 + r25.u64;
	// lwz r23,-248(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -248);
	// add r27,r28,r27
	r27.u64 = r28.u64 + r27.u64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// mullw r28,r26,r23
	r28.s64 = int64_t(r26.s32) * int64_t(r23.s32);
	// srawi r26,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	r26.s64 = r27.s32 >> 1;
	// lwz r27,-316(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// subf r21,r10,r11
	r21.s64 = r11.s64 - ctx.r10.s64;
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// add r27,r27,r28
	r27.u64 = r27.u64 + r28.u64;
	// subf r28,r6,r11
	r28.s64 = r11.s64 - ctx.r6.s64;
	// subf r20,r9,r6
	r20.s64 = ctx.r6.s64 - ctx.r9.s64;
	// add r22,r3,r5
	r22.u64 = ctx.r3.u64 + ctx.r5.u64;
	// subf r6,r9,r21
	ctx.r6.s64 = r21.s64 - ctx.r9.s64;
	// subf r21,r10,r8
	r21.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r3,r8,r11
	ctx.r3.s64 = r11.s64 - ctx.r8.s64;
	// rotlwi r25,r8,1
	r25.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// add r5,r6,r5
	ctx.r5.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mullw r26,r26,r4
	r26.s64 = int64_t(r26.s32) * int64_t(ctx.r4.s32);
	// mullw r8,r22,r29
	ctx.r8.s64 = int64_t(r22.s32) * int64_t(r29.s32);
	// mullw r5,r5,r7
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r7.s32);
	// add r8,r26,r8
	ctx.r8.u64 = r26.u64 + ctx.r8.u64;
	// add r25,r25,r10
	r25.u64 = r25.u64 + ctx.r10.u64;
	// add r5,r8,r5
	ctx.r5.u64 = ctx.r8.u64 + ctx.r5.u64;
	// rotlwi r8,r11,8
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 8);
	// mullw r11,r5,r24
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r24.s32);
	// rlwinm r5,r3,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r25,1,0,30
	r25.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 + ctx.r5.u64;
	// rlwinm r5,r28,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r25,r31,r25
	r25.s64 = r25.s64 - r31.s64;
	// subf r10,r10,r3
	ctx.r10.s64 = ctx.r3.s64 - ctx.r10.s64;
	// add r5,r28,r5
	ctx.r5.u64 = r28.u64 + ctx.r5.u64;
	// subf r25,r16,r25
	r25.s64 = r25.s64 - r16.s64;
	// add r3,r10,r31
	ctx.r3.u64 = ctx.r10.u64 + r31.u64;
	// subf r10,r9,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r9.s64;
	// srawi r25,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	r25.s64 = r25.s32 >> 1;
	// srawi r9,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r3.s32 >> 1;
	// add r5,r10,r30
	ctx.r5.u64 = ctx.r10.u64 + r30.u64;
	// mullw r10,r9,r29
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(r29.s32);
	// mullw r6,r25,r4
	ctx.r6.s64 = int64_t(r25.s32) * int64_t(ctx.r4.s32);
	// srawi r9,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r5.s32 >> 1;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// mullw r9,r9,r23
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r23.s32);
	// srawi r5,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	ctx.r5.s64 = r20.s32 >> 1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r5,r24
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(r24.s32);
	// srawi r6,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	ctx.r6.s64 = r21.s32 >> 1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r6,r7
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r11,r27,r11
	r11.u64 = r27.u64 + r11.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82665734
	if (!cr6.gt) goto loc_82665734;
	// li r11,255
	r11.s64 = 255;
	// b 0x82665740
	goto loc_82665740;
loc_82665734:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82665740
	if (!cr6.lt) goto loc_82665740;
	// li r11,0
	r11.s64 = 0;
loc_82665740:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r9,-256(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -256);
	// lwz r11,-288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// add r8,r11,r9
	ctx.r8.u64 = r11.u64 + ctx.r9.u64;
	// lwz r11,-324(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lbz r10,-4(r8)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + -4);
	// lbz r30,8(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 8);
	// lbz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + 4);
	// stw r11,-324(r1)
	PPC_STORE_U32(ctx.r1.u32 + -324, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,80(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// lbz r11,0(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r3,r9,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r23,r5,r8
	r23.u64 = ctx.r5.u64 + ctx.r8.u64;
	// addi r5,r9,-2
	ctx.r5.s64 = ctx.r9.s64 + -2;
	// addi r31,r9,4
	r31.s64 = ctx.r9.s64 + 4;
	// subf r25,r3,r8
	r25.s64 = ctx.r8.s64 - ctx.r3.s64;
	// addi r3,r9,2
	ctx.r3.s64 = ctx.r9.s64 + 2;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r28,0(r23)
	r28.u64 = PPC_LOAD_U8(r23.u32 + 0);
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r20,4(r23)
	r20.u64 = PPC_LOAD_U8(r23.u32 + 4);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r23,-4(r23)
	r23.u64 = PPC_LOAD_U8(r23.u32 + -4);
	// rlwinm r18,r3,1,0,30
	r18.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r26,4(r25)
	r26.u64 = PPC_LOAD_U8(r25.u32 + 4);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r22,8(r25)
	r22.u64 = PPC_LOAD_U8(r25.u32 + 8);
	// lbzx r27,r5,r8
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r8.u32);
	// rotlwi r19,r11,1
	r19.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lbzx r21,r31,r8
	r21.u64 = PPC_LOAD_U8(r31.u32 + ctx.r8.u32);
	// lbzx r5,r9,r8
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r8.u32);
	// lbz r9,0(r25)
	ctx.r9.u64 = PPC_LOAD_U8(r25.u32 + 0);
	// lbz r31,-4(r25)
	r31.u64 = PPC_LOAD_U8(r25.u32 + -4);
	// lbzx r25,r18,r8
	r25.u64 = PPC_LOAD_U8(r18.u32 + ctx.r8.u32);
	// subf r17,r26,r9
	r17.s64 = ctx.r9.s64 - r26.s64;
	// lbzx r8,r3,r8
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + ctx.r8.u32);
	// add r3,r19,r27
	ctx.r3.u64 = r19.u64 + r27.u64;
	// add r19,r5,r6
	r19.u64 = ctx.r5.u64 + ctx.r6.u64;
	// add r3,r3,r26
	ctx.r3.u64 = ctx.r3.u64 + r26.u64;
	// mulli r15,r19,13
	r15.s64 = r19.s64 * 13;
	// rotlwi r19,r11,2
	r19.u64 = __builtin_rotateleft32(r11.u32, 2);
	// rlwinm r18,r3,1,0,30
	r18.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r19,r11,r19
	r19.u64 = r11.u64 + r19.u64;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r17,r28,r17
	r17.s64 = r17.s64 - r28.s64;
	// stw r19,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r19.u32);
	// add r19,r10,r9
	r19.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r19,r19,1,0,30
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r19,r28
	ctx.r3.s64 = r28.s64 - r19.s64;
	// subf r19,r20,r18
	r19.s64 = r18.s64 - r20.s64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r18,r21,r19
	r18.s64 = r19.s64 - r21.s64;
	// add r19,r25,r31
	r19.u64 = r25.u64 + r31.u64;
	// rlwinm r14,r3,3,0,28
	r14.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r19,r19,1,0,30
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r3,r14
	ctx.r3.s64 = r14.s64 - ctx.r3.s64;
	// subf r19,r23,r19
	r19.s64 = r19.s64 - r23.s64;
	// rlwinm r14,r18,2,0,29
	r14.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r16,r22,r19
	r16.s64 = r19.s64 - r22.s64;
	// subf r19,r31,r17
	r19.s64 = r17.s64 - r31.s64;
	// rlwinm r17,r16,1,0,30
	r17.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// add r19,r19,r20
	r19.u64 = r19.u64 + r20.u64;
	// add r17,r17,r8
	r17.u64 = r17.u64 + ctx.r8.u64;
	// stw r14,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r14.u32);
	// add r19,r19,r22
	r19.u64 = r19.u64 + r22.u64;
	// rlwinm r17,r17,1,0,30
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r19,r19,1,0,30
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r19,r8,r19
	r19.s64 = r19.s64 - ctx.r8.s64;
	// stw r17,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r17.u32);
	// subf r17,r25,r5
	r17.s64 = ctx.r5.s64 - r25.s64;
	// lwz r14,-336(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// add r16,r19,r23
	r16.u64 = r19.u64 + r23.u64;
	// subf r17,r30,r17
	r17.s64 = r17.s64 - r30.s64;
	// subf r19,r5,r25
	r19.s64 = r25.s64 - ctx.r5.s64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// add r17,r17,r10
	r17.u64 = r17.u64 + ctx.r10.u64;
	// add r3,r3,r14
	ctx.r3.u64 = ctx.r3.u64 + r14.u64;
	// lwz r14,-332(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// add r18,r18,r14
	r18.u64 = r18.u64 + r14.u64;
	// subf r14,r27,r10
	r14.s64 = ctx.r10.s64 - r27.s64;
	// add r3,r3,r18
	ctx.r3.u64 = ctx.r3.u64 + r18.u64;
	// rlwinm r14,r14,1,0,30
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r15,r3
	ctx.r3.s64 = ctx.r3.s64 - r15.s64;
	// subf r15,r28,r6
	r15.s64 = ctx.r6.s64 - r28.s64;
	// subf r14,r30,r14
	r14.s64 = r14.s64 - r30.s64;
	// subf r15,r25,r15
	r15.s64 = r15.s64 - r25.s64;
	// subf r14,r31,r14
	r14.s64 = r14.s64 - r31.s64;
	// add r15,r15,r9
	r15.u64 = r15.u64 + ctx.r9.u64;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// subf r18,r6,r25
	r18.s64 = r25.s64 - ctx.r6.s64;
	// mullw r3,r3,r4
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r4.s32);
	// stw r14,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r14.u32);
	// stw r15,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r15.u32);
	// stw r3,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, ctx.r3.u32);
	// subf r15,r11,r9
	r15.s64 = ctx.r9.s64 - r11.s64;
	// rlwinm r14,r17,3,0,28
	r14.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r15,r31,r15
	r15.s64 = r15.s64 - r31.s64;
	// subf r17,r17,r14
	r17.s64 = r14.s64 - r17.s64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// add r17,r16,r17
	r17.u64 = r16.u64 + r17.u64;
	// subf r15,r28,r15
	r15.s64 = r15.s64 - r28.s64;
	// subf r3,r27,r21
	ctx.r3.s64 = r21.s64 - r27.s64;
	// add r15,r15,r23
	r15.u64 = r15.u64 + r23.u64;
	// stw r15,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r15.u32);
	// lwz r14,-332(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// lwz r15,-336(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// add r14,r14,r5
	r14.u64 = r14.u64 + ctx.r5.u64;
	// stw r17,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r17.u32);
	// add r15,r15,r23
	r15.u64 = r15.u64 + r23.u64;
	// rlwinm r16,r14,1,0,30
	r16.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r15,r15,r21
	r15.u64 = r15.u64 + r21.u64;
	// subf r17,r11,r6
	r17.s64 = ctx.r6.s64 - r11.s64;
	// rlwinm r15,r15,1,0,30
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// mulli r17,r17,11
	r17.s64 = r17.s64 * 11;
	// stw r16,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r16.u32);
	// rotlwi r16,r5,1
	r16.u64 = __builtin_rotateleft32(ctx.r5.u32, 1);
	// add r14,r16,r9
	r14.u64 = r16.u64 + ctx.r9.u64;
	// subf r16,r8,r15
	r16.s64 = r15.s64 - ctx.r8.s64;
	// rlwinm r15,r14,1,0,30
	r15.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r16,r16,r22
	r16.u64 = r16.u64 + r22.u64;
	// subf r15,r28,r15
	r15.s64 = r15.s64 - r28.s64;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r14,r11,r5
	r14.s64 = ctx.r5.s64 - r11.s64;
	// stw r16,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, r16.u32);
	// lwz r16,-328(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// subf r16,r16,r15
	r16.s64 = r15.s64 - r16.s64;
	// mulli r15,r14,11
	r15.s64 = r14.s64 * 11;
	// stw r16,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r16.u32);
	// stw r15,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, r15.u32);
	// rlwinm r15,r3,2,0,29
	r15.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r16,r18,1,0,30
	r16.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r3,r15
	ctx.r3.u64 = ctx.r3.u64 + r15.u64;
	// lwz r15,-336(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// add r16,r18,r16
	r16.u64 = r18.u64 + r16.u64;
	// add r3,r15,r3
	ctx.r3.u64 = r15.u64 + ctx.r3.u64;
	// lwz r15,-332(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// rotlwi r18,r27,2
	r18.u64 = __builtin_rotateleft32(r27.u32, 2);
	// add r16,r15,r16
	r16.u64 = r15.u64 + r16.u64;
	// add r18,r27,r18
	r18.u64 = r27.u64 + r18.u64;
	// rotlwi r15,r10,3
	r15.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r14,r3,r17
	r14.u64 = ctx.r3.u64 + r17.u64;
	// lwz r17,-316(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// subf r3,r18,r16
	ctx.r3.s64 = r16.s64 - r18.s64;
	// subf r18,r10,r15
	r18.s64 = r15.s64 - ctx.r10.s64;
	// rlwinm r16,r17,3,0,28
	r16.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// add r18,r3,r18
	r18.u64 = ctx.r3.u64 + r18.u64;
	// srawi r15,r14,1
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x1) != 0);
	r15.s64 = r14.s32 >> 1;
	// subf r3,r17,r16
	ctx.r3.s64 = r16.s64 - r17.s64;
	// srawi r16,r18,1
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x1) != 0);
	r16.s64 = r18.s32 >> 1;
	// lwz r18,-304(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// mullw r17,r15,r29
	r17.s64 = int64_t(r15.s32) * int64_t(r29.s32);
	// add r18,r18,r3
	r18.u64 = r18.u64 + ctx.r3.u64;
	// subf r3,r26,r20
	ctx.r3.s64 = r20.s64 - r26.s64;
	// rlwinm r15,r3,2,0,29
	r15.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r15,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r15.u32);
	// subf r15,r20,r28
	r15.s64 = r28.s64 - r20.s64;
	// lwz r14,-288(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// srawi r14,r14,1
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x1) != 0);
	r14.s64 = r14.s32 >> 1;
	// lwz r20,-260(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// mullw r16,r16,r7
	r16.s64 = int64_t(r16.s32) * int64_t(ctx.r7.s32);
	// add r17,r20,r17
	r17.u64 = r20.u64 + r17.u64;
	// lwz r20,-240(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -240);
	// subf r15,r21,r15
	r15.s64 = r15.s64 - r21.s64;
	// add r20,r18,r20
	r20.u64 = r18.u64 + r20.u64;
	// lwz r18,-336(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// rlwinm r21,r14,8,0,23
	r21.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 8) & 0xFFFFFF00;
	// add r18,r3,r18
	r18.u64 = ctx.r3.u64 + r18.u64;
	// add r3,r17,r16
	ctx.r3.u64 = r17.u64 + r16.u64;
	// add r20,r20,r18
	r20.u64 = r20.u64 + r18.u64;
	// subf r18,r10,r15
	r18.s64 = r15.s64 - ctx.r10.s64;
	// add r21,r3,r21
	r21.u64 = ctx.r3.u64 + r21.u64;
	// srawi r17,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r17.s64 = r20.s32 >> 1;
	// subf r3,r9,r18
	ctx.r3.s64 = r18.s64 - ctx.r9.s64;
	// lwz r20,-252(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -252);
	// subf r16,r11,r10
	r16.s64 = ctx.r10.s64 - r11.s64;
	// mullw r18,r17,r4
	r18.s64 = int64_t(r17.s32) * int64_t(ctx.r4.s32);
	// add r17,r3,r27
	r17.u64 = ctx.r3.u64 + r27.u64;
	// subf r15,r10,r27
	r15.s64 = r27.s64 - ctx.r10.s64;
	// subf r16,r31,r16
	r16.s64 = r16.s64 - r31.s64;
	// subf r14,r9,r26
	r14.s64 = r26.s64 - ctx.r9.s64;
	// add r27,r17,r30
	r27.u64 = r17.u64 + r30.u64;
	// rlwinm r17,r15,1,0,30
	r17.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r15,r14,1,0,30
	r15.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r14,r27,r26
	r14.u64 = r27.u64 + r26.u64;
	// subf r17,r23,r17
	r17.s64 = r17.s64 - r23.s64;
	// subf r27,r30,r16
	r27.s64 = r16.s64 - r30.s64;
	// subf r16,r25,r15
	r16.s64 = r15.s64 - r25.s64;
	// subf r25,r25,r17
	r25.s64 = r17.s64 - r25.s64;
	// subf r17,r6,r16
	r17.s64 = r16.s64 - ctx.r6.s64;
	// add r27,r27,r6
	r27.u64 = r27.u64 + ctx.r6.u64;
	// subf r25,r5,r25
	r25.s64 = r25.s64 - ctx.r5.s64;
	// subf r17,r10,r17
	r17.s64 = r17.s64 - ctx.r10.s64;
	// add r16,r27,r22
	r16.u64 = r27.u64 + r22.u64;
	// subf r27,r9,r25
	r27.s64 = r25.s64 - ctx.r9.s64;
	// subf r25,r22,r17
	r25.s64 = r17.s64 - r22.s64;
	// add r17,r27,r28
	r17.u64 = r27.u64 + r28.u64;
	// rlwinm r15,r14,1,0,30
	r15.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// add r25,r25,r5
	r25.u64 = r25.u64 + ctx.r5.u64;
	// rlwinm r27,r19,1,0,30
	r27.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r6,r11
	ctx.r3.s64 = r11.s64 - ctx.r6.s64;
	// add r17,r17,r6
	r17.u64 = r17.u64 + ctx.r6.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r3,r3,r19
	ctx.r3.u64 = ctx.r3.u64 + r19.u64;
	// stw r15,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r15.u32);
	// add r27,r19,r27
	r27.u64 = r19.u64 + r27.u64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// add r19,r25,r11
	r19.u64 = r25.u64 + r11.u64;
	// subf r15,r10,r11
	r15.s64 = r11.s64 - ctx.r10.s64;
	// rotlwi r25,r6,1
	r25.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// add r17,r17,r31
	r17.u64 = r17.u64 + r31.u64;
	// add r14,r19,r31
	r14.u64 = r19.u64 + r31.u64;
	// subf r19,r9,r15
	r19.s64 = r15.s64 - ctx.r9.s64;
	// add r15,r25,r10
	r15.u64 = r25.u64 + ctx.r10.u64;
	// mullw r25,r17,r7
	r25.s64 = int64_t(r17.s32) * int64_t(ctx.r7.s32);
	// rlwinm r17,r3,1,0,30
	r17.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r16,r16,1,0,30
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r3,r17
	ctx.r3.u64 = ctx.r3.u64 + r17.u64;
	// rotlwi r17,r26,2
	r17.u64 = __builtin_rotateleft32(r26.u32, 2);
	// mullw r20,r21,r20
	r20.s64 = int64_t(r21.s32) * int64_t(r20.s32);
	// stw r17,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r17.u32);
	// lwz r17,-336(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// add r3,r17,r3
	ctx.r3.u64 = r17.u64 + ctx.r3.u64;
	// lwz r17,-332(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// subf r21,r6,r11
	r21.s64 = r11.s64 - ctx.r6.s64;
	// subf r3,r23,r3
	ctx.r3.s64 = ctx.r3.s64 - r23.s64;
	// add r26,r26,r17
	r26.u64 = r26.u64 + r17.u64;
	// rotlwi r23,r9,3
	r23.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - r22.s64;
	// subf r26,r26,r16
	r26.s64 = r16.s64 - r26.s64;
	// subf r23,r9,r23
	r23.s64 = r23.s64 - ctx.r9.s64;
	// add r3,r3,r8
	ctx.r3.u64 = ctx.r3.u64 + ctx.r8.u64;
	// add r8,r26,r23
	ctx.r8.u64 = r26.u64 + r23.u64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// add r27,r8,r27
	r27.u64 = ctx.r8.u64 + r27.u64;
	// mullw r8,r3,r29
	ctx.r8.s64 = int64_t(ctx.r3.s32) * int64_t(r29.s32);
	// srawi r26,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	r26.s64 = r27.s32 >> 1;
	// add r27,r18,r8
	r27.u64 = r18.u64 + ctx.r8.u64;
	// mullw r3,r14,r29
	ctx.r3.s64 = int64_t(r14.s32) * int64_t(r29.s32);
	// add r31,r19,r31
	r31.u64 = r19.u64 + r31.u64;
	// mullw r8,r26,r4
	ctx.r8.s64 = int64_t(r26.s32) * int64_t(ctx.r4.s32);
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// mullw r31,r31,r7
	r31.s64 = int64_t(r31.s32) * int64_t(ctx.r7.s32);
	// rlwinm r26,r15,1,0,30
	r26.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lwz r31,-328(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// subf r26,r30,r26
	r26.s64 = r26.s64 - r30.s64;
	// add r25,r27,r25
	r25.u64 = r27.u64 + r25.u64;
	// subf r31,r31,r26
	r31.s64 = r26.s64 - r31.s64;
	// lwz r27,-248(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -248);
	// mullw r8,r25,r27
	ctx.r8.s64 = int64_t(r25.s32) * int64_t(r27.s32);
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// add r8,r20,r8
	ctx.r8.u64 = r20.u64 + ctx.r8.u64;
	// mullw r4,r31,r4
	ctx.r4.s64 = int64_t(r31.s32) * int64_t(ctx.r4.s32);
	// rlwinm r31,r21,1,0,30
	r31.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r3,r3,r24
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r24.s32);
	// add r31,r21,r31
	r31.u64 = r21.u64 + r31.u64;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// subf r8,r5,r11
	ctx.r8.s64 = r11.s64 - ctx.r5.s64;
	// subf r26,r9,r5
	r26.s64 = ctx.r5.s64 - ctx.r9.s64;
	// subf r5,r10,r31
	ctx.r5.s64 = r31.s64 - ctx.r10.s64;
	// subf r31,r10,r6
	r31.s64 = ctx.r6.s64 - ctx.r10.s64;
	// add r6,r5,r30
	ctx.r6.u64 = ctx.r5.u64 + r30.u64;
	// rotlwi r10,r11,8
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 8);
	// srawi r11,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r11.s64 = ctx.r6.s32 >> 1;
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// srawi r8,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	ctx.r8.s64 = r26.s32 >> 1;
	// mullw r9,r9,r27
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r27.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// mullw r9,r8,r24
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(r24.s32);
	// srawi r6,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	ctx.r6.s64 = r31.s32 >> 1;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// mullw r9,r6,r7
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82665bd0
	if (!cr6.gt) goto loc_82665BD0;
	// li r11,255
	r11.s64 = 255;
	// b 0x82665bdc
	goto loc_82665BDC;
loc_82665BD0:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82665bdc
	if (!cr6.lt) goto loc_82665BDC;
	// li r11,0
	r11.s64 = 0;
loc_82665BDC:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,-320(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// lwz r28,-264(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// li r26,128
	r26.s64 = 128;
	// lwz r7,-268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// lwz r25,-296(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// lwz r30,-232(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -232);
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r27,-256(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -256);
	// lwz r8,-324(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// b 0x82665e10
	goto loc_82665E10;
loc_82665C10:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
loc_82665C14:
	// blt cr6,0x82665d38
	if (cr6.lt) goto loc_82665D38;
	// lwz r11,80(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// srawi r6,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r6.s64 = r11.s32 >> 1;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// bge cr6,0x82665d38
	if (!cr6.lt) goto loc_82665D38;
	// rlwinm r6,r10,8,0,23
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lis r5,-32126
	ctx.r5.s64 = -2105409536;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r11,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r6,-15412(r5)
	PPC_STORE_U32(ctx.r5.u32 + -15412, ctx.r6.u32);
	// extsw r5,r6
	ctx.r5.s64 = ctx.r6.s32;
	// mullw r6,r11,r7
	ctx.r6.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// std r5,-216(r1)
	PPC_STORE_U64(ctx.r1.u32 + -216, ctx.r5.u64);
	// addi r5,r1,-276
	ctx.r5.s64 = ctx.r1.s64 + -276;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r11,1,0,30
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r30
	r11.u64 = ctx.r10.u64 + r30.u64;
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// lbzx r6,r4,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// lbzx r29,r31,r11
	r29.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// subf r23,r6,r29
	r23.s64 = r29.s64 - ctx.r6.s64;
	// mullw r29,r6,r24
	r29.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// lfd f13,-216(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -216);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f11,f13
	ctx.f13.f64 = f0.f64 * ctx.f11.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f13.u32);
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// subf r6,r5,r23
	ctx.r6.s64 = r23.s64 - ctx.r5.s64;
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// lwz r11,-276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// mullw r6,r6,r11
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// subfic r31,r11,256
	xer.ca = r11.u32 <= 256;
	r31.s64 = 256 - r11.s64;
	// mullw r6,r6,r24
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// subf r31,r24,r31
	r31.s64 = r31.s64 - r24.s64;
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// mullw r4,r31,r4
	ctx.r4.s64 = int64_t(r31.s32) * int64_t(ctx.r4.s32);
	// mullw r5,r5,r11
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r6,r6,r29
	ctx.r6.u64 = ctx.r6.u64 + r29.u64;
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// stb r6,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r6.u8);
	// lwz r6,80(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// rlwinm r23,r6,1,0,30
	r23.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r22,r6,2
	r22.s64 = ctx.r6.s64 + 2;
	// mullw r29,r31,r4
	r29.s64 = int64_t(r31.s32) * int64_t(ctx.r4.s32);
	// lbzx r6,r23,r10
	ctx.r6.u64 = PPC_LOAD_U8(r23.u32 + ctx.r10.u32);
	// rlwinm r23,r22,1,0,30
	r23.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r31,r5,r11
	r31.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// lbzx r10,r23,r10
	ctx.r10.u64 = PPC_LOAD_U8(r23.u32 + ctx.r10.u32);
	// subf r23,r6,r10
	r23.s64 = ctx.r10.s64 - ctx.r6.s64;
	// mullw r10,r6,r24
	ctx.r10.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// subf r6,r5,r23
	ctx.r6.s64 = r23.s64 - ctx.r5.s64;
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// mullw r11,r6,r11
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// mullw r11,r11,r24
	r11.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// b 0x82665e04
	goto loc_82665E04;
loc_82665D38:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82665dfc
	if (!cr6.gt) goto loc_82665DFC;
	// lwz r6,80(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// srawi r11,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r11.s64 = ctx.r6.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x82665dfc
	if (!cr6.lt) goto loc_82665DFC;
	// rlwinm r11,r10,8,0,23
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// lis r5,-32126
	ctx.r5.s64 = -2105409536;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-15412(r5)
	PPC_STORE_U32(ctx.r5.u32 + -15412, r11.u32);
	// extsw r5,r11
	ctx.r5.s64 = r11.s32;
	// mullw r11,r6,r7
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// std r5,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r5.u64);
	// addi r5,r1,-284
	ctx.r5.s64 = ctx.r1.s64 + -284;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// lbzx r6,r6,r10
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// mullw r4,r6,r24
	ctx.r4.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// lfd f13,-192(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f11,f13
	ctx.f13.f64 = f0.f64 * ctx.f11.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f13.u32);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r10,r11,r27
	ctx.r10.u64 = r11.u64 + r27.u64;
	// lwz r11,-284(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// subfic r6,r11,256
	xer.ca = r11.u32 <= 256;
	ctx.r6.s64 = 256 - r11.s64;
	// subf r6,r24,r6
	ctx.r6.s64 = ctx.r6.s64 - r24.s64;
	// add r31,r6,r11
	r31.u64 = ctx.r6.u64 + r11.u64;
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// mullw r11,r31,r5
	r11.s64 = int64_t(r31.s32) * int64_t(ctx.r5.s32);
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r11.u8);
	// lwz r5,80(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r11,r6,r11
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// lbzx r10,r5,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r10.u32);
	// mullw r10,r10,r24
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r24.s32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// b 0x82665e04
	goto loc_82665E04;
loc_82665DFC:
	// stb r26,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r26.u8);
	// stb r26,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r26.u8);
loc_82665E04:
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stw r8,-324(r1)
	PPC_STORE_U32(ctx.r1.u32 + -324, ctx.r8.u32);
loc_82665E10:
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// stw r9,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, ctx.r9.u32);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// stw r28,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, r28.u32);
	// blt cr6,0x82665244
	if (cr6.lt) goto loc_82665244;
	// b 0x8266621c
	goto loc_8266621C;
loc_82665E2C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
loc_82665E30:
	// blt cr6,0x8266607c
	if (cr6.lt) goto loc_8266607C;
	// lwz r11,84(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// bge cr6,0x8266607c
	if (!cr6.lt) goto loc_8266607C;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8266621c
	if (!cr6.gt) goto loc_8266621C;
loc_82665E54:
	// fadd f0,f10,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f10.f64 + f0.f64;
	// addi r11,r1,-292
	r11.s64 = ctx.r1.s64 + -292;
	// fmul f13,f0,f7
	ctx.f13.f64 = f0.f64 * ctx.f7.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,-292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x82666058
	if (cr6.lt) goto loc_82666058;
	// lwz r10,80(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// srawi r6,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 1;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// bge cr6,0x82665f94
	if (!cr6.lt) goto loc_82665F94;
	// rlwinm r6,r11,8,0,23
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// lis r5,-32126
	ctx.r5.s64 = -2105409536;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r10,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r6,-15412(r5)
	PPC_STORE_U32(ctx.r5.u32 + -15412, ctx.r6.u32);
	// extsw r5,r6
	ctx.r5.s64 = ctx.r6.s32;
	// mullw r6,r10,r7
	ctx.r6.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// std r5,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r5.u64);
	// addi r5,r1,-280
	ctx.r5.s64 = ctx.r1.s64 + -280;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// addi r6,r10,2
	ctx.r6.s64 = ctx.r10.s64 + 2;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r6,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r30
	r11.u64 = ctx.r10.u64 + r30.u64;
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// lbzx r6,r4,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// lbzx r29,r31,r11
	r29.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// subf r23,r6,r29
	r23.s64 = r29.s64 - ctx.r6.s64;
	// mullw r29,r6,r24
	r29.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// lfd f13,-176(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f11,f13
	ctx.f13.f64 = f0.f64 * ctx.f11.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f13.u32);
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// subf r6,r5,r23
	ctx.r6.s64 = r23.s64 - ctx.r5.s64;
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// lwz r11,-280(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// mullw r6,r6,r11
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// subfic r31,r11,256
	xer.ca = r11.u32 <= 256;
	r31.s64 = 256 - r11.s64;
	// mullw r6,r6,r24
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// subf r31,r24,r31
	r31.s64 = r31.s64 - r24.s64;
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// mullw r4,r31,r4
	ctx.r4.s64 = int64_t(r31.s32) * int64_t(ctx.r4.s32);
	// mullw r5,r5,r11
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r6,r6,r29
	ctx.r6.u64 = ctx.r6.u64 + r29.u64;
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// stb r6,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r6.u8);
	// lwz r6,80(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// rlwinm r23,r6,1,0,30
	r23.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r22,r6,2
	r22.s64 = ctx.r6.s64 + 2;
	// mullw r29,r31,r4
	r29.s64 = int64_t(r31.s32) * int64_t(ctx.r4.s32);
	// lbzx r6,r23,r10
	ctx.r6.u64 = PPC_LOAD_U8(r23.u32 + ctx.r10.u32);
	// rlwinm r23,r22,1,0,30
	r23.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r31,r5,r11
	r31.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// lbzx r10,r23,r10
	ctx.r10.u64 = PPC_LOAD_U8(r23.u32 + ctx.r10.u32);
	// subf r23,r6,r10
	r23.s64 = ctx.r10.s64 - ctx.r6.s64;
	// mullw r10,r6,r24
	ctx.r10.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// subf r6,r5,r23
	ctx.r6.s64 = r23.s64 - ctx.r5.s64;
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// mullw r11,r6,r11
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// mullw r11,r11,r24
	r11.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// b 0x82666060
	goto loc_82666060;
loc_82665F94:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82666058
	if (!cr6.gt) goto loc_82666058;
	// lwz r6,80(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// srawi r10,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 1;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82666058
	if (!cr6.lt) goto loc_82666058;
	// rlwinm r10,r11,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// lis r5,-32126
	ctx.r5.s64 = -2105409536;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,-15412(r5)
	PPC_STORE_U32(ctx.r5.u32 + -15412, ctx.r10.u32);
	// extsw r5,r10
	ctx.r5.s64 = ctx.r10.s32;
	// mullw r10,r6,r7
	ctx.r10.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// std r5,-224(r1)
	PPC_STORE_U64(ctx.r1.u32 + -224, ctx.r5.u64);
	// addi r5,r1,-308
	ctx.r5.s64 = ctx.r1.s64 + -308;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// lbzx r6,r6,r10
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// mullw r4,r6,r24
	ctx.r4.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// lfd f13,-224(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -224);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f11,f13
	ctx.f13.f64 = f0.f64 * ctx.f11.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f13.u32);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r10,r11,r27
	ctx.r10.u64 = r11.u64 + r27.u64;
	// lwz r11,-308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -308);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// subfic r6,r11,256
	xer.ca = r11.u32 <= 256;
	ctx.r6.s64 = 256 - r11.s64;
	// subf r6,r24,r6
	ctx.r6.s64 = ctx.r6.s64 - r24.s64;
	// add r31,r6,r11
	r31.u64 = ctx.r6.u64 + r11.u64;
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// mullw r11,r31,r5
	r11.s64 = int64_t(r31.s32) * int64_t(ctx.r5.s32);
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r11.u8);
	// lwz r5,80(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r11,r6,r11
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// lbzx r10,r5,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r10.u32);
	// mullw r10,r10,r24
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r24.s32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// b 0x82666060
	goto loc_82666060;
loc_82666058:
	// stb r26,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r26.u8);
	// stb r26,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r26.u8);
loc_82666060:
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// blt cr6,0x82665e54
	if (cr6.lt) goto loc_82665E54;
	// b 0x82666214
	goto loc_82666214;
loc_8266607C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x826661e4
	if (!cr6.gt) goto loc_826661E4;
	// lwz r11,84(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// bge cr6,0x826661e4
	if (!cr6.lt) goto loc_826661E4;
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8266621c
	if (!cr6.gt) goto loc_8266621C;
loc_826660A0:
	// fadd f0,f10,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f10.f64 + f0.f64;
	// addi r11,r1,-244
	r11.s64 = ctx.r1.s64 + -244;
	// fmul f13,f0,f7
	ctx.f13.f64 = f0.f64 * ctx.f7.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,-244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -244);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x826661c0
	if (cr6.lt) goto loc_826661C0;
	// lwz r5,80(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// srawi r10,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r5.s32 >> 1;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82666174
	if (!cr6.lt) goto loc_82666174;
	// rlwinm r10,r11,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// lis r6,-32126
	ctx.r6.s64 = -2105409536;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,-15412(r6)
	PPC_STORE_U32(ctx.r6.u32 + -15412, ctx.r10.u32);
	// extsw r6,r10
	ctx.r6.s64 = ctx.r10.s32;
	// mullw r10,r5,r7
	ctx.r10.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r7.s32);
	// std r6,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r6.u64);
	// addi r6,r1,-300
	ctx.r6.s64 = ctx.r1.s64 + -300;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r4,4(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// add r10,r11,r27
	ctx.r10.u64 = r11.u64 + r27.u64;
	// lfd f13,-208(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmsub f13,f0,f11,f13
	ctx.f13.f64 = f0.f64 * ctx.f11.f64 - ctx.f13.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f13.u32);
	// lwz r11,-300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// subfic r6,r11,256
	xer.ca = r11.u32 <= 256;
	ctx.r6.s64 = 256 - r11.s64;
	// mullw r4,r4,r11
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// subf r6,r24,r6
	ctx.r6.s64 = ctx.r6.s64 - r24.s64;
	// add r29,r6,r24
	r29.u64 = ctx.r6.u64 + r24.u64;
	// add r28,r6,r24
	r28.u64 = ctx.r6.u64 + r24.u64;
	// mullw r6,r29,r5
	ctx.r6.s64 = int64_t(r29.s32) * int64_t(ctx.r5.s32);
	// add r6,r4,r6
	ctx.r6.u64 = ctx.r4.u64 + ctx.r6.u64;
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// stb r6,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r6.u8);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// mullw r10,r28,r6
	ctx.r10.s64 = int64_t(r28.s32) * int64_t(ctx.r6.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// b 0x826661c8
	goto loc_826661C8;
loc_82666174:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826661c0
	if (!cr6.gt) goto loc_826661C0;
	// lwz r10,80(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// srawi r6,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 1;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// bge cr6,0x826661c0
	if (!cr6.lt) goto loc_826661C0;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r6,r10,r7
	ctx.r6.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// stw r11,-15412(r10)
	PPC_STORE_U32(ctx.r10.u32 + -15412, r11.u32);
	// add r11,r6,r5
	r11.u64 = ctx.r6.u64 + ctx.r5.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r10,r11,r30
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r30.u32);
	// stb r10,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r10.u8);
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// b 0x826661c8
	goto loc_826661C8;
loc_826661C0:
	// stb r26,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r26.u8);
	// stb r26,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r26.u8);
loc_826661C8:
	// lwz r11,88(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmpw cr6,r31,r11
	cr6.compare<int32_t>(r31.s32, r11.s32, xer);
	// blt cr6,0x826660a0
	if (cr6.lt) goto loc_826660A0;
	// b 0x82666214
	goto loc_82666214;
loc_826661E4:
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8266621c
	if (!cr6.gt) goto loc_8266621C;
loc_826661F4:
	// stb r26,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r26.u8);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stb r26,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r26.u8);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lwz r10,88(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x826661f4
	if (cr6.lt) goto loc_826661F4;
loc_82666214:
	// stw r9,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, ctx.r9.u32);
	// stw r8,-324(r1)
	PPC_STORE_U32(ctx.r1.u32 + -324, ctx.r8.u32);
loc_8266621C:
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// lwz r11,92(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 92);
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// stw r25,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r25.u32);
	// blt cr6,0x82665188
	if (cr6.lt) goto loc_82665188;
loc_82666230:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82666234"))) PPC_WEAK_FUNC(sub_82666234);
PPC_FUNC_IMPL(__imp__sub_82666234) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82666238"))) PPC_WEAK_FUNC(sub_82666238);
PPC_FUNC_IMPL(__imp__sub_82666238) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,12593
	r11.s64 = 825294848;
	// lis r31,12889
	r31.s64 = 844693504;
	// ori r10,r11,13392
	ctx.r10.u64 = r11.u64 | 13392;
	// lwz r11,220(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// ori r30,r31,21849
	r30.u64 = r31.u64 | 21849;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// lis r31,-32126
	r31.s64 = -2105409536;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// stw r10,3328(r31)
	PPC_STORE_U32(r31.u32 + 3328, ctx.r10.u32);
	// stw r7,112(r3)
	PPC_STORE_U32(ctx.r3.u32 + 112, ctx.r7.u32);
	// stw r8,116(r3)
	PPC_STORE_U32(ctx.r3.u32 + 116, ctx.r8.u32);
	// stw r9,120(r3)
	PPC_STORE_U32(ctx.r3.u32 + 120, ctx.r9.u32);
	// stw r4,100(r3)
	PPC_STORE_U32(ctx.r3.u32 + 100, ctx.r4.u32);
	// stw r5,104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 104, ctx.r5.u32);
	// stw r6,108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 108, ctx.r6.u32);
	// bne cr6,0x826662a0
	if (!cr6.eq) goto loc_826662A0;
	// li r8,1
	ctx.r8.s64 = 1;
	// bl 0x82664720
	sub_82664720(ctx, base);
	// b 0x826662d0
	goto loc_826662D0;
loc_826662A0:
	// lis r10,22870
	ctx.r10.s64 = 1498808320;
	// ori r10,r10,22869
	ctx.r10.u64 = ctx.r10.u64 | 22869;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x826662bc
	if (!cr6.eq) goto loc_826662BC;
	// li r8,0
	ctx.r8.s64 = 0;
	// bl 0x82664720
	sub_82664720(ctx, base);
	// b 0x826662d0
	goto loc_826662D0;
loc_826662BC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826662cc
	if (!cr6.eq) goto loc_826662CC;
	// bl 0x82663918
	sub_82663918(ctx, base);
	// b 0x826662d0
	goto loc_826662D0;
loc_826662CC:
	// bl 0x82661e10
	sub_82661E10(ctx, base);
loc_826662D0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826662E8"))) PPC_WEAK_FUNC(sub_826662E8);
PPC_FUNC_IMPL(__imp__sub_826662E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-1072(r1)
	ea = -1072 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r5,48(r1)
	PPC_STORE_U32(ctx.r1.u32 + 48, ctx.r5.u32);
	// vspltish v13,4
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// stw r3,32(r1)
	PPC_STORE_U32(ctx.r1.u32 + 32, ctx.r3.u32);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// stw r4,64(r1)
	PPC_STORE_U32(ctx.r1.u32 + 64, ctx.r4.u32);
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// li r5,-32
	ctx.r5.s64 = -32;
	// li r31,16
	r31.s64 = 16;
	// li r30,16
	r30.s64 = 16;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r6,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r6.u32);
	// vspltish v13,8
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// li r6,-16
	ctx.r6.s64 = -16;
	// rlwinm r7,r4,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// li r29,16
	r29.s64 = 16;
	// li r28,16
	r28.s64 = 16;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltisb v13,-9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0xFFFFFFF7)));
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r11.u32);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// addi r10,r10,3872
	ctx.r10.s64 = ctx.r10.s64 + 3872;
	// lvx128 v13,r5,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lvx128 v7,r6,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v2,r0,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,16
	ctx.r10.s64 = 16;
	// stvx v13,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r5,16
	ctx.r5.s64 = 16;
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v13,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v12,v12,v13
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// add r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
	// lvlx v8,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v10,r9,r5
	temp.u32 = ctx.r9.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v9,r8,r31
	temp.u32 = ctx.r8.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v1,v0,v12
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v5,r7,r29
	temp.u32 = ctx.r7.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v4,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrglb v31,v0,v12
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v17,v0,v10
	_mm_store_si128((__m128i*)v17.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v6,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrglb v16,v0,v10
	_mm_store_si128((__m128i*)v16.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v8,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r6,r1,48
	ctx.r6.s64 = ctx.r1.s64 + 48;
	// vor v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// lvx128 v13,r0,r6
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r6,r4,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// vsplth v11,v13,1
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xD0C))));
	// addi r27,r1,16
	r27.s64 = ctx.r1.s64 + 16;
	// add r6,r4,r6
	ctx.r6.u64 = ctx.r4.u64 + ctx.r6.u64;
	// vsubshs v6,v0,v11
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// lvx128 v13,r0,r27
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r27.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltb v13,v13,3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_set1_epi8(char(0xC))));
	// stvx v6,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v6,v4,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// rlwinm r10,r4,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lvrx v5,r6,r28
	temp.u32 = ctx.r6.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v4,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r6,16
	ctx.r6.s64 = 16;
	// vor v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// vcmpgtub v3,v12,v13
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_cmpgt_epu8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vcmpgtub v30,v10,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vcmpgtub v29,v9,v13
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx v27,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r8,r4,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// vmrghb v28,v0,v9
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// add r9,r4,r9
	ctx.r9.u64 = ctx.r4.u64 + ctx.r9.u64;
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// subf r8,r4,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r4.s64;
	// vmrghb v25,v0,v5
	_mm_store_si128((__m128i*)v25.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// vmrglb v24,v0,v5
	_mm_store_si128((__m128i*)v24.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// vcmpgtub v5,v5,v13
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_cmpgt_epu8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// rlwinm r7,r4,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r4,r7
	ctx.r7.u64 = ctx.r4.u64 + ctx.r7.u64;
	// lvlx v4,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// lvrx v12,r9,r6
	temp.u32 = ctx.r9.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v10,r8,r5
	temp.u32 = ctx.r8.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v10,v4,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v4,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v4,v27,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvrx v27,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)v27.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v26,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r1,864
	r11.s64 = ctx.r1.s64 + 864;
	// vmrghb v23,v0,v12
	_mm_store_si128((__m128i*)v23.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v22,v0,v12
	_mm_store_si128((__m128i*)v22.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcmpgtub v12,v12,v13
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_cmpgt_epu8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v21,v0,v10
	_mm_store_si128((__m128i*)v21.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v20,v0,v10
	_mm_store_si128((__m128i*)v20.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v3,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// vor v3,v26,v27
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vmrghb v27,v0,v6
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v26,v0,v6
	_mm_store_si128((__m128i*)v26.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcmpgtub v6,v6,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_cmpgt_epu8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v19,v0,v4
	_mm_store_si128((__m128i*)v19.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v1,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,320
	r11.s64 = ctx.r1.s64 + 320;
	// vor v1,v30,v30
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v30.u8));
	// vor v30,v28,v28
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)v28.u8));
	// vmrglb v28,v0,v8
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v31,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// vor v31,v29,v29
	_mm_store_si128((__m128i*)v31.u8, _mm_load_si128((__m128i*)v29.u8));
	// vmrghb v29,v0,v8
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcmpgtub v8,v8,v13
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_cmpgt_epu8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v17,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// stvx v16,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,192
	r11.s64 = ctx.r1.s64 + 192;
	// stvx v30,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,352
	r11.s64 = ctx.r1.s64 + 352;
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// stvx v29,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,368
	r11.s64 = ctx.r1.s64 + 368;
	// stvx v28,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// stvx v27,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,384
	r11.s64 = ctx.r1.s64 + 384;
	// stvx v26,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,240
	r11.s64 = ctx.r1.s64 + 240;
	// stvx v25,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,400
	r11.s64 = ctx.r1.s64 + 400;
	// stvx v24,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,256
	r11.s64 = ctx.r1.s64 + 256;
	// stvx v23,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,416
	r11.s64 = ctx.r1.s64 + 416;
	// stvx v22,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// stvx v21,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,432
	r11.s64 = ctx.r1.s64 + 432;
	// stvx v20,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,288
	r11.s64 = ctx.r1.s64 + 288;
	// stvx v19,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v19,v0,v4
	_mm_store_si128((__m128i*)v19.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r1,448
	r11.s64 = ctx.r1.s64 + 448;
	// stvx v19,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcmpgtub v18,v4,v13
	_mm_store_si128((__m128i*)v18.u8, _mm_cmpgt_epu8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// addi r11,r1,1008
	r11.s64 = ctx.r1.s64 + 1008;
	// vcmpgtub v10,v10,v13
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_cmpgt_epu8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r12,16
	r12.s64 = 16;
	// stvx128 v0,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcmpgtub v13,v3,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_cmpgt_epu8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r10,1
	ctx.r10.s64 = 1;
	// vaddsbs v4,v8,v6
	// vaddsbs v19,v5,v12
	// vaddsbs v18,v10,v18
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddsbs v13,v1,v31
	// vmrghb v1,v0,v3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r1,304
	r11.s64 = ctx.r1.s64 + 304;
	// vmrglb v3,v0,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddsbs v5,v4,v5
	// vaddsbs v6,v19,v6
	// stw r10,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r10.u32);
	// vaddsbs v10,v19,v10
	// vaddsbs v12,v18,v12
	// vaddshs v15,v28,v26
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v14,v25,v23
	_mm_store_si128((__m128i*)v14.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v23.s16)));
	// stvx v1,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,464
	r11.s64 = ctx.r1.s64 + 464;
	// addi r9,r1,288
	ctx.r9.s64 = ctx.r1.s64 + 288;
	// vaddshs v1,v24,v22
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v22.s16)));
	// stvx v3,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,864
	r11.s64 = ctx.r1.s64 + 864;
	// vaddshs v3,v17,v30
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v17,v16,v9
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v16,v29,v27
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v9,v15,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,1008
	r11.s64 = ctx.r1.s64 + 1008;
	// vaddsbs v0,v13,v0
	// vaddsbs v13,v13,v8
	// vaddsbs v8,v4,v31
	// vaddshs v31,v15,v24
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v24.s16)));
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,480
	r11.s64 = ctx.r1.s64 + 480;
	// vaddsbs v4,v18,v4
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,496
	r11.s64 = ctx.r1.s64 + 496;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,512
	r11.s64 = ctx.r1.s64 + 512;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,448
	ctx.r9.s64 = ctx.r1.s64 + 448;
	// vaddshs v13,v21,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,528
	r11.s64 = ctx.r1.s64 + 528;
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,544
	r11.s64 = ctx.r1.s64 + 544;
	// vaddshs v5,v17,v28
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v28,v14,v21
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v14.s16), _mm_load_si128((__m128i*)v21.s16)));
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,560
	r11.s64 = ctx.r1.s64 + 560;
	// vaddshs v6,v3,v29
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v1,v26
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v1,v1,v20
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v20.s16)));
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,576
	r11.s64 = ctx.r1.s64 + 576;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,592
	r11.s64 = ctx.r1.s64 + 592;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// vaddshs v12,v20,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// stvx v4,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,0
	r11.s64 = 0;
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,320
	ctx.r9.s64 = ctx.r1.s64 + 320;
	// vaddshs v10,v3,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v4,v16,v30
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v3,v16,v25
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v25.s16)));
	// stw r11,40(r1)
	PPC_STORE_U32(ctx.r1.u32 + 40, r11.u32);
	// vaddshs v30,v14,v27
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v14.s16), _mm_load_si128((__m128i*)v27.s16)));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,608
	ctx.r9.s64 = ctx.r1.s64 + 608;
	// vaddshs v8,v17,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v10,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,736
	ctx.r9.s64 = ctx.r1.s64 + 736;
	// stvx v8,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,624
	ctx.r9.s64 = ctx.r1.s64 + 624;
	// stvx v6,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,752
	ctx.r9.s64 = ctx.r1.s64 + 752;
	// vaddshs v10,v13,v23
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)v23.s16)));
	// li r12,16
	r12.s64 = 16;
	// lvx128 v0,r1,r12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,640
	ctx.r9.s64 = ctx.r1.s64 + 640;
	// stvx v4,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,768
	ctx.r9.s64 = ctx.r1.s64 + 768;
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,656
	ctx.r9.s64 = ctx.r1.s64 + 656;
	// stvx v3,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,784
	ctx.r9.s64 = ctx.r1.s64 + 784;
	// stvx v31,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,672
	ctx.r9.s64 = ctx.r1.s64 + 672;
	// stvx v30,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,800
	ctx.r9.s64 = ctx.r1.s64 + 800;
	// stvx v29,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,688
	ctx.r9.s64 = ctx.r1.s64 + 688;
	// stvx v28,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,304
	ctx.r9.s64 = ctx.r1.s64 + 304;
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,464
	ctx.r9.s64 = ctx.r1.s64 + 464;
	// vaddshs v13,v13,v9
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v9,v12,v22
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)v22.s16)));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,816
	ctx.r9.s64 = ctx.r1.s64 + 816;
	// vaddshs v12,v12,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v1,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,704
	ctx.r9.s64 = ctx.r1.s64 + 704;
	// stvx v10,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,832
	ctx.r9.s64 = ctx.r1.s64 + 832;
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,720
	ctx.r9.s64 = ctx.r1.s64 + 720;
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,848
	ctx.r9.s64 = ctx.r1.s64 + 848;
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82666758:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r1,480
	ctx.r9.s64 = ctx.r1.s64 + 480;
	// addi r8,r1,736
	ctx.r8.s64 = ctx.r1.s64 + 736;
	// addi r7,r1,608
	ctx.r7.s64 = ctx.r1.s64 + 608;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r6,r1,320
	ctx.r6.s64 = ctx.r1.s64 + 320;
	// lvx128 v12,r11,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// lvx128 v13,r11,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v4,v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v10,r11,r7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// vperm v3,v10,v13,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvx128 v8,r10,r6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r10,r5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// vperm v5,v12,v12,v6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vperm v6,v10,v13,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v13,v9,v8,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v8,v9,v8,v2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddsbs v5,v5,v4
	// vaddshs v8,v13,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddsbs v12,v12,v5
	// vaddshs v5,v6,v3
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vcmpequb v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_cmpeq_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v10,v10,v5
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcmpequb v12,v5,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_cmpeq_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// li r11,4
	r11.s64 = 4;
	// vsubshs v5,v13,v11
	// vaddshs v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v12,v8,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// vaddshs v8,v6,v13
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vmrghb v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vandc v4,v13,v12
	// vaddshs v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// vsrah v10,v10,v9
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v13,v10,v13
	// vcmpgtsh v9,v13,v11
	// vcmpgtsh v13,v8,v13
	// vand v8,v6,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vandc v10,v10,v9
	// vand v9,v5,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v10,v10,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vandc v13,v10,v13
	// vor v13,v13,v9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vand v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vor v13,v13,v4
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvewx v13,r0,r3
	ea = (ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 64);
	// lwz r9,32(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// lwz r11,40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 40);
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r3,32(r1)
	PPC_STORE_U32(ctx.r1.u32 + 32, ctx.r3.u32);
	// stw r11,40(r1)
	PPC_STORE_U32(ctx.r1.u32 + 40, r11.u32);
	// stw r10,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r10.u32);
	// blt cr6,0x82666758
	if (cr6.lt) goto loc_82666758;
	// addi r1,r1,1072
	ctx.r1.s64 = ctx.r1.s64 + 1072;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82666888"))) PPC_WEAK_FUNC(sub_82666888);
PPC_FUNC_IMPL(__imp__sub_82666888) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcd0
	// lis r11,-32243
	r11.s64 = -2113077248;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// addi r21,r11,3824
	r21.s64 = r11.s64 + 3824;
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// beq cr6,0x82666df0
	if (cr6.eq) goto loc_82666DF0;
	// stw r7,-496(r1)
	PPC_STORE_U32(ctx.r1.u32 + -496, ctx.r7.u32);
	// subf r11,r8,r4
	r11.s64 = ctx.r4.s64 - ctx.r8.s64;
	// add r6,r4,r8
	ctx.r6.u64 = ctx.r4.u64 + ctx.r8.u64;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// subf r9,r8,r11
	ctx.r9.s64 = r11.s64 - ctx.r8.s64;
	// lvx128 v1,r0,r21
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r21.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r3,r6,r8
	ctx.r3.u64 = ctx.r6.u64 + ctx.r8.u64;
	// vspltish v12,3
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// vspltish v24,4
	// add r29,r3,r8
	r29.u64 = ctx.r3.u64 + ctx.r8.u64;
	// stw r11,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, r11.u32);
	// subf r30,r8,r10
	r30.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r6,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, ctx.r6.u32);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// stw r9,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r9.u32);
	// li r23,2
	r23.s64 = 2;
	// stw r3,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, ctx.r3.u32);
	// li r28,16
	r28.s64 = 16;
	// stw r10,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, ctx.r10.u32);
	// li r26,16
	r26.s64 = 16;
	// stw r29,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, r29.u32);
	// stw r30,-392(r1)
	PPC_STORE_U32(ctx.r1.u32 + -392, r30.u32);
	// li r27,16
	r27.s64 = 16;
	// stw r31,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r31.u32);
	// li r25,16
	r25.s64 = 16;
	// stw r23,-148(r1)
	PPC_STORE_U32(ctx.r1.u32 + -148, r23.u32);
	// lvlx v11,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v13,r30,r28
	temp.u32 = r30.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r24,16
	r24.s64 = 16;
	// vor v20,v11,v13
	_mm_store_si128((__m128i*)v20.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v8,r9,r26
	temp.u32 = ctx.r9.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r22,16
	r22.s64 = 16;
	// vor v18,v13,v8
	_mm_store_si128((__m128i*)v18.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvrx v10,r10,r27
	temp.u32 = ctx.r10.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltisw v23,4
	_mm_store_si128((__m128i*)v23.u32, _mm_set1_epi32(int(0x4)));
	// lvrx v11,r11,r25
	temp.u32 = r11.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v19,v9,v10
	_mm_store_si128((__m128i*)v19.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v13,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v5,v0,v20
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v17,v13,v11
	_mm_store_si128((__m128i*)v17.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r4,r24
	temp.u32 = ctx.r4.u32 + r24.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r24,0
	r24.s64 = 0;
	// vor v16,v13,v10
	_mm_store_si128((__m128i*)v16.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v11,r6,r22
	temp.u32 = ctx.r6.u32 + r22.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v15,v13,v11
	_mm_store_si128((__m128i*)v15.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v13,r3,r28
	temp.u32 = ctx.r3.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v7,v0,v17
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v14,v11,v13
	_mm_store_si128((__m128i*)v14.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v11,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v6,v0,v16
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v15
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v14
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r28,r1,-496
	r28.s64 = ctx.r1.s64 + -496;
	// lvx128 v10,r0,r28
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r28,16
	r28.s64 = 16;
	// vsplth v26,v10,1
	_mm_store_si128((__m128i*)v26.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_set1_epi16(short(0xD0C))));
	// vmrghb v10,v0,v18
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v13,r29,r28
	temp.u32 = r29.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r28,r1,-496
	r28.s64 = ctx.r1.s64 + -496;
	// vor v13,v11,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v0,v19
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v21,v26,v26
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// stvx v13,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v4,v0,v13
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupkhsh v22,v21
	_mm_store_si128((__m128i*)v22.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)v21.s16))));
loc_826669CC:
	// vsubshs v13,v5,v11
	// li r12,-432
	r12.s64 = -432;
	// stvx128 v11,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v31,v10,v7
	// vsubshs v2,v11,v10
	// vsubshs v30,v7,v6
	// vsubshs v27,v0,v13
	// vsubshs v29,v6,v9
	// vsubshs v25,v0,v2
	// vsubshs v28,v9,v8
	// vmaxsh v13,v27,v13
	// vsubshs v27,v0,v31
	// vmaxsh v2,v25,v2
	// vsubshs v25,v0,v29
	// vsubshs v3,v8,v4
	// vmaxsh v31,v27,v31
	// vsubshs v27,v0,v30
	// vsubshs v11,v0,v28
	// vmaxsh v29,v25,v29
	// vcmpgtuh v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmaxsh v30,v27,v30
	// vmaxsh v28,v11,v28
	// vsubshs v27,v0,v3
	// vcmpgtuh v2,v12,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vcmpgtuh v31,v12,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vcmpgtuh v30,v12,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmaxsh v27,v27,v3
	// vcmpgtuh v29,v12,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vcmpgtuh v28,v12,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vcmpgtuh v27,v12,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vaddshs v31,v29,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v13,v27,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v13,v0,v13
	// vperm v2,v13,v13,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vcmpgtsh. v25,v2,v24
	// mfocrf r25,2
	r25.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r28,r25,0,26,26
	r28.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0x20;
	// li r12,-432
	r12.s64 = -432;
	// lvx128 v11,r1,r12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r25,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, r25.u32);
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// beq cr6,0x82666ccc
	if (cr6.eq) goto loc_82666CCC;
	// vmaxsh v30,v7,v6
	// vmaxsh v13,v11,v10
	// vminsh v31,v11,v10
	// vminsh v29,v7,v6
	// vmaxsh v28,v9,v8
	// vmaxsh v13,v13,v30
	// vminsh v27,v9,v8
	// vminsh v31,v31,v29
	// vmaxsh v13,v28,v13
	// vminsh v31,v27,v31
	// vsubshs v13,v13,v31
	// vupkhsh v31,v13
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// vupklsh v30,v13
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v13.s16)));
	// vcmpgtsh. v13,v21,v13
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r28,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, r28.u32);
	// vcmpgtsw. v31,v22,v31
	// vand v13,v13,v25
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v25.u8)));
	// mfocrf r27,2
	r27.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r27,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, r27.u32);
	// vcmpgtsw. v31,v22,v30
	// mfocrf r26,2
	r26.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v31,v2
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// stw r26,-140(r1)
	PPC_STORE_U32(ctx.r1.u32 + -140, r26.u32);
	// vcmpgtsw. v31,v31,v23
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r28,-132(r1)
	PPC_STORE_U32(ctx.r1.u32 + -132, r28.u32);
	// vcmpgtsw. v2,v2,v23
	// mfocrf r22,2
	r22.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r28,r28,0,26,26
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// mr r28,r22
	r28.u64 = r22.u64;
	// stw r28,-124(r1)
	PPC_STORE_U32(ctx.r1.u32 + -124, r28.u32);
	// beq cr6,0x82666b1c
	if (cr6.eq) goto loc_82666B1C;
	// rlwinm r27,r27,0,26,26
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r27,32
	cr6.compare<uint32_t>(r27.u32, 32, xer);
	// bne cr6,0x82666b34
	if (!cr6.eq) goto loc_82666B34;
loc_82666B1C:
	// rlwinm r28,r28,0,26,26
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// beq cr6,0x82666ccc
	if (cr6.eq) goto loc_82666CCC;
	// rlwinm r28,r26,0,26,26
	r28.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// beq cr6,0x82666ccc
	if (cr6.eq) goto loc_82666CCC;
loc_82666B34:
	// vsubshs v29,v0,v3
	// li r12,-416
	r12.s64 = -416;
	// stvx128 v0,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v2,v11,v5
	// addi r28,r1,-464
	r28.s64 = ctx.r1.s64 + -464;
	// vaddshs v30,v10,v7
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v31,v6,v9
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vmaxsh v29,v29,v3
	// vsubshs v3,v0,v2
	// vaddshs v27,v11,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vandc128 v63,v10,v13
	_mm_store_si128((__m128i*)v63.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vandc128 v62,v7,v13
	_mm_store_si128((__m128i*)v62.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmaxsh v28,v3,v2
	// vaddshs v3,v30,v24
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vcmpgtsh v2,v26,v29
	// vandc128 v61,v8,v13
	_mm_store_si128((__m128i*)v61.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vcmpgtsh v30,v26,v28
	// vandc v29,v8,v2
	// vand v4,v4,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vandc v2,v11,v30
	// vand v5,v5,v30
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vxor v4,v4,v29
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vxor v5,v5,v2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v30,v8,v4
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v0,v9,v4
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v29,v5,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v28,v29,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v28,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsubshs v31,v5,v9
	// vaddshs v28,v7,v8
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v31,v31,v29
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v5,v10
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v31,v31,v2
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v29,v29,v2
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v28,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v28,v4,v10
	// vsrah v31,v31,v12
	// vsrah v29,v29,v12
	// vsrah v2,v2,v12
	// vaddshs v30,v28,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v28,v0,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vand v31,v31,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v29,v29,v13
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r12,-432
	r12.s64 = -432;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v27,v3
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsrah v28,v28,v12
	// vand v2,v2,v13
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor128 v29,v29,v63
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v63.u8)));
	// vsrah v30,v30,v12
	// vandc128 v63,v9,v13
	_mm_store_si128((__m128i*)v63.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vand v28,v28,v13
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v30,v30,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r12,-432
	r12.s64 = -432;
	// lvx128 v27,r1,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v3,v27,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vandc v27,v11,v13
	// vsrah v3,v3,v12
	// vxor v31,v31,v27
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vandc v27,v6,v13
	// vand128 v60,v3,v13
	_mm_store_si128((__m128i*)v60.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v31,v31,v29
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vxor128 v13,v2,v62
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v62.u8)));
	// vxor v3,v30,v27
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vxor128 v2,v28,v63
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v63.u8)));
	// vxor128 v30,v60,v61
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v60.u8), _mm_load_si128((__m128i*)v61.u8)));
	// stvx v31,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v31,v13,v3
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// addi r28,r1,-448
	r28.s64 = ctx.r1.s64 + -448;
	// vpkshus v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// stvx v31,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,-480
	r28.s64 = ctx.r1.s64 + -480;
	// stvx v2,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r28,-464(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -464);
	// lwz r27,-456(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -456);
	// lwz r26,-460(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -460);
	// lwz r22,-452(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + -452);
	// stw r28,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r28.u32);
	// stw r27,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r27.u32);
	// lwz r28,-448(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r27,-440(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -440);
	// lwz r20,-480(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -480);
	// lwz r19,-472(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -472);
	// li r12,-416
	r12.s64 = -416;
	// lvx128 v0,r1,r12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r28,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r28.u32);
	// lwz r18,-444(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -444);
	// stw r27,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r27.u32);
	// lwz r28,-436(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -436);
	// stw r20,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r20.u32);
	// lwz r27,-476(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -476);
	// lwz r20,-468(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -468);
	// stw r19,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r19.u32);
	// stw r26,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r26.u32);
	// stw r22,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r22.u32);
	// stw r18,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r18.u32);
	// stw r28,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r28.u32);
	// stw r27,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, r27.u32);
	// stw r20,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r20.u32);
	// b 0x82666cd4
	goto loc_82666CD4;
loc_82666CCC:
	// vor v3,v6,v6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v13,v7,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
loc_82666CD4:
	// rlwinm r28,r25,0,24,24
	r28.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r28,128
	cr6.compare<uint32_t>(r28.u32, 128, xer);
	// beq cr6,0x82666d70
	if (cr6.eq) goto loc_82666D70;
	// vspltisb v31,-1
	_mm_store_si128((__m128i*)v31.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// vsubshs v2,v6,v7
	// vxor v30,v31,v25
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vsubshs v31,v0,v2
	// vmaxsh v31,v31,v2
	// vcmpgtsh v29,v31,v12
	// vcmpgtsh v28,v26,v31
	// vand v29,v29,v28
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vand v30,v29,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vcmpequh. v29,v0,v30
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r28,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, r28.u32);
	// rlwinm r28,r28,0,24,24
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r28,128
	cr6.compare<int32_t>(r28.s32, 128, xer);
	// beq cr6,0x82666d70
	if (cr6.eq) goto loc_82666D70;
	// vspltish v29,2
	// addi r28,r1,-480
	r28.s64 = ctx.r1.s64 + -480;
	// vspltish v28,15
	// vsrah v31,v31,v29
	// vsrah v2,v2,v28
	// vaddshs v29,v31,v31
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vand v2,v2,v29
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vsubshs v2,v31,v2
	// vand v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v3,v3,v2
	// vpkshus v13,v13,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r28,-480(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -480);
	// lwz r27,-476(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -476);
	// lwz r26,-472(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -472);
	// lwz r25,-468(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -468);
	// stw r28,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r28.u32);
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// stw r26,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r26.u32);
	// stw r25,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r25.u32);
loc_82666D70:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x82666dc0
	if (!cr6.eq) goto loc_82666DC0;
	// addi r28,r1,-496
	r28.s64 = ctx.r1.s64 + -496;
	// vmrglb v5,v0,v20
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// vmrglb v11,v0,v19
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// vmrglb v10,v0,v18
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// vmrglb v7,v0,v17
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// vmrglb v6,v0,v16
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v13,r0,r28
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// vmrglb v9,v0,v15
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// vmrglb v8,v0,v14
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// vmrglb v4,v0,v13
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_82666DC0:
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// cmpw cr6,r24,r23
	cr6.compare<int32_t>(r24.s32, r23.s32, xer);
	// blt cr6,0x826669cc
	if (cr6.lt) goto loc_826669CC;
	// stw r24,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, r24.u32);
	// stw r29,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, r29.u32);
	// stw r3,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, ctx.r3.u32);
	// stw r6,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, ctx.r6.u32);
	// stw r31,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r31.u32);
	// stw r11,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, r11.u32);
	// stw r9,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r9.u32);
	// stw r10,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, ctx.r10.u32);
	// stw r30,-392(r1)
	PPC_STORE_U32(ctx.r1.u32 + -392, r30.u32);
loc_82666DF0:
	// stw r7,-496(r1)
	PPC_STORE_U32(ctx.r1.u32 + -496, ctx.r7.u32);
	// rlwinm r11,r8,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// li r23,2
	r23.s64 = 2;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v1,r0,r21
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r21.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r28,16
	r28.s64 = 16;
	// vspltish v13,3
	// subf r10,r8,r11
	ctx.r10.s64 = r11.s64 - ctx.r8.s64;
	// vspltish v22,4
	// add r3,r11,r8
	ctx.r3.u64 = r11.u64 + ctx.r8.u64;
	// subf r6,r8,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r23,-248(r1)
	PPC_STORE_U32(ctx.r1.u32 + -248, r23.u32);
	// add r31,r3,r8
	r31.u64 = ctx.r3.u64 + ctx.r8.u64;
	// stw r11,-376(r1)
	PPC_STORE_U32(ctx.r1.u32 + -376, r11.u32);
	// subf r9,r8,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r8.s64;
	// add r29,r31,r8
	r29.u64 = r31.u64 + ctx.r8.u64;
	// stw r10,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, ctx.r10.u32);
	// subf r30,r8,r9
	r30.s64 = ctx.r9.s64 - ctx.r8.s64;
	// stw r3,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, ctx.r3.u32);
	// li r26,16
	r26.s64 = 16;
	// stw r6,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, ctx.r6.u32);
	// stw r31,-384(r1)
	PPC_STORE_U32(ctx.r1.u32 + -384, r31.u32);
	// li r25,16
	r25.s64 = 16;
	// stw r9,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r9.u32);
	// li r27,16
	r27.s64 = 16;
	// stw r29,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r29.u32);
	// li r24,16
	r24.s64 = 16;
	// stw r30,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, r30.u32);
	// lvrx v12,r30,r28
	temp.u32 = r30.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r22,16
	r22.s64 = 16;
	// vor v21,v11,v12
	_mm_store_si128((__m128i*)v21.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v8,r6,r26
	temp.u32 = ctx.r6.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v12,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v19,v12,v8
	_mm_store_si128((__m128i*)v19.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvrx v11,r10,r25
	temp.u32 = ctx.r10.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvrx v10,r9,r27
	temp.u32 = ctx.r9.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r25,0
	r25.s64 = 0;
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v5,v0,v21
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v20,v9,v10
	_mm_store_si128((__m128i*)v20.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vor v18,v12,v11
	_mm_store_si128((__m128i*)v18.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r11,r24
	temp.u32 = r11.u32 + r24.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v17,v12,v10
	_mm_store_si128((__m128i*)v17.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v11,r3,r22
	temp.u32 = ctx.r3.u32 + r22.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v12,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v16,v12,v11
	_mm_store_si128((__m128i*)v16.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v12,r31,r28
	temp.u32 = r31.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v7,v0,v18
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v15,v11,v12
	_mm_store_si128((__m128i*)v15.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvlx v11,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v6,v0,v17
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v16
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v15
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r28,r1,-496
	r28.s64 = ctx.r1.s64 + -496;
	// lvx128 v10,r0,r28
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r28,16
	r28.s64 = 16;
	// vsplth v26,v10,1
	_mm_store_si128((__m128i*)v26.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_set1_epi16(short(0xD0C))));
	// vmrghb v10,v0,v19
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v12,r29,r28
	temp.u32 = r29.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vaddshs v23,v26,v26
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vor v14,v11,v12
	_mm_store_si128((__m128i*)v14.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v11,v0,v20
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupkhsh v24,v23
	_mm_store_si128((__m128i*)v24.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v23.s16))));
	// vmrghb v4,v0,v14
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_82666F04:
	// vsubshs v12,v5,v11
	// li r12,-496
	r12.s64 = -496;
	// stvx128 v11,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v31,v10,v7
	// vsubshs v2,v11,v10
	// vsubshs v30,v7,v6
	// vsubshs v27,v0,v12
	// vsubshs v29,v6,v9
	// vsubshs v25,v0,v2
	// vsubshs v28,v9,v8
	// vmaxsh v12,v27,v12
	// vsubshs v27,v0,v31
	// vmaxsh v2,v25,v2
	// vsubshs v25,v0,v29
	// vsubshs v3,v8,v4
	// vmaxsh v31,v27,v31
	// vsubshs v27,v0,v30
	// vsubshs v11,v0,v28
	// vmaxsh v29,v25,v29
	// vcmpgtuh v12,v13,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmaxsh v30,v27,v30
	// vmaxsh v28,v11,v28
	// vsubshs v27,v0,v3
	// vcmpgtuh v2,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vcmpgtuh v31,v13,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vcmpgtuh v30,v13,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmaxsh v27,v27,v3
	// vcmpgtuh v29,v13,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vcmpgtuh v28,v13,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vaddshs v12,v12,v2
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vcmpgtuh v27,v13,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vaddshs v31,v29,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v12,v27,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v12,v12,v2
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v12,v0,v12
	// vperm v2,v12,v12,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vcmpgtsh. v25,v2,v22
	// mfocrf r24,2
	r24.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r28,r24,0,26,26
	r28.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x20;
	// li r12,-496
	r12.s64 = -496;
	// lvx128 v11,r1,r12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r24,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, r24.u32);
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// beq cr6,0x82667208
	if (cr6.eq) goto loc_82667208;
	// vmaxsh v30,v7,v6
	// vmaxsh v12,v11,v10
	// vminsh v31,v11,v10
	// vminsh v29,v7,v6
	// vmaxsh v28,v9,v8
	// vmaxsh v12,v12,v30
	// vminsh v27,v9,v8
	// vminsh v31,v31,v29
	// vmaxsh v12,v28,v12
	// vminsh v31,v27,v31
	// vsubshs v12,v12,v31
	// vupkhsh v31,v12
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16))));
	// vupklsh v30,v12
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v12.s16)));
	// vcmpgtsh. v12,v23,v12
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r28,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, r28.u32);
	// vcmpgtsw. v31,v24,v31
	// vand v12,v12,v25
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v25.u8)));
	// mfocrf r27,2
	r27.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r27,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, r27.u32);
	// vcmpgtsw. v31,v24,v30
	// mfocrf r26,2
	r26.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v31,v2
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vspltisw v30,4
	_mm_store_si128((__m128i*)v30.u32, _mm_set1_epi32(int(0x4)));
	// stw r26,-232(r1)
	PPC_STORE_U32(ctx.r1.u32 + -232, r26.u32);
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtsw. v31,v31,v30
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r28,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r28.u32);
	// vcmpgtsw. v2,v2,v30
	// mfocrf r22,2
	r22.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r28,r28,0,26,26
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// mr r28,r22
	r28.u64 = r22.u64;
	// stw r28,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, r28.u32);
	// beq cr6,0x82667058
	if (cr6.eq) goto loc_82667058;
	// rlwinm r27,r27,0,26,26
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r27,32
	cr6.compare<uint32_t>(r27.u32, 32, xer);
	// bne cr6,0x82667070
	if (!cr6.eq) goto loc_82667070;
loc_82667058:
	// rlwinm r28,r28,0,26,26
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// beq cr6,0x82667208
	if (cr6.eq) goto loc_82667208;
	// rlwinm r28,r26,0,26,26
	r28.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// beq cr6,0x82667208
	if (cr6.eq) goto loc_82667208;
loc_82667070:
	// vsubshs v29,v0,v3
	// li r12,-432
	r12.s64 = -432;
	// stvx128 v0,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v2,v11,v5
	// addi r28,r1,-448
	r28.s64 = ctx.r1.s64 + -448;
	// vaddshs v30,v10,v7
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v31,v6,v9
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vmaxsh v29,v29,v3
	// vsubshs v3,v0,v2
	// vaddshs v27,v11,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vandc128 v63,v10,v12
	_mm_store_si128((__m128i*)v63.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vandc128 v62,v7,v12
	_mm_store_si128((__m128i*)v62.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmaxsh v28,v3,v2
	// vaddshs v3,v30,v22
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vcmpgtsh v2,v26,v29
	// vandc128 v61,v8,v12
	_mm_store_si128((__m128i*)v61.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vcmpgtsh v30,v26,v28
	// vandc v29,v8,v2
	// vand v4,v4,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vandc v2,v11,v30
	// vand v5,v5,v30
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vxor v4,v4,v29
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vxor v5,v5,v2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v30,v8,v4
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v0,v9,v4
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v29,v5,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v28,v29,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v28,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsubshs v31,v5,v9
	// vaddshs v28,v7,v8
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v31,v31,v29
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v5,v10
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v31,v31,v2
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v29,v29,v2
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v28,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v28,v4,v10
	// vsrah v31,v31,v13
	// vsrah v29,v29,v13
	// vsrah v2,v2,v13
	// vaddshs v30,v28,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v28,v0,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vand v31,v31,v12
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v29,v29,v12
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// li r12,-496
	r12.s64 = -496;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v27,v3
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsrah v28,v28,v13
	// vand v2,v2,v12
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vxor128 v29,v29,v63
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v63.u8)));
	// vsrah v30,v30,v13
	// vandc128 v63,v9,v12
	_mm_store_si128((__m128i*)v63.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vand v28,v28,v12
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v30,v30,v12
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// li r12,-496
	r12.s64 = -496;
	// lvx128 v27,r1,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v3,v27,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vandc v27,v11,v12
	// vsrah v3,v3,v13
	// vxor v31,v31,v27
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vandc v27,v6,v12
	// vand128 v60,v3,v12
	_mm_store_si128((__m128i*)v60.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vpkshus v31,v31,v29
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vxor128 v12,v2,v62
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v62.u8)));
	// vxor v3,v30,v27
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vxor128 v2,v28,v63
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v63.u8)));
	// vxor128 v30,v60,v61
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v60.u8), _mm_load_si128((__m128i*)v61.u8)));
	// stvx v31,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v31,v12,v3
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// addi r28,r1,-464
	r28.s64 = ctx.r1.s64 + -464;
	// vpkshus v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// stvx v31,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,-480
	r28.s64 = ctx.r1.s64 + -480;
	// stvx v2,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r28,-448(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r27,-440(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -440);
	// lwz r26,-444(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -444);
	// lwz r22,-436(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + -436);
	// stw r28,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r28.u32);
	// stw r27,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r27.u32);
	// lwz r28,-464(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -464);
	// lwz r27,-456(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -456);
	// lwz r20,-480(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -480);
	// lwz r19,-472(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -472);
	// li r12,-432
	r12.s64 = -432;
	// lvx128 v0,r1,r12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r28,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r28.u32);
	// lwz r18,-460(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -460);
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// lwz r28,-452(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -452);
	// stw r20,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r20.u32);
	// lwz r27,-476(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -476);
	// lwz r20,-468(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -468);
	// stw r19,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r19.u32);
	// stw r26,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r26.u32);
	// stw r22,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, r22.u32);
	// stw r18,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r18.u32);
	// stw r28,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r28.u32);
	// stw r27,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r27.u32);
	// stw r20,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r20.u32);
	// b 0x82667210
	goto loc_82667210;
loc_82667208:
	// vor v3,v6,v6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v12,v7,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
loc_82667210:
	// rlwinm r28,r24,0,24,24
	r28.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r28,128
	cr6.compare<uint32_t>(r28.u32, 128, xer);
	// beq cr6,0x826672ac
	if (cr6.eq) goto loc_826672AC;
	// vspltisb v31,-1
	_mm_store_si128((__m128i*)v31.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// vsubshs v2,v6,v7
	// vxor v30,v31,v25
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vsubshs v31,v0,v2
	// vmaxsh v31,v31,v2
	// vcmpgtsh v29,v31,v13
	// vcmpgtsh v28,v26,v31
	// vand v29,v29,v28
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vand v30,v29,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vcmpequh. v29,v0,v30
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r28,-136(r1)
	PPC_STORE_U32(ctx.r1.u32 + -136, r28.u32);
	// rlwinm r28,r28,0,24,24
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r28,128
	cr6.compare<int32_t>(r28.s32, 128, xer);
	// beq cr6,0x826672ac
	if (cr6.eq) goto loc_826672AC;
	// vspltish v29,2
	// addi r28,r1,-480
	r28.s64 = ctx.r1.s64 + -480;
	// vspltish v28,15
	// vsrah v31,v31,v29
	// vsrah v2,v2,v28
	// vaddshs v29,v31,v31
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vand v2,v2,v29
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vsubshs v2,v31,v2
	// vand v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vaddshs v12,v12,v2
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v3,v3,v2
	// vpkshus v12,v12,v3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// stvx v12,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r28,-480(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -480);
	// lwz r27,-476(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -476);
	// lwz r26,-472(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -472);
	// lwz r24,-468(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -468);
	// stw r28,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r28.u32);
	// stw r27,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r27.u32);
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
	// stw r24,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r24.u32);
loc_826672AC:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x826672f4
	if (!cr6.eq) goto loc_826672F4;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// vmrglb v5,v0,v21
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// vmrglb v11,v0,v20
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// vmrglb v10,v0,v19
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// vmrglb v7,v0,v18
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// vmrglb v6,v0,v17
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// vmrglb v9,v0,v16
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// vmrglb v8,v0,v15
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// vmrglb v4,v0,v14
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_826672F4:
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// cmpw cr6,r25,r23
	cr6.compare<int32_t>(r25.s32, r23.s32, xer);
	// blt cr6,0x82666f04
	if (cr6.lt) goto loc_82666F04;
	// stw r11,-376(r1)
	PPC_STORE_U32(ctx.r1.u32 + -376, r11.u32);
	// addi r11,r4,4
	r11.s64 = ctx.r4.s64 + 4;
	// stw r10,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, ctx.r10.u32);
	// li r22,2
	r22.s64 = 2;
	// add r10,r11,r8
	ctx.r10.u64 = r11.u64 + ctx.r8.u64;
	// stw r9,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r9.u32);
	// stw r6,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, ctx.r6.u32);
	// rlwinm r28,r8,3,0,28
	r28.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r10,r8
	ctx.r9.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r3,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, ctx.r3.u32);
	// stw r31,-384(r1)
	PPC_STORE_U32(ctx.r1.u32 + -384, r31.u32);
	// li r27,16
	r27.s64 = 16;
	// add r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r30,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, r30.u32);
	// stw r29,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r29.u32);
	// li r26,16
	r26.s64 = 16;
	// add r3,r6,r8
	ctx.r3.u64 = ctx.r6.u64 + ctx.r8.u64;
	// stw r25,-216(r1)
	PPC_STORE_U32(ctx.r1.u32 + -216, r25.u32);
	// stw r11,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r11.u32);
	// li r25,16
	r25.s64 = 16;
	// add r31,r3,r8
	r31.u64 = ctx.r3.u64 + ctx.r8.u64;
	// stw r10,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, ctx.r10.u32);
	// stw r9,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r9.u32);
	// li r23,0
	r23.s64 = 0;
	// add r30,r31,r8
	r30.u64 = r31.u64 + ctx.r8.u64;
	// stw r6,-400(r1)
	PPC_STORE_U32(ctx.r1.u32 + -400, ctx.r6.u32);
	// stw r3,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, ctx.r3.u32);
	// add r29,r30,r8
	r29.u64 = r30.u64 + ctx.r8.u64;
	// stw r22,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, r22.u32);
	// stw r31,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r31.u32);
	// stw r28,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, r28.u32);
	// stw r30,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, r30.u32);
	// stw r7,-496(r1)
	PPC_STORE_U32(ctx.r1.u32 + -496, ctx.r7.u32);
	// stw r29,-396(r1)
	PPC_STORE_U32(ctx.r1.u32 + -396, r29.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r11,r27
	temp.u32 = r11.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvlx v10,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v11,r10,r27
	temp.u32 = ctx.r10.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v10,r9,r26
	temp.u32 = ctx.r9.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v8,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r6,r27
	temp.u32 = ctx.r6.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v6,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r31,r25
	temp.u32 = r31.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v5,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v7,r3,r26
	temp.u32 = ctx.r3.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v4,v5,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v8,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r30,r27
	temp.u32 = r30.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v3,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v6,v8,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvrx v5,r29,r26
	temp.u32 = r29.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// stw r23,-388(r1)
	PPC_STORE_U32(ctx.r1.u32 + -388, r23.u32);
	// vor v5,v3,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v4,v12,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v12,v12,v8
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghh v2,v10,v6
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghh v3,v11,v7
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v8,v9,v5
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrglh v10,v10,v6
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrglh v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrghh v7,v4,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglh v6,v4,v2
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghh v5,v3,v8
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrghh v4,v12,v10
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v8,v3,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrglh v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// addi r27,r1,-496
	r27.s64 = ctx.r1.s64 + -496;
	// vmrghh v3,v11,v9
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v2,v11,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvx128 v1,r0,r21
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r21.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghh v9,v6,v8
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrglh v6,v6,v8
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrghh v10,v7,v5
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// lvx128 v8,r0,r27
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r27.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglh v11,v7,v5
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsplth v24,v8,1
	_mm_store_si128((__m128i*)v24.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_set1_epi16(short(0xD0C))));
	// vmrghh v5,v4,v3
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglh v8,v4,v3
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghh v7,v12,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vaddshs v20,v24,v24
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vmrglh v4,v12,v2
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vupkhsh v21,v20
	_mm_store_si128((__m128i*)v21.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v20.s16))));
loc_82667484:
	// vsubshs v12,v10,v11
	// vsubshs v31,v9,v6
	// vsubshs v2,v11,v9
	// vsubshs v30,v6,v5
	// vsubshs v27,v0,v12
	// vsubshs v29,v5,v8
	// vsubshs v26,v0,v2
	// vsubshs v28,v8,v7
	// vmaxsh v12,v27,v12
	// vsubshs v27,v0,v31
	// vmaxsh v2,v26,v2
	// vsubshs v3,v7,v4
	// vsubshs v26,v0,v29
	// vmaxsh v31,v27,v31
	// vsubshs v27,v0,v30
	// vsubshs v25,v0,v28
	// vmaxsh v29,v26,v29
	// vcmpgtuh v12,v13,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmaxsh v30,v27,v30
	// vmaxsh v28,v25,v28
	// vsubshs v27,v0,v3
	// vcmpgtuh v2,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vcmpgtuh v31,v13,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vcmpgtuh v30,v13,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmaxsh v27,v27,v3
	// vcmpgtuh v29,v13,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vcmpgtuh v28,v13,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vaddshs v12,v12,v2
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vcmpgtuh v27,v13,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vaddshs v31,v29,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v12,v27,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v12,v12,v2
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v12,v0,v12
	// vperm v2,v12,v12,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vcmpgtsh. v23,v2,v22
	// mfocrf r24,2
	r24.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r27,r24,0,26,26
	r27.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x20;
	// stw r24,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, r24.u32);
	// cmplwi cr6,r27,32
	cr6.compare<uint32_t>(r27.u32, 32, xer);
	// beq cr6,0x826677ac
	if (cr6.eq) goto loc_826677AC;
	// vmaxsh v30,v6,v5
	// vmaxsh v12,v11,v9
	// vminsh v31,v11,v9
	// vminsh v29,v6,v5
	// vmaxsh v28,v8,v7
	// vmaxsh v12,v12,v30
	// vminsh v27,v8,v7
	// vminsh v31,v31,v29
	// vmaxsh v12,v28,v12
	// vminsh v31,v27,v31
	// vsubshs v12,v12,v31
	// vupkhsh v31,v12
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16))));
	// vupklsh v30,v12
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v12.s16)));
	// vcmpgtsh. v12,v20,v12
	// mfocrf r27,2
	r27.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r27,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, r27.u32);
	// vcmpgtsw. v31,v21,v31
	// vand v12,v12,v23
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v23.u8)));
	// mfocrf r26,2
	r26.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r26,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r26.u32);
	// vcmpgtsw. v31,v21,v30
	// mfocrf r25,2
	r25.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v31,v2
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vspltisw v30,4
	_mm_store_si128((__m128i*)v30.u32, _mm_set1_epi32(int(0x4)));
	// stw r25,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, r25.u32);
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtsw. v31,v31,v30
	// mfocrf r27,2
	r27.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r27,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, r27.u32);
	// vcmpgtsw. v2,v2,v30
	// mfocrf r20,2
	r20.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r27,r27,0,26,26
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r27,32
	cr6.compare<uint32_t>(r27.u32, 32, xer);
	// mr r27,r20
	r27.u64 = r20.u64;
	// stw r27,-252(r1)
	PPC_STORE_U32(ctx.r1.u32 + -252, r27.u32);
	// beq cr6,0x826675c8
	if (cr6.eq) goto loc_826675C8;
	// rlwinm r26,r26,0,26,26
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r26,32
	cr6.compare<uint32_t>(r26.u32, 32, xer);
	// bne cr6,0x826675e0
	if (!cr6.eq) goto loc_826675E0;
loc_826675C8:
	// rlwinm r27,r27,0,26,26
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r27,32
	cr6.compare<uint32_t>(r27.u32, 32, xer);
	// beq cr6,0x826677ac
	if (cr6.eq) goto loc_826677AC;
	// rlwinm r27,r25,0,26,26
	r27.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r27,32
	cr6.compare<uint32_t>(r27.u32, 32, xer);
	// beq cr6,0x826677ac
	if (cr6.eq) goto loc_826677AC;
loc_826675E0:
	// vsubshs v29,v0,v3
	// addi r27,r1,-448
	r27.s64 = ctx.r1.s64 + -448;
	// vsubshs v2,v11,v10
	// vaddshs v30,v9,v6
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vor v26,v4,v4
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmaxsh v29,v29,v3
	// vsubshs v3,v0,v2
	// vor v27,v10,v10
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vaddshs v31,v5,v8
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v25,v6,v7
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vmaxsh v28,v3,v2
	// vaddshs v3,v30,v22
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vcmpgtsh v2,v24,v29
	// vaddshs v19,v11,v5
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vcmpgtsh v30,v24,v28
	// vandc v29,v7,v2
	// vand v4,v4,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vandc v2,v11,v30
	// vand v10,v10,v30
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vxor v4,v4,v29
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vxor v10,v10,v2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v30,v7,v4
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v18,v8,v4
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v29,v10,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v28,v29,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v28,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsubshs v31,v10,v8
	// vaddshs v28,v10,v9
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v29,v31,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v31,v28,v2
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v28,v4,v9
	// vaddshs v29,v29,v2
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v25,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v17,v28,v30
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vsrah v31,v31,v13
	// vsrah v29,v29,v13
	// vsrah v2,v2,v13
	// vaddshs v30,v19,v3
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v28,v18,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v17,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vand v29,v29,v12
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vandc v25,v11,v12
	// vand v2,v2,v12
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v31,v31,v12
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vandc v19,v9,v12
	// vandc v18,v6,v12
	// vsrah v30,v30,v13
	// vsrah v28,v28,v13
	// vsrah v3,v3,v13
	// vxor v25,v29,v25
	_mm_store_si128((__m128i*)v25.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vxor v29,v2,v18
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vxor v31,v31,v19
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vand v30,v30,v12
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vandc v2,v5,v12
	// vand v18,v28,v12
	_mm_store_si128((__m128i*)v18.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vandc v19,v8,v12
	// vandc v17,v7,v12
	// vand v12,v3,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vpkshus v3,v31,v29
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vxor v28,v30,v2
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vxor v2,v18,v19
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vxor v30,v12,v17
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vpkshus v12,v27,v25
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vpkshus v2,v28,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vpkshus v31,v30,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vmrghb v30,v12,v3
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v3,v2,v31
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrglb v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v31,v30,v12
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrglb v12,v30,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v30.u8)));
	// stvx v31,r0,r27
	_mm_store_si128((__m128i*)(base + ((r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r27,r1,-464
	r27.s64 = ctx.r1.s64 + -464;
	// stvx v12,r0,r27
	_mm_store_si128((__m128i*)(base + ((r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v12,v3,v2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// addi r27,r1,-480
	r27.s64 = ctx.r1.s64 + -480;
	// vmrglb v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// stvx v12,r0,r27
	_mm_store_si128((__m128i*)(base + ((r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r27,r1,-416
	r27.s64 = ctx.r1.s64 + -416;
	// stvx v3,r0,r27
	_mm_store_si128((__m128i*)(base + ((r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r27,-448(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r26,-444(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -444);
	// lwz r25,-464(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -464);
	// lwz r20,-460(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -460);
	// lwz r19,-456(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -456);
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// stw r26,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r26.u32);
	// lwz r27,-440(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -440);
	// lwz r26,-436(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -436);
	// lwz r18,-452(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -452);
	// stw r27,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r27.u32);
	// stw r26,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r26.u32);
	// lwz r27,-480(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -480);
	// lwz r26,-476(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -476);
	// stw r25,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r25.u32);
	// stw r20,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r20.u32);
	// lwz r25,-472(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -472);
	// stw r19,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r19.u32);
	// lwz r20,-468(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -468);
	// stw r18,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r18.u32);
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// stw r26,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r26.u32);
	// stw r25,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r25.u32);
	// stw r20,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, r20.u32);
	// lwz r19,-416(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -416);
	// lwz r18,-412(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// lwz r27,-408(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// lwz r26,-404(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -404);
	// stw r19,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r19.u32);
	// stw r18,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r18.u32);
	// stw r27,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r27.u32);
	// stw r26,4(r29)
	PPC_STORE_U32(r29.u32 + 4, r26.u32);
	// b 0x826677b4
	goto loc_826677B4;
loc_826677AC:
	// vor v28,v5,v5
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vor v29,v6,v6
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
loc_826677B4:
	// rlwinm r27,r24,0,24,24
	r27.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r27,128
	cr6.compare<uint32_t>(r27.u32, 128, xer);
	// beq cr6,0x82667888
	if (cr6.eq) goto loc_82667888;
	// vspltisb v12,-1
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// vsubshs v3,v5,v6
	// vxor v2,v12,v23
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vsubshs v12,v0,v3
	// vmaxsh v12,v12,v3
	// vcmpgtsh v31,v12,v13
	// vcmpgtsh v30,v24,v12
	// vand v31,v31,v30
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vand v2,v31,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vcmpequh. v31,v0,v2
	// mfocrf r27,2
	r27.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r27,-244(r1)
	PPC_STORE_U32(ctx.r1.u32 + -244, r27.u32);
	// rlwinm r27,r27,0,24,24
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r27,128
	cr6.compare<int32_t>(r27.s32, 128, xer);
	// beq cr6,0x82667888
	if (cr6.eq) goto loc_82667888;
	// vspltish v31,2
	// addi r27,r1,-432
	r27.s64 = ctx.r1.s64 + -432;
	// vspltish v30,15
	// vsrah v12,v12,v31
	// vsrah v3,v3,v30
	// vaddshs v31,v12,v12
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vand v3,v3,v31
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vsubshs v12,v12,v3
	// vand v12,v12,v2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v3,v29,v12
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsubshs v12,v28,v12
	// vpkshus v3,v3,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vmrghb v12,v3,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghh v3,v0,v12
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// stvx v3,r0,r27
	_mm_store_si128((__m128i*)(base + ((r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r27,r1,-496
	r27.s64 = ctx.r1.s64 + -496;
	// stvx v12,r0,r27
	_mm_store_si128((__m128i*)(base + ((r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r27,-432(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -432);
	// lwz r26,-428(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -428);
	// lwz r25,-424(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -424);
	// lwz r24,-420(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -420);
	// lwz r20,-496(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -496);
	// sth r27,3(r11)
	PPC_STORE_U16(r11.u32 + 3, r27.u16);
	// lwz r19,-492(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -492);
	// sth r26,3(r10)
	PPC_STORE_U16(ctx.r10.u32 + 3, r26.u16);
	// lwz r27,-488(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -488);
	// lwz r26,-484(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -484);
	// sth r25,3(r9)
	PPC_STORE_U16(ctx.r9.u32 + 3, r25.u16);
	// sth r24,3(r6)
	PPC_STORE_U16(ctx.r6.u32 + 3, r24.u16);
	// sth r20,3(r3)
	PPC_STORE_U16(ctx.r3.u32 + 3, r20.u16);
	// sth r19,3(r31)
	PPC_STORE_U16(r31.u32 + 3, r19.u16);
	// sth r27,3(r30)
	PPC_STORE_U16(r30.u32 + 3, r27.u16);
	// sth r26,3(r29)
	PPC_STORE_U16(r29.u32 + 3, r26.u16);
loc_82667888:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x826679c4
	if (!cr6.eq) goto loc_826679C4;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// add r6,r6,r28
	ctx.r6.u64 = ctx.r6.u64 + r28.u64;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// add r31,r31,r28
	r31.u64 = r31.u64 + r28.u64;
	// stw r11,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r11.u32);
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// stw r10,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, ctx.r10.u32);
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stw r9,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r9.u32);
	// li r27,16
	r27.s64 = 16;
	// stw r6,-400(r1)
	PPC_STORE_U32(ctx.r1.u32 + -400, ctx.r6.u32);
	// li r26,16
	r26.s64 = 16;
	// stw r3,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, ctx.r3.u32);
	// stw r31,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r31.u32);
	// li r25,16
	r25.s64 = 16;
	// stw r30,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, r30.u32);
	// li r24,16
	r24.s64 = 16;
	// stw r29,-396(r1)
	PPC_STORE_U32(ctx.r1.u32 + -396, r29.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r11,r27
	temp.u32 = r11.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v10,r10,r26
	temp.u32 = ctx.r10.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v9,r9,r25
	temp.u32 = ctx.r9.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v8,r6,r24
	temp.u32 = ctx.r6.u32 + r24.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v7,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r31,r26
	temp.u32 = r31.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v8,r3,r27
	temp.u32 = ctx.r3.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v5,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vor v5,v5,v6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvrx v7,r30,r27
	temp.u32 = r30.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v6,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v4,v6,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v3,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r29,r26
	temp.u32 = r29.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v6,v8,v8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v8,v0,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v4,v11,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v11,v11,v8
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v8,v9,v7
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrglh v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrghh v7,v12,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v12,v12,v6
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghh v6,v10,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrglh v10,v10,v5
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghh v5,v4,v8
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglh v4,v4,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghh v8,v7,v6
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v3,v12,v10
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v2,v11,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v31,v11,v9
	_mm_store_si128((__m128i*)v31.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghh v10,v8,v5
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vmrglh v11,v8,v5
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vmrghh v9,v7,v4
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrglh v6,v7,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v5,v3,v2
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrglh v8,v3,v2
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrghh v7,v12,v31
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v4,v12,v31
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
loc_826679C4:
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// cmpw cr6,r23,r22
	cr6.compare<int32_t>(r23.s32, r22.s32, xer);
	// stw r23,-388(r1)
	PPC_STORE_U32(ctx.r1.u32 + -388, r23.u32);
	// blt cr6,0x82667484
	if (cr6.lt) goto loc_82667484;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x8266808c
	if (cr6.eq) goto loc_8266808C;
	// addi r11,r4,-4
	r11.s64 = ctx.r4.s64 + -4;
	// stw r7,-496(r1)
	PPC_STORE_U32(ctx.r1.u32 + -496, ctx.r7.u32);
	// li r26,2
	r26.s64 = 2;
	// lvx128 v1,r0,r21
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r21.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r8
	ctx.r10.u64 = r11.u64 + ctx.r8.u64;
	// li r30,16
	r30.s64 = 16;
	// add r9,r10,r8
	ctx.r9.u64 = ctx.r10.u64 + ctx.r8.u64;
	// li r29,16
	r29.s64 = 16;
	// stw r11,-380(r1)
	PPC_STORE_U32(ctx.r1.u32 + -380, r11.u32);
	// add r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r26,-236(r1)
	PPC_STORE_U32(ctx.r1.u32 + -236, r26.u32);
	// stw r10,-372(r1)
	PPC_STORE_U32(ctx.r1.u32 + -372, ctx.r10.u32);
	// li r28,16
	r28.s64 = 16;
	// add r5,r6,r8
	ctx.r5.u64 = ctx.r6.u64 + ctx.r8.u64;
	// stw r9,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, ctx.r9.u32);
	// li r27,0
	r27.s64 = 0;
	// add r4,r5,r8
	ctx.r4.u64 = ctx.r5.u64 + ctx.r8.u64;
	// stw r6,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r6.u32);
	// add r3,r4,r8
	ctx.r3.u64 = ctx.r4.u64 + ctx.r8.u64;
	// stw r5,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, ctx.r5.u32);
	// add r31,r3,r8
	r31.u64 = ctx.r3.u64 + ctx.r8.u64;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r4,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, ctx.r4.u32);
	// stw r3,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, ctx.r3.u32);
	// stw r31,-324(r1)
	PPC_STORE_U32(ctx.r1.u32 + -324, r31.u32);
	// stw r8,-228(r1)
	PPC_STORE_U32(ctx.r1.u32 + -228, ctx.r8.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v10,r10,r29
	temp.u32 = ctx.r10.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v7,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v10,r9,r30
	temp.u32 = ctx.r9.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v8,r6,r29
	temp.u32 = ctx.r6.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvrx v9,r5,r28
	temp.u32 = ctx.r5.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v5,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v6,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r4,r30
	temp.u32 = ctx.r4.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v6,v6,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vor v7,v5,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v9,r3,r29
	temp.u32 = ctx.r3.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v5,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v4,v5,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v3,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v5,r31,r30
	temp.u32 = r31.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v5,v3,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v8,v0,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r27,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r27.u32);
	// vmrghb v6,v0,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v4,v12,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v12,v12,v8
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghh v3,v11,v7
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v8,v10,v6
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghh v2,v9,v5
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrglh v10,v10,v6
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrglh v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrglh v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v7,v4,v8
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghh v6,v3,v2
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// addi r7,r1,-496
	ctx.r7.s64 = ctx.r1.s64 + -496;
	// vmrglh v8,v4,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglh v5,v3,v2
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrghh v4,v12,v10
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghh v3,v11,v9
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v2,v11,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghh v10,v7,v6
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrglh v11,v7,v6
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v9,v8,v5
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vmrglh v6,v8,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsplth v24,v8,1
	_mm_store_si128((__m128i*)v24.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_set1_epi16(short(0xD0C))));
	// vmrghh v5,v4,v3
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglh v8,v4,v3
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghh v7,v12,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vaddshs v20,v24,v24
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vmrglh v4,v12,v2
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vupkhsh v21,v20
	_mm_store_si128((__m128i*)v21.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v20.s16))));
loc_82667B3C:
	// vsubshs v12,v10,v11
	// vsubshs v31,v9,v6
	// vsubshs v2,v11,v9
	// vsubshs v30,v6,v5
	// vsubshs v27,v0,v12
	// vsubshs v29,v5,v8
	// vsubshs v26,v0,v2
	// vsubshs v28,v8,v7
	// vmaxsh v12,v27,v12
	// vsubshs v27,v0,v31
	// vmaxsh v2,v26,v2
	// vsubshs v3,v7,v4
	// vsubshs v26,v0,v29
	// vmaxsh v31,v27,v31
	// vsubshs v27,v0,v30
	// vsubshs v25,v0,v28
	// vmaxsh v29,v26,v29
	// vcmpgtuh v12,v13,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmaxsh v30,v27,v30
	// vmaxsh v28,v25,v28
	// vsubshs v27,v0,v3
	// vcmpgtuh v2,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vcmpgtuh v31,v13,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vcmpgtuh v30,v13,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmaxsh v27,v27,v3
	// vcmpgtuh v29,v13,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vcmpgtuh v28,v13,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vaddshs v12,v12,v2
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vcmpgtuh v27,v13,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vaddshs v31,v29,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v12,v27,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v12,v12,v2
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v12,v0,v12
	// vperm v2,v12,v12,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vcmpgtsh. v23,v2,v22
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r7,r28,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x20;
	// stw r28,-220(r1)
	PPC_STORE_U32(ctx.r1.u32 + -220, r28.u32);
	// cmplwi cr6,r7,32
	cr6.compare<uint32_t>(ctx.r7.u32, 32, xer);
	// beq cr6,0x82667e64
	if (cr6.eq) goto loc_82667E64;
	// vmaxsh v30,v6,v5
	// vmaxsh v12,v11,v9
	// vminsh v31,v11,v9
	// vminsh v29,v6,v5
	// vmaxsh v28,v8,v7
	// vmaxsh v12,v12,v30
	// vminsh v27,v8,v7
	// vminsh v31,v31,v29
	// vmaxsh v12,v28,v12
	// vminsh v31,v27,v31
	// vsubshs v12,v12,v31
	// vupkhsh v31,v12
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16))));
	// vupklsh v30,v12
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v12.s16)));
	// vcmpgtsh. v12,v20,v12
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r7,-212(r1)
	PPC_STORE_U32(ctx.r1.u32 + -212, ctx.r7.u32);
	// vcmpgtsw. v31,v21,v31
	// vand v12,v12,v23
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v23.u8)));
	// mfocrf r30,2
	r30.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r30,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, r30.u32);
	// vcmpgtsw. v31,v21,v30
	// mfocrf r29,2
	r29.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v31,v2
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vspltisw v30,4
	_mm_store_si128((__m128i*)v30.u32, _mm_set1_epi32(int(0x4)));
	// stw r29,-196(r1)
	PPC_STORE_U32(ctx.r1.u32 + -196, r29.u32);
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtsw. v31,v31,v30
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r7,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, ctx.r7.u32);
	// vcmpgtsw. v2,v2,v30
	// mfocrf r25,2
	r25.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r7,32
	cr6.compare<uint32_t>(ctx.r7.u32, 32, xer);
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// stw r7,-180(r1)
	PPC_STORE_U32(ctx.r1.u32 + -180, ctx.r7.u32);
	// beq cr6,0x82667c80
	if (cr6.eq) goto loc_82667C80;
	// rlwinm r30,r30,0,26,26
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r30,32
	cr6.compare<uint32_t>(r30.u32, 32, xer);
	// bne cr6,0x82667c98
	if (!cr6.eq) goto loc_82667C98;
loc_82667C80:
	// rlwinm r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r7,32
	cr6.compare<uint32_t>(ctx.r7.u32, 32, xer);
	// beq cr6,0x82667e64
	if (cr6.eq) goto loc_82667E64;
	// rlwinm r7,r29,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r7,32
	cr6.compare<uint32_t>(ctx.r7.u32, 32, xer);
	// beq cr6,0x82667e64
	if (cr6.eq) goto loc_82667E64;
loc_82667C98:
	// vsubshs v29,v0,v3
	// addi r7,r1,-496
	ctx.r7.s64 = ctx.r1.s64 + -496;
	// vsubshs v2,v11,v10
	// vaddshs v30,v9,v6
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vor v26,v4,v4
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmaxsh v29,v29,v3
	// vsubshs v3,v0,v2
	// vor v27,v10,v10
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vaddshs v31,v5,v8
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v25,v6,v7
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vmaxsh v28,v3,v2
	// vaddshs v3,v30,v22
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vcmpgtsh v2,v24,v29
	// vaddshs v19,v11,v5
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vcmpgtsh v30,v24,v28
	// vandc v29,v7,v2
	// vand v4,v4,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vandc v2,v11,v30
	// vand v10,v10,v30
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vxor v4,v4,v29
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vxor v10,v10,v2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v30,v7,v4
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v18,v8,v4
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v29,v10,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v28,v29,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v28,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsubshs v31,v10,v8
	// vaddshs v28,v10,v9
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v29,v31,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v31,v28,v2
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v28,v4,v9
	// vaddshs v29,v29,v2
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v25,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v17,v28,v30
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vsrah v31,v31,v13
	// vsrah v29,v29,v13
	// vsrah v2,v2,v13
	// vaddshs v30,v19,v3
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v28,v18,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v17,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vand v29,v29,v12
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vandc v25,v11,v12
	// vand v2,v2,v12
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v31,v31,v12
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vandc v19,v9,v12
	// vandc v18,v6,v12
	// vsrah v30,v30,v13
	// vsrah v28,v28,v13
	// vsrah v3,v3,v13
	// vxor v25,v29,v25
	_mm_store_si128((__m128i*)v25.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vxor v29,v2,v18
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vxor v31,v31,v19
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vand v30,v30,v12
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vandc v2,v5,v12
	// vand v18,v28,v12
	_mm_store_si128((__m128i*)v18.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vandc v19,v8,v12
	// vandc v17,v7,v12
	// vand v12,v3,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vpkshus v3,v31,v29
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vxor v28,v30,v2
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vxor v2,v18,v19
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vxor v30,v12,v17
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vpkshus v12,v27,v25
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vpkshus v2,v28,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vpkshus v31,v30,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vmrghb v30,v12,v3
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v3,v2,v31
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrglb v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v31,v30,v12
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrglb v12,v30,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v30.u8)));
	// stvx v31,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-432
	ctx.r7.s64 = ctx.r1.s64 + -432;
	// stvx v12,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v12,v3,v2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// addi r7,r1,-416
	ctx.r7.s64 = ctx.r1.s64 + -416;
	// vmrglb v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// stvx v12,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-448
	ctx.r7.s64 = ctx.r1.s64 + -448;
	// stvx v3,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r7,-496(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -496);
	// lwz r30,-492(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -492);
	// lwz r29,-432(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -432);
	// lwz r25,-428(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -428);
	// lwz r24,-424(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -424);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r30.u32);
	// lwz r7,-488(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -488);
	// lwz r30,-484(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -484);
	// lwz r23,-420(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -420);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// stw r30,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r30.u32);
	// lwz r7,-416(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -416);
	// lwz r30,-412(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// stw r29,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r29.u32);
	// stw r25,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r25.u32);
	// lwz r29,-408(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// stw r24,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r24.u32);
	// lwz r25,-404(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -404);
	// stw r23,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r23.u32);
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// stw r30,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r30.u32);
	// stw r29,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r29.u32);
	// stw r25,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, r25.u32);
	// lwz r24,-448(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r23,-444(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -444);
	// lwz r7,-440(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -440);
	// lwz r30,-436(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -436);
	// stw r24,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, r24.u32);
	// stw r23,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r23.u32);
	// stw r7,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r7.u32);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// b 0x82667e6c
	goto loc_82667E6C;
loc_82667E64:
	// vor v28,v5,v5
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vor v29,v6,v6
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
loc_82667E6C:
	// rlwinm r7,r28,0,24,24
	ctx.r7.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r7,128
	cr6.compare<uint32_t>(ctx.r7.u32, 128, xer);
	// beq cr6,0x82667f40
	if (cr6.eq) goto loc_82667F40;
	// vspltisb v12,-1
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// vsubshs v3,v5,v6
	// vxor v2,v12,v23
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vsubshs v12,v0,v3
	// vmaxsh v12,v12,v3
	// vcmpgtsh v31,v12,v13
	// vcmpgtsh v30,v24,v12
	// vand v31,v31,v30
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vand v2,v31,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vcmpequh. v31,v0,v2
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r7,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, ctx.r7.u32);
	// rlwinm r7,r7,0,24,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r7,128
	cr6.compare<int32_t>(ctx.r7.s32, 128, xer);
	// beq cr6,0x82667f40
	if (cr6.eq) goto loc_82667F40;
	// vspltish v31,2
	// addi r7,r1,-464
	ctx.r7.s64 = ctx.r1.s64 + -464;
	// vspltish v30,15
	// vsrah v12,v12,v31
	// vsrah v3,v3,v30
	// vaddshs v31,v12,v12
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vand v3,v3,v31
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vsubshs v12,v12,v3
	// vand v12,v12,v2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v3,v29,v12
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsubshs v12,v28,v12
	// vpkshus v3,v3,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vmrghb v12,v3,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghh v3,v0,v12
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// stvx v3,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-480
	ctx.r7.s64 = ctx.r1.s64 + -480;
	// stvx v12,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r7,-464(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -464);
	// lwz r30,-460(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -460);
	// lwz r29,-456(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -456);
	// lwz r28,-452(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -452);
	// lwz r25,-480(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -480);
	// sth r7,3(r11)
	PPC_STORE_U16(r11.u32 + 3, ctx.r7.u16);
	// lwz r24,-476(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -476);
	// sth r30,3(r10)
	PPC_STORE_U16(ctx.r10.u32 + 3, r30.u16);
	// lwz r7,-472(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -472);
	// lwz r30,-468(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -468);
	// sth r29,3(r9)
	PPC_STORE_U16(ctx.r9.u32 + 3, r29.u16);
	// sth r28,3(r6)
	PPC_STORE_U16(ctx.r6.u32 + 3, r28.u16);
	// sth r25,3(r5)
	PPC_STORE_U16(ctx.r5.u32 + 3, r25.u16);
	// sth r24,3(r4)
	PPC_STORE_U16(ctx.r4.u32 + 3, r24.u16);
	// sth r7,3(r3)
	PPC_STORE_U16(ctx.r3.u32 + 3, ctx.r7.u16);
	// sth r30,3(r31)
	PPC_STORE_U16(r31.u32 + 3, r30.u16);
loc_82667F40:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x8266807c
	if (!cr6.eq) goto loc_8266807C;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// stw r11,-380(r1)
	PPC_STORE_U32(ctx.r1.u32 + -380, r11.u32);
	// add r3,r3,r8
	ctx.r3.u64 = ctx.r3.u64 + ctx.r8.u64;
	// stw r10,-372(r1)
	PPC_STORE_U32(ctx.r1.u32 + -372, ctx.r10.u32);
	// add r31,r31,r8
	r31.u64 = r31.u64 + ctx.r8.u64;
	// stw r9,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, ctx.r9.u32);
	// li r7,16
	ctx.r7.s64 = 16;
	// stw r6,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r6.u32);
	// li r30,16
	r30.s64 = 16;
	// stw r5,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, ctx.r5.u32);
	// stw r4,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, ctx.r4.u32);
	// li r29,16
	r29.s64 = 16;
	// stw r3,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, ctx.r3.u32);
	// li r28,16
	r28.s64 = 16;
	// stw r31,-324(r1)
	PPC_STORE_U32(ctx.r1.u32 + -324, r31.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v10,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v9,r9,r29
	temp.u32 = ctx.r9.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v8,r6,r28
	temp.u32 = ctx.r6.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v7,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r4,r30
	temp.u32 = ctx.r4.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v8,r5,r7
	temp.u32 = ctx.r5.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v5,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vor v5,v5,v6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvrx v7,r3,r7
	temp.u32 = ctx.r3.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v6,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v4,v6,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v3,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r31,r30
	temp.u32 = r31.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v6,v8,v8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v8,v0,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v4,v11,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v11,v11,v8
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v8,v9,v7
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrglh v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrghh v7,v12,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v12,v12,v6
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghh v6,v10,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrglh v10,v10,v5
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghh v5,v4,v8
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglh v4,v4,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghh v8,v7,v6
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v3,v12,v10
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v2,v11,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v31,v11,v9
	_mm_store_si128((__m128i*)v31.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghh v10,v8,v5
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vmrglh v11,v8,v5
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vmrghh v9,v7,v4
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrglh v6,v7,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v5,v3,v2
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrglh v8,v3,v2
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrghh v7,v12,v31
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v4,v12,v31
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
loc_8266807C:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmpw cr6,r27,r26
	cr6.compare<int32_t>(r27.s32, r26.s32, xer);
	// stw r27,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r27.u32);
	// blt cr6,0x82667b3c
	if (cr6.lt) goto loc_82667B3C;
loc_8266808C:
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_82668090"))) PPC_WEAK_FUNC(sub_82668090);
PPC_FUNC_IMPL(__imp__sub_82668090) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcd0
	// lis r11,-32243
	r11.s64 = -2113077248;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// addi r25,r11,3824
	r25.s64 = r11.s64 + 3824;
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// beq cr6,0x826685f8
	if (cr6.eq) goto loc_826685F8;
	// stw r7,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, ctx.r7.u32);
	// subf r11,r8,r4
	r11.s64 = ctx.r4.s64 - ctx.r8.s64;
	// add r6,r4,r8
	ctx.r6.u64 = ctx.r4.u64 + ctx.r8.u64;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// subf r9,r8,r11
	ctx.r9.s64 = r11.s64 - ctx.r8.s64;
	// lvx128 v1,r0,r25
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r3,r6,r8
	ctx.r3.u64 = ctx.r6.u64 + ctx.r8.u64;
	// vspltish v12,3
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// vspltish v24,4
	// add r29,r3,r8
	r29.u64 = ctx.r3.u64 + ctx.r8.u64;
	// stw r11,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, r11.u32);
	// subf r30,r8,r10
	r30.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r6,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, ctx.r6.u32);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// stw r9,-216(r1)
	PPC_STORE_U32(ctx.r1.u32 + -216, ctx.r9.u32);
	// li r22,1
	r22.s64 = 1;
	// stw r3,-228(r1)
	PPC_STORE_U32(ctx.r1.u32 + -228, ctx.r3.u32);
	// li r28,16
	r28.s64 = 16;
	// stw r10,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, ctx.r10.u32);
	// li r26,16
	r26.s64 = 16;
	// stw r29,-248(r1)
	PPC_STORE_U32(ctx.r1.u32 + -248, r29.u32);
	// stw r30,-236(r1)
	PPC_STORE_U32(ctx.r1.u32 + -236, r30.u32);
	// li r27,16
	r27.s64 = 16;
	// stw r31,-232(r1)
	PPC_STORE_U32(ctx.r1.u32 + -232, r31.u32);
	// li r24,16
	r24.s64 = 16;
	// stw r22,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r22.u32);
	// lvlx v11,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v13,r30,r28
	temp.u32 = r30.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r23,16
	r23.s64 = 16;
	// vor v20,v11,v13
	_mm_store_si128((__m128i*)v20.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v8,r9,r26
	temp.u32 = ctx.r9.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r21,16
	r21.s64 = 16;
	// vor v18,v13,v8
	_mm_store_si128((__m128i*)v18.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvrx v10,r10,r27
	temp.u32 = ctx.r10.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltisw v23,4
	_mm_store_si128((__m128i*)v23.u32, _mm_set1_epi32(int(0x4)));
	// lvrx v11,r11,r24
	temp.u32 = r11.u32 + r24.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v19,v9,v10
	_mm_store_si128((__m128i*)v19.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v13,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v5,v0,v20
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v17,v13,v11
	_mm_store_si128((__m128i*)v17.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r4,r23
	temp.u32 = ctx.r4.u32 + r23.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r23,0
	r23.s64 = 0;
	// vor v16,v13,v10
	_mm_store_si128((__m128i*)v16.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v11,r6,r21
	temp.u32 = ctx.r6.u32 + r21.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v15,v13,v11
	_mm_store_si128((__m128i*)v15.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v13,r3,r28
	temp.u32 = ctx.r3.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v7,v0,v17
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v14,v11,v13
	_mm_store_si128((__m128i*)v14.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v11,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v6,v0,v16
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v15
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v14
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r28,r1,-368
	r28.s64 = ctx.r1.s64 + -368;
	// lvx128 v10,r0,r28
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r28,16
	r28.s64 = 16;
	// vsplth v26,v10,1
	_mm_store_si128((__m128i*)v26.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_set1_epi16(short(0xD0C))));
	// vmrghb v10,v0,v18
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v13,r29,r28
	temp.u32 = r29.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r28,r1,-368
	r28.s64 = ctx.r1.s64 + -368;
	// vor v13,v11,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v0,v19
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v21,v26,v26
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// stvx v13,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v4,v0,v13
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupkhsh v22,v21
	_mm_store_si128((__m128i*)v22.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)v21.s16))));
loc_826681D4:
	// vsubshs v13,v5,v11
	// li r12,-304
	r12.s64 = -304;
	// stvx128 v11,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v31,v10,v7
	// vsubshs v2,v11,v10
	// vsubshs v30,v7,v6
	// vsubshs v27,v0,v13
	// vsubshs v29,v6,v9
	// vsubshs v25,v0,v2
	// vsubshs v28,v9,v8
	// vmaxsh v13,v27,v13
	// vsubshs v27,v0,v31
	// vmaxsh v2,v25,v2
	// vsubshs v25,v0,v29
	// vsubshs v3,v8,v4
	// vmaxsh v31,v27,v31
	// vsubshs v27,v0,v30
	// vsubshs v11,v0,v28
	// vmaxsh v29,v25,v29
	// vcmpgtuh v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmaxsh v30,v27,v30
	// vmaxsh v28,v11,v28
	// vsubshs v27,v0,v3
	// vcmpgtuh v2,v12,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vcmpgtuh v31,v12,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vcmpgtuh v30,v12,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmaxsh v27,v27,v3
	// vcmpgtuh v29,v12,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vcmpgtuh v28,v12,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vcmpgtuh v27,v12,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vaddshs v31,v29,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v13,v27,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v13,v0,v13
	// vperm v2,v13,v13,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vcmpgtsh. v25,v2,v24
	// mfocrf r24,2
	r24.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r28,r24,0,26,26
	r28.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x20;
	// li r12,-304
	r12.s64 = -304;
	// lvx128 v11,r1,r12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r24,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r24.u32);
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// beq cr6,0x826684d4
	if (cr6.eq) goto loc_826684D4;
	// vmaxsh v30,v7,v6
	// vmaxsh v13,v11,v10
	// vminsh v31,v11,v10
	// vminsh v29,v7,v6
	// vmaxsh v28,v9,v8
	// vmaxsh v13,v13,v30
	// vminsh v27,v9,v8
	// vminsh v31,v31,v29
	// vmaxsh v13,v28,v13
	// vminsh v31,v27,v31
	// vsubshs v13,v13,v31
	// vupkhsh v31,v13
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// vupklsh v30,v13
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v13.s16)));
	// vcmpgtsh. v13,v21,v13
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r28,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, r28.u32);
	// vcmpgtsw. v31,v22,v31
	// vand v13,v13,v25
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v25.u8)));
	// mfocrf r27,2
	r27.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r27,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, r27.u32);
	// vcmpgtsw. v31,v22,v30
	// mfocrf r26,2
	r26.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v31,v2
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// stw r26,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, r26.u32);
	// vcmpgtsw. v31,v31,v23
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r28,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, r28.u32);
	// vcmpgtsw. v2,v2,v23
	// mfocrf r21,2
	r21.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r28,r28,0,26,26
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// mr r28,r21
	r28.u64 = r21.u64;
	// stw r28,-136(r1)
	PPC_STORE_U32(ctx.r1.u32 + -136, r28.u32);
	// beq cr6,0x82668324
	if (cr6.eq) goto loc_82668324;
	// rlwinm r27,r27,0,26,26
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r27,32
	cr6.compare<uint32_t>(r27.u32, 32, xer);
	// bne cr6,0x8266833c
	if (!cr6.eq) goto loc_8266833C;
loc_82668324:
	// rlwinm r28,r28,0,26,26
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// beq cr6,0x826684d4
	if (cr6.eq) goto loc_826684D4;
	// rlwinm r28,r26,0,26,26
	r28.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r28,32
	cr6.compare<uint32_t>(r28.u32, 32, xer);
	// beq cr6,0x826684d4
	if (cr6.eq) goto loc_826684D4;
loc_8266833C:
	// vsubshs v29,v0,v3
	// li r12,-288
	r12.s64 = -288;
	// stvx128 v0,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v2,v11,v5
	// addi r28,r1,-336
	r28.s64 = ctx.r1.s64 + -336;
	// vaddshs v30,v10,v7
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v31,v6,v9
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vmaxsh v29,v29,v3
	// vsubshs v3,v0,v2
	// vaddshs v27,v11,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vandc128 v63,v10,v13
	_mm_store_si128((__m128i*)v63.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vandc128 v62,v7,v13
	_mm_store_si128((__m128i*)v62.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmaxsh v28,v3,v2
	// vaddshs v3,v30,v24
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vcmpgtsh v2,v26,v29
	// vandc128 v61,v8,v13
	_mm_store_si128((__m128i*)v61.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vcmpgtsh v30,v26,v28
	// vandc v29,v8,v2
	// vand v4,v4,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vandc v2,v11,v30
	// vand v5,v5,v30
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vxor v4,v4,v29
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vxor v5,v5,v2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v30,v8,v4
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v0,v9,v4
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v29,v5,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v28,v29,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v28,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsubshs v31,v5,v9
	// vaddshs v28,v7,v8
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v31,v31,v29
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v5,v10
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v31,v31,v2
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v29,v29,v2
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v28,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v28,v4,v10
	// vsrah v31,v31,v12
	// vsrah v29,v29,v12
	// vsrah v2,v2,v12
	// vaddshs v30,v28,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v28,v0,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vand v31,v31,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v29,v29,v13
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r12,-304
	r12.s64 = -304;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v27,v3
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsrah v28,v28,v12
	// vand v2,v2,v13
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor128 v29,v29,v63
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v63.u8)));
	// vsrah v30,v30,v12
	// vandc128 v63,v9,v13
	_mm_store_si128((__m128i*)v63.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vand v28,v28,v13
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v30,v30,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r12,-304
	r12.s64 = -304;
	// lvx128 v27,r1,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v3,v27,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vandc v27,v11,v13
	// vsrah v3,v3,v12
	// vxor v31,v31,v27
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vandc v27,v6,v13
	// vand128 v60,v3,v13
	_mm_store_si128((__m128i*)v60.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v31,v31,v29
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vxor128 v13,v2,v62
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v62.u8)));
	// vxor v3,v30,v27
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vxor128 v2,v28,v63
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v63.u8)));
	// vxor128 v30,v60,v61
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v60.u8), _mm_load_si128((__m128i*)v61.u8)));
	// stvx v31,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v31,v13,v3
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// addi r28,r1,-320
	r28.s64 = ctx.r1.s64 + -320;
	// vpkshus v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// stvx v31,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,-352
	r28.s64 = ctx.r1.s64 + -352;
	// stvx v2,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r28,-336(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// lwz r27,-328(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// lwz r26,-332(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// lwz r21,-324(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// stw r28,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r28.u32);
	// stw r27,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r27.u32);
	// lwz r28,-320(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// lwz r27,-312(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// lwz r20,-352(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// lwz r19,-344(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// li r12,-288
	r12.s64 = -288;
	// lvx128 v0,r1,r12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r28,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r28.u32);
	// lwz r18,-316(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// stw r27,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r27.u32);
	// lwz r28,-308(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -308);
	// stw r20,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r20.u32);
	// lwz r27,-348(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// lwz r20,-340(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	// stw r19,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r19.u32);
	// stw r26,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r26.u32);
	// stw r21,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r21.u32);
	// stw r18,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r18.u32);
	// stw r28,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r28.u32);
	// stw r27,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, r27.u32);
	// stw r20,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r20.u32);
	// b 0x826684dc
	goto loc_826684DC;
loc_826684D4:
	// vor v3,v6,v6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v13,v7,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
loc_826684DC:
	// rlwinm r28,r24,0,24,24
	r28.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r28,128
	cr6.compare<uint32_t>(r28.u32, 128, xer);
	// beq cr6,0x82668578
	if (cr6.eq) goto loc_82668578;
	// vspltisb v31,-1
	_mm_store_si128((__m128i*)v31.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// vsubshs v2,v6,v7
	// vxor v30,v31,v25
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vsubshs v31,v0,v2
	// vmaxsh v31,v31,v2
	// vcmpgtsh v29,v31,v12
	// vcmpgtsh v28,v26,v31
	// vand v29,v29,v28
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vand v30,v29,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vcmpequh. v29,v0,v30
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r28,-196(r1)
	PPC_STORE_U32(ctx.r1.u32 + -196, r28.u32);
	// rlwinm r28,r28,0,24,24
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r28,128
	cr6.compare<int32_t>(r28.s32, 128, xer);
	// beq cr6,0x82668578
	if (cr6.eq) goto loc_82668578;
	// vspltish v29,2
	// addi r28,r1,-352
	r28.s64 = ctx.r1.s64 + -352;
	// vspltish v28,15
	// vsrah v31,v31,v29
	// vsrah v2,v2,v28
	// vaddshs v29,v31,v31
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vand v2,v2,v29
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vsubshs v2,v31,v2
	// vand v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v3,v3,v2
	// vpkshus v13,v13,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r28,-352(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// lwz r27,-348(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// lwz r26,-344(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// lwz r24,-340(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	// stw r28,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r28.u32);
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// stw r26,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r26.u32);
	// stw r24,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r24.u32);
loc_82668578:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x826685c8
	if (!cr6.eq) goto loc_826685C8;
	// addi r28,r1,-368
	r28.s64 = ctx.r1.s64 + -368;
	// vmrglb v5,v0,v20
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// vmrglb v11,v0,v19
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// vmrglb v10,v0,v18
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// vmrglb v7,v0,v17
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// vmrglb v6,v0,v16
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v13,r0,r28
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// vmrglb v9,v0,v15
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// vmrglb v8,v0,v14
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// vmrglb v4,v0,v13
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_826685C8:
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// cmpw cr6,r23,r22
	cr6.compare<int32_t>(r23.s32, r22.s32, xer);
	// blt cr6,0x826681d4
	if (cr6.lt) goto loc_826681D4;
	// stw r23,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, r23.u32);
	// stw r29,-248(r1)
	PPC_STORE_U32(ctx.r1.u32 + -248, r29.u32);
	// stw r3,-228(r1)
	PPC_STORE_U32(ctx.r1.u32 + -228, ctx.r3.u32);
	// stw r6,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, ctx.r6.u32);
	// stw r31,-232(r1)
	PPC_STORE_U32(ctx.r1.u32 + -232, r31.u32);
	// stw r11,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, r11.u32);
	// stw r9,-216(r1)
	PPC_STORE_U32(ctx.r1.u32 + -216, ctx.r9.u32);
	// stw r10,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, ctx.r10.u32);
	// stw r30,-236(r1)
	PPC_STORE_U32(ctx.r1.u32 + -236, r30.u32);
loc_826685F8:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82668cb8
	if (cr6.eq) goto loc_82668CB8;
	// addi r11,r4,-4
	r11.s64 = ctx.r4.s64 + -4;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// li r26,1
	r26.s64 = 1;
	// vspltisb v17,-1
	_mm_store_si128((__m128i*)v17.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// add r10,r11,r8
	ctx.r10.u64 = r11.u64 + ctx.r8.u64;
	// vspltish v18,2
	// li r30,16
	r30.s64 = 16;
	// vspltish v12,3
	// add r9,r10,r8
	ctx.r9.u64 = ctx.r10.u64 + ctx.r8.u64;
	// vspltish v22,4
	// stw r11,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, r11.u32);
	// li r29,16
	r29.s64 = 16;
	// add r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r26,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, r26.u32);
	// stw r10,-220(r1)
	PPC_STORE_U32(ctx.r1.u32 + -220, ctx.r10.u32);
	// li r27,0
	r27.s64 = 0;
	// add r5,r6,r8
	ctx.r5.u64 = ctx.r6.u64 + ctx.r8.u64;
	// vspltisw v21,4
	_mm_store_si128((__m128i*)v21.u32, _mm_set1_epi32(int(0x4)));
	// stw r9,-252(r1)
	PPC_STORE_U32(ctx.r1.u32 + -252, ctx.r9.u32);
	// add r4,r5,r8
	ctx.r4.u64 = ctx.r5.u64 + ctx.r8.u64;
	// stw r7,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, ctx.r7.u32);
	// stw r6,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r6.u32);
	// add r3,r4,r8
	ctx.r3.u64 = ctx.r4.u64 + ctx.r8.u64;
	// stw r5,-244(r1)
	PPC_STORE_U32(ctx.r1.u32 + -244, ctx.r5.u32);
	// add r31,r3,r8
	r31.u64 = ctx.r3.u64 + ctx.r8.u64;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r4,-212(r1)
	PPC_STORE_U32(ctx.r1.u32 + -212, ctx.r4.u32);
	// stw r3,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, ctx.r3.u32);
	// stw r31,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, r31.u32);
	// stw r8,-148(r1)
	PPC_STORE_U32(ctx.r1.u32 + -148, ctx.r8.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v13,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v13,v11,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v10,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v11,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v10,r9,r29
	temp.u32 = ctx.r9.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v8,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r6,r30
	temp.u32 = ctx.r6.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v13,v0,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v7,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r5,r29
	temp.u32 = ctx.r5.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v6,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r4,r30
	temp.u32 = ctx.r4.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v5,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r3,r29
	temp.u32 = ctx.r3.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvlx v4,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v5,r31,r30
	temp.u32 = r31.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r27,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, r27.u32);
	// vmrghb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v4,v13,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrghb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v3,v11,v7
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v2,v10,v6
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrglh v10,v10,v6
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghh v1,v9,v5
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrglh v13,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrglh v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrghh v8,v4,v2
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghh v6,v3,v1
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrglh v7,v4,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghh v4,v13,v10
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrglh v5,v3,v1
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrglh v13,v13,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrghh v3,v11,v9
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v2,v11,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v10,v8,v6
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vmrglh v11,v8,v6
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// addi r7,r1,-368
	ctx.r7.s64 = ctx.r1.s64 + -368;
	// vmrghh v9,v7,v5
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrglh v6,v7,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// lvx128 v1,r0,r25
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghh v5,v4,v3
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghh v7,v13,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsplth v24,v8,1
	_mm_store_si128((__m128i*)v24.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_set1_epi16(short(0xD0C))));
	// vmrglh v8,v4,v3
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglh v4,v13,v2
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vaddshs v19,v24,v24
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vupkhsh v20,v19
	_mm_store_si128((__m128i*)v20.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)v19.s16))));
loc_82668774:
	// vsubshs v13,v10,v11
	// vsubshs v31,v9,v6
	// vsubshs v2,v11,v9
	// vsubshs v30,v6,v5
	// vsubshs v27,v0,v13
	// vsubshs v29,v5,v8
	// vsubshs v26,v0,v2
	// vsubshs v28,v8,v7
	// vmaxsh v13,v27,v13
	// vsubshs v27,v0,v31
	// vmaxsh v2,v26,v2
	// vsubshs v3,v7,v4
	// vsubshs v26,v0,v29
	// vmaxsh v31,v27,v31
	// vsubshs v27,v0,v30
	// vsubshs v25,v0,v28
	// vmaxsh v29,v26,v29
	// vcmpgtuh v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmaxsh v30,v27,v30
	// vmaxsh v28,v25,v28
	// vsubshs v27,v0,v3
	// vcmpgtuh v2,v12,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vcmpgtuh v31,v12,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vcmpgtuh v30,v12,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmaxsh v27,v27,v3
	// vcmpgtuh v29,v12,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vcmpgtuh v28,v12,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vcmpgtuh v27,v12,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vaddshs v31,v29,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v13,v27,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v13,v0,v13
	// vperm v2,v13,v13,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vcmpgtsh. v23,v2,v22
	// mfocrf r28,2
	r28.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r7,r28,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x20;
	// stw r28,-180(r1)
	PPC_STORE_U32(ctx.r1.u32 + -180, r28.u32);
	// cmplwi cr6,r7,32
	cr6.compare<uint32_t>(ctx.r7.u32, 32, xer);
	// beq cr6,0x82668a98
	if (cr6.eq) goto loc_82668A98;
	// vmaxsh v30,v6,v5
	// vmaxsh v13,v11,v9
	// vminsh v31,v11,v9
	// vminsh v29,v6,v5
	// vmaxsh v28,v8,v7
	// vmaxsh v13,v13,v30
	// vminsh v27,v8,v7
	// vminsh v31,v31,v29
	// vmaxsh v13,v28,v13
	// vminsh v31,v27,v31
	// vsubshs v13,v13,v31
	// vupkhsh v31,v13
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// vupklsh v30,v13
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v13.s16)));
	// vcmpgtsh. v13,v19,v13
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r7,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, ctx.r7.u32);
	// vcmpgtsw. v31,v20,v31
	// vand v13,v13,v23
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v23.u8)));
	// mfocrf r30,2
	r30.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r30,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, r30.u32);
	// vcmpgtsw. v31,v20,v30
	// mfocrf r29,2
	r29.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v31,v2
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// stw r29,-140(r1)
	PPC_STORE_U32(ctx.r1.u32 + -140, r29.u32);
	// vcmpgtsw. v31,v31,v21
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r7,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, ctx.r7.u32);
	// vcmpgtsw. v2,v2,v21
	// mfocrf r25,2
	r25.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r7,32
	cr6.compare<uint32_t>(ctx.r7.u32, 32, xer);
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// stw r7,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r7.u32);
	// beq cr6,0x826688b4
	if (cr6.eq) goto loc_826688B4;
	// rlwinm r30,r30,0,26,26
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r30,32
	cr6.compare<uint32_t>(r30.u32, 32, xer);
	// bne cr6,0x826688cc
	if (!cr6.eq) goto loc_826688CC;
loc_826688B4:
	// rlwinm r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r7,32
	cr6.compare<uint32_t>(ctx.r7.u32, 32, xer);
	// beq cr6,0x82668a98
	if (cr6.eq) goto loc_82668A98;
	// rlwinm r7,r29,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r7,32
	cr6.compare<uint32_t>(ctx.r7.u32, 32, xer);
	// beq cr6,0x82668a98
	if (cr6.eq) goto loc_82668A98;
loc_826688CC:
	// vsubshs v29,v0,v3
	// addi r7,r1,-320
	ctx.r7.s64 = ctx.r1.s64 + -320;
	// vsubshs v2,v11,v10
	// vaddshs v30,v9,v6
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vor v26,v4,v4
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmaxsh v29,v29,v3
	// vsubshs v3,v0,v2
	// vor v27,v10,v10
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vaddshs v31,v5,v8
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v25,v6,v7
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vmaxsh v28,v3,v2
	// vaddshs v3,v30,v22
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vcmpgtsh v2,v24,v29
	// vaddshs v16,v11,v5
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vcmpgtsh v30,v24,v28
	// vandc v29,v7,v2
	// vand v4,v4,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vandc v2,v11,v30
	// vand v10,v10,v30
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vxor v4,v4,v29
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vxor v10,v10,v2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v30,v7,v4
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v15,v8,v4
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v29,v10,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v28,v29,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v28,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsubshs v31,v10,v8
	// vaddshs v28,v10,v9
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v29,v31,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v31,v28,v2
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubshs v28,v4,v9
	// vaddshs v29,v29,v2
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v25,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v14,v28,v30
	_mm_store_si128((__m128i*)v14.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vsrah v31,v31,v12
	// vsrah v29,v29,v12
	// vsrah v2,v2,v12
	// vaddshs v30,v16,v3
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v28,v15,v3
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v14,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v14.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vand v29,v29,v13
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v25,v11,v13
	// vand v2,v2,v13
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v31,v31,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v16,v9,v13
	// vandc v15,v6,v13
	// vsrah v30,v30,v12
	// vsrah v28,v28,v12
	// vsrah v3,v3,v12
	// vxor v25,v29,v25
	_mm_store_si128((__m128i*)v25.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vxor v29,v2,v15
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vxor v31,v31,v16
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vand v30,v30,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v2,v5,v13
	// vand v15,v28,v13
	_mm_store_si128((__m128i*)v15.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v16,v8,v13
	// vandc v14,v7,v13
	// vand v13,v3,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v3,v31,v29
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vxor v28,v30,v2
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vxor v2,v15,v16
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vxor v30,v13,v14
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vpkshus v13,v27,v25
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vpkshus v2,v28,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vpkshus v31,v30,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vmrghb v30,v13,v3
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v3,v2,v31
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrglb v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v31,v30,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrglb v13,v30,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v30.u8)));
	// stvx v31,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-336
	ctx.r7.s64 = ctx.r1.s64 + -336;
	// stvx v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v13,v3,v2
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// addi r7,r1,-352
	ctx.r7.s64 = ctx.r1.s64 + -352;
	// vmrglb v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// stvx v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-288
	ctx.r7.s64 = ctx.r1.s64 + -288;
	// stvx v3,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r7,-320(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// lwz r30,-316(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// lwz r29,-336(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// lwz r25,-332(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// lwz r24,-328(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r30.u32);
	// lwz r7,-312(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// lwz r30,-308(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -308);
	// lwz r23,-324(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// stw r30,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r30.u32);
	// lwz r7,-352(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// lwz r30,-348(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// stw r29,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r29.u32);
	// stw r25,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r25.u32);
	// lwz r29,-344(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// stw r24,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r24.u32);
	// lwz r25,-340(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	// stw r23,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r23.u32);
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// stw r30,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r30.u32);
	// stw r29,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r29.u32);
	// stw r25,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, r25.u32);
	// lwz r24,-288(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// lwz r23,-284(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// lwz r7,-280(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// lwz r30,-276(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// stw r24,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, r24.u32);
	// stw r23,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r23.u32);
	// stw r7,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r7.u32);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// b 0x82668aa0
	goto loc_82668AA0;
loc_82668A98:
	// vor v28,v5,v5
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vor v29,v6,v6
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
loc_82668AA0:
	// rlwinm r7,r28,0,24,24
	ctx.r7.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r7,128
	cr6.compare<uint32_t>(ctx.r7.u32, 128, xer);
	// beq cr6,0x82668b6c
	if (cr6.eq) goto loc_82668B6C;
	// vsubshs v3,v5,v6
	// vxor v2,v17,v23
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vsubshs v13,v0,v3
	// vmaxsh v13,v13,v3
	// vcmpgtsh v31,v13,v12
	// vcmpgtsh v30,v24,v13
	// vand v31,v31,v30
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vand v2,v31,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vcmpequh. v31,v0,v2
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r7,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, ctx.r7.u32);
	// rlwinm r7,r7,0,24,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r7,128
	cr6.compare<int32_t>(ctx.r7.s32, 128, xer);
	// beq cr6,0x82668b6c
	if (cr6.eq) goto loc_82668B6C;
	// vspltish v31,15
	// vsrah v13,v13,v18
	// addi r7,r1,-304
	ctx.r7.s64 = ctx.r1.s64 + -304;
	// vsrah v3,v3,v31
	// vaddshs v31,v13,v13
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vand v3,v3,v31
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vsubshs v13,v13,v3
	// vand v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v3,v29,v13
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vsubshs v13,v28,v13
	// vpkshus v3,v3,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vmrghb v13,v3,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghh v3,v0,v13
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v13,v0,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// stvx v3,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-368
	ctx.r7.s64 = ctx.r1.s64 + -368;
	// stvx v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r7,-304(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// lwz r30,-300(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// lwz r29,-296(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// lwz r28,-292(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// lwz r25,-368(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// sth r7,3(r11)
	PPC_STORE_U16(r11.u32 + 3, ctx.r7.u16);
	// lwz r24,-364(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// sth r30,3(r10)
	PPC_STORE_U16(ctx.r10.u32 + 3, r30.u16);
	// lwz r7,-360(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// lwz r30,-356(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -356);
	// sth r29,3(r9)
	PPC_STORE_U16(ctx.r9.u32 + 3, r29.u16);
	// sth r28,3(r6)
	PPC_STORE_U16(ctx.r6.u32 + 3, r28.u16);
	// sth r25,3(r5)
	PPC_STORE_U16(ctx.r5.u32 + 3, r25.u16);
	// sth r24,3(r4)
	PPC_STORE_U16(ctx.r4.u32 + 3, r24.u16);
	// sth r7,3(r3)
	PPC_STORE_U16(ctx.r3.u32 + 3, ctx.r7.u16);
	// sth r30,3(r31)
	PPC_STORE_U16(r31.u32 + 3, r30.u16);
loc_82668B6C:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x82668ca8
	if (!cr6.eq) goto loc_82668CA8;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// stw r11,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, r11.u32);
	// add r3,r3,r8
	ctx.r3.u64 = ctx.r3.u64 + ctx.r8.u64;
	// stw r10,-220(r1)
	PPC_STORE_U32(ctx.r1.u32 + -220, ctx.r10.u32);
	// add r31,r31,r8
	r31.u64 = r31.u64 + ctx.r8.u64;
	// stw r9,-252(r1)
	PPC_STORE_U32(ctx.r1.u32 + -252, ctx.r9.u32);
	// li r7,16
	ctx.r7.s64 = 16;
	// stw r6,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r6.u32);
	// li r30,16
	r30.s64 = 16;
	// stw r5,-244(r1)
	PPC_STORE_U32(ctx.r1.u32 + -244, ctx.r5.u32);
	// stw r4,-212(r1)
	PPC_STORE_U32(ctx.r1.u32 + -212, ctx.r4.u32);
	// li r29,16
	r29.s64 = 16;
	// stw r3,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, ctx.r3.u32);
	// li r28,16
	r28.s64 = 16;
	// stw r31,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, r31.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v13,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v13,v11,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v10,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v9,r9,r29
	temp.u32 = ctx.r9.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v8,r6,r28
	temp.u32 = ctx.r6.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v13,v0,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v7,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r4,r30
	temp.u32 = ctx.r4.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v8,r5,r7
	temp.u32 = ctx.r5.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v5,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vor v5,v5,v6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvrx v7,r3,r7
	temp.u32 = ctx.r3.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v6,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v4,v6,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v3,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r31,r30
	temp.u32 = r31.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v6,v8,v8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v8,v0,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v4,v11,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v11,v11,v8
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v8,v9,v7
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrglh v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrghh v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrglh v13,v13,v6
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrghh v6,v10,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrglh v10,v10,v5
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghh v5,v4,v8
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglh v4,v4,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghh v8,v7,v6
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v3,v13,v10
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrglh v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v2,v11,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v31,v11,v9
	_mm_store_si128((__m128i*)v31.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v13,v13,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrghh v10,v8,v5
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vmrglh v11,v8,v5
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vmrghh v9,v7,v4
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrglh v6,v7,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v5,v3,v2
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrglh v8,v3,v2
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrghh v7,v13,v31
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrglh v4,v13,v31
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
loc_82668CA8:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmpw cr6,r27,r26
	cr6.compare<int32_t>(r27.s32, r26.s32, xer);
	// stw r27,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, r27.u32);
	// blt cr6,0x82668774
	if (cr6.lt) goto loc_82668774;
loc_82668CB8:
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_82668CBC"))) PPC_WEAK_FUNC(sub_82668CBC);
PPC_FUNC_IMPL(__imp__sub_82668CBC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82668CC0"))) PPC_WEAK_FUNC(sub_82668CC0);
PPC_FUNC_IMPL(__imp__sub_82668CC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-624(r1)
	ea = -624 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// stw r4,652(r1)
	PPC_STORE_U32(ctx.r1.u32 + 652, ctx.r4.u32);
	// stw r5,660(r1)
	PPC_STORE_U32(ctx.r1.u32 + 660, ctx.r5.u32);
	// li r29,4
	r29.s64 = 4;
	// addi r22,r11,3824
	r22.s64 = r11.s64 + 3824;
	// stw r7,676(r1)
	PPC_STORE_U32(ctx.r1.u32 + 676, ctx.r7.u32);
	// stw r8,684(r1)
	PPC_STORE_U32(ctx.r1.u32 + 684, ctx.r8.u32);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// stw r22,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r22.u32);
	// beq cr6,0x826693c4
	if (cr6.eq) goto loc_826693C4;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// subf r9,r8,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r8.s64;
	// add r5,r4,r8
	ctx.r5.u64 = ctx.r4.u64 + ctx.r8.u64;
	// stw r4,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r4.u32);
	// subf r7,r8,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r8.s64;
	// lvx128 v13,r0,r22
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r31,r5,r8
	r31.u64 = ctx.r5.u64 + ctx.r8.u64;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// subf r6,r8,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r8.s64;
	// vspltish v14,2
	// add r30,r31,r8
	r30.u64 = r31.u64 + ctx.r8.u64;
	// stw r9,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r9.u32);
	// subf r10,r8,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r8.s64;
	// stw r5,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r5.u32);
	// add r11,r30,r8
	r11.u64 = r30.u64 + ctx.r8.u64;
	// stw r7,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r7.u32);
	// subf r4,r8,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r8.s64;
	// vspltish v6,3
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// stw r6,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r6.u32);
	// vspltish v7,4
	// stw r31,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r31.u32);
	// stw r10,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r10.u32);
	// vspltish v1,8
	// stw r11,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r30,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, r30.u32);
	// li r3,2
	ctx.r3.s64 = 2;
	// stvx v13,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r29,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, r29.u32);
	// stw r11,32(r1)
	PPC_STORE_U32(ctx.r1.u32 + 32, r11.u32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vsplth v16,v13,1
	_mm_store_si128((__m128i*)v16.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xD0C))));
	// vaddshs v13,v16,v16
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vupkhsh v18,v13
	_mm_store_si128((__m128i*)v18.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// stvx v18,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x82668da4
	goto loc_82668DA4;
loc_82668D90:
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r7,172(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// lwz r30,168(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// lwz r31,164(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// lwz r5,188(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
loc_82668DA4:
	// lwz r8,52(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// lvlx v12,r4,r11
	temp.u32 = ctx.r4.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,r7,r11
	temp.u32 = ctx.r7.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v28,v0,v12
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v13,r10,r11
	temp.u32 = ctx.r10.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v12,v0,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v11,r6,r11
	temp.u32 = ctx.r6.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v8,v0,v13
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v9,r9,r11
	temp.u32 = ctx.r9.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v4,v0,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v5,r8,r11
	temp.u32 = ctx.r8.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lwz r8,324(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// lvlx v3,r5,r11
	temp.u32 = ctx.r5.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v2,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v11,v0,v3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v31,r30,r11
	temp.u32 = r30.u32 + r11.u32;
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v3,v0,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v13,v28,v8
	// lvlx v30,r8,r11
	temp.u32 = ctx.r8.u32 + r11.u32;
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v5,v0,v31
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v26,v0,v30
	_mm_store_si128((__m128i*)v26.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v2,v8,v4
	// vsubshs v30,v9,v11
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// vsubshs v31,v12,v10
	// vsubshs v24,v0,v13
	// vsubshs v23,v0,v2
	// vsubshs v19,v0,v30
	// vsubshs v21,v4,v12
	// vsubshs v22,v0,v31
	// vsubshs v25,v10,v9
	// vsubshs v27,v3,v5
	// vsubshs v20,v11,v3
	// vsubshs v29,v5,v26
	// vmaxsh v13,v24,v13
	// vmaxsh v2,v23,v2
	// vmaxsh v30,v19,v30
	// vsubshs v24,v0,v21
	// vmaxsh v31,v22,v31
	// vsubshs v19,v0,v27
	// vsubshs v23,v0,v25
	// vsubshs v22,v0,v20
	// vsubshs v17,v0,v29
	// vmaxsh v24,v24,v21
	// vmaxsh v27,v19,v27
	// vmaxsh v23,v23,v25
	// vcmpgtuh v13,v6,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vcmpgtuh v2,v6,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmaxsh v22,v22,v20
	// vmaxsh v19,v17,v29
	// vcmpgtuh v31,v6,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vcmpgtuh v24,v6,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vcmpgtuh v30,v6,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vcmpgtuh v23,v6,v23
	_mm_store_si128((__m128i*)v23.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtuh v22,v6,v22
	_mm_store_si128((__m128i*)v22.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vcmpgtuh v27,v6,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vcmpgtuh v19,v6,v19
	_mm_store_si128((__m128i*)v19.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vaddshs v2,v24,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v31,v23,v30
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v30,v22,v27
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v13,v19,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v13,v30,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v13,v2,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v2,r0,r8
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v13,v0,v13
	// vperm v2,v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vspltish v13,5
	// vcmpgtsh. v15,v2,v13
	// mfocrf r8,2
	ctx.r8.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r7,r8,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x20;
	// stw r8,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, ctx.r8.u32);
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// beq cr6,0x82669278
	if (cr6.eq) goto loc_82669278;
	// vmaxsh v30,v12,v10
	// vminsh v27,v12,v10
	// vmaxsh v13,v8,v4
	// vminsh v31,v8,v4
	// vmaxsh v24,v9,v11
	// vminsh v23,v9,v11
	// vmaxsh v22,v3,v5
	// vminsh v19,v3,v5
	// vmaxsh v13,v13,v30
	// vminsh v31,v31,v27
	// vmaxsh v30,v24,v22
	// vminsh v27,v23,v19
	// vmaxsh v13,v13,v30
	// vminsh v31,v31,v27
	// vaddshs v27,v16,v16
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vsubshs v13,v13,v31
	// vupkhsh v31,v13
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// vupklsh v30,v13
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v13.s16)));
	// vcmpgtsh. v13,v27,v13
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r7,384(r1)
	PPC_STORE_U32(ctx.r1.u32 + 384, ctx.r7.u32);
	// vcmpgtsw. v31,v18,v31
	// vand v13,v13,v15
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v15.u8)));
	// mfocrf r6,2
	ctx.r6.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r6,464(r1)
	PPC_STORE_U32(ctx.r1.u32 + 464, ctx.r6.u32);
	// vcmpgtsw. v31,v18,v30
	// mfocrf r5,2
	ctx.r5.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v31,v2
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vspltisw v30,4
	_mm_store_si128((__m128i*)v30.u32, _mm_set1_epi32(int(0x4)));
	// stw r5,440(r1)
	PPC_STORE_U32(ctx.r1.u32 + 440, ctx.r5.u32);
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtsw. v31,v31,v30
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r7,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, ctx.r7.u32);
	// vcmpgtsw. v2,v2,v30
	// mfocrf r31,2
	r31.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// stw r7,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, ctx.r7.u32);
	// beq cr6,0x82668f88
	if (cr6.eq) goto loc_82668F88;
	// rlwinm r6,r6,0,26,26
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r6,32
	cr6.compare<int32_t>(ctx.r6.s32, 32, xer);
	// bne cr6,0x82668fa0
	if (!cr6.eq) goto loc_82668FA0;
loc_82668F88:
	// rlwinm r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// beq cr6,0x82669278
	if (cr6.eq) goto loc_82669278;
	// rlwinm r7,r5,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// beq cr6,0x82669278
	if (cr6.eq) goto loc_82669278;
loc_82668FA0:
	// vsubshs v24,v0,v29
	// vsubshs v27,v8,v28
	// vaddshs v2,v10,v9
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v30,v11,v3
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vmaxsh v29,v24,v29
	// vsubshs v24,v0,v27
	// vaddshs v31,v4,v12
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v18,v30,v30
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vcmpgtsh v29,v16,v29
	// vmaxsh v27,v24,v27
	// vaddshs v24,v2,v2
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v19,v31,v31
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vandc v23,v5,v29
	// vcmpgtsh v27,v16,v27
	// vand v29,v26,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vaddshs v17,v24,v30
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v24,v31,v24
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vand v28,v28,v27
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vandc v26,v8,v27
	// vxor v29,v29,v23
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vxor v28,v28,v26
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vaddshs v30,v29,v29
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v27,v5,v29
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v12,v12
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v31,v28,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// li r12,192
	r12.s64 = 192;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v26,v28,v8
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v28,v11,v11
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// li r12,240
	r12.s64 = 240;
	// stvx128 v29,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v23,v27,v27
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// li r12,80
	r12.s64 = 80;
	// stvx128 v31,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v22,v26,v26
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,256
	r12.s64 = 256;
	// stvx128 v28,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v18,v18,v23
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v19,v22,v19
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v30,v18,v1
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v18,v10,v10
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v31,v19,v1
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v19,v11,v5
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v28,v2,v30
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v29,v31,v2
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v24,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v31,v31,v17
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vaddshs v19,v18,v19
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v17,v8,v12
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsubshs v27,v2,v27
	// li r12,80
	r12.s64 = 80;
	// lvx128 v30,r1,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v29,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v30.s16)));
	// li r12,192
	r12.s64 = 192;
	// lvx128 v29,r1,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v29,v28,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v29.s16)));
	// li r12,192
	r12.s64 = 192;
	// stvx128 v0,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v28,v31,v26
	// li r12,240
	r12.s64 = 240;
	// lvx128 v26,r1,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v31,v31,v26
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,256
	r12.s64 = 256;
	// lvx128 v26,r1,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v2,v26
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,240
	r12.s64 = 240;
	// stvx128 v6,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v24,v29,v23
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v23,v4,v4
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v26,v22,v30
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v22,v3,v3
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v6,v10,v11
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v23,v30,v23
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v30,v12,v9
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v0,v9,v9
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v22,v29,v22
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vaddshs v29,v23,v6
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// li r12,80
	r12.s64 = 80;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v30,v26,v7
	// vaddshs v18,v0,v17
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vaddshs v28,v28,v19
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vsrah v2,v2,v7
	// vsrah v24,v24,v7
	// vaddshs v27,v27,v18
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v18.s16)));
	// li r12,80
	r12.s64 = 80;
	// lvx128 v23,r1,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v26,v22,v23
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vsrah v23,v31,v7
	// vand v31,v30,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsrah v30,v29,v7
	// vandc v22,v8,v13
	// vsrah v29,v28,v7
	// vsrah v28,v27,v7
	// vsrah v27,v26,v7
	// vandc v26,v12,v13
	// vandc v3,v3,v13
	// vand v23,v23,v13
	_mm_store_si128((__m128i*)v23.u8, _mm_and_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v27,v27,v13
	_mm_store_si128((__m128i*)v27.u8, _mm_and_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor v31,v31,v22
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v22.u8)));
	// vand v19,v29,v13
	_mm_store_si128((__m128i*)v19.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor v29,v23,v26
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vxor v3,v27,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vand v28,v28,v13
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v4,v4,v13
	// vand v30,v30,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v29,v29,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vand128 v63,v2,v13
	_mm_store_si128((__m128i*)v63.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v3,v3,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vandc v27,v5,v13
	// vand v26,v24,v13
	_mm_store_si128((__m128i*)v26.u8, _mm_and_si128(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v22,v10,v13
	// vandc v18,v9,v13
	// vandc v17,v11,v13
	// vpkshus v13,v31,v31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vxor v30,v30,v4
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vxor v4,v19,v22
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v22.u8)));
	// vxor v31,v26,v27
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vxor v2,v28,v18
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vpkshus v30,v30,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vxor128 v28,v63,v17
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v63.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vpkshus v27,v4,v4
	_mm_store_si128((__m128i*)v27.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// vpkshus v26,v2,v2
	_mm_store_si128((__m128i*)v26.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vpkshus v28,v28,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// vpkshus v13,v31,v31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// stvewx v30,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// stvewx v30,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// lwz r10,172(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// stvewx v29,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,172(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// stvewx v29,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// stvewx v27,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// stvewx v27,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// lwz r10,52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// stvewx v26,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// stvewx v26,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// lwz r10,188(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// stvewx v28,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,188(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// stvewx v28,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// lwz r10,164(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// stvewx v3,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v3.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,164(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// stvewx v3,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v3.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// lwz r10,168(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,168(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r9,44(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// lwz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r11,32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r8,340(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// li r12,192
	r12.s64 = 192;
	// lvx128 v0,r1,r12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,240
	r12.s64 = 240;
	// lvx128 v6,r1,r12
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v18,r0,r7
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x82669280
	goto loc_82669280;
loc_82669278:
	// vor v2,v9,v9
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v4,v10,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
loc_82669280:
	// rlwinm r8,r8,0,24,24
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r8,128
	cr6.compare<int32_t>(ctx.r8.s32, 128, xer);
	// beq cr6,0x82669398
	if (cr6.eq) goto loc_82669398;
	// vsubshs v12,v12,v11
	// vsubshs v13,v8,v10
	// vslh v11,v21,v14
	// vsubshs v8,v9,v5
	// vslh v5,v20,v14
	// vaddshs v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v11,v11,v21
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v21.s16)));
	// vsubshs v10,v9,v10
	// vaddshs v9,v8,v8
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v8,v5,v20
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v20.s16)));
	// vsubshs v13,v13,v11
	// vaddshs v5,v12,v12
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vslh v3,v25,v14
	// vaddshs v12,v13,v7
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsubshs v13,v9,v8
	// vaddshs v3,v3,v25
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vsubshs v9,v0,v12
	// vaddshs v11,v13,v7
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsubshs v13,v5,v3
	// vspltish v5,15
	// vmaxsh v12,v12,v9
	// vsubshs v8,v0,v11
	// vaddshs v13,v13,v7
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsrah v9,v12,v6
	// vmaxsh v12,v11,v8
	// vsrah v8,v13,v5
	// vsrah v11,v12,v6
	// vsubshs v12,v0,v13
	// vminsh v11,v9,v11
	// vmaxsh v12,v13,v12
	// vsrah v13,v25,v5
	// vmaxsh v5,v25,v10
	// vspltish v10,1
	// vsrah v12,v12,v6
	// vxor v9,v8,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsrah v5,v5,v10
	// vsubshs v10,v12,v11
	// vcmpgtsh v3,v16,v12
	// vcmpgtsh v12,v12,v11
	// vandc v11,v9,v15
	// vslh v9,v10,v14
	// vand v12,v3,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vsrah v10,v10,v6
	// vand v10,v10,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vminsh v12,v5,v12
	// vxor v12,v12,v13
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v13,v12,v13
	// vsubshs v12,v4,v13
	// vaddshs v13,v2,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvewx v12,r9,r11
	ea = (ctx.r9.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// stvewx v12,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// lwz r10,52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r9,44(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// lwz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r11,32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
loc_82669398:
	// lwz r8,28(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r11,32(r1)
	PPC_STORE_U32(ctx.r1.u32 + 32, r11.u32);
	// stw r8,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r8.u32);
	// bne cr6,0x82668d90
	if (!cr6.eq) goto loc_82668D90;
	// lwz r8,684(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	// lwz r7,676(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	// lwz r4,652(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 652);
loc_826693C4:
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// rlwinm r11,r8,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lvx128 v12,r0,r22
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r11,r4
	ctx.r7.u64 = r11.u64 + ctx.r4.u64;
	// vspltish v13,1
	// vspltish v18,15
	// stw r29,24(r1)
	PPC_STORE_U32(ctx.r1.u32 + 24, r29.u32);
	// subf r9,r8,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r8.s64;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// add r4,r7,r8
	ctx.r4.u64 = ctx.r7.u64 + ctx.r8.u64;
	// vspltish v14,2
	// subf r6,r8,r9
	ctx.r6.s64 = ctx.r9.s64 - ctx.r8.s64;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r3,r4,r8
	ctx.r3.u64 = ctx.r4.u64 + ctx.r8.u64;
	// vspltish v12,5
	// subf r5,r8,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r8.s64;
	// vspltish v6,3
	// add r31,r3,r8
	r31.u64 = ctx.r3.u64 + ctx.r8.u64;
	// stw r7,48(r1)
	PPC_STORE_U32(ctx.r1.u32 + 48, ctx.r7.u32);
	// subf r10,r8,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r8.s64;
	// vspltish v8,4
	// stw r9,40(r1)
	PPC_STORE_U32(ctx.r1.u32 + 40, ctx.r9.u32);
	// vspltish v1,8
	// subf r11,r8,r10
	r11.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r6,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r6.u32);
	// stw r4,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r4.u32);
	// vspltisw128 v63,4
	_mm_store_si128((__m128i*)v63.u32, _mm_set1_epi32(int(0x4)));
	// stw r5,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r5.u32);
	// stw r3,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r3.u32);
	// stw r10,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r10.u32);
	// stw r11,336(r1)
	PPC_STORE_U32(ctx.r1.u32 + 336, r11.u32);
	// add r11,r31,r8
	r11.u64 = r31.u64 + ctx.r8.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r31,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r31.u32);
	// stw r11,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, r11.u32);
	// li r11,2
	r11.s64 = 2;
	// stw r8,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r8.u32);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, r11.u32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,224
	ctx.r8.s64 = ctx.r1.s64 + 224;
	// vsplth v16,v9,1
	_mm_store_si128((__m128i*)v16.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_set1_epi16(short(0xD0C))));
	// stvx v13,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vaddshs v13,v16,v16
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// vupkhsh v17,v13
	_mm_store_si128((__m128i*)v17.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// stvx v18,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,208
	ctx.r8.s64 = ctx.r1.s64 + 208;
	// stvx v17,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x826694b8
	goto loc_826694B8;
loc_826694A0:
	// lwz r5,156(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// lwz r6,180(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r7,48(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 48);
	// lwz r4,152(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r31,160(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// lwz r3,144(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
loc_826694B8:
	// lwz r8,336(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	// lvlx v10,r6,r11
	temp.u32 = ctx.r6.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,r10,r11
	temp.u32 = ctx.r10.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,r5,r11
	temp.u32 = ctx.r5.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v7,v0,v13
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v9,r9,r11
	temp.u32 = ctx.r9.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v4,v0,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v5,r7,r11
	temp.u32 = ctx.r7.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v12,r8,r11
	temp.u32 = ctx.r8.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lwz r8,332(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// vmrghb v28,v0,v12
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v3,r4,r11
	temp.u32 = ctx.r4.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v12,v0,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v2,r3,r11
	temp.u32 = ctx.r3.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v31,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v9,v0,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v0,v3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v30,r8,r11
	temp.u32 = ctx.r8.u32 + r11.u32;
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v3,v0,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v13,v28,v7
	// vmrghb v26,v0,v30
	_mm_store_si128((__m128i*)v26.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v2,v7,v4
	// vmrghb v5,v0,v31
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v31,v12,v10
	// vsubshs v30,v9,v11
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// vsubshs v24,v0,v13
	// vsubshs v23,v0,v2
	// vsubshs v21,v4,v12
	// vsubshs v19,v0,v30
	// vsubshs v22,v0,v31
	// vsubshs v25,v10,v9
	// vsubshs v27,v3,v5
	// vsubshs v20,v11,v3
	// vsubshs v29,v5,v26
	// vmaxsh v13,v24,v13
	// vmaxsh v2,v23,v2
	// vmaxsh v30,v19,v30
	// vsubshs v24,v0,v21
	// vmaxsh v31,v22,v31
	// vsubshs v19,v0,v27
	// vsubshs v23,v0,v25
	// vsubshs v15,v0,v29
	// vsubshs v22,v0,v20
	// vmaxsh v24,v24,v21
	// vmaxsh v27,v19,v27
	// vmaxsh v23,v23,v25
	// vcmpgtuh v13,v6,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vcmpgtuh v2,v6,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmaxsh v19,v15,v29
	// vmaxsh v22,v22,v20
	// vcmpgtuh v31,v6,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vcmpgtuh v24,v6,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vcmpgtuh v30,v6,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vcmpgtuh v23,v6,v23
	_mm_store_si128((__m128i*)v23.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtuh v22,v6,v22
	_mm_store_si128((__m128i*)v22.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vcmpgtuh v27,v6,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vcmpgtuh v19,v6,v19
	_mm_store_si128((__m128i*)v19.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vaddshs v2,v24,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v31,v23,v30
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v30,v22,v27
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v13,v19,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v13,v30,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v13,v2,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v2,r0,r8
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vsubshs v13,v0,v13
	// vperm v2,v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcmpgtsh. v15,v2,v13
	// mfocrf r8,2
	ctx.r8.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r7,r8,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x20;
	// stw r8,328(r1)
	PPC_STORE_U32(ctx.r1.u32 + 328, ctx.r8.u32);
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// beq cr6,0x82669998
	if (cr6.eq) goto loc_82669998;
	// vmaxsh v30,v12,v10
	// vminsh v27,v12,v10
	// vmaxsh v13,v7,v4
	// vminsh v31,v7,v4
	// vmaxsh v24,v9,v11
	// vminsh v23,v9,v11
	// vmaxsh v22,v3,v5
	// vminsh v19,v3,v5
	// vmaxsh v13,v13,v30
	// vminsh v31,v31,v27
	// vmaxsh v30,v24,v22
	// vminsh v27,v23,v19
	// vmaxsh v13,v13,v30
	// vminsh v31,v31,v27
	// vaddshs v27,v16,v16
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vsubshs v13,v13,v31
	// vupkhsh v31,v13
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// vupklsh v30,v13
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v13.s16)));
	// vcmpgtsh. v13,v27,v13
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r7,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, ctx.r7.u32);
	// vcmpgtsw. v31,v17,v31
	// vand v13,v13,v15
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v15.u8)));
	// mfocrf r6,2
	ctx.r6.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r6,468(r1)
	PPC_STORE_U32(ctx.r1.u32 + 468, ctx.r6.u32);
	// vcmpgtsw. v31,v17,v30
	// mfocrf r5,2
	ctx.r5.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v31,v2
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vor128 v30,v63,v63
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)v63.u8));
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// stw r5,400(r1)
	PPC_STORE_U32(ctx.r1.u32 + 400, ctx.r5.u32);
	// vcmpgtsw. v31,v31,v30
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r7,432(r1)
	PPC_STORE_U32(ctx.r1.u32 + 432, ctx.r7.u32);
	// vcmpgtsw. v2,v2,v30
	// mfocrf r4,2
	ctx.r4.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// stw r7,408(r1)
	PPC_STORE_U32(ctx.r1.u32 + 408, ctx.r7.u32);
	// beq cr6,0x826696a0
	if (cr6.eq) goto loc_826696A0;
	// rlwinm r6,r6,0,26,26
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r6,32
	cr6.compare<int32_t>(ctx.r6.s32, 32, xer);
	// bne cr6,0x826696b8
	if (!cr6.eq) goto loc_826696B8;
loc_826696A0:
	// rlwinm r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// beq cr6,0x82669998
	if (cr6.eq) goto loc_82669998;
	// rlwinm r7,r5,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// beq cr6,0x82669998
	if (cr6.eq) goto loc_82669998;
loc_826696B8:
	// vsubshs v24,v0,v29
	// vsubshs v27,v7,v28
	// vaddshs v2,v10,v9
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v30,v11,v3
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vmaxsh v29,v24,v29
	// vsubshs v24,v0,v27
	// vaddshs v31,v4,v12
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v18,v30,v30
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vcmpgtsh v29,v16,v29
	// vmaxsh v27,v24,v27
	// vaddshs v24,v2,v2
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v19,v31,v31
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vandc v23,v5,v29
	// vcmpgtsh v27,v16,v27
	// vand v29,v26,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vaddshs v17,v24,v30
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v24,v31,v24
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vand v28,v28,v27
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vandc v26,v7,v27
	// vxor v29,v29,v23
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vxor v28,v28,v26
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vaddshs v30,v29,v29
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v27,v5,v29
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v12,v12
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v31,v28,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// li r12,192
	r12.s64 = 192;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v26,v28,v7
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v28,v11,v11
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// li r12,240
	r12.s64 = 240;
	// stvx128 v29,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v23,v27,v27
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// li r12,80
	r12.s64 = 80;
	// stvx128 v31,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v22,v26,v26
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,256
	r12.s64 = 256;
	// stvx128 v28,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v18,v18,v23
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v19,v22,v19
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v30,v18,v1
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v18,v10,v10
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v31,v19,v1
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v19,v11,v5
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v28,v2,v30
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v29,v31,v2
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v2,v24,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v31,v31,v17
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vaddshs v19,v18,v19
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v17,v7,v12
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsubshs v27,v2,v27
	// li r12,80
	r12.s64 = 80;
	// lvx128 v30,r1,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v29,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v30.s16)));
	// li r12,192
	r12.s64 = 192;
	// lvx128 v29,r1,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v29,v28,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v29.s16)));
	// li r12,192
	r12.s64 = 192;
	// stvx128 v0,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v28,v31,v26
	// li r12,240
	r12.s64 = 240;
	// lvx128 v26,r1,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v31,v31,v26
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,256
	r12.s64 = 256;
	// lvx128 v26,r1,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v2,v26
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,240
	r12.s64 = 240;
	// stvx128 v6,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v24,v29,v23
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v23,v4,v4
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v26,v22,v30
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v22,v3,v3
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v6,v10,v11
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v23,v30,v23
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v30,v12,v9
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v0,v9,v9
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v22,v29,v22
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vaddshs v29,v23,v6
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// li r12,80
	r12.s64 = 80;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v30,v26,v8
	// vaddshs v18,v0,v17
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vaddshs v28,v28,v19
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vsrah v2,v2,v8
	// vsrah v24,v24,v8
	// vaddshs v27,v27,v18
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v18.s16)));
	// li r12,80
	r12.s64 = 80;
	// lvx128 v23,r1,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v26,v22,v23
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vsrah v23,v31,v8
	// vand v31,v30,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsrah v30,v29,v8
	// vandc v22,v7,v13
	// vsrah v29,v28,v8
	// vsrah v28,v27,v8
	// vsrah v27,v26,v8
	// vandc v26,v12,v13
	// vandc v3,v3,v13
	// vand v23,v23,v13
	_mm_store_si128((__m128i*)v23.u8, _mm_and_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v27,v27,v13
	_mm_store_si128((__m128i*)v27.u8, _mm_and_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor v31,v31,v22
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v22.u8)));
	// vand v19,v29,v13
	_mm_store_si128((__m128i*)v19.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor v29,v23,v26
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vxor v3,v27,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vand v28,v28,v13
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v4,v4,v13
	// vand v30,v30,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v29,v29,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vand128 v62,v2,v13
	_mm_store_si128((__m128i*)v62.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v3,v3,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vandc v27,v5,v13
	// vand v26,v24,v13
	_mm_store_si128((__m128i*)v26.u8, _mm_and_si128(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v22,v10,v13
	// vandc v18,v9,v13
	// vandc v17,v11,v13
	// vpkshus v13,v31,v31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vxor v30,v30,v4
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vxor v4,v19,v22
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v22.u8)));
	// vxor v31,v26,v27
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vxor v2,v28,v18
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vpkshus v30,v30,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vxor128 v28,v62,v17
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v62.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vpkshus v27,v4,v4
	_mm_store_si128((__m128i*)v27.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// vpkshus v26,v2,v2
	_mm_store_si128((__m128i*)v26.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vpkshus v28,v28,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// vpkshus v13,v31,v31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// stvewx v30,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// stvewx v30,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// stvewx v29,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r10,180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// stvewx v29,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 40);
	// stvewx v27,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r10,40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 40);
	// stvewx v27,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 48);
	// stvewx v26,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r10,48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 48);
	// stvewx v26,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,152(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// stvewx v28,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r10,152(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// stvewx v28,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stvewx v3,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v3.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stvewx v3,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v3.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r9,40(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 40);
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lwz r8,328(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	// li r12,192
	r12.s64 = 192;
	// lvx128 v0,r1,r12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,240
	r12.s64 = 240;
	// lvx128 v6,r1,r12
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v18,r0,r7
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,208
	ctx.r7.s64 = ctx.r1.s64 + 208;
	// lvx128 v17,r0,r7
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x826699a0
	goto loc_826699A0;
loc_82669998:
	// vor v2,v9,v9
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v4,v10,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
loc_826699A0:
	// rlwinm r8,r8,0,24,24
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r8,128
	cr6.compare<int32_t>(ctx.r8.s32, 128, xer);
	// beq cr6,0x82669ab8
	if (cr6.eq) goto loc_82669AB8;
	// vsubshs v12,v12,v11
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// vsubshs v13,v7,v10
	// vslh v11,v21,v14
	// vsubshs v7,v9,v5
	// vslh v5,v20,v14
	// vaddshs v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v11,v11,v21
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v21.s16)));
	// vsubshs v10,v9,v10
	// vaddshs v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v7,v5,v20
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v20.s16)));
	// vsubshs v13,v13,v11
	// vaddshs v5,v12,v12
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vslh v3,v25,v14
	// vmaxsh v10,v25,v10
	// vaddshs v12,v13,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsubshs v13,v9,v7
	// vaddshs v3,v3,v25
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vsubshs v9,v0,v12
	// vaddshs v11,v13,v8
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsubshs v13,v5,v3
	// lvx128 v5,r0,r10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v5,v10,v5
	// vmaxsh v12,v12,v9
	// vsubshs v7,v0,v11
	// vaddshs v13,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsrah v9,v12,v6
	// vmaxsh v12,v11,v7
	// vsrah v7,v13,v18
	// vsrah v11,v12,v6
	// vsubshs v12,v0,v13
	// vminsh v11,v9,v11
	// vmaxsh v12,v13,v12
	// vsrah v13,v25,v18
	// vsrah v12,v12,v6
	// vxor v9,v7,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v10,v12,v11
	// vcmpgtsh v3,v16,v12
	// vcmpgtsh v12,v12,v11
	// vandc v11,v9,v15
	// vslh v9,v10,v14
	// vand v12,v3,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vsrah v10,v10,v6
	// vand v10,v10,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vminsh v12,v5,v12
	// vxor v12,v12,v13
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v13,v12,v13
	// vsubshs v12,v4,v13
	// vaddshs v13,v2,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvewx v12,r9,r11
	ea = (ctx.r9.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r10,40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 40);
	// stvewx v12,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 48);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r10,48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 48);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r9,40(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 40);
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
loc_82669AB8:
	// lwz r7,24(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r8,272(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stw r11,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, r11.u32);
	// stw r7,24(r1)
	PPC_STORE_U32(ctx.r1.u32 + 24, ctx.r7.u32);
	// lwz r7,16(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// stw r8,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r8.u32);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// blt cr6,0x826694a0
	if (cr6.lt) goto loc_826694A0;
	// lwz r11,676(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	// vspltish v13,5
	// lwz r23,684(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// lwz r11,652(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	// addi r31,r11,3
	r31.s64 = r11.s64 + 3;
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// add r30,r31,r23
	r30.u64 = r31.u64 + r23.u64;
	// add r29,r30,r23
	r29.u64 = r30.u64 + r23.u64;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// lvx128 v13,r0,r22
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r28,r29,r23
	r28.u64 = r29.u64 + r23.u64;
	// li r22,0
	r22.s64 = 0;
	// add r27,r28,r23
	r27.u64 = r28.u64 + r23.u64;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r26,r27,r23
	r26.u64 = r27.u64 + r23.u64;
	// li r11,0
	r11.s64 = 0;
	// add r25,r26,r23
	r25.u64 = r26.u64 + r23.u64;
	// add r24,r25,r23
	r24.u64 = r25.u64 + r23.u64;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// vsplth v23,v13,1
	_mm_store_si128((__m128i*)v23.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xD0C))));
	// vaddshs v15,v23,v23
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v23.s16)));
	// stvx v23,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,288
	ctx.r10.s64 = ctx.r1.s64 + 288;
	// stvx v15,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82669B54:
	// add r10,r11,r31
	ctx.r10.u64 = r11.u64 + r31.u64;
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// li r21,16
	r21.s64 = 16;
	// li r20,16
	r20.s64 = 16;
	// add r8,r11,r27
	ctx.r8.u64 = r11.u64 + r27.u64;
	// li r19,16
	r19.s64 = 16;
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r7,r11,r26
	ctx.r7.u64 = r11.u64 + r26.u64;
	// lvrx v13,r10,r21
	temp.u32 = ctx.r10.u32 + r21.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r18,16
	r18.s64 = 16;
	// lvrx v11,r9,r20
	temp.u32 = ctx.r9.u32 + r20.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvlx v10,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r8,r19
	temp.u32 = ctx.r8.u32 + r19.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r6,r11,r25
	ctx.r6.u64 = r11.u64 + r25.u64;
	// vor v10,v10,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvlx v12,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r7,r18
	temp.u32 = ctx.r7.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r17,16
	r17.s64 = 16;
	// vor v9,v12,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// add r5,r11,r29
	ctx.r5.u64 = r11.u64 + r29.u64;
	// vor v12,v11,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// li r16,16
	r16.s64 = 16;
	// vor v11,v10,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// add r4,r11,r28
	ctx.r4.u64 = r11.u64 + r28.u64;
	// li r15,16
	r15.s64 = 16;
	// vor v10,v9,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// lvrx v7,r6,r17
	temp.u32 = ctx.r6.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r3,r11,r24
	ctx.r3.u64 = r11.u64 + r24.u64;
	// vor v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v5,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r5,r16
	temp.u32 = ctx.r5.u32 + r16.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r14,16
	r14.s64 = 16;
	// vor v7,v5,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v4,r4,r15
	temp.u32 = ctx.r4.u32 + r15.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v5,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvlx v4,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v3,r3,r14
	temp.u32 = ctx.r3.u32 + r14.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v4,v4,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v3,v13,v11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v12,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v10,v7,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v2,v5,v4
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrglb v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrglb v7,v5,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v5,v3,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v4,v11,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrglb v10,v3,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v3,v13,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v11,v11,v2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrglb v13,v13,v9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v9,v12,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v7,v5,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v2,v10,v11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vmrglb v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v29,v3,v9
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v30,v10,v11
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vmrglb v21,v3,v9
	_mm_store_si128((__m128i*)v21.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v20,v13,v12
	_mm_store_si128((__m128i*)v20.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v19,v13,v12
	_mm_store_si128((__m128i*)v19.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v31,v0,v7
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v4,v0,v5
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v12,v0,v5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v0,v30
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v13,v31,v7
	// vmrglb v3,v0,v30
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v2,v7,v4
	// vmrghb v5,v0,v29
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v14,v4,v12
	// vmrglb v27,v0,v29
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v22,v10,v9
	// vsubshs v25,v0,v13
	// li r12,80
	r12.s64 = 80;
	// stvx128 v12,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v30,v12,v10
	// li r12,192
	r12.s64 = 192;
	// stvx128 v11,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v24,v0,v14
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// vsubshs v29,v9,v11
	// vmaxsh v13,v25,v13
	// vsubshs v25,v0,v2
	// vsubshs v17,v0,v22
	// vsubshs v1,v11,v3
	// vsubshs v28,v5,v27
	// vsubshs v26,v3,v5
	// vmaxsh v2,v25,v2
	// vsubshs v18,v0,v30
	// vmaxsh v25,v24,v14
	// vsubshs v16,v0,v29
	// vmaxsh v24,v17,v22
	// vsubshs v11,v0,v26
	// vsubshs v12,v0,v1
	// vsubshs v17,v0,v28
	// vmaxsh v30,v18,v30
	// vmaxsh v29,v16,v29
	// vcmpgtuh v13,v6,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vcmpgtuh v2,v6,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmaxsh v18,v12,v1
	// vmaxsh v26,v11,v26
	// vmaxsh v17,v17,v28
	// vcmpgtuh v30,v6,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vcmpgtuh v25,v6,v25
	_mm_store_si128((__m128i*)v25.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vcmpgtuh v29,v6,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vcmpgtuh v24,v6,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtuh v18,v6,v18
	_mm_store_si128((__m128i*)v18.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v18.u16)));
	// vcmpgtuh v26,v6,v26
	_mm_store_si128((__m128i*)v26.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vcmpgtuh v17,v6,v17
	_mm_store_si128((__m128i*)v17.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v17.u16)));
	// vaddshs v2,v25,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v30,v24,v29
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v18,v26
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v13,v17,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v13,v29,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v13,v2,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v2,r0,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// vsubshs v13,v0,v13
	// vperm v2,v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,304
	ctx.r10.s64 = ctx.r1.s64 + 304;
	// vcmpgtsh. v16,v2,v13
	// stvx v16,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mfocrf r9,2
	ctx.r9.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r10,r9,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20;
	// li r12,80
	r12.s64 = 80;
	// lvx128 v12,r1,r12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,192
	r12.s64 = 192;
	// lvx128 v11,r1,r12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// beq cr6,0x8266a158
	if (cr6.eq) goto loc_8266A158;
	// vmaxsh v29,v12,v10
	// vminsh v26,v12,v10
	// vmaxsh v13,v7,v4
	// vminsh v30,v7,v4
	// vmaxsh v25,v9,v11
	// vminsh v24,v9,v11
	// vmaxsh v18,v3,v5
	// vminsh v17,v3,v5
	// vmaxsh v13,v13,v29
	// vminsh v30,v30,v26
	// vmaxsh v29,v25,v18
	// vminsh v26,v24,v17
	// vmaxsh v13,v13,v29
	// vminsh v30,v30,v26
	// vsubshs v13,v13,v30
	// vupkhsh v29,v13
	_mm_store_si128((__m128i*)v29.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// vupklsh v26,v13
	_mm_store_si128((__m128i*)v26.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v13.s16)));
	// vcmpgtsh. v13,v15,v13
	// mfocrf r10,2
	ctx.r10.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v30,v15
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v15.s16))));
	// stw r10,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, ctx.r10.u32);
	// vand v13,v13,v16
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vcmpgtsw. v29,v30,v29
	// mfocrf r8,2
	ctx.r8.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vcmpgtsw. v30,v30,v26
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v30,v2
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vupklsh v29,v2
	_mm_store_si128((__m128i*)v29.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// vor128 v2,v63,v63
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v63.u8));
	// vcmpgtsw. v30,v30,v2
	// mfocrf r10,2
	ctx.r10.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vcmpgtsw. v2,v29,v2
	// mfocrf r6,2
	ctx.r6.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// beq cr6,0x82669e2c
	if (cr6.eq) goto loc_82669E2C;
	// rlwinm r8,r8,0,26,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r8,32
	cr6.compare<int32_t>(ctx.r8.s32, 32, xer);
	// bne cr6,0x82669e44
	if (!cr6.eq) goto loc_82669E44;
loc_82669E2C:
	// rlwinm r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// beq cr6,0x8266a158
	if (cr6.eq) goto loc_8266A158;
	// rlwinm r10,r7,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// beq cr6,0x8266a158
	if (cr6.eq) goto loc_8266A158;
loc_82669E44:
	// vsubshs v25,v0,v28
	// li r12,256
	r12.s64 = 256;
	// stvx128 v0,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v26,v7,v31
	// vor v18,v31,v31
	_mm_store_si128((__m128i*)v18.u8, _mm_load_si128((__m128i*)v31.u8));
	// vor v17,v27,v27
	_mm_store_si128((__m128i*)v17.u8, _mm_load_si128((__m128i*)v27.u8));
	// vmaxsh v28,v25,v28
	// vsubshs v25,v0,v26
	// vaddshs v2,v10,v9
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v29,v11,v3
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vcmpgtsh v28,v23,v28
	// vmaxsh v26,v25,v26
	// vaddshs v25,v2,v2
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v30,v4,v12
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vandc v24,v5,v28
	// vcmpgtsh v26,v23,v26
	// vand v28,v27,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vaddshs v15,v29,v29
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v25,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vand v31,v31,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vandc v27,v7,v26
	// vxor v28,v28,v24
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v25,v30,v25
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vaddshs v16,v30,v30
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vxor v31,v31,v27
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vaddshs v27,v5,v28
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v30,v28,v28
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v28,v12,v12
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v26,v31,v7
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v31,v31,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// li r12,240
	r12.s64 = 240;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v23,v27,v27
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// li r12,192
	r12.s64 = 192;
	// stvx128 v28,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v28,8
	// vaddshs v24,v26,v26
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,80
	r12.s64 = 80;
	// stvx128 v31,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v30,v15,v23
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v15,v10,v10
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v16,v24,v16
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v30,v30,v28
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v31,v16,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v16,v11,v5
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v28,v31,v2
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v31,v31,v29
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v2,v30
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v2,v25,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v16,v15,v16
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v25,v2,v0
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v0,v7,v12
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// li r12,80
	r12.s64 = 80;
	// lvx128 v30,r1,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v28,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v30.s16)));
	// li r12,80
	r12.s64 = 80;
	// stvx128 v6,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v28,v31,v26
	// li r12,192
	r12.s64 = 192;
	// lvx128 v26,r1,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v26,v31,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,240
	r12.s64 = 240;
	// lvx128 v31,r1,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v31,v29,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v31.s16)));
	// li r12,192
	r12.s64 = 192;
	// stvx128 v1,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v29,v2,v27
	// vaddshs v2,v24,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v24,v4,v4
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v6,v9,v9
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v27,v31,v23
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v23,v3,v3
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v30,v30,v24
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vaddshs v1,v10,v11
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v15,v6,v0
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v23,v31,v23
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v24,v12,v9
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vsrah v2,v2,v8
	// vaddshs v31,v30,v1
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v28,v28,v16
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v29,v29,v15
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v15.s16)));
	// vaddshs v30,v23,v24
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vand v23,v2,v13
	_mm_store_si128((__m128i*)v23.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsrah v2,v31,v8
	// vandc v24,v7,v13
	// vsrah v26,v26,v8
	// vsrah v27,v27,v8
	// vsrah v31,v28,v8
	// vsrah v29,v29,v8
	// vsrah v28,v25,v8
	// add r10,r11,r31
	ctx.r10.u64 = r11.u64 + r31.u64;
	// vand v27,v27,v13
	_mm_store_si128((__m128i*)v27.u8, _mm_and_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r8,16
	ctx.r8.s64 = 16;
	// vandc v25,v5,v13
	// li r7,16
	ctx.r7.s64 = 16;
	// vsrah v30,v30,v8
	// li r6,16
	ctx.r6.s64 = 16;
	// vand v2,v2,v13
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r5,16
	ctx.r5.s64 = 16;
	// vandc v4,v4,v13
	// li r4,16
	ctx.r4.s64 = 16;
	// vxor v27,v27,v25
	_mm_store_si128((__m128i*)v27.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vandc v25,v12,v13
	// vand v26,v26,v13
	_mm_store_si128((__m128i*)v26.u8, _mm_and_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor v24,v23,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vand v31,v31,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v23,v10,v13
	// vandc128 v62,v3,v13
	_mm_store_si128((__m128i*)v62.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vand v30,v30,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v29,v29,v13
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v28,v28,v13
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v16,v9,v13
	// vandc v15,v11,v13
	// vpkshus v13,v18,v24
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vxor v3,v2,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vpkshus v4,v27,v17
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vxor v2,v26,v25
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vxor v25,v31,v23
	_mm_store_si128((__m128i*)v25.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vxor128 v30,v30,v62
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v62.u8)));
	// vxor v24,v29,v16
	_mm_store_si128((__m128i*)v24.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vxor v31,v28,v15
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vpkshus v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vpkshus v2,v25,v24
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vpkshus v31,v31,v30
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vmrghb v30,v13,v4
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v4
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v29,v2,v20
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v4,v3,v21
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v28,v31,v19
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrglb v2,v2,v20
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrglb v3,v3,v21
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v31,v31,v19
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrghb v27,v30,v29
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrghb v26,v4,v28
	_mm_store_si128((__m128i*)v26.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrglb v30,v30,v29
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrglb v4,v4,v28
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrghb v29,v13,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v28,v3,v31
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v3,v3,v31
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v31,v27,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vmrghb v2,v27,v26
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vmrghb v27,v30,v4
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrglb v4,v30,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrghb v30,v29,v28
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vmrglb v29,v29,v28
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vor v23,v4,v4
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmrghb v28,v2,v30
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v21,v4,v4
	_mm_store_si128((__m128i*)v21.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmrglb v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v30,v13,v3
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v4,v31,v29
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrglb v3,v31,v29
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vor v20,v2,v2
	_mm_store_si128((__m128i*)v20.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vor v29,v2,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vmrglb v31,v27,v30
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v27.u8)));
	// stvlx v28,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v28.u8[15 - i]);
	// vmrghb v2,v27,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v27.u8)));
	// stvrx v28,r10,r8
	ea = ctx.r10.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v28.u8[i]);
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// vmrghb v30,v23,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vmrglb v13,v21,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v21.u8)));
	// stvlx v20,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v20.u8[15 - i]);
	// stvrx v29,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v29.u8[i]);
	// add r10,r11,r29
	ctx.r10.u64 = r11.u64 + r29.u64;
	// stvlx v4,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v4.u8[15 - i]);
	// stvrx v4,r10,r6
	ea = ctx.r10.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v4.u8[i]);
	// add r10,r11,r28
	ctx.r10.u64 = r11.u64 + r28.u64;
	// stvlx v3,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v3.u8[15 - i]);
	// stvrx v3,r10,r5
	ea = ctx.r10.u32 + ctx.r5.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v3.u8[i]);
	// add r10,r11,r27
	ctx.r10.u64 = r11.u64 + r27.u64;
	// li r12,256
	r12.s64 = 256;
	// lvx128 v0,r1,r12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,80
	r12.s64 = 80;
	// lvx128 v6,r1,r12
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,192
	r12.s64 = 192;
	// lvx128 v1,r1,r12
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvlx v2,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v2.u8[15 - i]);
	// stvrx v2,r10,r4
	ea = ctx.r10.u32 + ctx.r4.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v2.u8[i]);
	// add r10,r11,r26
	ctx.r10.u64 = r11.u64 + r26.u64;
	// stvlx v31,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v31.u8[15 - i]);
	// stvrx v31,r10,r8
	ea = ctx.r10.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v31.u8[i]);
	// add r10,r11,r25
	ctx.r10.u64 = r11.u64 + r25.u64;
	// stvlx v30,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v30.u8[15 - i]);
	// stvrx v30,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v30.u8[i]);
	// add r10,r11,r24
	ctx.r10.u64 = r11.u64 + r24.u64;
	// stvlx v13,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// stvrx v13,r10,r6
	ea = ctx.r10.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v23,r0,r10
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,304
	ctx.r10.s64 = ctx.r1.s64 + 304;
	// lvx128 v16,r0,r10
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,288
	ctx.r10.s64 = ctx.r1.s64 + 288;
	// lvx128 v15,r0,r10
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x8266a160
	goto loc_8266A160;
loc_8266A158:
	// vor v24,v9,v9
	_mm_store_si128((__m128i*)v24.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v25,v10,v10
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
loc_8266A160:
	// rlwinm r10,r9,0,24,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r10,128
	cr6.compare<int32_t>(ctx.r10.s32, 128, xer);
	// beq cr6,0x8266a2a8
	if (cr6.eq) goto loc_8266A2A8;
	// vsubshs v12,v12,v11
	// vspltish v11,2
	// vsubshs v13,v7,v10
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// vsubshs v7,v9,v5
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// vsubshs v9,v9,v10
	// vslh v10,v14,v11
	// vaddshs v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vslh v5,v1,v11
	// vaddshs v7,v7,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v10,v10,v14
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vaddshs v4,v12,v12
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v5,v5,v1
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vslh v3,v22,v11
	// vsubshs v13,v13,v10
	// vmaxsh v9,v22,v9
	// vaddshs v3,v3,v22
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vaddshs v12,v13,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsubshs v13,v7,v5
	// vsubshs v7,v0,v12
	// vaddshs v10,v13,v8
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsubshs v13,v4,v3
	// vmaxsh v12,v12,v7
	// vsubshs v5,v0,v10
	// vaddshs v13,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsrah v7,v12,v6
	// vmaxsh v12,v10,v5
	// vspltish v10,15
	// vsrah v5,v12,v6
	// vsubshs v12,v0,v13
	// vsrah v4,v13,v10
	// vmaxsh v12,v13,v12
	// vsrah v13,v22,v10
	// vspltish v10,1
	// vsrah v12,v12,v6
	// vsrah v3,v9,v10
	// vminsh v10,v7,v5
	// vxor v7,v4,v13
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vcmpgtsh v5,v23,v12
	// vsubshs v9,v12,v10
	// vcmpgtsh v12,v12,v10
	// vandc v10,v7,v16
	// vslh v11,v9,v11
	// vand v12,v5,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vsrah v11,v11,v6
	// vand v11,v11,v12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vminsh v12,v3,v12
	// vxor v12,v12,v13
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v13,v12,v13
	// vsubshs v12,v25,v13
	// vaddshs v13,v24,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vmrghh v11,v12,v13
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vpkshus v13,v11,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vmrghh v12,v0,v13
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v13,v0,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r7,120(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r6,124(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r5,128(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// sthx r9,r10,r31
	PPC_STORE_U16(ctx.r10.u32 + r31.u32, ctx.r9.u16);
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// sthx r8,r10,r30
	PPC_STORE_U16(ctx.r10.u32 + r30.u32, ctx.r8.u16);
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r8,140(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// sthx r7,r10,r29
	PPC_STORE_U16(ctx.r10.u32 + r29.u32, ctx.r7.u16);
	// sthx r6,r10,r28
	PPC_STORE_U16(ctx.r10.u32 + r28.u32, ctx.r6.u16);
	// sthx r5,r10,r27
	PPC_STORE_U16(ctx.r10.u32 + r27.u32, ctx.r5.u16);
	// sthx r4,r10,r26
	PPC_STORE_U16(ctx.r10.u32 + r26.u32, ctx.r4.u16);
	// sthx r9,r10,r25
	PPC_STORE_U16(ctx.r10.u32 + r25.u32, ctx.r9.u16);
	// sthx r8,r10,r24
	PPC_STORE_U16(ctx.r10.u32 + r24.u32, ctx.r8.u16);
loc_8266A2A8:
	// rlwinm r10,r23,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// li r10,2
	ctx.r10.s64 = 2;
	// cmpw cr6,r22,r10
	cr6.compare<int32_t>(r22.s32, ctx.r10.s32, xer);
	// blt cr6,0x82669b54
	if (cr6.lt) goto loc_82669B54;
	// lwz r11,660(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8266ab00
	if (cr6.eq) goto loc_8266AB00;
	// lwz r11,676(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	// vspltish v13,1
	// vspltish v15,2
	// li r22,0
	r22.s64 = 0;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// lwz r11,652(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	// addi r31,r11,-5
	r31.s64 = r11.s64 + -5;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// add r30,r31,r23
	r30.u64 = r31.u64 + r23.u64;
	// add r29,r30,r23
	r29.u64 = r30.u64 + r23.u64;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,240
	r11.s64 = ctx.r1.s64 + 240;
	// vspltish v13,5
	// add r28,r29,r23
	r28.u64 = r29.u64 + r23.u64;
	// add r27,r28,r23
	r27.u64 = r28.u64 + r23.u64;
	// stvx v15,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,304
	r11.s64 = ctx.r1.s64 + 304;
	// add r26,r27,r23
	r26.u64 = r27.u64 + r23.u64;
	// add r25,r26,r23
	r25.u64 = r26.u64 + r23.u64;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v13,8
	// addi r11,r1,368
	r11.s64 = ctx.r1.s64 + 368;
	// add r24,r25,r23
	r24.u64 = r25.u64 + r23.u64;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v13,15
	// addi r11,r1,192
	r11.s64 = ctx.r1.s64 + 192;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// stvx128 v63,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v63.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,2
	r11.s64 = 2;
	// stw r11,320(r1)
	PPC_STORE_U32(ctx.r1.u32 + 320, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,288
	r11.s64 = ctx.r1.s64 + 288;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,0
	r11.s64 = 0;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// vsplth v23,v13,1
	_mm_store_si128((__m128i*)v23.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xD0C))));
	// vaddshs v13,v23,v23
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v23.s16)));
	// stvx v23,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8266A37C:
	// add r10,r11,r31
	ctx.r10.u64 = r11.u64 + r31.u64;
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// li r21,16
	r21.s64 = 16;
	// li r20,16
	r20.s64 = 16;
	// add r8,r11,r27
	ctx.r8.u64 = r11.u64 + r27.u64;
	// li r19,16
	r19.s64 = 16;
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r7,r11,r26
	ctx.r7.u64 = r11.u64 + r26.u64;
	// lvrx v13,r10,r21
	temp.u32 = ctx.r10.u32 + r21.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r18,16
	r18.s64 = 16;
	// lvrx v11,r9,r20
	temp.u32 = ctx.r9.u32 + r20.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvlx v10,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r8,r19
	temp.u32 = ctx.r8.u32 + r19.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r6,r11,r25
	ctx.r6.u64 = r11.u64 + r25.u64;
	// vor v10,v10,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvlx v12,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r7,r18
	temp.u32 = ctx.r7.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r17,16
	r17.s64 = 16;
	// vor v9,v12,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// add r5,r11,r29
	ctx.r5.u64 = r11.u64 + r29.u64;
	// vor v12,v11,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// li r16,16
	r16.s64 = 16;
	// vor v11,v10,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// add r4,r11,r28
	ctx.r4.u64 = r11.u64 + r28.u64;
	// li r15,16
	r15.s64 = 16;
	// vor v10,v9,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// lvrx v7,r6,r17
	temp.u32 = ctx.r6.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r3,r11,r24
	ctx.r3.u64 = r11.u64 + r24.u64;
	// vor v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v5,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r5,r16
	temp.u32 = ctx.r5.u32 + r16.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r14,16
	r14.s64 = 16;
	// vor v7,v5,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v4,r4,r15
	temp.u32 = ctx.r4.u32 + r15.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v5,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvlx v4,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v3,r3,r14
	temp.u32 = ctx.r3.u32 + r14.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v4,v4,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v3,v13,v11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v12,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v10,v7,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v2,v5,v4
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrglb v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrglb v7,v5,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v5,v3,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v4,v11,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrglb v10,v3,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v3,v13,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v11,v11,v2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrglb v13,v13,v9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v9,v12,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v7,v5,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v2,v10,v11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vmrglb v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v29,v3,v9
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v30,v10,v11
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vmrglb v21,v3,v9
	_mm_store_si128((__m128i*)v21.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v20,v13,v12
	_mm_store_si128((__m128i*)v20.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v19,v13,v12
	_mm_store_si128((__m128i*)v19.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v31,v0,v7
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v4,v0,v5
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v12,v0,v5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v0,v30
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v13,v31,v7
	// vmrglb v3,v0,v30
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v2,v7,v4
	// vmrghb v5,v0,v29
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v14,v4,v12
	// vmrglb v27,v0,v29
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v22,v10,v9
	// vsubshs v25,v0,v13
	// li r12,416
	r12.s64 = 416;
	// stvx128 v12,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v30,v12,v10
	// li r12,352
	r12.s64 = 352;
	// stvx128 v11,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v24,v0,v14
	// addi r10,r1,288
	ctx.r10.s64 = ctx.r1.s64 + 288;
	// vsubshs v29,v9,v11
	// vmaxsh v13,v25,v13
	// vsubshs v25,v0,v2
	// vsubshs v17,v0,v22
	// vsubshs v1,v11,v3
	// vsubshs v28,v5,v27
	// vsubshs v26,v3,v5
	// vmaxsh v2,v25,v2
	// vsubshs v18,v0,v30
	// vmaxsh v25,v24,v14
	// vsubshs v16,v0,v29
	// vmaxsh v24,v17,v22
	// vsubshs v11,v0,v26
	// vsubshs v12,v0,v1
	// vsubshs v17,v0,v28
	// vmaxsh v30,v18,v30
	// vmaxsh v29,v16,v29
	// vcmpgtuh v13,v6,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vcmpgtuh v2,v6,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmaxsh v18,v12,v1
	// vmaxsh v26,v11,v26
	// vmaxsh v17,v17,v28
	// vcmpgtuh v30,v6,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vcmpgtuh v25,v6,v25
	_mm_store_si128((__m128i*)v25.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vcmpgtuh v29,v6,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vcmpgtuh v24,v6,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtuh v18,v6,v18
	_mm_store_si128((__m128i*)v18.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v18.u16)));
	// vcmpgtuh v26,v6,v26
	_mm_store_si128((__m128i*)v26.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vcmpgtuh v17,v6,v17
	_mm_store_si128((__m128i*)v17.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v17.u16)));
	// vaddshs v2,v25,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v30,v24,v29
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v18,v26
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v13,v17,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v13,v29,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v13,v2,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v2,r0,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,304
	ctx.r10.s64 = ctx.r1.s64 + 304;
	// vsubshs v13,v0,v13
	// vperm v2,v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,256
	ctx.r10.s64 = ctx.r1.s64 + 256;
	// vcmpgtsh. v16,v2,v13
	// stvx v16,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mfocrf r9,2
	ctx.r9.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r10,r9,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20;
	// li r12,416
	r12.s64 = 416;
	// lvx128 v12,r1,r12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,352
	r12.s64 = 352;
	// lvx128 v11,r1,r12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// beq cr6,0x8266a994
	if (cr6.eq) goto loc_8266A994;
	// vmaxsh v29,v12,v10
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// vminsh v26,v12,v10
	// vmaxsh v13,v7,v4
	// vminsh v30,v7,v4
	// vmaxsh v25,v9,v11
	// vminsh v24,v9,v11
	// vmaxsh v18,v3,v5
	// vminsh v17,v3,v5
	// vmaxsh v13,v13,v29
	// vminsh v30,v30,v26
	// vmaxsh v29,v25,v18
	// vminsh v26,v24,v17
	// vmaxsh v13,v13,v29
	// vminsh v30,v30,v26
	// vsubshs v13,v13,v30
	// lvx128 v30,r0,r10
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupkhsh v29,v13
	_mm_store_si128((__m128i*)v29.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// vupklsh v26,v13
	_mm_store_si128((__m128i*)v26.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v13.s16)));
	// vcmpgtsh. v13,v30,v13
	// mfocrf r10,2
	ctx.r10.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v30,v30
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16))));
	// stw r10,392(r1)
	PPC_STORE_U32(ctx.r1.u32 + 392, ctx.r10.u32);
	// vand v13,v13,v16
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vcmpgtsw. v29,v30,v29
	// mfocrf r8,2
	ctx.r8.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vcmpgtsw. v30,v30,v26
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// vupkhsh v30,v2
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// lvx128 v29,r0,r10
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcmpgtsw. v30,v30,v29
	// mfocrf r10,2
	ctx.r10.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vcmpgtsw. v2,v2,v29
	// mfocrf r6,2
	ctx.r6.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// beq cr6,0x8266a660
	if (cr6.eq) goto loc_8266A660;
	// rlwinm r8,r8,0,26,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r8,32
	cr6.compare<int32_t>(ctx.r8.s32, 32, xer);
	// bne cr6,0x8266a678
	if (!cr6.eq) goto loc_8266A678;
loc_8266A660:
	// rlwinm r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// beq cr6,0x8266a994
	if (cr6.eq) goto loc_8266A994;
	// rlwinm r10,r7,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// beq cr6,0x8266a994
	if (cr6.eq) goto loc_8266A994;
loc_8266A678:
	// vsubshs v25,v0,v28
	// addi r10,r1,368
	ctx.r10.s64 = ctx.r1.s64 + 368;
	// vsubshs v26,v7,v31
	// li r12,64
	r12.s64 = 64;
	// stvx128 v0,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v18,v31,v31
	_mm_store_si128((__m128i*)v18.u8, _mm_load_si128((__m128i*)v31.u8));
	// vor v17,v27,v27
	_mm_store_si128((__m128i*)v17.u8, _mm_load_si128((__m128i*)v27.u8));
	// vmaxsh v28,v25,v28
	// vsubshs v25,v0,v26
	// vaddshs v2,v10,v9
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v29,v11,v3
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vcmpgtsh v28,v23,v28
	// vmaxsh v26,v25,v26
	// vaddshs v25,v2,v2
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v30,v4,v12
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vandc v24,v5,v28
	// vcmpgtsh v26,v23,v26
	// vand v28,v27,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vaddshs v15,v29,v29
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v25,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vand v31,v31,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vandc v27,v7,v26
	// vxor v28,v28,v24
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v25,v30,v25
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vaddshs v16,v30,v30
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vxor v31,v31,v27
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vaddshs v27,v5,v28
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v30,v28,v28
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v28,v12,v12
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v26,v31,v7
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v31,v31,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// li r12,448
	r12.s64 = 448;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v23,v27,v27
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// li r12,416
	r12.s64 = 416;
	// stvx128 v28,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v24,v26,v26
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,352
	r12.s64 = 352;
	// stvx128 v31,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v15,v23
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v23.s16)));
	// lvx128 v15,r0,r10
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v28,v15,v15
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)v15.u8));
	// vaddshs v16,v24,v16
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v15,v10,v10
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v30,v30,v28
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v31,v16,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v16,v11,v5
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v28,v31,v2
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v31,v31,v29
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v2,v30
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v2,v25,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v16,v15,v16
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v25,v2,v0
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v0,v7,v12
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// li r12,352
	r12.s64 = 352;
	// lvx128 v30,r1,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v28,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v30.s16)));
	// li r12,352
	r12.s64 = 352;
	// stvx128 v1,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v28,v31,v26
	// li r12,416
	r12.s64 = 416;
	// lvx128 v26,r1,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v26,v31,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,448
	r12.s64 = 448;
	// lvx128 v31,r1,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v31,v29,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v31.s16)));
	// li r12,448
	r12.s64 = 448;
	// stvx128 v6,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v29,v2,v27
	// vaddshs v2,v24,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v24,v4,v4
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v27,v31,v23
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v23,v3,v3
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v6,v9,v9
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v30,v30,v24
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vaddshs v1,v10,v11
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v23,v31,v23
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v24,v12,v9
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vsrah v2,v2,v8
	// vaddshs v31,v30,v1
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v15,v6,v0
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v30,v23,v24
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vand v23,v2,v13
	_mm_store_si128((__m128i*)v23.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v28,v28,v16
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v29,v29,v15
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v15.s16)));
	// vandc v24,v7,v13
	// vsrah v26,v26,v8
	// vsrah v27,v27,v8
	// vsrah v2,v31,v8
	// vsrah v31,v28,v8
	// add r10,r11,r31
	ctx.r10.u64 = r11.u64 + r31.u64;
	// vsrah v28,v25,v8
	// li r8,16
	ctx.r8.s64 = 16;
	// vand v27,v27,v13
	_mm_store_si128((__m128i*)v27.u8, _mm_and_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r7,16
	ctx.r7.s64 = 16;
	// vandc v25,v5,v13
	// li r6,16
	ctx.r6.s64 = 16;
	// vsrah v30,v30,v8
	// li r5,16
	ctx.r5.s64 = 16;
	// vsrah v29,v29,v8
	// li r4,16
	ctx.r4.s64 = 16;
	// vand v2,v2,v13
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v4,v4,v13
	// vxor v27,v27,v25
	_mm_store_si128((__m128i*)v27.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vandc v25,v12,v13
	// vand v26,v26,v13
	_mm_store_si128((__m128i*)v26.u8, _mm_and_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor v24,v23,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vand v31,v31,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v23,v10,v13
	// vandc128 v63,v3,v13
	_mm_store_si128((__m128i*)v63.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vand v30,v30,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v29,v29,v13
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v28,v28,v13
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v16,v9,v13
	// vandc v15,v11,v13
	// vpkshus v13,v18,v24
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vxor v3,v2,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vpkshus v4,v27,v17
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vxor v2,v26,v25
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vxor v25,v31,v23
	_mm_store_si128((__m128i*)v25.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vxor128 v30,v30,v63
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v63.u8)));
	// vxor v24,v29,v16
	_mm_store_si128((__m128i*)v24.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vxor v31,v28,v15
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vpkshus v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vpkshus v2,v25,v24
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vpkshus v31,v31,v30
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vmrghb v30,v13,v4
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v4
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v29,v2,v20
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v4,v3,v21
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v28,v31,v19
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrglb v2,v2,v20
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrglb v3,v3,v21
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v31,v31,v19
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrghb v27,v30,v29
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrghb v26,v4,v28
	_mm_store_si128((__m128i*)v26.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrglb v30,v30,v29
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrglb v4,v4,v28
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrghb v29,v13,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v28,v3,v31
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v3,v3,v31
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v31,v27,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vmrghb v2,v27,v26
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vmrghb v27,v30,v4
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrglb v4,v30,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrghb v30,v29,v28
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vmrglb v29,v29,v28
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vor v23,v4,v4
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmrghb v28,v2,v30
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v21,v4,v4
	_mm_store_si128((__m128i*)v21.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmrglb v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v30,v13,v3
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v4,v31,v29
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vor v20,v2,v2
	_mm_store_si128((__m128i*)v20.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vmrglb v3,v31,v29
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vor v29,v2,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vmrglb v31,v27,v30
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v27.u8)));
	// stvlx v28,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v28.u8[15 - i]);
	// vmrghb v2,v27,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v27.u8)));
	// stvrx v28,r10,r8
	ea = ctx.r10.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v28.u8[i]);
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// vmrghb v30,v23,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vmrglb v13,v21,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v21.u8)));
	// stvlx v20,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v20.u8[15 - i]);
	// stvrx v29,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v29.u8[i]);
	// add r10,r11,r29
	ctx.r10.u64 = r11.u64 + r29.u64;
	// stvlx v4,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v4.u8[15 - i]);
	// stvrx v4,r10,r6
	ea = ctx.r10.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v4.u8[i]);
	// add r10,r11,r28
	ctx.r10.u64 = r11.u64 + r28.u64;
	// li r12,64
	r12.s64 = 64;
	// lvx128 v0,r1,r12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,448
	r12.s64 = 448;
	// lvx128 v6,r1,r12
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,352
	r12.s64 = 352;
	// lvx128 v1,r1,r12
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvlx v3,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v3.u8[15 - i]);
	// stvrx v3,r10,r5
	ea = ctx.r10.u32 + ctx.r5.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v3.u8[i]);
	// add r10,r11,r27
	ctx.r10.u64 = r11.u64 + r27.u64;
	// stvlx v2,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v2.u8[15 - i]);
	// stvrx v2,r10,r4
	ea = ctx.r10.u32 + ctx.r4.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v2.u8[i]);
	// add r10,r11,r26
	ctx.r10.u64 = r11.u64 + r26.u64;
	// stvlx v31,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v31.u8[15 - i]);
	// stvrx v31,r10,r8
	ea = ctx.r10.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v31.u8[i]);
	// add r10,r11,r25
	ctx.r10.u64 = r11.u64 + r25.u64;
	// stvlx v30,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v30.u8[15 - i]);
	// stvrx v30,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v30.u8[i]);
	// add r10,r11,r24
	ctx.r10.u64 = r11.u64 + r24.u64;
	// stvlx v13,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// stvrx v13,r10,r6
	ea = ctx.r10.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v23,r0,r10
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,256
	ctx.r10.s64 = ctx.r1.s64 + 256;
	// lvx128 v16,r0,r10
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// lvx128 v15,r0,r10
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x8266a99c
	goto loc_8266A99C;
loc_8266A994:
	// vor v24,v9,v9
	_mm_store_si128((__m128i*)v24.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v25,v10,v10
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
loc_8266A99C:
	// rlwinm r10,r9,0,24,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r10,128
	cr6.compare<int32_t>(ctx.r10.s32, 128, xer);
	// beq cr6,0x8266aae8
	if (cr6.eq) goto loc_8266AAE8;
	// vsubshs v12,v12,v11
	// addi r9,r1,192
	ctx.r9.s64 = ctx.r1.s64 + 192;
	// vsubshs v13,v7,v10
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// vslh v11,v14,v15
	// vsubshs v7,v9,v5
	// vslh v5,v1,v15
	// vaddshs v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v11,v11,v14
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vsubshs v10,v9,v10
	// vaddshs v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v7,v5,v1
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsubshs v13,v13,v11
	// vaddshs v5,v12,v12
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vslh v4,v22,v15
	// vmaxsh v10,v22,v10
	// vaddshs v12,v13,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsubshs v13,v9,v7
	// vaddshs v4,v4,v22
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vsubshs v9,v0,v12
	// vaddshs v11,v13,v8
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsubshs v13,v5,v4
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// vmaxsh v12,v12,v9
	// vsubshs v7,v0,v11
	// vaddshs v13,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsrah v9,v12,v6
	// vmaxsh v12,v11,v7
	// vsrah v7,v13,v5
	// vsrah v11,v12,v6
	// vsubshs v12,v0,v13
	// vminsh v11,v9,v11
	// vmaxsh v12,v13,v12
	// vsrah v13,v22,v5
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v5,v10,v5
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// vsrah v12,v12,v6
	// vxor v9,v7,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v10,v12,v11
	// vcmpgtsh v4,v23,v12
	// vcmpgtsh v12,v12,v11
	// vandc v11,v9,v16
	// vslh v9,v10,v15
	// vand v12,v4,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vsrah v10,v10,v6
	// vand v10,v10,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vminsh v12,v5,v12
	// vxor v12,v12,v13
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v13,v12,v13
	// vsubshs v12,v25,v13
	// vaddshs v13,v24,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vmrghh v11,v12,v13
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vpkshus v13,v11,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vmrghh v12,v0,v13
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v13,v0,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r9,128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r8,132(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r7,136(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r6,140(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// sthx r9,r31,r10
	PPC_STORE_U16(r31.u32 + ctx.r10.u32, ctx.r9.u16);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// sthx r8,r30,r10
	PPC_STORE_U16(r30.u32 + ctx.r10.u32, ctx.r8.u16);
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r8,124(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// sthx r7,r29,r10
	PPC_STORE_U16(r29.u32 + ctx.r10.u32, ctx.r7.u16);
	// sthx r6,r28,r10
	PPC_STORE_U16(r28.u32 + ctx.r10.u32, ctx.r6.u16);
	// sthx r5,r27,r10
	PPC_STORE_U16(r27.u32 + ctx.r10.u32, ctx.r5.u16);
	// sthx r4,r26,r10
	PPC_STORE_U16(r26.u32 + ctx.r10.u32, ctx.r4.u16);
	// sthx r9,r25,r10
	PPC_STORE_U16(r25.u32 + ctx.r10.u32, ctx.r9.u16);
	// sthx r8,r24,r10
	PPC_STORE_U16(r24.u32 + ctx.r10.u32, ctx.r8.u16);
loc_8266AAE8:
	// rlwinm r10,r23,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r10,320(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// cmpw cr6,r22,r10
	cr6.compare<int32_t>(r22.s32, ctx.r10.s32, xer);
	// blt cr6,0x8266a37c
	if (cr6.lt) goto loc_8266A37C;
loc_8266AB00:
	// addi r1,r1,624
	ctx.r1.s64 = ctx.r1.s64 + 624;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8266AB08"))) PPC_WEAK_FUNC(sub_8266AB08);
PPC_FUNC_IMPL(__imp__sub_8266AB08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// lis r11,-32243
	r11.s64 = -2113077248;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// addi r22,r11,3824
	r22.s64 = r11.s64 + 3824;
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// beq cr6,0x8266b1cc
	if (cr6.eq) goto loc_8266B1CC;
	// stw r7,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, ctx.r7.u32);
	// subf r3,r8,r4
	ctx.r3.s64 = ctx.r4.s64 - ctx.r8.s64;
	// add r7,r4,r8
	ctx.r7.u64 = ctx.r4.u64 + ctx.r8.u64;
	// vspltish v13,1
	// addi r11,r1,-336
	r11.s64 = ctx.r1.s64 + -336;
	// stw r4,-400(r1)
	PPC_STORE_U32(ctx.r1.u32 + -400, ctx.r4.u32);
	// subf r10,r8,r3
	ctx.r10.s64 = ctx.r3.s64 - ctx.r8.s64;
	// vspltish v10,15
	// add r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 + ctx.r8.u64;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// subf r9,r8,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r3,-404(r1)
	PPC_STORE_U32(ctx.r1.u32 + -404, ctx.r3.u32);
	// add r5,r6,r8
	ctx.r5.u64 = ctx.r6.u64 + ctx.r8.u64;
	// stw r7,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r7.u32);
	// li r29,0
	r29.s64 = 0;
	// vspltish v18,2
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// subf r11,r8,r9
	r11.s64 = ctx.r9.s64 - ctx.r8.s64;
	// add r30,r5,r8
	r30.u64 = ctx.r5.u64 + ctx.r8.u64;
	// stw r10,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, ctx.r10.u32);
	// li r28,4
	r28.s64 = 4;
	// stw r9,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, ctx.r9.u32);
	// stw r6,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, ctx.r6.u32);
	// subf r31,r8,r11
	r31.s64 = r11.s64 - ctx.r8.s64;
	// stw r5,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, ctx.r5.u32);
	// vspltish v9,3
	// stw r11,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, r11.u32);
	// vspltish v12,4
	// stw r30,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, r30.u32);
	// vspltish v23,5
	// stw r29,-408(r1)
	PPC_STORE_U32(ctx.r1.u32 + -408, r29.u32);
	// vspltish v19,8
	// stw r28,-412(r1)
	PPC_STORE_U32(ctx.r1.u32 + -412, r28.u32);
	// lvlx v8,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,-272
	ctx.r10.s64 = ctx.r1.s64 + -272;
	// lvlx v13,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltisw v14,4
	_mm_store_si128((__m128i*)v14.u32, _mm_set1_epi32(int(0x4)));
	// lvlx v7,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v10,v8,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// lvlx v6,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v8,v7,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v6,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// lvlx v4,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v28,v0,v4
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v5,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v2,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v4,v0,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v31,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v3,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v10,v0,v5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v21,v4,v11
	// vmrghb v3,v0,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v26,v0,v31
	_mm_store_si128((__m128i*)v26.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v30,v11,v8
	// vsubshs v25,v8,v7
	// vsubshs v20,v10,v3
	// vsubshs v31,v5,v26
	// vsubshs v29,v7,v10
	// vsubshs v27,v3,v5
	// addi r10,r1,-304
	ctx.r10.s64 = ctx.r1.s64 + -304;
	// lvx128 v6,r0,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-288
	ctx.r10.s64 = ctx.r1.s64 + -288;
	// vsplth v1,v6,1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_set1_epi16(short(0xD0C))));
	// vmrghb v6,v0,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v24,v1,v1
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsubshs v2,v6,v4
	// vupkhsh v13,v24
	_mm_store_si128((__m128i*)ctx.v13.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v24.s16))));
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v13,v28,v6
	// vsubshs v22,v0,v13
	// vmaxsh v13,v22,v13
	// vsubshs v22,v0,v2
	// vmaxsh v2,v22,v2
	// vsubshs v22,v0,v30
	// vmaxsh v30,v22,v30
	// vsubshs v16,v0,v29
	// li r12,-304
	r12.s64 = -304;
	// stvx128 v12,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v22,v0,v21
	// li r12,-256
	r12.s64 = -256;
	// stvx128 v11,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v15,v0,v20
	// li r12,-240
	r12.s64 = -240;
	// stvx128 v7,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v17,v0,v25
	// lvx128 v7,r0,r22
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v12,v0,v27
	// vsubshs v11,v0,v31
	// vmaxsh v29,v16,v29
	// vmaxsh v22,v22,v21
	// vmaxsh v16,v15,v20
	// vmaxsh v17,v17,v25
	// vcmpgtuh v13,v9,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vcmpgtuh v2,v9,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmaxsh v27,v12,v27
	// vmaxsh v15,v11,v31
	// vcmpgtuh v30,v9,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vcmpgtuh v22,v9,v22
	_mm_store_si128((__m128i*)v22.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vcmpgtuh v29,v9,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vcmpgtuh v17,v9,v17
	_mm_store_si128((__m128i*)v17.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)v17.u16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtuh v15,v9,v15
	_mm_store_si128((__m128i*)v15.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)v15.u16)));
	// vcmpgtuh v16,v9,v16
	_mm_store_si128((__m128i*)v16.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)v16.u16)));
	// vcmpgtuh v27,v9,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vaddshs v2,v22,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v30,v17,v29
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v13,v15,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v29,v16,v27
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v13,v29,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v13,v2,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vsubshs v13,v0,v13
	// vperm v2,v13,v13,v7
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vcmpgtsh. v15,v2,v23
	// mfocrf r10,2
	ctx.r10.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r9,r10,0,26,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	// li r12,-304
	r12.s64 = -304;
	// lvx128 v12,r1,r12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-256
	r12.s64 = -256;
	// lvx128 v11,r1,r12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r10,-416(r1)
	PPC_STORE_U32(ctx.r1.u32 + -416, ctx.r10.u32);
	// li r12,-240
	r12.s64 = -240;
	// lvx128 v7,r1,r12
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// beq cr6,0x8266b0a8
	if (cr6.eq) goto loc_8266B0A8;
	// vmaxsh v29,v11,v8
	// vminsh v27,v11,v8
	// vmaxsh v13,v6,v4
	// vminsh v30,v6,v4
	// vmaxsh v23,v7,v10
	// vminsh v22,v7,v10
	// vmaxsh v17,v3,v5
	// vminsh v16,v3,v5
	// vmaxsh v13,v13,v29
	// vminsh v30,v30,v27
	// vmaxsh v29,v23,v17
	// vminsh v27,v22,v16
	// vmaxsh v13,v13,v29
	// vminsh v30,v30,v27
	// vsubshs v13,v13,v30
	// vupkhsh v30,v13
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// vupklsh v29,v13
	_mm_store_si128((__m128i*)v29.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v13.s16)));
	// vcmpgtsh. v13,v24,v13
	// mfocrf r9,2
	ctx.r9.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// stw r9,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, ctx.r9.u32);
	// addi r9,r1,-288
	ctx.r9.s64 = ctx.r1.s64 + -288;
	// vand v13,v13,v15
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v15.u8)));
	// lvx128 v27,r0,r9
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcmpgtsw. v30,v27,v30
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vcmpgtsw. v30,v27,v29
	// mfocrf r6,2
	ctx.r6.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v30,v2
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtsw. v30,v30,v14
	// mfocrf r9,2
	ctx.r9.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vcmpgtsw. v2,v2,v14
	// mfocrf r5,2
	ctx.r5.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r9,r9,0,26,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// beq cr6,0x8266addc
	if (cr6.eq) goto loc_8266ADDC;
	// rlwinm r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// bne cr6,0x8266adf4
	if (!cr6.eq) goto loc_8266ADF4;
loc_8266ADDC:
	// rlwinm r9,r9,0,26,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// beq cr6,0x8266b0a8
	if (cr6.eq) goto loc_8266B0A8;
	// rlwinm r9,r6,0,26,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// beq cr6,0x8266b0a8
	if (cr6.eq) goto loc_8266B0A8;
loc_8266ADF4:
	// vsubshs v24,v0,v31
	// vsubshs v27,v6,v28
	// vaddshs v2,v8,v7
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v30,v4,v11
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vmaxsh v31,v24,v31
	// vsubshs v24,v0,v27
	// vaddshs v29,v10,v3
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v17,v30,v30
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vcmpgtsh v31,v1,v31
	// vmaxsh v27,v24,v27
	// vaddshs v24,v2,v2
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v16,v29,v29
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vandc v23,v5,v31
	// vand v31,v26,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vcmpgtsh v27,v1,v27
	// vaddshs v29,v24,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v24,v30,v24
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vxor v31,v31,v23
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vand v28,v28,v27
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vandc v26,v6,v27
	// vaddshs v30,v11,v11
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v27,v5,v31
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v31,v31,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vxor v28,v28,v26
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v26.u8)));
	// li r12,-256
	r12.s64 = -256;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v23,v27,v27
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// li r12,-304
	r12.s64 = -304;
	// stvx128 v31,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v26,v28,v6
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v14,v28,v28
	_mm_store_si128((__m128i*)v14.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v16,v16,v23
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v28,v10,v10
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v22,v26,v26
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v30,v16,v19
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v19.s16)));
	// li r12,-240
	r12.s64 = -240;
	// stvx128 v28,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v16,v6,v11
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v17,v22,v17
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vaddshs v31,v17,v19
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v17,v8,v8
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v19,v10,v5
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v28,v31,v2
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v31,v31,v29
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v2,v30
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v2,v24,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v30,v28,v14
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vsubshs v28,v31,v26
	// vaddshs v19,v17,v19
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vsubshs v27,v2,v27
	// vaddshs v26,v22,v30
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v22,v3,v3
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v28,v28,v19
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v19.s16)));
	// li r12,-304
	r12.s64 = -304;
	// lvx128 v14,r1,r12
	_mm_store_si128((__m128i*)v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v29,v29,v14
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v14.s16)));
	// li r12,-256
	r12.s64 = -256;
	// lvx128 v14,r1,r12
	_mm_store_si128((__m128i*)v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v31,v31,v14
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v14.s16)));
	// li r12,-256
	r12.s64 = -256;
	// stvx128 v0,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v8,v10
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v24,v29,v23
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v23,v4,v4
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v22,v29,v22
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v22.s16)));
	// li r12,-240
	r12.s64 = -240;
	// lvx128 v14,r1,r12
	_mm_store_si128((__m128i*)v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v2,v14
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vaddshs v23,v30,v23
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v30,v11,v7
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v14,v7,v7
	_mm_store_si128((__m128i*)v14.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsrah v2,v2,v12
	// vaddshs v29,v23,v0
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// li r12,-304
	r12.s64 = -304;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v30,v26,v12
	// vaddshs v17,v14,v16
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v14.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vsrah v24,v24,v12
	// vaddshs v27,v27,v17
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v17.s16)));
	// li r12,-304
	r12.s64 = -304;
	// lvx128 v23,r1,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v26,v22,v23
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vsrah v23,v31,v12
	// vand v31,v30,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v22,v6,v13
	// vsrah v30,v29,v12
	// vsrah v29,v28,v12
	// vsrah v28,v27,v12
	// vsrah v27,v26,v12
	// vxor v31,v31,v22
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v22.u8)));
	// vand v27,v27,v13
	_mm_store_si128((__m128i*)v27.u8, _mm_and_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r12,-256
	r12.s64 = -256;
	// lvx128 v0,r1,r12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vandc v26,v11,v13
	// vandc v3,v3,v13
	// vand v23,v23,v13
	_mm_store_si128((__m128i*)v23.u8, _mm_and_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v19,v29,v13
	_mm_store_si128((__m128i*)v19.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v28,v28,v13
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor v3,v27,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vxor v29,v23,v26
	_mm_store_si128((__m128i*)v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vandc v4,v4,v13
	// vand v30,v30,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v14,v2,v13
	_mm_store_si128((__m128i*)v14.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v3,v3,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vandc v27,v5,v13
	// vpkshus v29,v29,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vand v26,v24,v13
	_mm_store_si128((__m128i*)v26.u8, _mm_and_si128(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v22,v8,v13
	// vandc v17,v7,v13
	// vandc v16,v10,v13
	// vpkshus v13,v31,v31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vxor v30,v30,v4
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vxor v4,v19,v22
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v22.u8)));
	// vxor v31,v26,v27
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vxor v2,v28,v17
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vpkshus v30,v30,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vxor v28,v14,v16
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vpkshus v27,v4,v4
	_mm_store_si128((__m128i*)v27.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// stvewx v13,r11,r29
	ea = (r11.u32 + r29.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// lwz r10,-320(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// vpkshus v26,v2,v2
	_mm_store_si128((__m128i*)v26.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vpkshus v28,v28,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-408(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// lwz r10,-364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// vpkshus v13,v31,v31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// stvewx v30,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// lwz r10,-364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// stvewx v30,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-408(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// lwz r10,-368(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// stvewx v29,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// lwz r10,-368(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// stvewx v29,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-408(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// lwz r10,-404(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -404);
	// stvewx v27,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// lwz r10,-404(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -404);
	// stvewx v27,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-408(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// lwz r10,-400(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -400);
	// stvewx v26,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// lwz r10,-400(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -400);
	// stvewx v26,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-408(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// lwz r10,-356(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -356);
	// stvewx v28,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// lwz r10,-356(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -356);
	// stvewx v28,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-408(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// lwz r10,-360(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// stvewx v3,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v3.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// lwz r10,-360(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// stvewx v3,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v3.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-408(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// lwz r10,-352(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,-352(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// lwz r11,-412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r8,60(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r3,-404(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -404);
	// lwz r29,-408(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// lwz r10,-416(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -416);
	// b 0x8266b0b0
	goto loc_8266B0B0;
loc_8266B0A8:
	// vor v2,v7,v7
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v4,v8,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
loc_8266B0B0:
	// rlwinm r11,r10,0,24,24
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r11,128
	cr6.compare<int32_t>(r11.s32, 128, xer);
	// beq cr6,0x8266b1cc
	if (cr6.eq) goto loc_8266B1CC;
	// vsubshs v11,v11,v10
	// addi r11,r1,-272
	r11.s64 = ctx.r1.s64 + -272;
	// vsubshs v13,v6,v8
	// vslh v10,v21,v18
	// vsubshs v6,v7,v5
	// vslh v5,v20,v18
	// vaddshs v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v10,v10,v21
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)v21.s16)));
	// vsubshs v8,v7,v8
	// vaddshs v7,v6,v6
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v6,v5,v20
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v20.s16)));
	// vsubshs v13,v13,v10
	// vslh v3,v25,v18
	// vaddshs v5,v11,v11
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vmaxsh v8,v25,v8
	// vaddshs v11,v13,v12
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsubshs v13,v7,v6
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v3,v3,v25
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v25.s16)));
	// addi r11,r1,-336
	r11.s64 = ctx.r1.s64 + -336;
	// vaddshs v10,v13,v12
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsubshs v13,v5,v3
	// vaddshs v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsubshs v12,v0,v11
	// vmaxsh v12,v11,v12
	// vsubshs v11,v0,v10
	// vsubshs v0,v0,v13
	// vsrah v12,v12,v9
	// vmaxsh v11,v10,v11
	// vmaxsh v0,v13,v0
	// vsrah v10,v13,v7
	// vsrah v13,v25,v7
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v11,v11,v9
	// vsrah v0,v0,v9
	// vsrah v8,v8,v7
	// vxor v10,v10,v13
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vminsh v12,v12,v11
	// vcmpgtsh v7,v1,v0
	// vsubshs v11,v0,v12
	// vcmpgtsh v0,v0,v12
	// vandc v12,v10,v15
	// vslh v10,v11,v18
	// vand v0,v7,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v11,v11,v9
	// vand v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v0,v11,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vminsh v0,v8,v0
	// vxor v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v0,v0,v13
	// vsubshs v13,v4,v0
	// vaddshs v0,v2,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvewx v13,r3,r29
	ea = (ctx.r3.u32 + r29.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// lwz r10,-404(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -404);
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-408(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	// lwz r10,-400(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -400);
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	// lwz r10,-400(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -400);
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r8,60(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
loc_8266B1CC:
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8266b9b0
	if (cr6.eq) goto loc_8266B9B0;
	// lwz r11,52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// addi r10,r1,-240
	ctx.r10.s64 = ctx.r1.s64 + -240;
	// lvx128 v13,r0,r22
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r4,-5
	r30.s64 = ctx.r4.s64 + -5;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// li r21,0
	r21.s64 = 0;
	// add r29,r30,r8
	r29.u64 = r30.u64 + ctx.r8.u64;
	// vspltish v6,3
	// vspltish v8,4
	// stw r11,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r11.u32);
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r28,r29,r8
	r28.u64 = r29.u64 + ctx.r8.u64;
	// li r11,0
	r11.s64 = 0;
	// add r27,r28,r8
	r27.u64 = r28.u64 + ctx.r8.u64;
	// add r26,r27,r8
	r26.u64 = r27.u64 + ctx.r8.u64;
	// add r25,r26,r8
	r25.u64 = r26.u64 + ctx.r8.u64;
	// add r24,r25,r8
	r24.u64 = r25.u64 + ctx.r8.u64;
	// add r23,r24,r8
	r23.u64 = r24.u64 + ctx.r8.u64;
	// addi r10,r1,-336
	ctx.r10.s64 = ctx.r1.s64 + -336;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-336
	ctx.r10.s64 = ctx.r1.s64 + -336;
	// vsplth v16,v13,1
	_mm_store_si128((__m128i*)v16.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xD0C))));
	// vaddshs v15,v16,v16
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v16,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-304
	ctx.r10.s64 = ctx.r1.s64 + -304;
	// stvx v15,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8266B240:
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// add r9,r11,r29
	ctx.r9.u64 = r11.u64 + r29.u64;
	// li r22,16
	r22.s64 = 16;
	// li r20,16
	r20.s64 = 16;
	// add r7,r11,r26
	ctx.r7.u64 = r11.u64 + r26.u64;
	// li r19,16
	r19.s64 = 16;
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r6,r11,r25
	ctx.r6.u64 = r11.u64 + r25.u64;
	// lvrx v13,r10,r22
	temp.u32 = ctx.r10.u32 + r22.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r18,16
	r18.s64 = 16;
	// lvrx v11,r9,r20
	temp.u32 = ctx.r9.u32 + r20.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvlx v10,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r7,r19
	temp.u32 = ctx.r7.u32 + r19.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r5,r11,r24
	ctx.r5.u64 = r11.u64 + r24.u64;
	// vor v10,v10,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvlx v12,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r6,r18
	temp.u32 = ctx.r6.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r17,16
	r17.s64 = 16;
	// vor v9,v12,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// add r4,r11,r28
	ctx.r4.u64 = r11.u64 + r28.u64;
	// vor v12,v11,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// li r16,16
	r16.s64 = 16;
	// vor v11,v10,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// li r15,16
	r15.s64 = 16;
	// vor v10,v9,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// lvrx v7,r5,r17
	temp.u32 = ctx.r5.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r31,r11,r23
	r31.u64 = r11.u64 + r23.u64;
	// vor v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v5,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r4,r16
	temp.u32 = ctx.r4.u32 + r16.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r14,16
	r14.s64 = 16;
	// vor v7,v5,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v4,r3,r15
	temp.u32 = ctx.r3.u32 + r15.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v5,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvlx v4,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v3,r31,r14
	temp.u32 = r31.u32 + r14.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v4,v4,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v3,v13,v11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v12,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v10,v7,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v2,v5,v4
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrglb v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrglb v7,v5,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v5,v3,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v4,v11,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrglb v10,v3,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v3,v13,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v11,v11,v2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrglb v13,v13,v9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v9,v12,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v7,v5,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v2,v10,v11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vmrglb v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v29,v3,v9
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v30,v10,v11
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vmrglb v21,v3,v9
	_mm_store_si128((__m128i*)v21.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v20,v13,v12
	_mm_store_si128((__m128i*)v20.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v19,v13,v12
	_mm_store_si128((__m128i*)v19.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v31,v0,v7
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v4,v0,v5
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v12,v0,v5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v0,v30
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v13,v31,v7
	// vmrglb v3,v0,v30
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v2,v7,v4
	// vmrghb v5,v0,v29
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v14,v4,v12
	// vmrglb v27,v0,v29
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v22,v10,v9
	// vsubshs v25,v0,v13
	// li r12,-176
	r12.s64 = -176;
	// stvx128 v12,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v30,v12,v10
	// li r12,-224
	r12.s64 = -224;
	// stvx128 v11,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v24,v0,v14
	// addi r10,r1,-240
	ctx.r10.s64 = ctx.r1.s64 + -240;
	// vsubshs v29,v9,v11
	// vmaxsh v13,v25,v13
	// vsubshs v25,v0,v2
	// vsubshs v18,v0,v22
	// vsubshs v1,v11,v3
	// vsubshs v28,v5,v27
	// vsubshs v26,v3,v5
	// vmaxsh v2,v25,v2
	// vsubshs v23,v0,v30
	// vmaxsh v25,v24,v14
	// vsubshs v17,v0,v29
	// vmaxsh v24,v18,v22
	// vsubshs v11,v0,v26
	// vsubshs v12,v0,v1
	// vsubshs v18,v0,v28
	// vmaxsh v30,v23,v30
	// vmaxsh v29,v17,v29
	// vcmpgtuh v13,v6,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vcmpgtuh v2,v6,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmaxsh v23,v12,v1
	// vmaxsh v26,v11,v26
	// vmaxsh v18,v18,v28
	// vcmpgtuh v30,v6,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vcmpgtuh v25,v6,v25
	_mm_store_si128((__m128i*)v25.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vcmpgtuh v29,v6,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vcmpgtuh v24,v6,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vaddshs v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcmpgtuh v23,v6,v23
	_mm_store_si128((__m128i*)v23.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vcmpgtuh v26,v6,v26
	_mm_store_si128((__m128i*)v26.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vcmpgtuh v18,v6,v18
	_mm_store_si128((__m128i*)v18.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v18.u16)));
	// vaddshs v2,v25,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v30,v24,v29
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v23,v26
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v13,v18,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v13,v29,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v13,v2,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v2,r0,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-256
	ctx.r10.s64 = ctx.r1.s64 + -256;
	// vsubshs v13,v0,v13
	// vperm v2,v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vspltish v13,5
	// vcmpgtsh. v17,v2,v13
	// stvx v17,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mfocrf r9,2
	ctx.r9.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r10,r9,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20;
	// li r12,-176
	r12.s64 = -176;
	// lvx128 v12,r1,r12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-224
	r12.s64 = -224;
	// lvx128 v11,r1,r12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// beq cr6,0x8266b848
	if (cr6.eq) goto loc_8266B848;
	// vmaxsh v29,v12,v10
	// vminsh v26,v12,v10
	// vmaxsh v13,v7,v4
	// vminsh v30,v7,v4
	// vmaxsh v25,v9,v11
	// vminsh v24,v9,v11
	// vmaxsh v23,v3,v5
	// vminsh v18,v3,v5
	// vmaxsh v13,v13,v29
	// vminsh v30,v30,v26
	// vmaxsh v29,v25,v23
	// vminsh v26,v24,v18
	// vmaxsh v13,v13,v29
	// vminsh v30,v30,v26
	// vsubshs v13,v13,v30
	// vupkhsh v29,v13
	_mm_store_si128((__m128i*)v29.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16))));
	// vupklsh v26,v13
	_mm_store_si128((__m128i*)v26.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v13.s16)));
	// vcmpgtsh. v13,v15,v13
	// mfocrf r10,2
	ctx.r10.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v30,v15
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v15.s16))));
	// stw r10,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r10.u32);
	// vand v13,v13,v17
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vcmpgtsw. v29,v30,v29
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vcmpgtsw. v30,v30,v26
	// mfocrf r6,2
	ctx.r6.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vupkhsh v30,v2
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vupklsh v29,v2
	_mm_store_si128((__m128i*)v29.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// vspltisw v2,4
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_set1_epi32(int(0x4)));
	// vcmpgtsw. v30,v30,v2
	// mfocrf r10,2
	ctx.r10.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vcmpgtsw. v2,v29,v2
	// mfocrf r5,2
	ctx.r5.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// rlwinm r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// beq cr6,0x8266b514
	if (cr6.eq) goto loc_8266B514;
	// rlwinm r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// bne cr6,0x8266b52c
	if (!cr6.eq) goto loc_8266B52C;
loc_8266B514:
	// rlwinm r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// beq cr6,0x8266b848
	if (cr6.eq) goto loc_8266B848;
	// rlwinm r10,r6,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x20;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// beq cr6,0x8266b848
	if (cr6.eq) goto loc_8266B848;
loc_8266B52C:
	// vsubshs v25,v0,v28
	// addi r10,r1,-336
	ctx.r10.s64 = ctx.r1.s64 + -336;
	// vsubshs v26,v7,v31
	// li r12,-384
	r12.s64 = -384;
	// stvx128 v0,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v18,v31,v31
	_mm_store_si128((__m128i*)v18.u8, _mm_load_si128((__m128i*)v31.u8));
	// vor v17,v27,v27
	_mm_store_si128((__m128i*)v17.u8, _mm_load_si128((__m128i*)v27.u8));
	// vmaxsh v28,v25,v28
	// vsubshs v25,v0,v26
	// lvx128 v24,r0,r10
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v10,v9
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v29,v11,v3
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vcmpgtsh v28,v24,v28
	// vmaxsh v26,v25,v26
	// vaddshs v25,v2,v2
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v30,v4,v12
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v15,v29,v29
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vcmpgtsh v26,v24,v26
	// vandc v24,v5,v28
	// vand v28,v27,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vaddshs v29,v25,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vand v31,v31,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vandc v27,v7,v26
	// vxor v28,v28,v24
	_mm_store_si128((__m128i*)v28.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v25,v30,v25
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vaddshs v16,v30,v30
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vxor v31,v31,v27
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vaddshs v27,v5,v28
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v30,v28,v28
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v28,v12,v12
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v26,v31,v7
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v31,v31,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// li r12,-192
	r12.s64 = -192;
	// stvx128 v30,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v23,v27,v27
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// li r12,-176
	r12.s64 = -176;
	// stvx128 v28,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v28,8
	// vaddshs v24,v26,v26
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,-224
	r12.s64 = -224;
	// stvx128 v31,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v30,v15,v23
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v15,v10,v10
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v16,v24,v16
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v30,v30,v28
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v31,v16,v28
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v16,v11,v5
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v28,v31,v2
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v31,v31,v29
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v29,v2,v30
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v2,v25,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v16,v15,v16
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v25,v2,v0
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v0,v7,v12
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// li r12,-224
	r12.s64 = -224;
	// lvx128 v30,r1,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v28,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v30.s16)));
	// li r12,-224
	r12.s64 = -224;
	// stvx128 v1,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v28,v31,v26
	// li r12,-176
	r12.s64 = -176;
	// lvx128 v26,r1,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v26,v31,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v26.s16)));
	// li r12,-192
	r12.s64 = -192;
	// lvx128 v31,r1,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v31,v29,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v31.s16)));
	// li r12,-192
	r12.s64 = -192;
	// stvx128 v6,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v29,v2,v27
	// vaddshs v2,v24,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v24,v4,v4
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v27,v31,v23
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v23,v3,v3
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v6,v9,v9
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v30,v30,v24
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vaddshs v1,v10,v11
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v23,v31,v23
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v24,v12,v9
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vsrah v2,v2,v8
	// vaddshs v31,v30,v1
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v15,v6,v0
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v30,v23,v24
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vand v23,v2,v13
	_mm_store_si128((__m128i*)v23.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v28,v28,v16
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v29,v29,v15
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v15.s16)));
	// vandc v24,v7,v13
	// vsrah v26,v26,v8
	// vsrah v27,v27,v8
	// vsrah v2,v31,v8
	// vsrah v31,v28,v8
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// vsrah v28,v25,v8
	// li r7,16
	ctx.r7.s64 = 16;
	// vand v27,v27,v13
	_mm_store_si128((__m128i*)v27.u8, _mm_and_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r6,16
	ctx.r6.s64 = 16;
	// vandc v25,v5,v13
	// li r5,16
	ctx.r5.s64 = 16;
	// vsrah v30,v30,v8
	// li r4,16
	ctx.r4.s64 = 16;
	// vsrah v29,v29,v8
	// li r3,16
	ctx.r3.s64 = 16;
	// vand v2,v2,v13
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v4,v4,v13
	// vxor v27,v27,v25
	_mm_store_si128((__m128i*)v27.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vandc v25,v12,v13
	// vand v26,v26,v13
	_mm_store_si128((__m128i*)v26.u8, _mm_and_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor v24,v23,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vand v31,v31,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v23,v10,v13
	// vandc128 v63,v3,v13
	_mm_store_si128((__m128i*)v63.u8, _mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vand v30,v30,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v29,v29,v13
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vand v28,v28,v13
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vandc v16,v9,v13
	// vandc v15,v11,v13
	// vpkshus v13,v18,v24
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vxor v3,v2,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vpkshus v4,v27,v17
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vxor v2,v26,v25
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vxor v25,v31,v23
	_mm_store_si128((__m128i*)v25.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vxor128 v30,v30,v63
	_mm_store_si128((__m128i*)v30.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v63.u8)));
	// vxor v24,v29,v16
	_mm_store_si128((__m128i*)v24.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vxor v31,v28,v15
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vpkshus v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vpkshus v2,v25,v24
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vpkshus v31,v31,v30
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vmrghb v30,v13,v4
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v4
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v29,v2,v20
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v4,v3,v21
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v28,v31,v19
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrglb v2,v2,v20
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrglb v3,v3,v21
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v31,v31,v19
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrghb v27,v30,v29
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrghb v26,v4,v28
	_mm_store_si128((__m128i*)v26.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrglb v30,v30,v29
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrglb v4,v4,v28
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrghb v29,v13,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v28,v3,v31
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v3,v3,v31
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v31,v27,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vmrghb v2,v27,v26
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vmrghb v27,v30,v4
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrglb v4,v30,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrghb v30,v29,v28
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vmrglb v29,v29,v28
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vor v23,v4,v4
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmrghb v28,v2,v30
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v21,v4,v4
	_mm_store_si128((__m128i*)v21.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmrglb v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v30,v13,v3
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v4,v31,v29
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vor v20,v2,v2
	_mm_store_si128((__m128i*)v20.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vmrglb v3,v31,v29
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vor v29,v2,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vmrglb v31,v27,v30
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v27.u8)));
	// stvlx v28,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v28.u8[15 - i]);
	// vmrghb v2,v27,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v27.u8)));
	// stvrx v28,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v28.u8[i]);
	// add r10,r11,r29
	ctx.r10.u64 = r11.u64 + r29.u64;
	// vmrghb v30,v23,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vmrglb v13,v21,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v21.u8)));
	// stvlx v20,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v20.u8[15 - i]);
	// stvrx v29,r10,r6
	ea = ctx.r10.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v29.u8[i]);
	// add r10,r11,r28
	ctx.r10.u64 = r11.u64 + r28.u64;
	// stvlx v4,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v4.u8[15 - i]);
	// stvrx v4,r10,r5
	ea = ctx.r10.u32 + ctx.r5.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v4.u8[i]);
	// add r10,r11,r27
	ctx.r10.u64 = r11.u64 + r27.u64;
	// li r12,-384
	r12.s64 = -384;
	// lvx128 v0,r1,r12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-192
	r12.s64 = -192;
	// lvx128 v6,r1,r12
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-224
	r12.s64 = -224;
	// lvx128 v1,r1,r12
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvlx v3,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v3.u8[15 - i]);
	// stvrx v3,r10,r4
	ea = ctx.r10.u32 + ctx.r4.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v3.u8[i]);
	// add r10,r11,r26
	ctx.r10.u64 = r11.u64 + r26.u64;
	// stvlx v2,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v2.u8[15 - i]);
	// stvrx v2,r10,r3
	ea = ctx.r10.u32 + ctx.r3.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v2.u8[i]);
	// add r10,r11,r25
	ctx.r10.u64 = r11.u64 + r25.u64;
	// stvlx v31,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v31.u8[15 - i]);
	// stvrx v31,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v31.u8[i]);
	// add r10,r11,r24
	ctx.r10.u64 = r11.u64 + r24.u64;
	// stvlx v30,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v30.u8[15 - i]);
	// stvrx v30,r10,r6
	ea = ctx.r10.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v30.u8[i]);
	// add r10,r11,r23
	ctx.r10.u64 = r11.u64 + r23.u64;
	// stvlx v13,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// stvrx v13,r10,r5
	ea = ctx.r10.u32 + ctx.r5.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// addi r10,r1,-336
	ctx.r10.s64 = ctx.r1.s64 + -336;
	// lvx128 v16,r0,r10
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-256
	ctx.r10.s64 = ctx.r1.s64 + -256;
	// lvx128 v17,r0,r10
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-304
	ctx.r10.s64 = ctx.r1.s64 + -304;
	// lvx128 v15,r0,r10
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x8266b850
	goto loc_8266B850;
loc_8266B848:
	// vor v24,v9,v9
	_mm_store_si128((__m128i*)v24.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v25,v10,v10
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
loc_8266B850:
	// rlwinm r10,r9,0,24,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r10,128
	cr6.compare<int32_t>(ctx.r10.s32, 128, xer);
	// beq cr6,0x8266b998
	if (cr6.eq) goto loc_8266B998;
	// vsubshs v12,v12,v11
	// vspltish v11,2
	// vsubshs v13,v7,v10
	// addi r9,r1,-272
	ctx.r9.s64 = ctx.r1.s64 + -272;
	// vsubshs v7,v9,v5
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// vsubshs v9,v9,v10
	// vslh v10,v14,v11
	// vaddshs v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vslh v5,v1,v11
	// vaddshs v7,v7,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v10,v10,v14
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vaddshs v4,v12,v12
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v5,v5,v1
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vslh v3,v22,v11
	// vsubshs v13,v13,v10
	// vmaxsh v9,v22,v9
	// vaddshs v3,v3,v22
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vaddshs v12,v13,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsubshs v13,v7,v5
	// vsubshs v7,v0,v12
	// vaddshs v10,v13,v8
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsubshs v13,v4,v3
	// vmaxsh v12,v12,v7
	// vsubshs v5,v0,v10
	// vaddshs v13,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsrah v7,v12,v6
	// vmaxsh v12,v10,v5
	// vspltish v10,15
	// vsrah v5,v12,v6
	// vsubshs v12,v0,v13
	// vsrah v4,v13,v10
	// vmaxsh v12,v13,v12
	// vsrah v13,v22,v10
	// vspltish v10,1
	// vsrah v12,v12,v6
	// vsrah v3,v9,v10
	// vminsh v10,v7,v5
	// vxor v7,v4,v13
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vcmpgtsh v5,v16,v12
	// vsubshs v9,v12,v10
	// vcmpgtsh v12,v12,v10
	// vandc v10,v7,v17
	// vslh v11,v9,v11
	// vand v12,v5,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vsrah v11,v11,v6
	// vand v11,v11,v12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vand v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vminsh v12,v3,v12
	// vxor v12,v12,v13
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v13,v12,v13
	// vsubshs v12,v25,v13
	// vaddshs v13,v24,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vmrghh v11,v12,v13
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vpkshus v13,v11,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vmrghh v12,v0,v13
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v13,v0,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-288
	ctx.r9.s64 = ctx.r1.s64 + -288;
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r9,-272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// lwz r7,-268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// lwz r6,-264(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// lwz r5,-260(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// lwz r4,-288(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// sthx r9,r30,r10
	PPC_STORE_U16(r30.u32 + ctx.r10.u32, ctx.r9.u16);
	// lwz r3,-284(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// sthx r7,r29,r10
	PPC_STORE_U16(r29.u32 + ctx.r10.u32, ctx.r7.u16);
	// lwz r9,-280(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// lwz r7,-276(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// sthx r6,r28,r10
	PPC_STORE_U16(r28.u32 + ctx.r10.u32, ctx.r6.u16);
	// sthx r5,r27,r10
	PPC_STORE_U16(r27.u32 + ctx.r10.u32, ctx.r5.u16);
	// sthx r4,r26,r10
	PPC_STORE_U16(r26.u32 + ctx.r10.u32, ctx.r4.u16);
	// sthx r3,r25,r10
	PPC_STORE_U16(r25.u32 + ctx.r10.u32, ctx.r3.u16);
	// sthx r9,r24,r10
	PPC_STORE_U16(r24.u32 + ctx.r10.u32, ctx.r9.u16);
	// sthx r7,r23,r10
	PPC_STORE_U16(r23.u32 + ctx.r10.u32, ctx.r7.u16);
loc_8266B998:
	// rlwinm r10,r8,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpw cr6,r21,r10
	cr6.compare<int32_t>(r21.s32, ctx.r10.s32, xer);
	// blt cr6,0x8266b240
	if (cr6.lt) goto loc_8266B240;
loc_8266B9B0:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8266B9B4"))) PPC_WEAK_FUNC(sub_8266B9B4);
PPC_FUNC_IMPL(__imp__sub_8266B9B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266B9B8"))) PPC_WEAK_FUNC(sub_8266B9B8);
PPC_FUNC_IMPL(__imp__sub_8266B9B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r2{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v69{};
	PPCVRegister v72{};
	uint32_t ea{};
	// mr r12,r9
	r12.u64 = ctx.r9.u64;
	// lvx v8,r0,r5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,16
	ctx.r9.s64 = 16;
	// lvx v28,r0,r6
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,32
	ctx.r10.s64 = 32;
	// lvx v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,48
	r11.s64 = 48;
	// vupkhsh v10,v0
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16))));
	// rldicr r2,r7,32,31
	r2.u64 = __builtin_rotateleft64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// vupklsh v0,v0
	_mm_store_si128((__m128i*)ctx.v0.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v0.s16)));
	// li r6,80
	ctx.r6.s64 = 80;
	// vspltish v29,-1
	// lvx v9,r9,r5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r2,r8,r2
	r2.u64 = ctx.r8.u64 + r2.u64;
	// lvx v18,r10,r5
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,96
	ctx.r8.s64 = 96;
	// lvx v19,r11,r5
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r5,r12
	ctx.r5.u64 = r12.u64;
	// li r12,64
	r12.s64 = 64;
	// lvx v1,r9,r3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx v2,r10,r3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupkhsh v11,v1
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v1.s16))));
	// lvx v3,r11,r3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupkhsh v12,v2
	_mm_store_si128((__m128i*)ctx.v12.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// lvx v5,r6,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupkhsh v13,v3
	_mm_store_si128((__m128i*)ctx.v13.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16))));
	// lvx v6,r8,r3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupkhsh v15,v5
	_mm_store_si128((__m128i*)v15.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v5.s16))));
	// lvx v4,r12,r3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r12.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupkhsh v16,v6
	_mm_store_si128((__m128i*)v16.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v6.s16))));
	// vupkhsh v14,v4
	_mm_store_si128((__m128i*)v14.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16))));
	// li r7,112
	ctx.r7.s64 = 112;
	// vcfsx v10,v10,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v10.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// vupklsh v1,v1
	_mm_store_si128((__m128i*)ctx.v1.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v1.s16)));
	// vcfsx v11,v11,0
	_mm_store_ps(ctx.v11.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)));
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcfsx v12,v12,0
	_mm_store_ps(ctx.v12.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v12.u32)));
	// vupklsh v3,v3
	_mm_store_si128((__m128i*)ctx.v3.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v3.s16)));
	// vcfsx v13,v13,0
	_mm_store_ps(ctx.v13.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)));
	// vupklsh v4,v4
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v4.s16)));
	// vcfsx v14,v14,0
	_mm_store_ps(v14.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v14.u32)));
	// lvx v7,r7,r3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v15,v15,0
	_mm_store_ps(v15.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v15.u32)));
	// vupkhsh v17,v7
	_mm_store_si128((__m128i*)v17.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16))));
	// vcfsx v16,v16,0
	_mm_store_ps(v16.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v16.u32)));
	// vupklsh v5,v5
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v5.s16)));
	// vupklsh v6,v6
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v6.s16)));
	// vcfsx v0,v0,0
	_mm_store_ps(ctx.v0.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)));
	// vupklsh v7,v7
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v7.s16)));
	// vcfsx v1,v1,0
	_mm_store_ps(ctx.v1.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v1.u32)));
	// vcfsx v17,v17,0
	_mm_store_ps(v17.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v17.u32)));
	// vspltish v30,0
	// vcfsx v2,v2,0
	_mm_store_ps(ctx.v2.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v2.u32)));
	// vcfsx v3,v3,0
	_mm_store_ps(ctx.v3.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v3.u32)));
	// vcfsx v4,v4,0
	_mm_store_ps(ctx.v4.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)));
	// vcfsx v5,v5,0
	_mm_store_ps(ctx.v5.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// vcfsx v6,v6,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// vcfsx v7,v7,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// vmulfp128 v10,v10,v9
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v11,v11,v8
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v12,v12,v8
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v13,v13,v8
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v14,v14,v8
	_mm_store_ps(v14.f32, _mm_mul_ps(_mm_load_ps(v14.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v15,v15,v8
	_mm_store_ps(v15.f32, _mm_mul_ps(_mm_load_ps(v15.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v16,v16,v8
	_mm_store_ps(v16.f32, _mm_mul_ps(_mm_load_ps(v16.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v17,v17,v8
	_mm_store_ps(v17.f32, _mm_mul_ps(_mm_load_ps(v17.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v0,v0,v8
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v1,v1,v8
	_mm_store_ps(ctx.v1.f32, _mm_mul_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v2,v2,v8
	_mm_store_ps(ctx.v2.f32, _mm_mul_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v3,v3,v8
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v4,v4,v8
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v5,v5,v8
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v6,v6,v8
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v7,v7,v8
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v8.f32)));
	// vctsxs v10,v10,0
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_vctsxs(_mm_load_ps(ctx.v10.f32)));
	// vctsxs v11,v11,0
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_vctsxs(_mm_load_ps(ctx.v11.f32)));
	// vctsxs v12,v12,0
	_mm_store_si128((__m128i*)ctx.v12.s32, _mm_vctsxs(_mm_load_ps(ctx.v12.f32)));
	// vctsxs v13,v13,0
	_mm_store_si128((__m128i*)ctx.v13.s32, _mm_vctsxs(_mm_load_ps(ctx.v13.f32)));
	// vctsxs v14,v14,0
	_mm_store_si128((__m128i*)v14.s32, _mm_vctsxs(_mm_load_ps(v14.f32)));
	// vctsxs v15,v15,0
	_mm_store_si128((__m128i*)v15.s32, _mm_vctsxs(_mm_load_ps(v15.f32)));
	// vctsxs v16,v16,0
	_mm_store_si128((__m128i*)v16.s32, _mm_vctsxs(_mm_load_ps(v16.f32)));
	// vctsxs v3,v3,0
	_mm_store_si128((__m128i*)ctx.v3.s32, _mm_vctsxs(_mm_load_ps(ctx.v3.f32)));
	// vspltish v31,4
	// vctsxs v4,v4,0
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_vctsxs(_mm_load_ps(ctx.v4.f32)));
	// vctsxs v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.s32, _mm_vctsxs(_mm_load_ps(ctx.v0.f32)));
	// vctsxs v1,v1,0
	_mm_store_si128((__m128i*)ctx.v1.s32, _mm_vctsxs(_mm_load_ps(ctx.v1.f32)));
	// vctsxs v17,v17,0
	_mm_store_si128((__m128i*)v17.s32, _mm_vctsxs(_mm_load_ps(v17.f32)));
	// vctsxs v7,v7,0
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vctsxs v2,v2,0
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_vctsxs(_mm_load_ps(ctx.v2.f32)));
	// vctsxs v5,v5,0
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_vctsxs(_mm_load_ps(ctx.v5.f32)));
	// vctsxs v6,v6,0
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_vctsxs(_mm_load_ps(ctx.v6.f32)));
	// vpkswss v23,v13,v3
	// vpkswss v24,v14,v4
	// vpkswss v20,v10,v0
	// vpkswss v21,v11,v1
	// vsrah v13,v23,v29
	// vpkswss v27,v17,v7
	// vsrah v14,v24,v29
	// vpkswss v22,v12,v2
	// vsrah v10,v20,v29
	// vpkswss v25,v15,v5
	// vsrah v11,v21,v29
	// vpkswss v26,v16,v6
	// vcmpequh v4,v30,v24
	// vsrah v17,v27,v29
	// vsrah v12,v22,v29
	// vsrah v15,v25,v29
	// vsrah v16,v26,v29
	// vspltish v29,1
	// vcmpequh v3,v30,v23
	// vsel v13,v18,v19,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vsel v14,v18,v19,v14
	_mm_store_si128((__m128i*)v14.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vsel v10,v18,v19,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vcmpequh v0,v30,v20
	// vsel v11,v18,v19,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vcmpequh v1,v30,v21
	// vcmpequh v7,v30,v27
	// vsel v17,v18,v19,v17
	_mm_store_si128((__m128i*)v17.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vcmpequh v5,v30,v25
	// vcmpequh v6,v30,v26
	// vcmpequh v2,v30,v22
	// vsel v12,v18,v19,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vsel v15,v18,v19,v15
	_mm_store_si128((__m128i*)v15.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vsel v16,v18,v19,v16
	_mm_store_si128((__m128i*)v16.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vsel v13,v13,v30,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v14,v14,v30,v4
	_mm_store_si128((__m128i*)v14.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v14.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v10,v10,v30,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v11,v11,v30,v1
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v11.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v17,v17,v30,v7
	_mm_store_si128((__m128i*)v17.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v17.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v12,v12,v30,v2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v12.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v15,v15,v30,v5
	_mm_store_si128((__m128i*)v15.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v15.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v16,v16,v30,v6
	_mm_store_si128((__m128i*)v16.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v16.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vspltish v30,2
	// vaddshs v2,v24,v14
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vaddshs v8,v23,v13
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vspltish v13,3
	// vand v10,v10,v28
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vaddshs v5,v21,v11
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v6,v27,v17
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vslh v24,v2,v30
	// vaddshs v1,v20,v10
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v4,v22,v12
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vslh v2,v2,v13
	// vaddshs v12,v5,v6
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v7,v25,v15
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v15.s16)));
	// vslh v25,v1,v30
	// vaddshs v2,v2,v24
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vaddshs v3,v26,v16
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vslh v1,v1,v13
	// vslh v24,v12,v31
	// vslh v26,v5,v29
	// vslh v11,v5,v30
	// vaddshs v1,v1,v25
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vsubuhm v9,v24,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v10,v6,v31
	// vaddshs v11,v26,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vslh v25,v6,v13
	// vor128 v14,v69,v69
	_mm_store_si128((__m128i*)v14.u8, _mm_load_si128((__m128i*)v69.u8));
	// vsubuhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsubuhm v11,v9,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vor128 v15,v72,v72
	_mm_store_si128((__m128i*)v15.u8, _mm_load_si128((__m128i*)v72.u8));
	// vaddshs v1,v1,v31
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vslh v27,v5,v30
	// vslh v24,v6,v30
	// vslh v9,v12,v30
	// vslh v6,v6,v31
	// vslh v5,v5,v13
	// vaddshs v12,v7,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v26,v7,v30
	// vsubuhm v6,v9,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vaddshs v5,v27,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vslh v27,v7,v13
	// vsubuhm v10,v10,v25
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vsubuhm v6,v6,v24
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v5,v9,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vslh v9,v12,v30
	// vslh v24,v8,v30
	// vaddshs v26,v26,v27
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vslh v25,v8,v31
	// vslh v27,v8,v31
	// vsubuhm v24,v9,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v26,v9,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vsubuhm v24,v24,v25
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vsubuhm v10,v10,v26
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vslh v26,v12,v31
	// vslh v25,v7,v30
	// vaddshs v11,v11,v24
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vslh v24,v7,v29
	// vsubuhm v9,v26,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v26,v8,v13
	// vaddshs v24,v24,v25
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vslh v25,v4,v30
	// vsubuhm v26,v9,v26
	_mm_store_si128((__m128i*)v26.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vsubuhm v24,v9,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v9,v1,v2
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v26,v26,v27
	_mm_store_si128((__m128i*)v26.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vsubuhm v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v2,v3,v3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vaddshs v5,v5,v24
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vaddshs v6,v6,v26
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vslh v24,v4,v29
	// vslh v27,v2,v30
	// vslh v26,v2,v29
	// vslh v4,v4,v31
	// vaddshs v24,v24,v25
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vslh v3,v3,v31
	// vaddshs v26,v26,v27
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vsubuhm v3,v24,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vaddshs v4,v26,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v8,v9,v4
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsubuhm v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vaddshs v4,v1,v3
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsubuhm v1,v1,v3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vaddshs v24,v8,v5
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v27,v9,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v25,v4,v10
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v26,v1,v11
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubuhm v29,v1,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vsubuhm v28,v9,v6
	_mm_store_si128((__m128i*)v28.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsubuhm v30,v4,v10
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsubuhm v31,v8,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v24,v24,v13
	// vsrah v25,v25,v13
	// vsrah v26,v26,v13
	// vsrah v27,v27,v13
	// vsrah v28,v28,v13
	// vsrah v29,v29,v13
	// vmrglh v20,v24,v25
	_mm_store_si128((__m128i*)v20.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v30,v30,v13
	// vmrghh v16,v24,v25
	_mm_store_si128((__m128i*)v16.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v31,v31,v13
	// vmrghh v17,v26,v27
	_mm_store_si128((__m128i*)v17.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrglh v21,v26,v27
	_mm_store_si128((__m128i*)v21.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrghh v18,v28,v29
	_mm_store_si128((__m128i*)v18.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vmrglh v22,v28,v29
	_mm_store_si128((__m128i*)v22.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vmrghh v19,v30,v31
	_mm_store_si128((__m128i*)v19.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmrglh v23,v30,v31
	_mm_store_si128((__m128i*)v23.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmrghw v24,v16,v17
	_mm_store_si128((__m128i*)v24.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v17.u32), _mm_load_si128((__m128i*)v16.u32)));
	// vmrglw v25,v16,v17
	_mm_store_si128((__m128i*)v25.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v17.u32), _mm_load_si128((__m128i*)v16.u32)));
	// vmrghw v28,v18,v19
	_mm_store_si128((__m128i*)v28.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v19.u32), _mm_load_si128((__m128i*)v18.u32)));
	// vmrglw v29,v18,v19
	_mm_store_si128((__m128i*)v29.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v19.u32), _mm_load_si128((__m128i*)v18.u32)));
	// vmrghw v26,v20,v21
	_mm_store_si128((__m128i*)v26.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v21.u32), _mm_load_si128((__m128i*)v20.u32)));
	// vmrglw v27,v20,v21
	_mm_store_si128((__m128i*)v27.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v21.u32), _mm_load_si128((__m128i*)v20.u32)));
	// vmrglw v31,v22,v23
	_mm_store_si128((__m128i*)v31.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v23.u32), _mm_load_si128((__m128i*)v22.u32)));
	// vperm v5,v24,v28,v15
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vmrghw v30,v22,v23
	_mm_store_si128((__m128i*)v30.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v23.u32), _mm_load_si128((__m128i*)v22.u32)));
	// vperm v8,v25,v29,v15
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vperm v6,v27,v31,v15
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vperm v3,v27,v31,v14
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vspltish v27,3
	// vperm v4,v25,v29,v14
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vaddshs v13,v5,v6
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vspltish v25,1
	// vperm v7,v26,v30,v15
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vperm v2,v26,v30,v14
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vslh v10,v6,v27
	// vperm v1,v24,v28,v14
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vspltish v26,2
	// vslh v9,v13,v27
	// vspltish v17,8
	// vslh v20,v6,v25
	// vslh v19,v2,v25
	// vspltish v21,6
	// vslh v18,v1,v25
	// vslh v2,v2,v26
	// vslh v1,v1,v26
	// vsubuhm v9,v9,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v29,v17,v26
	// vaddshs v2,v2,v19
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v1,v1,v18
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vsubuhm v11,v9,v5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vslh v18,v5,v25
	// vslh v19,v5,v25
	// vsubuhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vslh v17,v6,v26
	// vslh v5,v5,v26
	// vslh v6,v6,v27
	// vslh v9,v13,v25
	// vsubuhm v10,v10,v17
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vaddshs v5,v5,v19
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vsubuhm v11,v11,v18
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vaddshs v12,v7,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v18,v7,v26
	// vslh v17,v7,v25
	// vsubuhm v6,v9,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vaddshs v5,v9,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vslh v9,v12,v25
	// vaddshs v17,v17,v18
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vslh v19,v8,v25
	// vsubuhm v6,v6,v20
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vslh v20,v8,v27
	// vaddshs v17,v9,v17
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vsubuhm v19,v9,v19
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vslh v9,v12,v27
	// vaddshs v1,v1,v29
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vsubuhm v10,v10,v17
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vsubuhm v19,v19,v20
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vslh v17,v7,v25
	// vsubuhm v9,v9,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v20,v8,v27
	// vaddshs v11,v11,v19
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v17,v17,v7
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vslh v19,v8,v26
	// vslh v7,v3,v26
	// vsrah v23,v12,v25
	// vsubuhm v17,v9,v17
	_mm_store_si128((__m128i*)v17.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vsubuhm v19,v9,v19
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vaddshs v9,v1,v2
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v2,v3,v4
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v5,v5,v17
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vaddshs v7,v7,v3
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsubuhm v19,v19,v20
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vslh v2,v2,v27
	// vslh v17,v4,v25
	// vslh v18,v4,v27
	// vaddshs v6,v6,v19
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vsubuhm v7,v2,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsubuhm v2,v2,v17
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vsrah v22,v13,v25
	// vaddshs v5,v5,v23
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v8,v9,v7
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v6,v6,v23
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vsubuhm v2,v2,v18
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vsubuhm v2,v2,v4
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// rldicl r3,r2,32,32
	ctx.r3.u64 = __builtin_rotateleft64(r2.u64, 32) & 0xFFFFFFFF;
	// vsubuhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// clrldi r2,r2,32
	r2.u64 = r2.u64 & 0xFFFFFFFF;
	// vadduhm v10,v10,v22
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vaddshs v11,v11,v22
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vsubuhm v7,v1,v2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v24,v8,v5
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v27,v9,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v15,v7,v10
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v26,v1,v11
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubuhm v28,v9,v6
	_mm_store_si128((__m128i*)v28.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsrah v24,v24,v21
	// vsubuhm v29,v1,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vsrah v15,v15,v21
	// vsubuhm v30,v7,v10
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsrah v26,v26,v21
	// stvx v24,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubuhm v31,v8,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v27,v27,v21
	// stvx v15,r9,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v28,v28,v21
	// vsrah v29,v29,v21
	// stvx v26,r10,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v30,v30,v21
	// vsrah v31,v31,v21
	// stvx v27,r11,r4
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v28,r12,r4
	_mm_store_si128((__m128i*)(base + ((r12.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v29,r6,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v30,r8,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v31,r7,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r3,4
	ctx.r4.s64 = ctx.r3.s64 + 4;
	// lvx v14,r0,r5
	_mm_store_si128((__m128i*)v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rldicr r5,r2,1,62
	ctx.r5.u64 = __builtin_rotateleft64(r2.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// vaddshs v24,v24,v14
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v14.s16)));
	// rldicr r7,r2,2,61
	ctx.r7.u64 = __builtin_rotateleft64(r2.u64, 2) & 0xFFFFFFFFFFFFFFFC;
	// vaddshs v15,v15,v14
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v14.s16)));
	// add r6,r5,r2
	ctx.r6.u64 = ctx.r5.u64 + r2.u64;
	// vaddshs v26,v26,v14
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v14.s16)));
	// add r8,r7,r2
	ctx.r8.u64 = ctx.r7.u64 + r2.u64;
	// vaddshs v27,v27,v14
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v14.s16)));
	// add r9,r7,r5
	ctx.r9.u64 = ctx.r7.u64 + ctx.r5.u64;
	// vpkshus v24,v24,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vaddshs v28,v28,v14
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vpkshus v15,v15,v15
	_mm_store_si128((__m128i*)v15.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v15.s16)));
	// vaddshs v29,v29,v14
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vpkshus v26,v26,v26
	_mm_store_si128((__m128i*)v26.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v30,v30,v14
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vpkshus v27,v27,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v31,v31,v14
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vpkshus v28,v28,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// add r10,r7,r6
	ctx.r10.u64 = ctx.r7.u64 + ctx.r6.u64;
	// vpkshus v29,v29,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vpkshus v30,v30,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// stvewx v24,r0,r3
	ea = (ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v24.u32[3 - ((ea & 0xF) >> 2)]);
	// vpkshus v31,v31,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// stvewx v24,r0,r4
	ea = (ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v24.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v15,r2,r3
	ea = (r2.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v15.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v15,r2,r4
	ea = (r2.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v15.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v26,r5,r3
	ea = (ctx.r5.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v26,r5,r4
	ea = (ctx.r5.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v27,r6,r3
	ea = (ctx.r6.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v27,r6,r4
	ea = (ctx.r6.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v28,r7,r3
	ea = (ctx.r7.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v28,r7,r4
	ea = (ctx.r7.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v29,r8,r3
	ea = (ctx.r8.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v29,r8,r4
	ea = (ctx.r8.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v30,r9,r3
	ea = (ctx.r9.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v30,r9,r4
	ea = (ctx.r9.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v31,r10,r3
	ea = (ctx.r10.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v31.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v31,r10,r4
	ea = (ctx.r10.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v31.u32[3 - ((ea & 0xF) >> 2)]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266BFEC"))) PPC_WEAK_FUNC(sub_8266BFEC);
PPC_FUNC_IMPL(__imp__sub_8266BFEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266BFF0"))) PPC_WEAK_FUNC(sub_8266BFF0);
PPC_FUNC_IMPL(__imp__sub_8266BFF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r2{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v69{};
	PPCVRegister v72{};
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// dcbt r0,r11
	// mr r12,r9
	r12.u64 = ctx.r9.u64;
	// lvx v8,r0,r5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,16
	ctx.r9.s64 = 16;
	// lvx v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,32
	ctx.r10.s64 = 32;
	// vupkhsh v10,v0
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16))));
	// li r11,48
	r11.s64 = 48;
	// lvx v28,r0,r6
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rldicr r2,r7,32,31
	r2.u64 = __builtin_rotateleft64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// vupklsh v0,v0
	_mm_store_si128((__m128i*)ctx.v0.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v0.s16)));
	// li r6,80
	ctx.r6.s64 = 80;
	// vspltish v29,-1
	// lvx v9,r9,r5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r2,r8,r2
	r2.u64 = ctx.r8.u64 + r2.u64;
	// lvx v18,r10,r5
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,96
	ctx.r8.s64 = 96;
	// lvx v19,r11,r5
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r5,r12
	ctx.r5.u64 = r12.u64;
	// li r12,64
	r12.s64 = 64;
	// lvx v1,r9,r3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx v2,r10,r3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupkhsh v11,v1
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v1.s16))));
	// lvx v3,r11,r3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupkhsh v12,v2
	_mm_store_si128((__m128i*)ctx.v12.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16))));
	// vupkhsh v13,v3
	_mm_store_si128((__m128i*)ctx.v13.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16))));
	// li r7,112
	ctx.r7.s64 = 112;
	// vcfsx v10,v10,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v10.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// lvx v5,r6,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx v4,r12,r3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r12.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v11,v11,0
	_mm_store_ps(ctx.v11.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)));
	// vupkhsh v14,v4
	_mm_store_si128((__m128i*)v14.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16))));
	// vcfsx v12,v12,0
	_mm_store_ps(ctx.v12.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v12.u32)));
	// vcfsx v13,v13,0
	_mm_store_ps(ctx.v13.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)));
	// lvx v6,r8,r3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx v7,r7,r3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupkhsh v15,v5
	_mm_store_si128((__m128i*)v15.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v5.s16))));
	// vupkhsh v16,v6
	_mm_store_si128((__m128i*)v16.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v6.s16))));
	// vcfsx v0,v0,0
	_mm_store_ps(ctx.v0.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)));
	// vcfsx v14,v14,0
	_mm_store_ps(v14.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v14.u32)));
	// vupkhsh v17,v7
	_mm_store_si128((__m128i*)v17.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16))));
	// vupklsh v1,v1
	_mm_store_si128((__m128i*)ctx.v1.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v1.s16)));
	// vupklsh v2,v2
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// vcfsx v15,v15,0
	_mm_store_ps(v15.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v15.u32)));
	// vupklsh v3,v3
	_mm_store_si128((__m128i*)ctx.v3.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v3.s16)));
	// vcfsx v16,v16,0
	_mm_store_ps(v16.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v16.u32)));
	// vupklsh v4,v4
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v4.s16)));
	// vcfsx v17,v17,0
	_mm_store_ps(v17.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v17.u32)));
	// vupklsh v5,v5
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v5.s16)));
	// vcfsx v1,v1,0
	_mm_store_ps(ctx.v1.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v1.u32)));
	// vupklsh v6,v6
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v6.s16)));
	// vcfsx v2,v2,0
	_mm_store_ps(ctx.v2.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v2.u32)));
	// vupklsh v7,v7
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v7.s16)));
	// vcfsx v3,v3,0
	_mm_store_ps(ctx.v3.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v3.u32)));
	// vcfsx v4,v4,0
	_mm_store_ps(ctx.v4.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)));
	// vspltish v30,0
	// vcfsx v5,v5,0
	_mm_store_ps(ctx.v5.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// vcfsx v6,v6,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// vcfsx v7,v7,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// vmulfp128 v10,v10,v9
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v11,v11,v8
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v12,v12,v8
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v13,v13,v8
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v14,v14,v8
	_mm_store_ps(v14.f32, _mm_mul_ps(_mm_load_ps(v14.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v15,v15,v8
	_mm_store_ps(v15.f32, _mm_mul_ps(_mm_load_ps(v15.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v16,v16,v8
	_mm_store_ps(v16.f32, _mm_mul_ps(_mm_load_ps(v16.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v17,v17,v8
	_mm_store_ps(v17.f32, _mm_mul_ps(_mm_load_ps(v17.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v0,v0,v8
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v1,v1,v8
	_mm_store_ps(ctx.v1.f32, _mm_mul_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v2,v2,v8
	_mm_store_ps(ctx.v2.f32, _mm_mul_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v3,v3,v8
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v4,v4,v8
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v5,v5,v8
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v6,v6,v8
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v7,v7,v8
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v8.f32)));
	// vctsxs v10,v10,0
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_vctsxs(_mm_load_ps(ctx.v10.f32)));
	// vctsxs v11,v11,0
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_vctsxs(_mm_load_ps(ctx.v11.f32)));
	// vctsxs v12,v12,0
	_mm_store_si128((__m128i*)ctx.v12.s32, _mm_vctsxs(_mm_load_ps(ctx.v12.f32)));
	// vctsxs v13,v13,0
	_mm_store_si128((__m128i*)ctx.v13.s32, _mm_vctsxs(_mm_load_ps(ctx.v13.f32)));
	// vctsxs v14,v14,0
	_mm_store_si128((__m128i*)v14.s32, _mm_vctsxs(_mm_load_ps(v14.f32)));
	// vctsxs v3,v3,0
	_mm_store_si128((__m128i*)ctx.v3.s32, _mm_vctsxs(_mm_load_ps(ctx.v3.f32)));
	// vspltish v31,4
	// vctsxs v4,v4,0
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_vctsxs(_mm_load_ps(ctx.v4.f32)));
	// vctsxs v1,v1,0
	_mm_store_si128((__m128i*)ctx.v1.s32, _mm_vctsxs(_mm_load_ps(ctx.v1.f32)));
	// vctsxs v17,v17,0
	_mm_store_si128((__m128i*)v17.s32, _mm_vctsxs(_mm_load_ps(v17.f32)));
	// vctsxs v7,v7,0
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vctsxs v15,v15,0
	_mm_store_si128((__m128i*)v15.s32, _mm_vctsxs(_mm_load_ps(v15.f32)));
	// vctsxs v2,v2,0
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_vctsxs(_mm_load_ps(ctx.v2.f32)));
	// vctsxs v16,v16,0
	_mm_store_si128((__m128i*)v16.s32, _mm_vctsxs(_mm_load_ps(v16.f32)));
	// vctsxs v5,v5,0
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_vctsxs(_mm_load_ps(ctx.v5.f32)));
	// vctsxs v6,v6,0
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_vctsxs(_mm_load_ps(ctx.v6.f32)));
	// vctsxs v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.s32, _mm_vctsxs(_mm_load_ps(ctx.v0.f32)));
	// vpkswss v23,v13,v3
	// vpkswss v24,v14,v4
	// vpkswss v21,v11,v1
	// vpkswss v27,v17,v7
	// vsrah v13,v23,v29
	// vsrah v14,v24,v29
	// vpkswss v22,v12,v2
	// vsrah v11,v21,v29
	// vcmpequh v4,v30,v24
	// vpkswss v25,v15,v5
	// vsrah v17,v27,v29
	// vpkswss v26,v16,v6
	// vcmpequh v3,v30,v23
	// vpkswss v20,v10,v0
	// vsrah v12,v22,v29
	// vsel v13,v18,v19,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vsrah v15,v25,v29
	// vsrah v16,v26,v29
	// vsrah v10,v20,v29
	// vspltish v29,1
	// vsel v14,v18,v19,v14
	_mm_store_si128((__m128i*)v14.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vcmpequh v1,v30,v21
	// vcmpequh v7,v30,v27
	// vsel v11,v18,v19,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vsel v17,v18,v19,v17
	_mm_store_si128((__m128i*)v17.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vsel v10,v18,v19,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vcmpequh v5,v30,v25
	// vcmpequh v6,v30,v26
	// vcmpequh v2,v30,v22
	// vsel v12,v18,v19,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vsel v15,v18,v19,v15
	_mm_store_si128((__m128i*)v15.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vcmpequh v0,v30,v20
	// vsel v16,v18,v19,v16
	_mm_store_si128((__m128i*)v16.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)v18.u8)), _mm_and_si128(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)v19.u8))));
	// vsel v13,v13,v30,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v14,v14,v30,v4
	_mm_store_si128((__m128i*)v14.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v14.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v11,v11,v30,v1
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v11.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v17,v17,v30,v7
	_mm_store_si128((__m128i*)v17.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v17.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v12,v12,v30,v2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v12.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v10,v10,v30,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v15,v15,v30,v5
	_mm_store_si128((__m128i*)v15.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v15.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vsel v16,v16,v30,v6
	_mm_store_si128((__m128i*)v16.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v16.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v30.u8))));
	// vspltish v30,2
	// vaddshs v2,v24,v14
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vaddshs v8,v23,v13
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vspltish v13,3
	// vaddshs v6,v27,v17
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vaddshs v5,v21,v11
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vand v10,v10,v28
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vslh v24,v2,v30
	// vaddshs v4,v22,v12
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vslh v2,v2,v13
	// vaddshs v12,v5,v6
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v1,v20,v10
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v7,v25,v15
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v15.s16)));
	// vaddshs v2,v2,v24
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vslh v24,v12,v31
	// vslh v25,v1,v30
	// vslh v1,v1,v13
	// vslh v10,v6,v31
	// vsubuhm v9,v24,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v3,v26,v16
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v1,v1,v25
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vslh v25,v6,v13
	// vsubuhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vor128 v14,v69,v69
	_mm_store_si128((__m128i*)v14.u8, _mm_load_si128((__m128i*)v69.u8));
	// vor128 v15,v72,v72
	_mm_store_si128((__m128i*)v15.u8, _mm_load_si128((__m128i*)v72.u8));
	// vaddshs v1,v1,v31
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsubuhm v10,v10,v25
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vslh v26,v5,v29
	// vslh v11,v5,v30
	// vslh v27,v5,v30
	// vslh v24,v6,v30
	// vslh v6,v6,v31
	// vaddshs v11,v26,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vslh v5,v5,v13
	// vslh v26,v7,v30
	// vslh v25,v8,v31
	// vsubuhm v11,v9,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vslh v9,v12,v30
	// vaddshs v5,v27,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v12,v7,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v27,v7,v13
	// vsubuhm v6,v9,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vaddshs v5,v9,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vslh v9,v12,v30
	// vaddshs v26,v26,v27
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vsubuhm v6,v6,v24
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vslh v24,v8,v30
	// vslh v27,v8,v31
	// vaddshs v26,v9,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vsubuhm v24,v9,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vsubuhm v10,v10,v26
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vslh v26,v12,v31
	// vsubuhm v24,v24,v25
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vslh v25,v7,v30
	// vsubuhm v9,v26,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v11,v11,v24
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vslh v24,v7,v29
	// vslh v26,v8,v13
	// vaddshs v24,v24,v25
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vsubuhm v26,v9,v26
	_mm_store_si128((__m128i*)v26.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vslh v25,v4,v30
	// vsubuhm v24,v9,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v9,v1,v2
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v26,v26,v27
	_mm_store_si128((__m128i*)v26.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vsubuhm v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v2,v3,v3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vaddshs v5,v5,v24
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vaddshs v6,v6,v26
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vslh v24,v4,v29
	// vslh v26,v2,v29
	// vslh v27,v2,v30
	// vslh v4,v4,v31
	// vaddshs v24,v24,v25
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vslh v3,v3,v31
	// vaddshs v26,v26,v27
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vsubuhm v3,v24,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vaddshs v4,v26,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v8,v9,v4
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsubuhm v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vaddshs v4,v1,v3
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsubuhm v1,v1,v3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vaddshs v24,v8,v5
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v27,v9,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v25,v4,v10
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v26,v1,v11
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubuhm v28,v9,v6
	_mm_store_si128((__m128i*)v28.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsubuhm v29,v1,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vsubuhm v30,v4,v10
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsubuhm v31,v8,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v24,v24,v13
	// vsrah v25,v25,v13
	// vsrah v28,v28,v13
	// vsrah v29,v29,v13
	// vsrah v26,v26,v13
	// vsrah v27,v27,v13
	// vmrghh v16,v24,v25
	_mm_store_si128((__m128i*)v16.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v30,v30,v13
	// vmrglh v20,v24,v25
	_mm_store_si128((__m128i*)v20.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v31,v31,v13
	// vmrghh v18,v28,v29
	_mm_store_si128((__m128i*)v18.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vmrglh v22,v28,v29
	_mm_store_si128((__m128i*)v22.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vmrghh v17,v26,v27
	_mm_store_si128((__m128i*)v17.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrglh v21,v26,v27
	_mm_store_si128((__m128i*)v21.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrghh v19,v30,v31
	_mm_store_si128((__m128i*)v19.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmrglh v23,v30,v31
	_mm_store_si128((__m128i*)v23.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmrghw v24,v16,v17
	_mm_store_si128((__m128i*)v24.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v17.u32), _mm_load_si128((__m128i*)v16.u32)));
	// vmrglw v25,v16,v17
	_mm_store_si128((__m128i*)v25.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v17.u32), _mm_load_si128((__m128i*)v16.u32)));
	// vmrghw v28,v18,v19
	_mm_store_si128((__m128i*)v28.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v19.u32), _mm_load_si128((__m128i*)v18.u32)));
	// vmrglw v29,v18,v19
	_mm_store_si128((__m128i*)v29.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v19.u32), _mm_load_si128((__m128i*)v18.u32)));
	// vmrglw v27,v20,v21
	_mm_store_si128((__m128i*)v27.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v21.u32), _mm_load_si128((__m128i*)v20.u32)));
	// vmrglw v31,v22,v23
	_mm_store_si128((__m128i*)v31.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v23.u32), _mm_load_si128((__m128i*)v22.u32)));
	// vperm v5,v24,v28,v15
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vmrghw v26,v20,v21
	_mm_store_si128((__m128i*)v26.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v21.u32), _mm_load_si128((__m128i*)v20.u32)));
	// vmrghw v30,v22,v23
	_mm_store_si128((__m128i*)v30.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v23.u32), _mm_load_si128((__m128i*)v22.u32)));
	// vperm v6,v27,v31,v15
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vperm v3,v27,v31,v14
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vspltish v27,3
	// vperm v8,v25,v29,v15
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vaddshs v13,v5,v6
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vperm v4,v25,v29,v14
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vspltish v25,1
	// vperm v7,v26,v30,v15
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vslh v10,v6,v27
	// vperm v2,v26,v30,v14
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vperm v1,v24,v28,v14
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vslh v9,v13,v27
	// vspltish v26,2
	// vslh v20,v6,v25
	// vspltish v17,8
	// vaddshs v12,v7,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v19,v2,v25
	// vspltish v21,6
	// vslh v18,v1,v25
	// vslh v2,v2,v26
	// vslh v1,v1,v26
	// vsubuhm v9,v9,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v29,v17,v26
	// vaddshs v2,v2,v19
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v1,v1,v18
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vsubuhm v11,v9,v5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vslh v18,v5,v25
	// vslh v19,v5,v25
	// vslh v5,v5,v26
	// vslh v17,v6,v26
	// vsubuhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vslh v6,v6,v27
	// vaddshs v5,v5,v19
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vslh v9,v13,v25
	// vsubuhm v10,v10,v17
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vsubuhm v11,v11,v18
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vslh v18,v7,v26
	// vslh v17,v7,v25
	// vsubuhm v6,v9,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vaddshs v5,v9,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vslh v9,v12,v25
	// vslh v19,v8,v25
	// vaddshs v17,v17,v18
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vsubuhm v6,v6,v20
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vslh v20,v8,v27
	// vsubuhm v19,v9,v19
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vaddshs v17,v9,v17
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vslh v9,v12,v27
	// vaddshs v1,v1,v29
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vsubuhm v19,v19,v20
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vsubuhm v10,v10,v17
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vslh v17,v7,v25
	// vsubuhm v9,v9,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v11,v11,v19
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vslh v19,v8,v26
	// vaddshs v17,v17,v7
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vslh v20,v8,v27
	// vslh v7,v3,v26
	// vsubuhm v19,v9,v19
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vsubuhm v17,v9,v17
	_mm_store_si128((__m128i*)v17.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vaddshs v9,v1,v2
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v2,v3,v4
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsubuhm v19,v19,v20
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vaddshs v5,v5,v17
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vaddshs v7,v7,v3
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vslh v2,v2,v27
	// vsrah v23,v12,v25
	// vaddshs v6,v6,v19
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vslh v17,v4,v25
	// vsubuhm v7,v2,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vslh v18,v4,v27
	// vsrah v22,v13,v25
	// vsubuhm v2,v2,v17
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vaddshs v5,v5,v23
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v6,v6,v23
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vsubuhm v2,v2,v18
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vaddshs v8,v9,v7
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsubuhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vadduhm v10,v10,v22
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vsubuhm v2,v2,v4
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vaddshs v11,v11,v22
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vaddshs v24,v8,v5
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v27,v9,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vsubuhm v7,v1,v2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v28,v9,v6
	_mm_store_si128((__m128i*)v28.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsrah v24,v24,v21
	// vaddshs v15,v7,v10
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v26,v1,v11
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubuhm v29,v1,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vsubuhm v30,v7,v10
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// stvx v24,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v15,v15,v21
	// vsubuhm v31,v8,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v26,v26,v21
	// vsrah v27,v27,v21
	// vsrah v28,v28,v21
	// stvx v15,r9,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v29,v29,v21
	// vsrah v30,v30,v21
	// stvx v26,r10,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v31,v31,v21
	// stvx v27,r11,r4
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v28,r12,r4
	_mm_store_si128((__m128i*)(base + ((r12.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v29,r6,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v30,r8,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v31,r7,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266C584"))) PPC_WEAK_FUNC(sub_8266C584);
PPC_FUNC_IMPL(__imp__sub_8266C584) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266C588"))) PPC_WEAK_FUNC(sub_8266C588);
PPC_FUNC_IMPL(__imp__sub_8266C588) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// li r31,1
	r31.s64 = 1;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// subfic r6,r9,8
	xer.ca = ctx.r9.u32 <= 8;
	ctx.r6.s64 = 8 - ctx.r9.s64;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stw r6,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, ctx.r6.u32);
	// slw r29,r30,r10
	r29.u64 = ctx.r10.u8 & 0x20 ? 0 : (r30.u32 << (ctx.r10.u8 & 0x3F));
	// stw r29,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, r29.u32);
	// slw r11,r31,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r31.u32 << (r11.u8 & 0x3F));
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8266c7a4
	if (!cr6.gt) goto loc_8266C7A4;
	// addi r27,r5,1
	r27.s64 = ctx.r5.s64 + 1;
	// mr r26,r11
	r26.u64 = r11.u64;
	// extsh r28,r8
	r28.s64 = ctx.r8.s16;
	// stw r27,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, r27.u32);
	// stw r26,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, r26.u32);
loc_8266C5E8:
	// li r20,0
	r20.s64 = 0;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x8266c6d0
	if (!cr6.gt) goto loc_8266C6D0;
	// addi r11,r29,-1
	r11.s64 = r29.s64 + -1;
	// addi r8,r4,1
	ctx.r8.s64 = ctx.r4.s64 + 1;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r1,-190
	ctx.r10.s64 = ctx.r1.s64 + -190;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// addi r11,r3,1
	r11.s64 = ctx.r3.s64 + 1;
	// addi r24,r4,1
	r24.s64 = ctx.r4.s64 + 1;
	// addi r23,r4,2
	r23.s64 = ctx.r4.s64 + 2;
	// subf r22,r8,r4
	r22.s64 = ctx.r4.s64 - ctx.r8.s64;
	// addi r21,r8,-1
	r21.s64 = ctx.r8.s64 + -1;
	// rlwinm r20,r9,2,0,29
	r20.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_8266C620:
	// add r5,r21,r11
	ctx.r5.u64 = r21.u64 + r11.u64;
	// lbz r8,-1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsh r6,r28
	ctx.r6.s64 = r28.s16;
	// lbz r29,2(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// extsh r3,r28
	ctx.r3.s64 = r28.s16;
	// lbz r30,1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r19,r28
	r19.s64 = r28.s16;
	// lbzx r27,r24,r11
	r27.u64 = PPC_LOAD_U8(r24.u32 + r11.u32);
	// mr r15,r29
	r15.u64 = r29.u64;
	// lbzx r25,r5,r22
	r25.u64 = PPC_LOAD_U8(ctx.r5.u32 + r22.u32);
	// extsh r18,r28
	r18.s64 = r28.s16;
	// lbz r5,0(r5)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// subf r17,r30,r27
	r17.s64 = r27.s64 - r30.s64;
	// lbzx r26,r23,r11
	r26.u64 = PPC_LOAD_U8(r23.u32 + r11.u32);
	// subf r25,r8,r25
	r25.s64 = r25.s64 - ctx.r8.s64;
	// subf r5,r31,r5
	ctx.r5.s64 = ctx.r5.s64 - r31.s64;
	// subf r16,r29,r26
	r16.s64 = r26.s64 - r29.s64;
	// mullw r26,r25,r6
	r26.s64 = int64_t(r25.s32) * int64_t(ctx.r6.s32);
	// rotlwi r25,r8,2
	r25.u64 = __builtin_rotateleft32(ctx.r8.u32, 2);
	// rotlwi r27,r31,2
	r27.u64 = __builtin_rotateleft32(r31.u32, 2);
	// mullw r29,r5,r3
	r29.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r3.s32);
	// mullw r31,r17,r19
	r31.s64 = int64_t(r17.s32) * int64_t(r19.s32);
	// rotlwi r30,r30,2
	r30.u64 = __builtin_rotateleft32(r30.u32, 2);
	// mullw r8,r16,r18
	ctx.r8.s64 = int64_t(r16.s32) * int64_t(r18.s32);
	// rlwinm r5,r15,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r26,r25
	ctx.r6.u64 = r26.u64 + r25.u64;
	// add r3,r29,r27
	ctx.r3.u64 = r29.u64 + r27.u64;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// sth r6,-2(r10)
	PPC_STORE_U16(ctx.r10.u32 + -2, ctx.r6.u16);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// sth r3,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r3.u16);
	// sth r31,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, r31.u16);
	// sth r8,4(r10)
	PPC_STORE_U16(ctx.r10.u32 + 4, ctx.r8.u16);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x8266c620
	if (!cr6.eq) goto loc_8266C620;
	// lwz r6,68(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r29,-200(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -200);
	// lwz r26,-208(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -208);
	// lwz r27,-204(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -204);
loc_8266C6D0:
	// add r11,r20,r3
	r11.u64 = r20.u64 + ctx.r3.u64;
	// extsh r9,r28
	ctx.r9.s64 = r28.s16;
	// rlwinm r8,r20,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r1,-192
	ctx.r5.s64 = ctx.r1.s64 + -192;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbzx r11,r11,r4
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r4.u32);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sthx r11,r8,r5
	PPC_STORE_U16(ctx.r8.u32 + ctx.r5.u32, r11.u16);
	// ble cr6,0x8266c780
	if (!cr6.gt) goto loc_8266C780;
	// addi r11,r29,-1
	r11.s64 = r29.s64 + -1;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// rlwinm r9,r11,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r11,r1,-190
	r11.s64 = ctx.r1.s64 + -190;
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
loc_8266C718:
	// addi r5,r11,-2
	ctx.r5.s64 = r11.s64 + -2;
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// lhz r31,0(r5)
	r31.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// lhz r30,4(r5)
	r30.u64 = PPC_LOAD_U16(ctx.r5.u32 + 4);
	// extsh r5,r31
	ctx.r5.s64 = r31.s16;
	// extsh r31,r30
	r31.s64 = r30.s16;
	// rlwinm r30,r9,2,0,29
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r31,r9,r31
	r31.s64 = r31.s64 - ctx.r9.s64;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r9,r9,r7
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// mullw r31,r31,r7
	r31.s64 = int64_t(r31.s32) * int64_t(ctx.r7.s32);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r31,r31,r6
	r31.u64 = r31.u64 + ctx.r6.u64;
	// srawi r9,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// srawi r5,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	ctx.r5.s64 = r31.s32 >> 4;
	// stb r9,-1(r10)
	PPC_STORE_U8(ctx.r10.u32 + -1, ctx.r9.u8);
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bne cr6,0x8266c718
	if (!cr6.eq) goto loc_8266C718;
loc_8266C780:
	// lwz r11,44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// stw r26,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, r26.u32);
	// stw r27,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, r27.u32);
	// bne cr6,0x8266c5e8
	if (!cr6.eq) goto loc_8266C5E8;
loc_8266C7A4:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8266C7A8"))) PPC_WEAK_FUNC(sub_8266C7A8);
PPC_FUNC_IMPL(__imp__sub_8266C7A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r29,r9,3
	r29.s64 = ctx.r9.s64 + 3;
	// li r30,1
	r30.s64 = 1;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// rlwinm r31,r6,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r11,30952
	r11.s64 = r11.s64 + 30952;
	// rlwinm r9,r7,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// clrlwi r10,r8,24
	ctx.r10.u64 = ctx.r8.u32 & 0xFF;
	// add r26,r31,r11
	r26.u64 = r31.u64 + r11.u64;
	// add r23,r9,r11
	r23.u64 = ctx.r9.u64 + r11.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// slw r17,r30,r29
	r17.u64 = r29.u8 & 0x20 ? 0 : (r30.u32 << (r29.u8 & 0x3F));
	// bne cr6,0x8266c800
	if (!cr6.eq) goto loc_8266C800;
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// li r25,4
	r25.s64 = 4;
	// beq cr6,0x8266c7f0
	if (cr6.eq) goto loc_8266C7F0;
	// li r25,6
	r25.s64 = 6;
loc_8266C7F0:
	// li r22,0
	r22.s64 = 0;
	// li r21,0
	r21.s64 = 0;
	// addi r19,r17,1
	r19.s64 = r17.s64 + 1;
	// b 0x8266c868
	goto loc_8266C868;
loc_8266C800:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne cr6,0x8266c834
	if (!cr6.eq) goto loc_8266C834;
	// li r25,0
	r25.s64 = 0;
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// li r22,4
	r22.s64 = 4;
	// beq cr6,0x8266c81c
	if (cr6.eq) goto loc_8266C81C;
	// li r22,6
	r22.s64 = 6;
loc_8266C81C:
	// addi r11,r22,-1
	r11.s64 = r22.s64 + -1;
	// li r24,0
	r24.s64 = 0;
	// addi r19,r17,3
	r19.s64 = r17.s64 + 3;
	// slw r11,r30,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r30.u32 << (r11.u8 & 0x3F));
	// subf r21,r10,r11
	r21.s64 = r11.s64 - ctx.r10.s64;
	// b 0x8266c878
	goto loc_8266C878;
loc_8266C834:
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// li r9,4
	ctx.r9.s64 = 4;
	// beq cr6,0x8266c844
	if (cr6.eq) goto loc_8266C844;
	// li r9,6
	ctx.r9.s64 = 6;
loc_8266C844:
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// li r11,4
	r11.s64 = 4;
	// beq cr6,0x8266c854
	if (cr6.eq) goto loc_8266C854;
	// li r11,6
	r11.s64 = 6;
loc_8266C854:
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// li r22,7
	r22.s64 = 7;
	// addi r25,r11,-7
	r25.s64 = r11.s64 + -7;
	// subfic r21,r10,64
	xer.ca = ctx.r10.u32 <= 64;
	r21.s64 = 64 - ctx.r10.s64;
	// addi r19,r17,3
	r19.s64 = r17.s64 + 3;
loc_8266C868:
	// addi r11,r25,-1
	r11.s64 = r25.s64 + -1;
	// slw r11,r30,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r30.u32 << (r11.u8 & 0x3F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r24,r11,-1
	r24.s64 = r11.s64 + -1;
loc_8266C878:
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// ble cr6,0x8266c9f0
	if (!cr6.gt) goto loc_8266C9F0;
	// addi r10,r1,-220
	ctx.r10.s64 = ctx.r1.s64 + -220;
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// subf r18,r10,r5
	r18.s64 = ctx.r5.s64 - ctx.r10.s64;
	// addi r20,r11,-1
	r20.s64 = r11.s64 + -1;
	// mr r16,r17
	r16.u64 = r17.u64;
loc_8266C894:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x8266c938
	if (!cr6.gt) goto loc_8266C938;
	// lhz r11,6(r23)
	r11.u64 = PPC_LOAD_U16(r23.u32 + 6);
	// rlwinm r7,r4,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r4,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r10,4(r23)
	ctx.r10.u64 = PPC_LOAD_U16(r23.u32 + 4);
	// extsh r5,r11
	ctx.r5.s64 = r11.s16;
	// lhz r11,0(r23)
	r11.u64 = PPC_LOAD_U16(r23.u32 + 0);
	// lhz r9,2(r23)
	ctx.r9.u64 = PPC_LOAD_U16(r23.u32 + 2);
	// add r6,r4,r6
	ctx.r6.u64 = ctx.r4.u64 + ctx.r6.u64;
	// extsh r29,r11
	r29.s64 = r11.s16;
	// subf r11,r4,r7
	r11.s64 = ctx.r7.s64 - ctx.r4.s64;
	// extsh r31,r10
	r31.s64 = ctx.r10.s16;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// extsh r30,r9
	r30.s64 = ctx.r9.s16;
	// addi r8,r1,-224
	ctx.r8.s64 = ctx.r1.s64 + -224;
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// subf r28,r7,r6
	r28.s64 = ctx.r6.s64 - ctx.r7.s64;
	// subf r27,r7,r4
	r27.s64 = ctx.r4.s64 - ctx.r7.s64;
loc_8266C8E8:
	// lbzx r7,r27,r11
	ctx.r7.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbzx r6,r28,r11
	ctx.r6.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// mullw r7,r7,r30
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r30.s32);
	// lbz r15,0(r9)
	r15.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r14,0(r11)
	r14.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mullw r6,r6,r5
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r5.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r15,r29
	ctx.r6.s64 = int64_t(r15.s32) * int64_t(r29.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r14,r31
	ctx.r6.s64 = int64_t(r14.s32) * int64_t(r31.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r7,r7,r24
	ctx.r7.u64 = ctx.r7.u64 + r24.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sraw r7,r7,r25
	temp.u32 = r25.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r7.s32 < 0) & (((ctx.r7.s32 >> temp.u32) << temp.u32) != ctx.r7.s32);
	ctx.r7.s64 = ctx.r7.s32 >> temp.u32;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bne cr6,0x8266c8e8
	if (!cr6.eq) goto loc_8266C8E8;
loc_8266C938:
	// addi r11,r1,-220
	r11.s64 = ctx.r1.s64 + -220;
	// mr r8,r17
	ctx.r8.u64 = r17.u64;
loc_8266C940:
	// lhz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// lhz r9,2(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 2);
	// lhz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r6,0(r26)
	ctx.r6.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r31,4(r26)
	r31.u64 = PPC_LOAD_U16(r26.u32 + 4);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lhz r30,6(r26)
	r30.u64 = PPC_LOAD_U16(r26.u32 + 6);
	// mullw r9,r7,r6
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// extsh r7,r5
	ctx.r7.s64 = ctx.r5.s16;
	// extsh r6,r31
	ctx.r6.s64 = r31.s16;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r7,r6
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r5,r30
	ctx.r5.s64 = r30.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// mullw r9,r9,r5
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r10,r10,r21
	ctx.r10.u64 = ctx.r10.u64 + r21.u64;
	// sraw r10,r10,r22
	temp.u32 = r22.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x8266c9b4
	if (!cr6.lt) goto loc_8266C9B4;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x8266c9c0
	goto loc_8266C9C0;
loc_8266C9B4:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x8266c9c0
	if (!cr6.gt) goto loc_8266C9C0;
	// li r10,255
	ctx.r10.s64 = 255;
loc_8266C9C0:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// sthx r10,r18,r11
	PPC_STORE_U16(r18.u32 + r11.u32, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x8266c940
	if (!cr6.eq) goto loc_8266C940;
	// addi r16,r16,-1
	r16.s64 = r16.s64 + -1;
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r20,r20,r4
	r20.u64 = r20.u64 + ctx.r4.u64;
	// addi r18,r18,48
	r18.s64 = r18.s64 + 48;
	// cmplwi cr6,r16,0
	cr6.compare<uint32_t>(r16.u32, 0, xer);
	// bne cr6,0x8266c894
	if (!cr6.eq) goto loc_8266C894;
loc_8266C9F0:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8266C9F4"))) PPC_WEAK_FUNC(sub_8266C9F4);
PPC_FUNC_IMPL(__imp__sub_8266C9F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266C9F8"))) PPC_WEAK_FUNC(sub_8266C9F8);
PPC_FUNC_IMPL(__imp__sub_8266C9F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc0
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r31,1
	r31.s64 = 1;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// addi r6,r10,3
	ctx.r6.s64 = ctx.r10.s64 + 3;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r30,r8,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// rlwinm r29,r7,3,0,28
	r29.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// clrlwi r10,r9,24
	ctx.r10.u64 = ctx.r9.u32 & 0xFF;
	// addi r28,r11,3
	r28.s64 = r11.s64 + 3;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// addi r11,r11,30952
	r11.s64 = r11.s64 + 30952;
	// add r21,r30,r11
	r21.u64 = r30.u64 + r11.u64;
	// add r24,r29,r11
	r24.u64 = r29.u64 + r11.u64;
	// slw r16,r31,r6
	r16.u64 = ctx.r6.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r6.u8 & 0x3F));
	// slw r30,r31,r28
	r30.u64 = r28.u8 & 0x20 ? 0 : (r31.u32 << (r28.u8 & 0x3F));
	// bne cr6,0x8266ca6c
	if (!cr6.eq) goto loc_8266CA6C;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// li r23,4
	r23.s64 = 4;
	// beq cr6,0x8266ca5c
	if (cr6.eq) goto loc_8266CA5C;
	// li r23,6
	r23.s64 = 6;
loc_8266CA5C:
	// li r20,0
	r20.s64 = 0;
	// li r19,0
	r19.s64 = 0;
	// addi r17,r16,1
	r17.s64 = r16.s64 + 1;
	// b 0x8266cad4
	goto loc_8266CAD4;
loc_8266CA6C:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x8266caa0
	if (!cr6.eq) goto loc_8266CAA0;
	// li r23,0
	r23.s64 = 0;
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// li r20,4
	r20.s64 = 4;
	// beq cr6,0x8266ca88
	if (cr6.eq) goto loc_8266CA88;
	// li r20,6
	r20.s64 = 6;
loc_8266CA88:
	// addi r11,r20,-1
	r11.s64 = r20.s64 + -1;
	// li r22,0
	r22.s64 = 0;
	// addi r17,r16,3
	r17.s64 = r16.s64 + 3;
	// slw r11,r31,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r31.u32 << (r11.u8 & 0x3F));
	// subf r19,r10,r11
	r19.s64 = r11.s64 - ctx.r10.s64;
	// b 0x8266cae4
	goto loc_8266CAE4;
loc_8266CAA0:
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// li r9,4
	ctx.r9.s64 = 4;
	// beq cr6,0x8266cab0
	if (cr6.eq) goto loc_8266CAB0;
	// li r9,6
	ctx.r9.s64 = 6;
loc_8266CAB0:
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// li r11,4
	r11.s64 = 4;
	// beq cr6,0x8266cac0
	if (cr6.eq) goto loc_8266CAC0;
	// li r11,6
	r11.s64 = 6;
loc_8266CAC0:
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// li r20,7
	r20.s64 = 7;
	// addi r23,r11,-7
	r23.s64 = r11.s64 + -7;
	// subfic r19,r10,64
	xer.ca = ctx.r10.u32 <= 64;
	r19.s64 = 64 - ctx.r10.s64;
	// addi r17,r16,3
	r17.s64 = r16.s64 + 3;
loc_8266CAD4:
	// addi r11,r23,-1
	r11.s64 = r23.s64 + -1;
	// slw r11,r31,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r31.u32 << (r11.u8 & 0x3F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r22,r11,-1
	r22.s64 = r11.s64 + -1;
loc_8266CAE4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x8266cc64
	if (!cr6.gt) goto loc_8266CC64;
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// mr r15,r30
	r15.u64 = r30.u64;
	// addi r18,r11,-1
	r18.s64 = r11.s64 + -1;
loc_8266CAF8:
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// ble cr6,0x8266cba4
	if (!cr6.gt) goto loc_8266CBA4;
	// lhz r11,6(r21)
	r11.u64 = PPC_LOAD_U16(r21.u32 + 6);
	// rlwinm r7,r4,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r4,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r10,4(r21)
	ctx.r10.u64 = PPC_LOAD_U16(r21.u32 + 4);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// lhz r11,0(r21)
	r11.u64 = PPC_LOAD_U16(r21.u32 + 0);
	// lhz r9,2(r21)
	ctx.r9.u64 = PPC_LOAD_U16(r21.u32 + 2);
	// add r6,r4,r31
	ctx.r6.u64 = ctx.r4.u64 + r31.u64;
	// extsh r27,r11
	r27.s64 = r11.s16;
	// subf r11,r4,r7
	r11.s64 = ctx.r7.s64 - ctx.r4.s64;
	// extsh r29,r10
	r29.s64 = ctx.r10.s16;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// extsh r28,r9
	r28.s64 = ctx.r9.s16;
	// addi r8,r1,-224
	ctx.r8.s64 = ctx.r1.s64 + -224;
	// mr r9,r18
	ctx.r9.u64 = r18.u64;
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// subf r26,r7,r6
	r26.s64 = ctx.r6.s64 - ctx.r7.s64;
	// subf r25,r7,r4
	r25.s64 = ctx.r4.s64 - ctx.r7.s64;
loc_8266CB4C:
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbzx r6,r26,r11
	ctx.r6.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// lbzx r7,r25,r11
	ctx.r7.u64 = PPC_LOAD_U8(r25.u32 + r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lbz r14,0(r9)
	r14.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r7,r7,r28
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r28.s32);
	// stw r31,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, r31.u32);
	// mullw r31,r6,r30
	r31.s64 = int64_t(ctx.r6.s32) * int64_t(r30.s32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// mullw r31,r14,r27
	r31.s64 = int64_t(r14.s32) * int64_t(r27.s32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r6,-240(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -240);
	// mullw r31,r6,r29
	r31.s64 = int64_t(ctx.r6.s32) * int64_t(r29.s32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// add r7,r7,r22
	ctx.r7.u64 = ctx.r7.u64 + r22.u64;
	// sraw r7,r7,r23
	temp.u32 = r23.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r7.s32 < 0) & (((ctx.r7.s32 >> temp.u32) << temp.u32) != ctx.r7.s32);
	ctx.r7.s64 = ctx.r7.s32 >> temp.u32;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bne cr6,0x8266cb4c
	if (!cr6.eq) goto loc_8266CB4C;
loc_8266CBA4:
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// ble cr6,0x8266cc48
	if (!cr6.gt) goto loc_8266CC48;
	// addi r11,r1,-220
	r11.s64 = ctx.r1.s64 + -220;
loc_8266CBB4:
	// lhz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// lhz r9,0(r24)
	ctx.r9.u64 = PPC_LOAD_U16(r24.u32 + 0);
	// lhz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r6,6(r24)
	ctx.r6.u64 = PPC_LOAD_U16(r24.u32 + 6);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r31,-2(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r30,2(r24)
	r30.u64 = PPC_LOAD_U16(r24.u32 + 2);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lhz r29,0(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r28,4(r24)
	r28.u64 = PPC_LOAD_U16(r24.u32 + 4);
	// mullw r9,r7,r6
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// extsh r7,r31
	ctx.r7.s64 = r31.s16;
	// extsh r6,r30
	ctx.r6.s64 = r30.s16;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r7,r6
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// extsh r31,r29
	r31.s64 = r29.s16;
	// extsh r7,r28
	ctx.r7.s64 = r28.s16;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r31,r7
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(ctx.r7.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r10,r10,r19
	ctx.r10.u64 = ctx.r10.u64 + r19.u64;
	// sraw r10,r10,r20
	temp.u32 = r20.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x8266cc28
	if (!cr6.lt) goto loc_8266CC28;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x8266cc34
	goto loc_8266CC34;
loc_8266CC28:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x8266cc34
	if (!cr6.gt) goto loc_8266CC34;
	// li r10,255
	ctx.r10.s64 = 255;
loc_8266CC34:
	// stbx r10,r8,r5
	PPC_STORE_U8(ctx.r8.u32 + ctx.r5.u32, ctx.r10.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpw cr6,r8,r16
	cr6.compare<int32_t>(ctx.r8.s32, r16.s32, xer);
	// blt cr6,0x8266cbb4
	if (cr6.lt) goto loc_8266CBB4;
loc_8266CC48:
	// lwz r11,44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r15,r15,-1
	r15.s64 = r15.s64 + -1;
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r18,r18,r4
	r18.u64 = r18.u64 + ctx.r4.u64;
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// bne cr6,0x8266caf8
	if (!cr6.eq) goto loc_8266CAF8;
loc_8266CC64:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8266CC68"))) PPC_WEAK_FUNC(sub_8266CC68);
PPC_FUNC_IMPL(__imp__sub_8266CC68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r29,r9,3
	r29.s64 = ctx.r9.s64 + 3;
	// li r30,1
	r30.s64 = 1;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// rlwinm r31,r6,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r11,30952
	r11.s64 = r11.s64 + 30952;
	// rlwinm r9,r7,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// clrlwi r10,r8,24
	ctx.r10.u64 = ctx.r8.u32 & 0xFF;
	// add r25,r31,r11
	r25.u64 = r31.u64 + r11.u64;
	// add r22,r9,r11
	r22.u64 = ctx.r9.u64 + r11.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// slw r17,r30,r29
	r17.u64 = r29.u8 & 0x20 ? 0 : (r30.u32 << (r29.u8 & 0x3F));
	// bne cr6,0x8266ccc0
	if (!cr6.eq) goto loc_8266CCC0;
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// li r24,4
	r24.s64 = 4;
	// beq cr6,0x8266ccb0
	if (cr6.eq) goto loc_8266CCB0;
	// li r24,6
	r24.s64 = 6;
loc_8266CCB0:
	// li r21,0
	r21.s64 = 0;
	// li r20,0
	r20.s64 = 0;
	// addi r18,r17,1
	r18.s64 = r17.s64 + 1;
	// b 0x8266cd28
	goto loc_8266CD28;
loc_8266CCC0:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne cr6,0x8266ccf4
	if (!cr6.eq) goto loc_8266CCF4;
	// li r24,0
	r24.s64 = 0;
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// li r21,4
	r21.s64 = 4;
	// beq cr6,0x8266ccdc
	if (cr6.eq) goto loc_8266CCDC;
	// li r21,6
	r21.s64 = 6;
loc_8266CCDC:
	// addi r11,r21,-1
	r11.s64 = r21.s64 + -1;
	// li r23,0
	r23.s64 = 0;
	// addi r18,r17,3
	r18.s64 = r17.s64 + 3;
	// slw r11,r30,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r30.u32 << (r11.u8 & 0x3F));
	// subf r20,r10,r11
	r20.s64 = r11.s64 - ctx.r10.s64;
	// b 0x8266cd38
	goto loc_8266CD38;
loc_8266CCF4:
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// li r9,4
	ctx.r9.s64 = 4;
	// beq cr6,0x8266cd04
	if (cr6.eq) goto loc_8266CD04;
	// li r9,6
	ctx.r9.s64 = 6;
loc_8266CD04:
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// li r11,4
	r11.s64 = 4;
	// beq cr6,0x8266cd14
	if (cr6.eq) goto loc_8266CD14;
	// li r11,6
	r11.s64 = 6;
loc_8266CD14:
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// li r21,7
	r21.s64 = 7;
	// addi r24,r11,-7
	r24.s64 = r11.s64 + -7;
	// subfic r20,r10,64
	xer.ca = ctx.r10.u32 <= 64;
	r20.s64 = 64 - ctx.r10.s64;
	// addi r18,r17,3
	r18.s64 = r17.s64 + 3;
loc_8266CD28:
	// addi r11,r24,-1
	r11.s64 = r24.s64 + -1;
	// slw r11,r30,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r30.u32 << (r11.u8 & 0x3F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r23,r11,-1
	r23.s64 = r11.s64 + -1;
loc_8266CD38:
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// ble cr6,0x8266cea4
	if (!cr6.gt) goto loc_8266CEA4;
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// mr r16,r17
	r16.u64 = r17.u64;
	// addi r19,r11,-1
	r19.s64 = r11.s64 + -1;
loc_8266CD4C:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// ble cr6,0x8266cdf0
	if (!cr6.gt) goto loc_8266CDF0;
	// lhz r11,6(r22)
	r11.u64 = PPC_LOAD_U16(r22.u32 + 6);
	// rlwinm r7,r4,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r4,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r10,4(r22)
	ctx.r10.u64 = PPC_LOAD_U16(r22.u32 + 4);
	// extsh r31,r11
	r31.s64 = r11.s16;
	// lhz r11,0(r22)
	r11.u64 = PPC_LOAD_U16(r22.u32 + 0);
	// lhz r9,2(r22)
	ctx.r9.u64 = PPC_LOAD_U16(r22.u32 + 2);
	// add r6,r4,r6
	ctx.r6.u64 = ctx.r4.u64 + ctx.r6.u64;
	// extsh r28,r11
	r28.s64 = r11.s16;
	// subf r11,r4,r7
	r11.s64 = ctx.r7.s64 - ctx.r4.s64;
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// extsh r29,r9
	r29.s64 = ctx.r9.s16;
	// addi r8,r1,-224
	ctx.r8.s64 = ctx.r1.s64 + -224;
	// mr r9,r19
	ctx.r9.u64 = r19.u64;
	// mr r10,r18
	ctx.r10.u64 = r18.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// subf r27,r7,r6
	r27.s64 = ctx.r6.s64 - ctx.r7.s64;
	// subf r26,r7,r4
	r26.s64 = ctx.r4.s64 - ctx.r7.s64;
loc_8266CDA0:
	// lbzx r7,r26,r11
	ctx.r7.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbzx r6,r27,r11
	ctx.r6.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// mullw r7,r7,r29
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r29.s32);
	// lbz r15,0(r9)
	r15.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r14,0(r11)
	r14.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mullw r6,r6,r31
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r31.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r15,r28
	ctx.r6.s64 = int64_t(r15.s32) * int64_t(r28.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r14,r30
	ctx.r6.s64 = int64_t(r14.s32) * int64_t(r30.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r7,r7,r23
	ctx.r7.u64 = ctx.r7.u64 + r23.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sraw r7,r7,r24
	temp.u32 = r24.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r7.s32 < 0) & (((ctx.r7.s32 >> temp.u32) << temp.u32) != ctx.r7.s32);
	ctx.r7.s64 = ctx.r7.s32 >> temp.u32;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bne cr6,0x8266cda0
	if (!cr6.eq) goto loc_8266CDA0;
loc_8266CDF0:
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r11,r1,-220
	r11.s64 = ctx.r1.s64 + -220;
loc_8266CDF8:
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r9,6(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 6);
	// lhz r7,-2(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r6,2(r25)
	ctx.r6.u64 = PPC_LOAD_U16(r25.u32 + 2);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r31,-4(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r30,0(r25)
	r30.u64 = PPC_LOAD_U16(r25.u32 + 0);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lhz r29,0(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r28,4(r25)
	r28.u64 = PPC_LOAD_U16(r25.u32 + 4);
	// mullw r9,r7,r6
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// extsh r7,r31
	ctx.r7.s64 = r31.s16;
	// extsh r6,r30
	ctx.r6.s64 = r30.s16;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r7,r6
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// extsh r31,r29
	r31.s64 = r29.s16;
	// extsh r7,r28
	ctx.r7.s64 = r28.s16;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r31,r7
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(ctx.r7.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r10,r10,r20
	ctx.r10.u64 = ctx.r10.u64 + r20.u64;
	// sraw r10,r10,r21
	temp.u32 = r21.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x8266ce6c
	if (!cr6.lt) goto loc_8266CE6C;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x8266ce78
	goto loc_8266CE78;
loc_8266CE6C:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x8266ce78
	if (!cr6.gt) goto loc_8266CE78;
	// li r10,255
	ctx.r10.s64 = 255;
loc_8266CE78:
	// stbx r10,r8,r5
	PPC_STORE_U8(ctx.r8.u32 + ctx.r5.u32, ctx.r10.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpw cr6,r8,r17
	cr6.compare<int32_t>(ctx.r8.s32, r17.s32, xer);
	// blt cr6,0x8266cdf8
	if (cr6.lt) goto loc_8266CDF8;
	// addi r16,r16,-1
	r16.s64 = r16.s64 + -1;
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r19,r19,r4
	r19.u64 = r19.u64 + ctx.r4.u64;
	// add r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 + ctx.r5.u64;
	// cmplwi cr6,r16,0
	cr6.compare<uint32_t>(r16.u32, 0, xer);
	// bne cr6,0x8266cd4c
	if (!cr6.eq) goto loc_8266CD4C;
loc_8266CEA4:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8266CEA8"))) PPC_WEAK_FUNC(sub_8266CEA8);
PPC_FUNC_IMPL(__imp__sub_8266CEA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// lis r11,-32144
	r11.s64 = -2106589184;
	// lis r14,-32144
	r14.s64 = -2106589184;
	// lis r15,-32144
	r15.s64 = -2106589184;
	// addi r14,r14,4816
	r14.s64 = r14.s64 + 4816;
	// lis r16,-32144
	r16.s64 = -2106589184;
	// stw r11,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r11.u32);
	// lis r17,-32144
	r17.s64 = -2106589184;
	// lis r18,-32144
	r18.s64 = -2106589184;
	// lis r19,-32144
	r19.s64 = -2106589184;
	// stw r14,536(r4)
	PPC_STORE_U32(ctx.r4.u32 + 536, r14.u32);
	// lis r20,-32144
	r20.s64 = -2106589184;
	// lis r21,-32144
	r21.s64 = -2106589184;
	// lis r22,-32144
	r22.s64 = -2106589184;
	// lis r23,-32144
	r23.s64 = -2106589184;
	// lis r24,-32144
	r24.s64 = -2106589184;
	// lis r25,-32144
	r25.s64 = -2106589184;
	// lis r26,-32144
	r26.s64 = -2106589184;
	// lis r27,-32144
	r27.s64 = -2106589184;
	// lis r28,-32144
	r28.s64 = -2106589184;
	// lis r29,-32144
	r29.s64 = -2106589184;
	// lis r30,-32144
	r30.s64 = -2106589184;
	// lis r31,-32144
	r31.s64 = -2106589184;
	// lis r3,-32144
	ctx.r3.s64 = -2106589184;
	// lis r5,-32144
	ctx.r5.s64 = -2106589184;
	// lis r6,-32144
	ctx.r6.s64 = -2106589184;
	// lis r7,-32143
	ctx.r7.s64 = -2106523648;
	// lis r8,-32143
	ctx.r8.s64 = -2106523648;
	// lis r9,-32143
	ctx.r9.s64 = -2106523648;
	// addi r15,r15,5176
	r15.s64 = r15.s64 + 5176;
	// addi r16,r16,5248
	r16.s64 = r16.s64 + 5248;
	// addi r17,r17,5320
	r17.s64 = r17.s64 + 5320;
	// addi r18,r18,5392
	r18.s64 = r18.s64 + 5392;
	// addi r19,r19,5520
	r19.s64 = r19.s64 + 5520;
	// addi r20,r20,6816
	r20.s64 = r20.s64 + 6816;
	// stw r15,540(r4)
	PPC_STORE_U32(ctx.r4.u32 + 540, r15.u32);
	// addi r21,r21,8000
	r21.s64 = r21.s64 + 8000;
	// stw r16,544(r4)
	PPC_STORE_U32(ctx.r4.u32 + 544, r16.u32);
	// addi r22,r22,8120
	r22.s64 = r22.s64 + 8120;
	// stw r17,548(r4)
	PPC_STORE_U32(ctx.r4.u32 + 548, r17.u32);
	// addi r23,r23,9304
	r23.s64 = r23.s64 + 9304;
	// stw r18,552(r4)
	PPC_STORE_U32(ctx.r4.u32 + 552, r18.u32);
	// addi r24,r24,9432
	r24.s64 = r24.s64 + 9432;
	// stw r19,556(r4)
	PPC_STORE_U32(ctx.r4.u32 + 556, r19.u32);
	// addi r25,r25,10616
	r25.s64 = r25.s64 + 10616;
	// stw r20,564(r4)
	PPC_STORE_U32(ctx.r4.u32 + 564, r20.u32);
	// addi r26,r26,10744
	r26.s64 = r26.s64 + 10744;
	// stw r21,568(r4)
	PPC_STORE_U32(ctx.r4.u32 + 568, r21.u32);
	// addi r27,r27,11928
	r27.s64 = r27.s64 + 11928;
	// stw r22,572(r4)
	PPC_STORE_U32(ctx.r4.u32 + 572, r22.u32);
	// addi r28,r28,12056
	r28.s64 = r28.s64 + 12056;
	// stw r23,576(r4)
	PPC_STORE_U32(ctx.r4.u32 + 576, r23.u32);
	// addi r29,r29,26632
	r29.s64 = r29.s64 + 26632;
	// stw r24,580(r4)
	PPC_STORE_U32(ctx.r4.u32 + 580, r24.u32);
	// addi r30,r30,30296
	r30.s64 = r30.s64 + 30296;
	// stw r25,584(r4)
	PPC_STORE_U32(ctx.r4.u32 + 584, r25.u32);
	// addi r31,r31,30472
	r31.s64 = r31.s64 + 30472;
	// stw r26,588(r4)
	PPC_STORE_U32(ctx.r4.u32 + 588, r26.u32);
	// addi r3,r3,30384
	ctx.r3.s64 = ctx.r3.s64 + 30384;
	// stw r27,592(r4)
	PPC_STORE_U32(ctx.r4.u32 + 592, r27.u32);
	// addi r5,r5,30552
	ctx.r5.s64 = ctx.r5.s64 + 30552;
	// stw r28,596(r4)
	PPC_STORE_U32(ctx.r4.u32 + 596, r28.u32);
	// addi r6,r6,31816
	ctx.r6.s64 = ctx.r6.s64 + 31816;
	// stw r29,664(r4)
	PPC_STORE_U32(ctx.r4.u32 + 664, r29.u32);
	// addi r7,r7,-27320
	ctx.r7.s64 = ctx.r7.s64 + -27320;
	// stw r30,668(r4)
	PPC_STORE_U32(ctx.r4.u32 + 668, r30.u32);
	// addi r8,r8,-32136
	ctx.r8.s64 = ctx.r8.s64 + -32136;
	// stw r31,672(r4)
	PPC_STORE_U32(ctx.r4.u32 + 672, r31.u32);
	// addi r9,r9,-26184
	ctx.r9.s64 = ctx.r9.s64 + -26184;
	// stw r3,676(r4)
	PPC_STORE_U32(ctx.r4.u32 + 676, ctx.r3.u32);
	// lis r10,-32143
	ctx.r10.s64 = -2106523648;
	// stw r5,680(r4)
	PPC_STORE_U32(ctx.r4.u32 + 680, ctx.r5.u32);
	// lis r11,-32143
	r11.s64 = -2106523648;
	// stw r6,684(r4)
	PPC_STORE_U32(ctx.r4.u32 + 684, ctx.r6.u32);
	// addi r10,r10,-25696
	ctx.r10.s64 = ctx.r10.s64 + -25696;
	// stw r7,688(r4)
	PPC_STORE_U32(ctx.r4.u32 + 688, ctx.r7.u32);
	// addi r11,r11,-22736
	r11.s64 = r11.s64 + -22736;
	// stw r8,692(r4)
	PPC_STORE_U32(ctx.r4.u32 + 692, ctx.r8.u32);
	// stw r9,696(r4)
	PPC_STORE_U32(ctx.r4.u32 + 696, ctx.r9.u32);
	// lwz r14,-160(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// addi r14,r14,6688
	r14.s64 = r14.s64 + 6688;
	// stw r14,560(r4)
	PPC_STORE_U32(ctx.r4.u32 + 560, r14.u32);
	// stw r11,704(r4)
	PPC_STORE_U32(ctx.r4.u32 + 704, r11.u32);
	// lis r11,-32143
	r11.s64 = -2106523648;
	// lis r14,-32143
	r14.s64 = -2106523648;
	// stw r10,700(r4)
	PPC_STORE_U32(ctx.r4.u32 + 700, ctx.r10.u32);
	// lis r15,-32144
	r15.s64 = -2106589184;
	// addi r14,r14,-24232
	r14.s64 = r14.s64 + -24232;
	// lis r16,-32143
	r16.s64 = -2106523648;
	// stw r11,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r11.u32);
	// lis r17,-32143
	r17.s64 = -2106523648;
	// lis r18,-32143
	r18.s64 = -2106523648;
	// lis r19,-32143
	r19.s64 = -2106523648;
	// stw r14,708(r4)
	PPC_STORE_U32(ctx.r4.u32 + 708, r14.u32);
	// lis r20,-32143
	r20.s64 = -2106523648;
	// lis r21,-32143
	r21.s64 = -2106523648;
	// lis r22,-32143
	r22.s64 = -2106523648;
	// lis r23,-32143
	r23.s64 = -2106523648;
	// lis r24,-32143
	r24.s64 = -2106523648;
	// lis r25,-32143
	r25.s64 = -2106523648;
	// lis r26,-32143
	r26.s64 = -2106523648;
	// lis r27,-32143
	r27.s64 = -2106523648;
	// lis r28,-32143
	r28.s64 = -2106523648;
	// lis r29,-32143
	r29.s64 = -2106523648;
	// lis r30,-32143
	r30.s64 = -2106523648;
	// lis r31,-32143
	r31.s64 = -2106523648;
	// lis r3,-32143
	ctx.r3.s64 = -2106523648;
	// lis r5,-32143
	ctx.r5.s64 = -2106523648;
	// lis r6,-32144
	ctx.r6.s64 = -2106589184;
	// lis r7,-32143
	ctx.r7.s64 = -2106523648;
	// addi r15,r15,31184
	r15.s64 = r15.s64 + 31184;
	// addi r16,r16,-30528
	r16.s64 = r16.s64 + -30528;
	// addi r17,r17,-26752
	r17.s64 = r17.s64 + -26752;
	// addi r18,r18,-28936
	r18.s64 = r18.s64 + -28936;
	// addi r19,r19,1264
	r19.s64 = r19.s64 + 1264;
	// addi r20,r20,1952
	r20.s64 = r20.s64 + 1952;
	// stw r15,712(r4)
	PPC_STORE_U32(ctx.r4.u32 + 712, r15.u32);
	// addi r21,r21,2032
	r21.s64 = r21.s64 + 2032;
	// stw r16,716(r4)
	PPC_STORE_U32(ctx.r4.u32 + 716, r16.u32);
	// addi r22,r22,2112
	r22.s64 = r22.s64 + 2112;
	// stw r17,720(r4)
	PPC_STORE_U32(ctx.r4.u32 + 720, r17.u32);
	// addi r23,r23,2728
	r23.s64 = r23.s64 + 2728;
	// stw r18,724(r4)
	PPC_STORE_U32(ctx.r4.u32 + 724, r18.u32);
	// addi r24,r24,4352
	r24.s64 = r24.s64 + 4352;
	// stw r19,796(r4)
	PPC_STORE_U32(ctx.r4.u32 + 796, r19.u32);
	// addi r25,r25,4952
	r25.s64 = r25.s64 + 4952;
	// stw r20,804(r4)
	PPC_STORE_U32(ctx.r4.u32 + 804, r20.u32);
	// addi r26,r26,6600
	r26.s64 = r26.s64 + 6600;
	// stw r21,808(r4)
	PPC_STORE_U32(ctx.r4.u32 + 808, r21.u32);
	// addi r27,r27,7088
	r27.s64 = r27.s64 + 7088;
	// stw r22,812(r4)
	PPC_STORE_U32(ctx.r4.u32 + 812, r22.u32);
	// addi r28,r28,8592
	r28.s64 = r28.s64 + 8592;
	// stw r23,816(r4)
	PPC_STORE_U32(ctx.r4.u32 + 816, r23.u32);
	// addi r29,r29,9072
	r29.s64 = r29.s64 + 9072;
	// stw r24,820(r4)
	PPC_STORE_U32(ctx.r4.u32 + 820, r24.u32);
	// addi r30,r30,10608
	r30.s64 = r30.s64 + 10608;
	// stw r25,824(r4)
	PPC_STORE_U32(ctx.r4.u32 + 824, r25.u32);
	// addi r31,r31,11224
	r31.s64 = r31.s64 + 11224;
	// stw r26,828(r4)
	PPC_STORE_U32(ctx.r4.u32 + 828, r26.u32);
	// addi r3,r3,12856
	ctx.r3.s64 = ctx.r3.s64 + 12856;
	// stw r27,832(r4)
	PPC_STORE_U32(ctx.r4.u32 + 832, r27.u32);
	// addi r5,r5,13456
	ctx.r5.s64 = ctx.r5.s64 + 13456;
	// stw r28,836(r4)
	PPC_STORE_U32(ctx.r4.u32 + 836, r28.u32);
	// addi r6,r6,26632
	ctx.r6.s64 = ctx.r6.s64 + 26632;
	// stw r29,840(r4)
	PPC_STORE_U32(ctx.r4.u32 + 840, r29.u32);
	// addi r7,r7,-15520
	ctx.r7.s64 = ctx.r7.s64 + -15520;
	// stw r30,844(r4)
	PPC_STORE_U32(ctx.r4.u32 + 844, r30.u32);
	// lis r8,-32143
	ctx.r8.s64 = -2106523648;
	// stw r31,848(r4)
	PPC_STORE_U32(ctx.r4.u32 + 848, r31.u32);
	// lis r9,-32143
	ctx.r9.s64 = -2106523648;
	// stw r3,852(r4)
	PPC_STORE_U32(ctx.r4.u32 + 852, ctx.r3.u32);
	// lis r10,-32143
	ctx.r10.s64 = -2106523648;
	// stw r5,856(r4)
	PPC_STORE_U32(ctx.r4.u32 + 856, ctx.r5.u32);
	// lis r11,-32143
	r11.s64 = -2106523648;
	// stw r6,728(r4)
	PPC_STORE_U32(ctx.r4.u32 + 728, ctx.r6.u32);
	// addi r8,r8,-15488
	ctx.r8.s64 = ctx.r8.s64 + -15488;
	// stw r7,732(r4)
	PPC_STORE_U32(ctx.r4.u32 + 732, ctx.r7.u32);
	// addi r9,r9,-15456
	ctx.r9.s64 = ctx.r9.s64 + -15456;
	// addi r10,r10,-15424
	ctx.r10.s64 = ctx.r10.s64 + -15424;
	// addi r11,r11,-15392
	r11.s64 = r11.s64 + -15392;
	// lwz r14,-160(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// addi r14,r14,1872
	r14.s64 = r14.s64 + 1872;
	// stw r14,800(r4)
	PPC_STORE_U32(ctx.r4.u32 + 800, r14.u32);
	// stw r10,744(r4)
	PPC_STORE_U32(ctx.r4.u32 + 744, ctx.r10.u32);
	// lis r10,-32143
	ctx.r10.s64 = -2106523648;
	// lis r14,-32143
	r14.s64 = -2106523648;
	// stw r8,736(r4)
	PPC_STORE_U32(ctx.r4.u32 + 736, ctx.r8.u32);
	// lis r31,-32144
	r31.s64 = -2106589184;
	// stw r9,740(r4)
	PPC_STORE_U32(ctx.r4.u32 + 740, ctx.r9.u32);
	// addi r14,r14,-14784
	r14.s64 = r14.s64 + -14784;
	// stw r11,748(r4)
	PPC_STORE_U32(ctx.r4.u32 + 748, r11.u32);
	// lis r3,-32144
	ctx.r3.s64 = -2106589184;
	// stw r10,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r10.u32);
	// lis r15,-32143
	r15.s64 = -2106523648;
	// lis r16,-32143
	r16.s64 = -2106523648;
	// lis r17,-32143
	r17.s64 = -2106523648;
	// stw r14,752(r4)
	PPC_STORE_U32(ctx.r4.u32 + 752, r14.u32);
	// lis r18,-32143
	r18.s64 = -2106523648;
	// lis r19,-32143
	r19.s64 = -2106523648;
	// lis r20,-32143
	r20.s64 = -2106523648;
	// lis r21,-32143
	r21.s64 = -2106523648;
	// lis r22,-32143
	r22.s64 = -2106523648;
	// lis r23,-32144
	r23.s64 = -2106589184;
	// lis r24,-32144
	r24.s64 = -2106589184;
	// lis r25,-32144
	r25.s64 = -2106589184;
	// lis r26,-32144
	r26.s64 = -2106589184;
	// lis r27,-32144
	r27.s64 = -2106589184;
	// lis r28,-32144
	r28.s64 = -2106589184;
	// lis r29,-32144
	r29.s64 = -2106589184;
	// lis r30,-32144
	r30.s64 = -2106589184;
	// lis r5,-32144
	ctx.r5.s64 = -2106589184;
	// lis r6,-32144
	ctx.r6.s64 = -2106589184;
	// addi r31,r31,21696
	r31.s64 = r31.s64 + 21696;
	// addi r3,r3,21720
	ctx.r3.s64 = ctx.r3.s64 + 21720;
	// addi r15,r15,-13272
	r15.s64 = r15.s64 + -13272;
	// addi r16,r16,-13184
	r16.s64 = r16.s64 + -13184;
	// addi r17,r17,-13152
	r17.s64 = r17.s64 + -13152;
	// addi r18,r18,-12544
	r18.s64 = r18.s64 + -12544;
	// stw r31,632(r4)
	PPC_STORE_U32(ctx.r4.u32 + 632, r31.u32);
	// addi r19,r19,-11032
	r19.s64 = r19.s64 + -11032;
	// stw r3,636(r4)
	PPC_STORE_U32(ctx.r4.u32 + 636, ctx.r3.u32);
	// addi r20,r20,-10912
	r20.s64 = r20.s64 + -10912;
	// stw r15,756(r4)
	PPC_STORE_U32(ctx.r4.u32 + 756, r15.u32);
	// addi r21,r21,-10304
	r21.s64 = r21.s64 + -10304;
	// stw r16,760(r4)
	PPC_STORE_U32(ctx.r4.u32 + 760, r16.u32);
	// addi r22,r22,-8792
	r22.s64 = r22.s64 + -8792;
	// stw r17,764(r4)
	PPC_STORE_U32(ctx.r4.u32 + 764, r17.u32);
	// addi r23,r23,4816
	r23.s64 = r23.s64 + 4816;
	// stw r18,768(r4)
	PPC_STORE_U32(ctx.r4.u32 + 768, r18.u32);
	// addi r24,r24,17240
	r24.s64 = r24.s64 + 17240;
	// stw r19,772(r4)
	PPC_STORE_U32(ctx.r4.u32 + 772, r19.u32);
	// addi r25,r25,17776
	r25.s64 = r25.s64 + 17776;
	// stw r20,780(r4)
	PPC_STORE_U32(ctx.r4.u32 + 780, r20.u32);
	// addi r26,r26,17800
	r26.s64 = r26.s64 + 17800;
	// stw r21,784(r4)
	PPC_STORE_U32(ctx.r4.u32 + 784, r21.u32);
	// addi r27,r27,18328
	r27.s64 = r27.s64 + 18328;
	// stw r22,788(r4)
	PPC_STORE_U32(ctx.r4.u32 + 788, r22.u32);
	// addi r28,r28,18352
	r28.s64 = r28.s64 + 18352;
	// stw r23,600(r4)
	PPC_STORE_U32(ctx.r4.u32 + 600, r23.u32);
	// addi r29,r29,19336
	r29.s64 = r29.s64 + 19336;
	// stw r24,604(r4)
	PPC_STORE_U32(ctx.r4.u32 + 604, r24.u32);
	// addi r30,r30,21224
	r30.s64 = r30.s64 + 21224;
	// stw r25,608(r4)
	PPC_STORE_U32(ctx.r4.u32 + 608, r25.u32);
	// addi r5,r5,22336
	ctx.r5.s64 = ctx.r5.s64 + 22336;
	// stw r26,612(r4)
	PPC_STORE_U32(ctx.r4.u32 + 612, r26.u32);
	// addi r6,r6,23856
	ctx.r6.s64 = ctx.r6.s64 + 23856;
	// stw r27,616(r4)
	PPC_STORE_U32(ctx.r4.u32 + 616, r27.u32);
	// lis r7,-32144
	ctx.r7.s64 = -2106589184;
	// stw r28,620(r4)
	PPC_STORE_U32(ctx.r4.u32 + 620, r28.u32);
	// lis r8,-32144
	ctx.r8.s64 = -2106589184;
	// stw r29,624(r4)
	PPC_STORE_U32(ctx.r4.u32 + 624, r29.u32);
	// lis r9,-32144
	ctx.r9.s64 = -2106589184;
	// stw r30,628(r4)
	PPC_STORE_U32(ctx.r4.u32 + 628, r30.u32);
	// lis r10,-32144
	ctx.r10.s64 = -2106589184;
	// stw r5,640(r4)
	PPC_STORE_U32(ctx.r4.u32 + 640, ctx.r5.u32);
	// addi r11,r4,864
	r11.s64 = ctx.r4.s64 + 864;
	// stw r6,644(r4)
	PPC_STORE_U32(ctx.r4.u32 + 644, ctx.r6.u32);
	// addi r7,r7,23936
	ctx.r7.s64 = ctx.r7.s64 + 23936;
	// addi r8,r8,23960
	ctx.r8.s64 = ctx.r8.s64 + 23960;
	// addi r31,r9,24576
	r31.s64 = ctx.r9.s64 + 24576;
	// addi r3,r10,26096
	ctx.r3.s64 = ctx.r10.s64 + 26096;
	// lwz r14,-160(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// addi r14,r14,-10944
	r14.s64 = r14.s64 + -10944;
	// stw r14,776(r4)
	PPC_STORE_U32(ctx.r4.u32 + 776, r14.u32);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// stw r7,648(r4)
	PPC_STORE_U32(ctx.r4.u32 + 648, ctx.r7.u32);
	// li r9,4
	ctx.r9.s64 = 4;
	// stw r8,652(r4)
	PPC_STORE_U32(ctx.r4.u32 + 652, ctx.r8.u32);
	// stw r31,656(r4)
	PPC_STORE_U32(ctx.r4.u32 + 656, r31.u32);
	// stw r3,660(r4)
	PPC_STORE_U32(ctx.r4.u32 + 660, ctx.r3.u32);
loc_8266D294:
	// lis r5,-32153
	ctx.r5.s64 = -2107179008;
	// lis r6,-32153
	ctx.r6.s64 = -2107179008;
	// lis r7,-32153
	ctx.r7.s64 = -2107179008;
	// lis r8,-32153
	ctx.r8.s64 = -2107179008;
	// addi r5,r5,-14968
	ctx.r5.s64 = ctx.r5.s64 + -14968;
	// addi r6,r6,-14968
	ctx.r6.s64 = ctx.r6.s64 + -14968;
	// addi r7,r7,-14968
	ctx.r7.s64 = ctx.r7.s64 + -14968;
	// addi r8,r8,-14968
	ctx.r8.s64 = ctx.r8.s64 + -14968;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r5,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r5.u32);
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// stw r8,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r8.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne cr6,0x8266d294
	if (!cr6.eq) goto loc_8266D294;
	// lis r24,-32143
	r24.s64 = -2106523648;
	// lis r30,-32143
	r30.s64 = -2106523648;
	// addi r24,r24,-6752
	r24.s64 = r24.s64 + -6752;
	// lis r23,-32143
	r23.s64 = -2106523648;
	// lis r25,-32143
	r25.s64 = -2106523648;
	// lis r26,-32143
	r26.s64 = -2106523648;
	// lis r27,-32143
	r27.s64 = -2106523648;
	// stw r24,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r24.u32);
	// addi r11,r30,-4016
	r11.s64 = r30.s64 + -4016;
	// lis r28,-32143
	r28.s64 = -2106523648;
	// lis r29,-32143
	r29.s64 = -2106523648;
	// lis r31,-32143
	r31.s64 = -2106523648;
	// lis r3,-32143
	ctx.r3.s64 = -2106523648;
	// lis r5,-32143
	ctx.r5.s64 = -2106523648;
	// stw r11,888(r4)
	PPC_STORE_U32(ctx.r4.u32 + 888, r11.u32);
	// lis r6,-32143
	ctx.r6.s64 = -2106523648;
	// lis r7,-32143
	ctx.r7.s64 = -2106523648;
	// lis r8,-32143
	ctx.r8.s64 = -2106523648;
	// lis r9,-32143
	ctx.r9.s64 = -2106523648;
	// lis r10,-32143
	ctx.r10.s64 = -2106523648;
	// addi r23,r23,-7352
	r23.s64 = r23.s64 + -7352;
	// addi r25,r25,-6656
	r25.s64 = r25.s64 + -6656;
	// addi r26,r26,-6560
	r26.s64 = r26.s64 + -6560;
	// addi r27,r27,-6464
	r27.s64 = r27.s64 + -6464;
	// addi r28,r28,-6368
	r28.s64 = r28.s64 + -6368;
	// addi r29,r29,-5712
	r29.s64 = r29.s64 + -5712;
	// stw r23,860(r4)
	PPC_STORE_U32(ctx.r4.u32 + 860, r23.u32);
	// addi r31,r31,-3888
	r31.s64 = r31.s64 + -3888;
	// stw r25,868(r4)
	PPC_STORE_U32(ctx.r4.u32 + 868, r25.u32);
	// addi r3,r3,-3792
	ctx.r3.s64 = ctx.r3.s64 + -3792;
	// stw r26,872(r4)
	PPC_STORE_U32(ctx.r4.u32 + 872, r26.u32);
	// addi r5,r5,-3136
	ctx.r5.s64 = ctx.r5.s64 + -3136;
	// stw r27,876(r4)
	PPC_STORE_U32(ctx.r4.u32 + 876, r27.u32);
	// addi r6,r6,-1440
	ctx.r6.s64 = ctx.r6.s64 + -1440;
	// stw r28,880(r4)
	PPC_STORE_U32(ctx.r4.u32 + 880, r28.u32);
	// addi r7,r7,-1312
	ctx.r7.s64 = ctx.r7.s64 + -1312;
	// stw r29,884(r4)
	PPC_STORE_U32(ctx.r4.u32 + 884, r29.u32);
	// addi r11,r8,-1216
	r11.s64 = ctx.r8.s64 + -1216;
	// stw r31,892(r4)
	PPC_STORE_U32(ctx.r4.u32 + 892, r31.u32);
	// addi r9,r9,-560
	ctx.r9.s64 = ctx.r9.s64 + -560;
	// stw r3,896(r4)
	PPC_STORE_U32(ctx.r4.u32 + 896, ctx.r3.u32);
	// addi r10,r10,1136
	ctx.r10.s64 = ctx.r10.s64 + 1136;
	// stw r5,900(r4)
	PPC_STORE_U32(ctx.r4.u32 + 900, ctx.r5.u32);
	// stw r6,904(r4)
	PPC_STORE_U32(ctx.r4.u32 + 904, ctx.r6.u32);
	// stw r7,908(r4)
	PPC_STORE_U32(ctx.r4.u32 + 908, ctx.r7.u32);
	// stw r11,912(r4)
	PPC_STORE_U32(ctx.r4.u32 + 912, r11.u32);
	// stw r9,916(r4)
	PPC_STORE_U32(ctx.r4.u32 + 916, ctx.r9.u32);
	// stw r10,920(r4)
	PPC_STORE_U32(ctx.r4.u32 + 920, ctx.r10.u32);
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8266D398"))) PPC_WEAK_FUNC(sub_8266D398);
PPC_FUNC_IMPL(__imp__sub_8266D398) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r6,r9
	r11.u64 = ctx.r6.u64 + ctx.r9.u64;
	// stvx v0,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r31,0(r7)
	r31.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// add r6,r3,r9
	ctx.r6.u64 = ctx.r3.u64 + ctx.r9.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// std r31,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, r31.u64);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r4,r4,r10
	ctx.r4.u64 = ctx.r4.u64 + ctx.r10.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// std r3,0(r5)
	PPC_STORE_U64(ctx.r5.u32 + 0, ctx.r3.u64);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// std r3,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r3.u64);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r4,r4,r10
	ctx.r4.u64 = ctx.r4.u64 + ctx.r10.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// std r3,0(r5)
	PPC_STORE_U64(ctx.r5.u32 + 0, ctx.r3.u64);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// std r3,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r3.u64);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r4,r4,r10
	ctx.r4.u64 = ctx.r4.u64 + ctx.r10.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// std r3,0(r5)
	PPC_STORE_U64(ctx.r5.u32 + 0, ctx.r3.u64);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// std r3,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r3.u64);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r4,r4,r10
	ctx.r4.u64 = ctx.r4.u64 + ctx.r10.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// std r3,0(r5)
	PPC_STORE_U64(ctx.r5.u32 + 0, ctx.r3.u64);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// std r3,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r3.u64);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r4,r4,r10
	ctx.r4.u64 = ctx.r4.u64 + ctx.r10.u64;
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// std r3,0(r5)
	PPC_STORE_U64(ctx.r5.u32 + 0, ctx.r3.u64);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// std r3,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r3.u64);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r4,r4,r10
	ctx.r4.u64 = ctx.r4.u64 + ctx.r10.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// std r3,0(r5)
	PPC_STORE_U64(ctx.r5.u32 + 0, ctx.r3.u64);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// std r3,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r3.u64);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// std r3,0(r5)
	PPC_STORE_U64(ctx.r5.u32 + 0, ctx.r3.u64);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ldx r7,r7,r10
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r7.u32 + ctx.r10.u32);
	// stdx r7,r4,r10
	PPC_STORE_U64(ctx.r4.u32 + ctx.r10.u32, ctx.r7.u64);
	// lvx128 v0,r11,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r6,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ldx r11,r8,r10
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + ctx.r10.u32);
	// stdx r11,r5,r10
	PPC_STORE_U64(ctx.r5.u32 + ctx.r10.u32, r11.u64);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266D574"))) PPC_WEAK_FUNC(sub_8266D574);
PPC_FUNC_IMPL(__imp__sub_8266D574) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266D578"))) PPC_WEAK_FUNC(sub_8266D578);
PPC_FUNC_IMPL(__imp__sub_8266D578) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// ori r8,r10,22857
	ctx.r8.u64 = ctx.r10.u64 | 22857;
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lis r11,12338
	r11.s64 = 808583168;
	// ori r6,r11,13385
	ctx.r6.u64 = r11.u64 | 13385;
	// lis r11,12849
	r11.s64 = 842072064;
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// ori r5,r11,22105
	ctx.r5.u64 = r11.u64 | 22105;
	// lis r11,12850
	r11.s64 = 842137600;
	// ori r10,r11,13392
	ctx.r10.u64 = r11.u64 | 13392;
	// beq cr6,0x8266d5d8
	if (cr6.eq) goto loc_8266D5D8;
	// cmplw cr6,r9,r6
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, xer);
	// beq cr6,0x8266d5d8
	if (cr6.eq) goto loc_8266D5D8;
	// cmplw cr6,r9,r5
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r5.u32, xer);
	// beq cr6,0x8266d5d8
	if (cr6.eq) goto loc_8266D5D8;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x8266d5d8
	if (cr6.eq) goto loc_8266D5D8;
	// lis r11,12593
	r11.s64 = 825294848;
	// ori r11,r11,13392
	r11.u64 = r11.u64 | 13392;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x8266d5d8
	if (cr6.eq) goto loc_8266D5D8;
	// li r3,3
	ctx.r3.s64 = 3;
	// blr 
	return;
loc_8266D5D8:
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r11,16(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// beq cr6,0x8266d658
	if (cr6.eq) goto loc_8266D658;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// beq cr6,0x8266d658
	if (cr6.eq) goto loc_8266D658;
	// cmplw cr6,r11,r5
	cr6.compare<uint32_t>(r11.u32, ctx.r5.u32, xer);
	// beq cr6,0x8266d658
	if (cr6.eq) goto loc_8266D658;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x8266d658
	if (cr6.eq) goto loc_8266D658;
	// lis r10,12889
	ctx.r10.s64 = 844693504;
	// ori r10,r10,21849
	ctx.r10.u64 = ctx.r10.u64 | 21849;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x8266d658
	if (cr6.eq) goto loc_8266D658;
	// lis r10,22870
	ctx.r10.s64 = 1498808320;
	// ori r10,r10,22869
	ctx.r10.u64 = ctx.r10.u64 | 22869;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x8266d658
	if (cr6.eq) goto loc_8266D658;
	// lis r10,21849
	ctx.r10.s64 = 1431896064;
	// ori r10,r10,22105
	ctx.r10.u64 = ctx.r10.u64 | 22105;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x8266d658
	if (cr6.eq) goto loc_8266D658;
	// lis r10,12849
	ctx.r10.s64 = 842072064;
	// ori r10,r10,22094
	ctx.r10.u64 = ctx.r10.u64 | 22094;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x8266d658
	if (cr6.eq) goto loc_8266D658;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8266d668
	if (cr6.eq) goto loc_8266D668;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// beq cr6,0x8266d658
	if (cr6.eq) goto loc_8266D658;
loc_8266D650:
	// li r3,4
	ctx.r3.s64 = 4;
	// blr 
	return;
loc_8266D658:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8266d668
	if (cr6.eq) goto loc_8266D668;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x8266d68c
	if (!cr6.eq) goto loc_8266D68C;
loc_8266D668:
	// lhz r10,14(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 14);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// beq cr6,0x8266d68c
	if (cr6.eq) goto loc_8266D68C;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// beq cr6,0x8266d68c
	if (cr6.eq) goto loc_8266D68C;
	// cmplwi cr6,r10,24
	cr6.compare<uint32_t>(ctx.r10.u32, 24, xer);
	// beq cr6,0x8266d68c
	if (cr6.eq) goto loc_8266D68C;
	// cmplwi cr6,r10,32
	cr6.compare<uint32_t>(ctx.r10.u32, 32, xer);
	// bne cr6,0x8266d650
	if (!cr6.eq) goto loc_8266D650;
loc_8266D68C:
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// beq cr6,0x8266d6a4
	if (cr6.eq) goto loc_8266D6A4;
	// cmplw cr6,r9,r6
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, xer);
	// beq cr6,0x8266d6a4
	if (cr6.eq) goto loc_8266D6A4;
	// cmplw cr6,r9,r5
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r5.u32, xer);
	// bne cr6,0x8266d6c0
	if (!cr6.eq) goto loc_8266D6C0;
loc_8266D6A4:
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x8266d6c0
	if (!cr6.eq) goto loc_8266D6C0;
	// lhz r10,14(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 14);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// bne cr6,0x8266d6c0
	if (!cr6.eq) goto loc_8266D6C0;
	// li r3,5
	ctx.r3.s64 = 5;
	// blr 
	return;
loc_8266D6C0:
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// beq cr6,0x8266d6d8
	if (cr6.eq) goto loc_8266D6D8;
	// cmplw cr6,r9,r6
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, xer);
	// beq cr6,0x8266d6d8
	if (cr6.eq) goto loc_8266D6D8;
	// cmplw cr6,r9,r5
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r5.u32, xer);
	// bne cr6,0x8266d6f0
	if (!cr6.eq) goto loc_8266D6F0;
loc_8266D6D8:
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// beq cr6,0x8266d6fc
	if (cr6.eq) goto loc_8266D6FC;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// beq cr6,0x8266d6fc
	if (cr6.eq) goto loc_8266D6FC;
	// cmplw cr6,r11,r5
	cr6.compare<uint32_t>(r11.u32, ctx.r5.u32, xer);
	// beq cr6,0x8266d6fc
	if (cr6.eq) goto loc_8266D6FC;
loc_8266D6F0:
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// li r3,7
	ctx.r3.s64 = 7;
	// beqlr cr6
	if (cr6.eq) return;
loc_8266D6FC:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266D704"))) PPC_WEAK_FUNC(sub_8266D704);
PPC_FUNC_IMPL(__imp__sub_8266D704) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266D708"))) PPC_WEAK_FUNC(sub_8266D708);
PPC_FUNC_IMPL(__imp__sub_8266D708) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce8
	// lwz r11,14624(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14624);
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// subf r4,r7,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8266d754
	if (cr6.eq) goto loc_8266D754;
	// lwz r31,14480(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14480);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r10,14628(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14628);
	// srawi r26,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	r26.s64 = r31.s32 >> 2;
	// lwz r30,14632(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14632);
	// mullw r8,r11,r7
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// subf r27,r31,r11
	r27.s64 = r11.s64 - r31.s64;
	// addze r11,r26
	temp.s64 = r26.s64 + xer.ca;
	xer.ca = temp.u32 < r26.u32;
	r11.s64 = temp.s64;
	// mullw r29,r10,r7
	r29.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// subf r26,r11,r10
	r26.s64 = ctx.r10.s64 - r11.s64;
	// subf r25,r11,r30
	r25.s64 = r30.s64 - r11.s64;
	// b 0x8266d774
	goto loc_8266D774;
loc_8266D754:
	// lwz r11,14588(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// lwz r10,14480(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14480);
	// mullw r8,r11,r7
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// subf r27,r10,r11
	r27.s64 = r11.s64 - ctx.r10.s64;
	// srawi r29,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r29.s64 = ctx.r8.s32 >> 2;
	// srawi r11,r27,2
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x3) != 0);
	r11.s64 = r27.s32 >> 2;
	// addze r26,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r26.s64 = temp.s64;
	// mr r25,r26
	r25.u64 = r26.u64;
loc_8266D774:
	// lwz r31,14588(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lwz r10,14608(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14608);
	// lwz r11,14604(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14604);
	// mullw r10,r10,r31
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r31.s32);
	// lwz r24,14492(r9)
	r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// lwz r30,14500(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14500);
	// srawi r31,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r31.s64 = r11.s32 >> 2;
	// mullw r7,r24,r7
	ctx.r7.s64 = int64_t(r24.s32) * int64_t(ctx.r7.s32);
	// addze r31,r31
	temp.s64 = r31.s64 + xer.ca;
	xer.ca = temp.u32 < r31.u32;
	r31.s64 = temp.s64;
	// srawi r24,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r24.s64 = ctx.r10.s32 >> 2;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// addze r11,r24
	temp.s64 = r24.s64 + xer.ca;
	xer.ca = temp.u32 < r24.u32;
	r11.s64 = temp.s64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r3,r7,r3
	ctx.r3.u64 = ctx.r7.u64 + ctx.r3.u64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r8,r11,r5
	ctx.r8.u64 = r11.u64 + ctx.r5.u64;
	// add r7,r11,r6
	ctx.r7.u64 = r11.u64 + ctx.r6.u64;
	// ble cr6,0x8266d858
	if (!cr6.gt) goto loc_8266D858;
	// lwz r5,14480(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14480);
loc_8266D7D0:
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x8266d838
	if (!cr6.gt) goto loc_8266D838;
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
loc_8266D7E0:
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// stb r5,-4(r11)
	PPC_STORE_U8(r11.u32 + -4, ctx.r5.u8);
	// lbz r5,1(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// stb r5,-2(r11)
	PPC_STORE_U8(r11.u32 + -2, ctx.r5.u8);
	// lbz r5,2(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// stb r5,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r5.u8);
	// lbz r5,3(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stb r5,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r5.u8);
	// lbz r5,0(r8)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r5,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r5.u8);
	// stb r5,-3(r11)
	PPC_STORE_U8(r11.u32 + -3, ctx.r5.u8);
	// lbz r5,0(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stb r5,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r5.u8);
	// stb r5,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, ctx.r5.u8);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r5,14480(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14480);
	// cmpw cr6,r6,r5
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, xer);
	// blt cr6,0x8266d7e0
	if (cr6.lt) goto loc_8266D7E0;
loc_8266D838:
	// lwz r11,14492(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// add r10,r27,r10
	ctx.r10.u64 = r27.u64 + ctx.r10.u64;
	// add r8,r26,r8
	ctx.r8.u64 = r26.u64 + ctx.r8.u64;
	// add r7,r25,r7
	ctx.r7.u64 = r25.u64 + ctx.r7.u64;
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x8266d7d0
	if (!cr6.eq) goto loc_8266D7D0;
loc_8266D858:
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_8266D85C"))) PPC_WEAK_FUNC(sub_8266D85C);
PPC_FUNC_IMPL(__imp__sub_8266D85C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266D860"))) PPC_WEAK_FUNC(sub_8266D860);
PPC_FUNC_IMPL(__imp__sub_8266D860) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bccc
	// subf r30,r7,r8
	r30.s64 = ctx.r8.s64 - ctx.r7.s64;
	// lwz r8,14492(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// lwz r31,14500(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14500);
	// mullw r8,r8,r7
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// lwz r10,14588(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// lwz r11,14604(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14604);
	// lwz r29,14608(r9)
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14608);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// add r19,r8,r3
	r19.u64 = ctx.r8.u64 + ctx.r3.u64;
	// mullw r8,r10,r7
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// mullw r10,r29,r10
	ctx.r10.s64 = int64_t(r29.s32) * int64_t(ctx.r10.s32);
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// srawi r3,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r3.s64 = r11.s32 >> 1;
	// mr r31,r19
	r31.u64 = r19.u64;
	// addze r3,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r3.s64 = temp.s64;
	// srawi r29,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r29.s64 = ctx.r10.s32 >> 2;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// addze r11,r29
	temp.s64 = r29.s64 + xer.ca;
	xer.ca = temp.u32 < r29.u32;
	r11.s64 = temp.s64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// add r4,r10,r4
	ctx.r4.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r18,r11,r5
	r18.u64 = r11.u64 + ctx.r5.u64;
	// add r17,r11,r6
	r17.u64 = r11.u64 + ctx.r6.u64;
	// ble cr6,0x8266d934
	if (!cr6.gt) goto loc_8266D934;
	// lwz r7,14524(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
loc_8266D8DC:
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8266d918
	if (!cr6.gt) goto loc_8266D918;
loc_8266D8F0:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r7,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r7.u8);
	// lbz r7,1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stb r7,2(r10)
	PPC_STORE_U8(ctx.r10.u32 + 2, ctx.r7.u8);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r7,14524(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// blt cr6,0x8266d8f0
	if (cr6.lt) goto loc_8266D8F0;
loc_8266D918:
	// lwz r10,14492(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// lwz r11,14588(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// add r31,r10,r31
	r31.u64 = ctx.r10.u64 + r31.u64;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x8266d8dc
	if (!cr6.eq) goto loc_8266D8DC;
loc_8266D934:
	// lwz r8,14496(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14496);
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// lwz r10,14588(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// li r21,2
	r21.s64 = 2;
	// addze r20,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r20.s64 = temp.s64;
	// add r6,r8,r19
	ctx.r6.u64 = ctx.r8.u64 + r19.u64;
	// mr r11,r18
	r11.u64 = r18.u64;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// add r24,r10,r18
	r24.u64 = ctx.r10.u64 + r18.u64;
	// add r23,r10,r17
	r23.u64 = ctx.r10.u64 + r17.u64;
	// cmpwi cr6,r20,2
	cr6.compare<int32_t>(r20.s32, 2, xer);
	// add r25,r8,r6
	r25.u64 = ctx.r8.u64 + ctx.r6.u64;
	// ble cr6,0x8266da84
	if (!cr6.gt) goto loc_8266DA84;
	// addi r10,r20,-3
	ctx.r10.s64 = r20.s64 + -3;
	// lwz r7,14524(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r22,r10,1
	r22.s64 = ctx.r10.s64 + 1;
	// addi r10,r22,1
	ctx.r10.s64 = r22.s64 + 1;
	// rlwinm r21,r10,1,0,30
	r21.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
loc_8266D980:
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8266da58
	if (!cr6.gt) goto loc_8266DA58;
	// addi r8,r6,1
	ctx.r8.s64 = ctx.r6.s64 + 1;
	// addi r10,r25,3
	ctx.r10.s64 = r25.s64 + 3;
	// subf r28,r25,r6
	r28.s64 = ctx.r6.s64 - r25.s64;
	// subf r31,r11,r24
	r31.s64 = r24.s64 - r11.s64;
	// subf r27,r11,r23
	r27.s64 = r23.s64 - r11.s64;
	// subf r26,r11,r5
	r26.s64 = ctx.r5.s64 - r11.s64;
loc_8266D9A4:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r6,r11,r27
	ctx.r6.u64 = r11.u64 + r27.u64;
	// lbzx r4,r11,r31
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + r31.u32);
	// add r5,r11,r26
	ctx.r5.u64 = r11.u64 + r26.u64;
	// rotlwi r30,r7,2
	r30.u64 = __builtin_rotateleft32(ctx.r7.u32, 2);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// add r30,r7,r30
	r30.u64 = ctx.r7.u64 + r30.u64;
	// rotlwi r7,r4,1
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r4.u32, 1);
	// add r7,r4,r7
	ctx.r7.u64 = ctx.r4.u64 + ctx.r7.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lbzx r7,r11,r31
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + r31.u32);
	// lbz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rotlwi r30,r7,3
	r30.u64 = __builtin_rotateleft32(ctx.r7.u32, 3);
	// subf r7,r7,r30
	ctx.r7.s64 = r30.s64 - ctx.r7.s64;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// stb r7,-2(r10)
	PPC_STORE_U8(ctx.r10.u32 + -2, ctx.r7.u8);
	// lbz r7,0(r5)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// lbz r4,0(r6)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// rotlwi r29,r7,2
	r29.u64 = __builtin_rotateleft32(ctx.r7.u32, 2);
	// rotlwi r30,r4,1
	r30.u64 = __builtin_rotateleft32(ctx.r4.u32, 1);
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// stbx r7,r28,r10
	PPC_STORE_U8(r28.u32 + ctx.r10.u32, ctx.r7.u8);
	// lbz r7,0(r6)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lbz r6,0(r5)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// rotlwi r5,r7,3
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r7.u32, 3);
	// subf r7,r7,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r7.s64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// stb r7,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r7.u8);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r7,14524(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// cmpw cr6,r3,r7
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r7.s32, xer);
	// blt cr6,0x8266d9a4
	if (cr6.lt) goto loc_8266D9A4;
loc_8266DA58:
	// lwz r10,14496(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14496);
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// lwz r8,14588(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// mr r11,r24
	r11.u64 = r24.u64;
	// add r6,r10,r25
	ctx.r6.u64 = ctx.r10.u64 + r25.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// add r24,r8,r24
	r24.u64 = ctx.r8.u64 + r24.u64;
	// add r23,r8,r23
	r23.u64 = ctx.r8.u64 + r23.u64;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// add r25,r10,r6
	r25.u64 = ctx.r10.u64 + ctx.r6.u64;
	// bne cr6,0x8266d980
	if (!cr6.eq) goto loc_8266D980;
loc_8266DA84:
	// cmpw cr6,r21,r20
	cr6.compare<int32_t>(r21.s32, r20.s32, xer);
	// bne cr6,0x8266dac4
	if (!cr6.eq) goto loc_8266DAC4;
	// lwz r8,14524(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x8266dac4
	if (!cr6.gt) goto loc_8266DAC4;
	// addi r8,r6,3
	ctx.r8.s64 = ctx.r6.s64 + 3;
loc_8266DAA0:
	// lbzx r7,r10,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// stb r7,-2(r8)
	PPC_STORE_U8(ctx.r8.u32 + -2, ctx.r7.u8);
	// lbzx r7,r10,r5
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r5.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lwz r7,14524(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// blt cr6,0x8266daa0
	if (cr6.lt) goto loc_8266DAA0;
loc_8266DAC4:
	// lwz r11,14492(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// li r21,3
	r21.s64 = 3;
	// lwz r10,14588(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// cmpwi cr6,r20,3
	cr6.compare<int32_t>(r20.s32, 3, xer);
	// rlwinm r6,r11,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,14496(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14496);
	// srawi r8,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 1;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// add r6,r11,r19
	ctx.r6.u64 = r11.u64 + r19.u64;
	// add r11,r8,r18
	r11.u64 = ctx.r8.u64 + r18.u64;
	// add r5,r8,r17
	ctx.r5.u64 = ctx.r8.u64 + r17.u64;
	// add r25,r7,r6
	r25.u64 = ctx.r7.u64 + ctx.r6.u64;
	// add r24,r10,r11
	r24.u64 = ctx.r10.u64 + r11.u64;
	// add r23,r10,r5
	r23.u64 = ctx.r10.u64 + ctx.r5.u64;
	// ble cr6,0x8266dc24
	if (!cr6.gt) goto loc_8266DC24;
	// addi r10,r20,-4
	ctx.r10.s64 = r20.s64 + -4;
	// lwz r7,14524(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r22,r10,1
	r22.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r22,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r21,r10,3
	r21.s64 = ctx.r10.s64 + 3;
loc_8266DB1C:
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8266dbf8
	if (!cr6.gt) goto loc_8266DBF8;
	// addi r8,r6,1
	ctx.r8.s64 = ctx.r6.s64 + 1;
	// addi r10,r25,3
	ctx.r10.s64 = r25.s64 + 3;
	// subf r29,r25,r6
	r29.s64 = ctx.r6.s64 - r25.s64;
	// subf r28,r11,r24
	r28.s64 = r24.s64 - r11.s64;
	// subf r27,r11,r23
	r27.s64 = r23.s64 - r11.s64;
	// subf r26,r11,r5
	r26.s64 = ctx.r5.s64 - r11.s64;
loc_8266DB40:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r6,r11,r28
	ctx.r6.u64 = r11.u64 + r28.u64;
	// add r5,r11,r26
	ctx.r5.u64 = r11.u64 + r26.u64;
	// rotlwi r30,r7,3
	r30.u64 = __builtin_rotateleft32(ctx.r7.u32, 3);
	// add r4,r11,r27
	ctx.r4.u64 = r11.u64 + r27.u64;
	// subf r7,r7,r30
	ctx.r7.s64 = r30.s64 - ctx.r7.s64;
	// lbz r31,0(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lbz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rotlwi r30,r6,2
	r30.u64 = __builtin_rotateleft32(ctx.r6.u32, 2);
	// rotlwi r31,r7,1
	r31.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// stb r7,-2(r10)
	PPC_STORE_U8(ctx.r10.u32 + -2, ctx.r7.u8);
	// lbz r7,0(r5)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// lbz r6,0(r4)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rotlwi r31,r7,3
	r31.u64 = __builtin_rotateleft32(ctx.r7.u32, 3);
	// subf r7,r7,r31
	ctx.r7.s64 = r31.s64 - ctx.r7.s64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// stbx r7,r10,r29
	PPC_STORE_U8(ctx.r10.u32 + r29.u32, ctx.r7.u8);
	// lbz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lbz r6,0(r5)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// rotlwi r4,r7,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r7.u32, 2);
	// rotlwi r5,r6,1
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// stb r7,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r7.u8);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r7,14524(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// cmpw cr6,r3,r7
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r7.s32, xer);
	// blt cr6,0x8266db40
	if (cr6.lt) goto loc_8266DB40;
loc_8266DBF8:
	// lwz r10,14496(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14496);
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// lwz r8,14588(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// mr r11,r24
	r11.u64 = r24.u64;
	// add r6,r10,r25
	ctx.r6.u64 = ctx.r10.u64 + r25.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// add r24,r8,r24
	r24.u64 = ctx.r8.u64 + r24.u64;
	// add r23,r8,r23
	r23.u64 = ctx.r8.u64 + r23.u64;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// add r25,r10,r6
	r25.u64 = ctx.r10.u64 + ctx.r6.u64;
	// bne cr6,0x8266db1c
	if (!cr6.eq) goto loc_8266DB1C;
loc_8266DC24:
	// lwz r10,14524(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8266dc84
	if (!cr6.gt) goto loc_8266DC84;
	// addi r10,r6,3
	ctx.r10.s64 = ctx.r6.s64 + 3;
	// subf r4,r6,r25
	ctx.r4.s64 = r25.s64 - ctx.r6.s64;
	// subf r6,r11,r5
	ctx.r6.s64 = ctx.r5.s64 - r11.s64;
loc_8266DC40:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmpw cr6,r21,r20
	cr6.compare<int32_t>(r21.s32, r20.s32, xer);
	// stb r8,-2(r10)
	PPC_STORE_U8(ctx.r10.u32 + -2, ctx.r8.u8);
	// lbzx r8,r11,r6
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// bne cr6,0x8266dc6c
	if (!cr6.eq) goto loc_8266DC6C;
	// lbz r5,-2(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// add r8,r4,r10
	ctx.r8.u64 = ctx.r4.u64 + ctx.r10.u64;
	// stb r5,-2(r8)
	PPC_STORE_U8(ctx.r8.u32 + -2, ctx.r5.u8);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// stb r5,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r5.u8);
loc_8266DC6C:
	// lwz r8,14524(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r7,r8
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r8.s32, xer);
	// blt cr6,0x8266dc40
	if (cr6.lt) goto loc_8266DC40;
loc_8266DC84:
	// lwz r10,14588(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// lwz r11,14492(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// li r7,0
	ctx.r7.s64 = 0;
	// srawi r6,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 1;
	// lwz r5,14524(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// add r10,r11,r19
	ctx.r10.u64 = r11.u64 + r19.u64;
	// addze r11,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	r11.s64 = temp.s64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// add r6,r11,r18
	ctx.r6.u64 = r11.u64 + r18.u64;
	// add r5,r11,r17
	ctx.r5.u64 = r11.u64 + r17.u64;
	// ble cr6,0x8266dd04
	if (!cr6.gt) goto loc_8266DD04;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// addi r11,r19,3
	r11.s64 = r19.s64 + 3;
	// subf r4,r18,r17
	ctx.r4.s64 = r17.s64 - r18.s64;
loc_8266DCC0:
	// lbz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stb r3,-2(r11)
	PPC_STORE_U8(r11.u32 + -2, ctx.r3.u8);
	// lbzx r3,r4,r8
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r8.u32);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r3,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r3.u8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lbz r3,0(r6)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// stb r3,-2(r10)
	PPC_STORE_U8(ctx.r10.u32 + -2, ctx.r3.u8);
	// lbz r3,0(r5)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stb r3,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r3.u8);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r3,14524(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// cmpw cr6,r7,r3
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r3.s32, xer);
	// blt cr6,0x8266dcc0
	if (cr6.lt) goto loc_8266DCC0;
loc_8266DD04:
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_8266DD08"))) PPC_WEAK_FUNC(sub_8266DD08);
PPC_FUNC_IMPL(__imp__sub_8266DD08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcf0
	// lwz r11,14624(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14624);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// subf r3,r7,r8
	ctx.r3.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8266dd54
	if (cr6.eq) goto loc_8266DD54;
	// lwz r30,14480(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14480);
	// rotlwi r8,r11,0
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r31,14628(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14628);
	// srawi r27,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r27.s64 = r30.s32 >> 1;
	// lwz r26,14632(r9)
	r26.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14632);
	// mullw r10,r8,r7
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// subf r28,r30,r8
	r28.s64 = ctx.r8.s64 - r30.s64;
	// addze r8,r27
	temp.s64 = r27.s64 + xer.ca;
	xer.ca = temp.u32 < r27.u32;
	ctx.r8.s64 = temp.s64;
	// mullw r11,r31,r7
	r11.s64 = int64_t(r31.s32) * int64_t(ctx.r7.s32);
	// subf r27,r8,r31
	r27.s64 = r31.s64 - ctx.r8.s64;
	// subf r26,r8,r26
	r26.s64 = r26.s64 - ctx.r8.s64;
	// b 0x8266dd74
	goto loc_8266DD74;
loc_8266DD54:
	// lwz r11,14588(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// lwz r8,14480(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14480);
	// mullw r10,r11,r7
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// subf r28,r8,r11
	r28.s64 = r11.s64 - ctx.r8.s64;
	// srawi r11,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r11.s64 = ctx.r10.s32 >> 1;
	// srawi r8,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	ctx.r8.s64 = r28.s32 >> 1;
	// addze r27,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	r27.s64 = temp.s64;
	// mr r26,r27
	r26.u64 = r27.u64;
loc_8266DD74:
	// lwz r8,14544(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14544);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// lwz r30,14548(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14548);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lwz r31,14540(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14540);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// lwz r31,14500(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14500);
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// lwz r11,14492(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r4,r11,r29
	ctx.r4.u64 = r11.u64 + r29.u64;
	// ble cr6,0x8266de28
	if (!cr6.gt) goto loc_8266DE28;
	// lwz r5,14480(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14480);
loc_8266DDB8:
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x8266de08
	if (!cr6.gt) goto loc_8266DE08;
	// addi r11,r4,1
	r11.s64 = ctx.r4.s64 + 1;
loc_8266DDC8:
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// stb r5,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, ctx.r5.u8);
	// lbz r5,1(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// stb r5,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r5.u8);
	// lbz r5,0(r8)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r5,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r5.u8);
	// lbz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// stb r5,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r5.u8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r5,14480(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14480);
	// cmpw cr6,r7,r5
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r5.s32, xer);
	// blt cr6,0x8266ddc8
	if (cr6.lt) goto loc_8266DDC8;
loc_8266DE08:
	// lwz r11,14492(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// add r10,r28,r10
	ctx.r10.u64 = r28.u64 + ctx.r10.u64;
	// add r8,r27,r8
	ctx.r8.u64 = r27.u64 + ctx.r8.u64;
	// add r6,r26,r6
	ctx.r6.u64 = r26.u64 + ctx.r6.u64;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x8266ddb8
	if (!cr6.eq) goto loc_8266DDB8;
loc_8266DE28:
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8266DE2C"))) PPC_WEAK_FUNC(sub_8266DE2C);
PPC_FUNC_IMPL(__imp__sub_8266DE2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266DE30"))) PPC_WEAK_FUNC(sub_8266DE30);
PPC_FUNC_IMPL(__imp__sub_8266DE30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r11.u32);
	// bne cr6,0x8266de60
	if (!cr6.eq) goto loc_8266DE60;
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// bne cr6,0x8266deac
	if (!cr6.eq) goto loc_8266DEAC;
	// li r11,2
	r11.s64 = 2;
	// li r3,31
	ctx.r3.s64 = 31;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r11.u32);
	// blr 
	return;
loc_8266DE60:
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// bne cr6,0x8266de84
	if (!cr6.eq) goto loc_8266DE84;
	// cmpwi cr6,r4,3
	cr6.compare<int32_t>(ctx.r4.s32, 3, xer);
	// bne cr6,0x8266def4
	if (!cr6.eq) goto loc_8266DEF4;
	// li r11,2
	r11.s64 = 2;
	// li r3,13
	ctx.r3.s64 = 13;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r11.u32);
	// blr 
	return;
loc_8266DE84:
	// cmpwi cr6,r3,5
	cr6.compare<int32_t>(ctx.r3.s32, 5, xer);
	// bne cr6,0x8266decc
	if (!cr6.eq) goto loc_8266DECC;
	// cmpwi cr6,r4,3
	cr6.compare<int32_t>(ctx.r4.s32, 3, xer);
	// bne cr6,0x8266df14
	if (!cr6.eq) goto loc_8266DF14;
	// li r11,4
	r11.s64 = 4;
	// li r10,3
	ctx.r10.s64 = 3;
	// li r3,53
	ctx.r3.s64 = 53;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// stw r10,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
	// blr 
	return;
loc_8266DEAC:
	// cmpwi cr6,r4,5
	cr6.compare<int32_t>(ctx.r4.s32, 5, xer);
	// bne cr6,0x8266df14
	if (!cr6.eq) goto loc_8266DF14;
	// li r11,4
	r11.s64 = 4;
	// li r10,3
	ctx.r10.s64 = 3;
	// li r3,35
	ctx.r3.s64 = 35;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// stw r10,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
	// blr 
	return;
loc_8266DECC:
	// cmpwi cr6,r3,7
	cr6.compare<int32_t>(ctx.r3.s32, 7, xer);
	// bne cr6,0x8266df14
	if (!cr6.eq) goto loc_8266DF14;
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// bne cr6,0x8266df14
	if (!cr6.eq) goto loc_8266DF14;
	// li r11,4
	r11.s64 = 4;
	// li r10,3
	ctx.r10.s64 = 3;
	// li r3,71
	ctx.r3.s64 = 71;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// stw r10,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
	// blr 
	return;
loc_8266DEF4:
	// cmpwi cr6,r4,7
	cr6.compare<int32_t>(ctx.r4.s32, 7, xer);
	// bne cr6,0x8266df14
	if (!cr6.eq) goto loc_8266DF14;
	// li r11,4
	r11.s64 = 4;
	// li r10,3
	ctx.r10.s64 = 3;
	// li r3,17
	ctx.r3.s64 = 17;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// stw r10,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
	// blr 
	return;
loc_8266DF14:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266DF1C"))) PPC_WEAK_FUNC(sub_8266DF1C);
PPC_FUNC_IMPL(__imp__sub_8266DF1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266DF20"))) PPC_WEAK_FUNC(sub_8266DF20);
PPC_FUNC_IMPL(__imp__sub_8266DF20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// lwz r28,420(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// lwz r27,412(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// mr r17,r4
	r17.u64 = ctx.r4.u64;
	// mr r16,r5
	r16.u64 = ctx.r5.u64;
	// mr r24,r6
	r24.u64 = ctx.r6.u64;
	// mr r21,r7
	r21.u64 = ctx.r7.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// addi r6,r1,100
	ctx.r6.s64 = ctx.r1.s64 + 100;
	// stw r20,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r20.u32);
	// addi r5,r1,92
	ctx.r5.s64 = ctx.r1.s64 + 92;
	// stw r17,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, r17.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r16,308(r1)
	PPC_STORE_U32(ctx.r1.u32 + 308, r16.u32);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r24,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r24.u32);
	// stw r21,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, r21.u32);
	// mr r15,r10
	r15.u64 = ctx.r10.u64;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// bl 0x8266de30
	sub_8266DE30(ctx, base);
	// add r29,r24,r8
	r29.u64 = r24.u64 + ctx.r8.u64;
	// lwz r26,396(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// add r19,r17,r21
	r19.u64 = r17.u64 + r21.u64;
	// lwz r4,404(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// add r11,r29,r8
	r11.u64 = r29.u64 + ctx.r8.u64;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// add r30,r16,r8
	r30.u64 = r16.u64 + ctx.r8.u64;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// stw r19,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r19.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// mr r31,r20
	r31.u64 = r20.u64;
	// add r14,r20,r21
	r14.u64 = r20.u64 + r21.u64;
	// mr r18,r17
	r18.u64 = r17.u64;
	// add r7,r30,r8
	ctx.r7.u64 = r30.u64 + ctx.r8.u64;
	// bl 0x8266de30
	sub_8266DE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8266e210
	if (cr6.eq) goto loc_8266E210;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x8266e210
	if (cr6.eq) goto loc_8266E210;
	// lwz r11,380(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// lwz r10,364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// ble cr6,0x8266e140
	if (!cr6.gt) goto loc_8266E140;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r25,88(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r24,92(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r23,96(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r22,100(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// b 0x8266e018
	goto loc_8266E018;
loc_8266E014:
	// lwz r19,84(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_8266E018:
	// lwz r6,356(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x8266e0fc
	if (!cr6.gt) goto loc_8266E0FC;
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// mr r11,r31
	r11.u64 = r31.u64;
	// subf r21,r7,r30
	r21.s64 = r30.s64 - ctx.r7.s64;
	// subf r20,r31,r14
	r20.s64 = r14.s64 - r31.s64;
	// subf r19,r31,r19
	r19.s64 = r19.s64 - r31.s64;
	// subf r18,r31,r18
	r18.s64 = r18.s64 - r31.s64;
	// subf r17,r7,r5
	r17.s64 = ctx.r5.s64 - ctx.r7.s64;
	// subf r16,r7,r29
	r16.s64 = r29.s64 - ctx.r7.s64;
loc_8266E048:
	// add r5,r20,r11
	ctx.r5.u64 = r20.u64 + r11.u64;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r3,r19,r11
	ctx.r3.u64 = r19.u64 + r11.u64;
	// mullw r29,r31,r26
	r29.s64 = int64_t(r31.s32) * int64_t(r26.s32);
	// lbz r30,0(r5)
	r30.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// mullw r30,r30,r4
	r30.s64 = int64_t(r30.s32) * int64_t(ctx.r4.s32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// add r31,r18,r11
	r31.u64 = r18.u64 + r11.u64;
	// add r30,r30,r25
	r30.u64 = r30.u64 + r25.u64;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// sraw r30,r30,r23
	temp.u32 = r23.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r30.s32 < 0) & (((r30.s32 >> temp.u32) << temp.u32) != r30.s32);
	r30.s64 = r30.s32 >> temp.u32;
	// stbx r30,r21,r10
	PPC_STORE_U8(r21.u32 + ctx.r10.u32, r30.u8);
	// lbz r5,0(r5)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// mullw r5,r5,r28
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r28.s32);
	// mullw r30,r30,r27
	r30.s64 = int64_t(r30.s32) * int64_t(r27.s32);
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// add r5,r5,r24
	ctx.r5.u64 = ctx.r5.u64 + r24.u64;
	// sraw r5,r5,r22
	temp.u32 = r22.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r5.s32 < 0) & (((ctx.r5.s32 >> temp.u32) << temp.u32) != ctx.r5.s32);
	ctx.r5.s64 = ctx.r5.s32 >> temp.u32;
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// lbz r30,0(r3)
	r30.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// mullw r30,r30,r4
	r30.s64 = int64_t(r30.s32) * int64_t(ctx.r4.s32);
	// mullw r5,r5,r26
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r26.s32);
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// add r5,r5,r25
	ctx.r5.u64 = ctx.r5.u64 + r25.u64;
	// sraw r5,r5,r23
	temp.u32 = r23.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r5.s32 < 0) & (((ctx.r5.s32 >> temp.u32) << temp.u32) != ctx.r5.s32);
	ctx.r5.s64 = ctx.r5.s32 >> temp.u32;
	// stbx r5,r16,r10
	PPC_STORE_U8(r16.u32 + ctx.r10.u32, ctx.r5.u8);
	// lbz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// lbz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// mullw r5,r5,r27
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r27.s32);
	// mullw r3,r3,r28
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r28.s32);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// add r5,r5,r24
	ctx.r5.u64 = ctx.r5.u64 + r24.u64;
	// sraw r5,r5,r22
	temp.u32 = r22.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r5.s32 < 0) & (((ctx.r5.s32 >> temp.u32) << temp.u32) != ctx.r5.s32);
	ctx.r5.s64 = ctx.r5.s32 >> temp.u32;
	// stbx r5,r17,r10
	PPC_STORE_U8(r17.u32 + ctx.r10.u32, ctx.r5.u8);
	// add r10,r10,r15
	ctx.r10.u64 = ctx.r10.u64 + r15.u64;
	// bne cr6,0x8266e048
	if (!cr6.eq) goto loc_8266E048;
	// lwz r21,324(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// lwz r16,308(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lwz r17,300(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r20,292(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r19,84(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_8266E0FC:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r10,r19,r21
	ctx.r10.u64 = r19.u64 + r21.u64;
	// add r30,r7,r8
	r30.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r29,r11,r8
	r29.u64 = r11.u64 + ctx.r8.u64;
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// mr r31,r14
	r31.u64 = r14.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// add r10,r29,r8
	ctx.r10.u64 = r29.u64 + ctx.r8.u64;
	// mr r18,r19
	r18.u64 = r19.u64;
	// add r14,r14,r21
	r14.u64 = r14.u64 + r21.u64;
	// add r7,r30,r8
	ctx.r7.u64 = r30.u64 + ctx.r8.u64;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// bne cr6,0x8266e014
	if (!cr6.eq) goto loc_8266E014;
	// lwz r24,316(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
loc_8266E140:
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8266e190
	if (cr6.eq) goto loc_8266E190;
	// lwz r8,356(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// mr r11,r20
	r11.u64 = r20.u64;
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x8266e190
	if (!cr6.gt) goto loc_8266E190;
	// subf r6,r20,r17
	ctx.r6.s64 = r17.s64 - r20.s64;
	// subf r5,r16,r24
	ctx.r5.s64 = r24.s64 - r16.s64;
	// rotlwi r8,r8,0
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
loc_8266E16C:
	// lbz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stb r4,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r4.u8);
	// lbzx r4,r6,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stbx r4,r5,r10
	PPC_STORE_U8(ctx.r5.u32 + ctx.r10.u32, ctx.r4.u8);
	// add r10,r10,r15
	ctx.r10.u64 = ctx.r10.u64 + r15.u64;
	// bne cr6,0x8266e16c
	if (!cr6.eq) goto loc_8266E16C;
loc_8266E190:
	// lwz r11,380(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8266e210
	if (cr6.eq) goto loc_8266E210;
	// lwz r3,388(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x8266e210
	if (!cr6.gt) goto loc_8266E210;
	// lwz r11,356(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// mr r8,r31
	ctx.r8.u64 = r31.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8266e210
	if (!cr6.gt) goto loc_8266E210;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r5,r30,r7
	ctx.r5.s64 = ctx.r7.s64 - r30.s64;
	// mr r11,r30
	r11.u64 = r30.u64;
	// subf r4,r30,r10
	ctx.r4.s64 = ctx.r10.s64 - r30.s64;
	// lwz r10,356(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// subf r6,r31,r18
	ctx.r6.s64 = r18.s64 - r31.s64;
	// subf r7,r30,r29
	ctx.r7.s64 = r29.s64 - r30.s64;
loc_8266E1D4:
	// lbz r31,0(r8)
	r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// stb r31,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r31.u8);
	// lbzx r31,r6,r8
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r8.u32);
	// stbx r31,r7,r11
	PPC_STORE_U8(ctx.r7.u32 + r11.u32, r31.u8);
	// ble cr6,0x8266e1fc
	if (!cr6.gt) goto loc_8266E1FC;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// stbx r31,r5,r11
	PPC_STORE_U8(ctx.r5.u32 + r11.u32, r31.u8);
	// lbzx r31,r7,r11
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stbx r31,r4,r11
	PPC_STORE_U8(ctx.r4.u32 + r11.u32, r31.u8);
loc_8266E1FC:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r11,r11,r15
	r11.u64 = r11.u64 + r15.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8266e1d4
	if (!cr6.eq) goto loc_8266E1D4;
loc_8266E210:
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8266E218"))) PPC_WEAK_FUNC(sub_8266E218);
PPC_FUNC_IMPL(__imp__sub_8266E218) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bccc
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// lwz r11,-15740(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -15740);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8266e354
	if (cr6.eq) goto loc_8266E354;
	// lwz r27,14588(r9)
	r27.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// srawi r11,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	r11.s64 = ctx.r7.s32 >> 1;
	// lwz r28,14540(r9)
	r28.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14540);
	// subf r25,r7,r8
	r25.s64 = ctx.r8.s64 - ctx.r7.s64;
	// mullw r29,r27,r7
	r29.s64 = int64_t(r27.s32) * int64_t(ctx.r7.s32);
	// lwz r26,14492(r9)
	r26.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// lwz r23,14644(r9)
	r23.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14644);
	// lwz r30,14500(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14500);
	// lwz r10,14544(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14544);
	// lwz r31,14548(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14548);
	// lwz r24,14480(r9)
	r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14480);
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// add r29,r29,r4
	r29.u64 = r29.u64 + ctx.r4.u64;
	// mullw r4,r26,r7
	ctx.r4.s64 = int64_t(r26.s32) * int64_t(ctx.r7.s32);
	// mullw r11,r11,r23
	r11.s64 = int64_t(r11.s32) * int64_t(r23.s32);
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r30,r4,r3
	r30.u64 = ctx.r4.u64 + ctx.r3.u64;
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// add r3,r10,r5
	ctx.r3.u64 = ctx.r10.u64 + ctx.r5.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// add r4,r11,r6
	ctx.r4.u64 = r11.u64 + ctx.r6.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// ble cr6,0x8266e2e0
	if (!cr6.gt) goto loc_8266E2E0;
	// mr r31,r25
	r31.u64 = r25.u64;
loc_8266E29C:
	// mr r11,r29
	r11.u64 = r29.u64;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x8266e2cc
	if (!cr6.gt) goto loc_8266E2CC;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
loc_8266E2B0:
	// lbz r28,0(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// stb r28,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r28.u8);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bne cr6,0x8266e2b0
	if (!cr6.eq) goto loc_8266E2B0;
loc_8266E2CC:
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// add r5,r5,r26
	ctx.r5.u64 = ctx.r5.u64 + r26.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8266e29c
	if (!cr6.eq) goto loc_8266E29C;
loc_8266E2E0:
	// lwz r11,14484(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14484);
	// srawi r10,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	ctx.r10.s64 = r25.s32 >> 1;
	// lwz r31,14524(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14524);
	// cntlzw r5,r7
	ctx.r5.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// lwz r7,14644(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14644);
	// lwz r8,14492(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// cntlzw r9,r11
	ctx.r9.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// li r6,3
	ctx.r6.s64 = 3;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// li r11,1
	r11.s64 = 1;
	// rlwinm r29,r5,27,31,31
	r29.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x1;
	// rlwinm r31,r9,27,31,31
	r31.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// li r10,4
	ctx.r10.s64 = 4;
	// stw r6,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r6.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r6,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r6.u32);
	// addi r5,r30,1
	ctx.r5.s64 = r30.s64 + 1;
	// addi r6,r30,3
	ctx.r6.s64 = r30.s64 + 3;
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// bl 0x8266df20
	sub_8266DF20(ctx, base);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd1c
	return;
loc_8266E354:
	// lwz r10,14492(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// subf r28,r7,r8
	r28.s64 = ctx.r8.s64 - ctx.r7.s64;
	// lwz r31,14500(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14500);
	// mullw r10,r10,r7
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// lwz r29,14588(r9)
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// lwz r11,14604(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14604);
	// lwz r30,14608(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14608);
	// lwz r27,14516(r9)
	r27.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14516);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// mullw r8,r30,r29
	ctx.r8.s64 = int64_t(r30.s32) * int64_t(r29.s32);
	// add r31,r10,r3
	r31.u64 = ctx.r10.u64 + ctx.r3.u64;
	// mullw r10,r29,r7
	ctx.r10.s64 = int64_t(r29.s32) * int64_t(ctx.r7.s32);
	// srawi r7,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 2;
	// mr r22,r31
	r22.u64 = r31.u64;
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// srawi r3,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r3.s64 = r11.s32 >> 1;
	// addze r3,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r3.s64 = temp.s64;
	// srawi r30,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r30.s64 = ctx.r8.s32 >> 2;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// addze r11,r30
	temp.s64 = r30.s64 + xer.ca;
	xer.ca = temp.u32 < r30.u32;
	r11.s64 = temp.s64;
	// srawi r30,r27,3
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7) != 0);
	r30.s64 = r27.s32 >> 3;
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// addze r3,r30
	temp.s64 = r30.s64 + xer.ca;
	xer.ca = temp.u32 < r30.u32;
	ctx.r3.s64 = temp.s64;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r27,r10,r4
	r27.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r10,r11,r7
	ctx.r10.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r26,r3,1,0,30
	r26.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r8,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	ctx.r8.s64 = r28.s32 >> 1;
	// add r24,r10,r6
	r24.u64 = ctx.r10.u64 + ctx.r6.u64;
	// add r25,r10,r5
	r25.u64 = ctx.r10.u64 + ctx.r5.u64;
	// subf r10,r26,r29
	ctx.r10.s64 = r29.s64 - r26.s64;
	// addze r20,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	r20.s64 = temp.s64;
	// srawi r6,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 1;
	// mr r11,r27
	r11.u64 = r27.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// addze r28,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	r28.s64 = temp.s64;
	// ble cr6,0x8266e4a0
	if (!cr6.gt) goto loc_8266E4A0;
	// add r21,r10,r29
	r21.u64 = ctx.r10.u64 + r29.u64;
	// mr r23,r20
	r23.u64 = r20.u64;
loc_8266E3FC:
	// lwz r5,14492(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// add r10,r11,r29
	ctx.r10.u64 = r11.u64 + r29.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x8266e480
	if (!cr6.gt) goto loc_8266E480;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_8266E418:
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// lbz r19,0(r8)
	r19.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// rotlwi r30,r30,16
	r30.u64 = __builtin_rotateleft32(r30.u32, 16);
	// lbz r18,1(r11)
	r18.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r17,0(r11)
	r17.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// or r30,r30,r19
	r30.u64 = r30.u64 | r19.u64;
	// rotlwi r19,r18,16
	r19.u64 = __builtin_rotateleft32(r18.u32, 16);
	// rlwinm r30,r30,8,0,23
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 8) & 0xFFFFFF00;
	// or r19,r19,r17
	r19.u64 = r19.u64 | r17.u64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// or r19,r19,r30
	r19.u64 = r19.u64 | r30.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r19,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r19.u32);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// lbz r19,1(r10)
	r19.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r18,0(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// rotlwi r19,r19,16
	r19.u64 = __builtin_rotateleft32(r19.u32, 16);
	// or r19,r19,r18
	r19.u64 = r19.u64 | r18.u64;
	// or r30,r19,r30
	r30.u64 = r19.u64 | r30.u64;
	// stw r30,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r30.u32);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne cr6,0x8266e418
	if (!cr6.eq) goto loc_8266E418;
loc_8266E480:
	// lwz r10,14496(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14496);
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + r28.u64;
	// add r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 + r28.u64;
	// add r31,r10,r31
	r31.u64 = ctx.r10.u64 + r31.u64;
	// add r11,r21,r11
	r11.u64 = r21.u64 + r11.u64;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x8266e3fc
	if (!cr6.eq) goto loc_8266E3FC;
loc_8266E4A0:
	// lwz r8,14516(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14516);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r7,14588(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14588);
	// add r11,r26,r27
	r11.u64 = r26.u64 + r27.u64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// add r30,r10,r22
	r30.u64 = ctx.r10.u64 + r22.u64;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// add r6,r3,r25
	ctx.r6.u64 = ctx.r3.u64 + r25.u64;
	// subf r28,r3,r8
	r28.s64 = ctx.r8.s64 - ctx.r3.s64;
	// add r5,r3,r24
	ctx.r5.u64 = ctx.r3.u64 + r24.u64;
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// li r31,0
	r31.s64 = 0;
	// subf r27,r10,r7
	r27.s64 = ctx.r7.s64 - ctx.r10.s64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// srawi r10,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r10.s64 = r27.s32 >> 1;
	// addze r26,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r26.s64 = temp.s64;
	// ble cr6,0x8266e598
	if (!cr6.gt) goto loc_8266E598;
	// addi r25,r20,-1
	r25.s64 = r20.s64 + -1;
loc_8266E4E8:
	// lwz r7,14492(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14492);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// add r10,r11,r29
	ctx.r10.u64 = r11.u64 + r29.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x8266e56c
	if (!cr6.gt) goto loc_8266E56C;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
loc_8266E504:
	// lbz r3,0(r5)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// lbz r24,0(r6)
	r24.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// rotlwi r3,r3,16
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r3.u32, 16);
	// lbz r23,1(r11)
	r23.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r22,0(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// or r3,r3,r24
	ctx.r3.u64 = ctx.r3.u64 | r24.u64;
	// rotlwi r24,r23,16
	r24.u64 = __builtin_rotateleft32(r23.u32, 16);
	// rlwinm r3,r3,8,0,23
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 8) & 0xFFFFFF00;
	// or r24,r24,r22
	r24.u64 = r24.u64 | r22.u64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// or r24,r24,r3
	r24.u64 = r24.u64 | ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r24,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r24.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lbz r24,1(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r23,0(r10)
	r23.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// rotlwi r24,r24,16
	r24.u64 = __builtin_rotateleft32(r24.u32, 16);
	// or r24,r24,r23
	r24.u64 = r24.u64 | r23.u64;
	// or r3,r24,r3
	ctx.r3.u64 = r24.u64 | ctx.r3.u64;
	// stw r3,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r3.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// bne cr6,0x8266e504
	if (!cr6.eq) goto loc_8266E504;
loc_8266E56C:
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// add r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 + r26.u64;
	// add r5,r5,r26
	ctx.r5.u64 = ctx.r5.u64 + r26.u64;
	// cmpw cr6,r31,r25
	cr6.compare<int32_t>(r31.s32, r25.s32, xer);
	// bge cr6,0x8266e588
	if (!cr6.lt) goto loc_8266E588;
	// lwz r10,14496(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14496);
	// add r30,r10,r30
	r30.u64 = ctx.r10.u64 + r30.u64;
loc_8266E588:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// cmpw cr6,r31,r20
	cr6.compare<int32_t>(r31.s32, r20.s32, xer);
	// blt cr6,0x8266e4e8
	if (cr6.lt) goto loc_8266E4E8;
loc_8266E598:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_8266E5A0"))) PPC_WEAK_FUNC(sub_8266E5A0);
PPC_FUNC_IMPL(__imp__sub_8266E5A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8266e5bc
	if (cr6.eq) goto loc_8266E5BC;
	// cmplwi cr6,r9,3
	cr6.compare<uint32_t>(ctx.r9.u32, 3, xer);
	// li r6,0
	ctx.r6.s64 = 0;
	// bne cr6,0x8266e5c0
	if (!cr6.eq) goto loc_8266E5C0;
loc_8266E5BC:
	// li r6,1
	ctx.r6.s64 = 1;
loc_8266E5C0:
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r10,12849
	ctx.r10.s64 = 842072064;
	// lis r8,12338
	ctx.r8.s64 = 808583168;
	// ori r10,r10,22105
	ctx.r10.u64 = ctx.r10.u64 | 22105;
	// lis r7,22101
	ctx.r7.s64 = 1448411136;
	// ori r8,r8,13385
	ctx.r8.u64 = ctx.r8.u64 | 13385;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// ori r7,r7,22857
	ctx.r7.u64 = ctx.r7.u64 | 22857;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x8266e668
	if (cr6.gt) goto loc_8266E668;
	// beq cr6,0x8266e688
	if (cr6.eq) goto loc_8266E688;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// beq cr6,0x8266e688
	if (cr6.eq) goto loc_8266E688;
	// lis r10,12593
	ctx.r10.s64 = 825294848;
	// ori r10,r10,13392
	ctx.r10.u64 = ctx.r10.u64 | 13392;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x8266e680
	if (!cr6.eq) goto loc_8266E680;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x8266e690
	if (!cr6.eq) goto loc_8266E690;
	// lis r11,12889
	r11.s64 = 844693504;
	// ori r11,r11,21849
	r11.u64 = r11.u64 | 21849;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x8266e630
	if (!cr6.eq) goto loc_8266E630;
	// lis r11,-32153
	r11.s64 = -2107179008;
	// addi r11,r11,-10488
	r11.s64 = r11.s64 + -10488;
	// stw r11,14664(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14664, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8266E630:
	// lis r11,22101
	r11.s64 = 1448411136;
	// ori r11,r11,22857
	r11.u64 = r11.u64 | 22857;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x8266e648
	if (cr6.eq) goto loc_8266E648;
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bne cr6,0x8266e690
	if (!cr6.eq) goto loc_8266E690;
loc_8266E648:
	// lwz r11,14620(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14620);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x8266e690
	if (cr6.eq) goto loc_8266E690;
	// lis r11,-32153
	r11.s64 = -2107179008;
	// addi r11,r11,-904
	r11.s64 = r11.s64 + -904;
	// stw r11,14660(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14660, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8266E668:
	// lis r5,12850
	ctx.r5.s64 = 842137600;
	// ori r5,r5,13392
	ctx.r5.u64 = ctx.r5.u64 | 13392;
	// cmplw cr6,r11,r5
	cr6.compare<uint32_t>(r11.u32, ctx.r5.u32, xer);
	// beq cr6,0x8266e708
	if (cr6.eq) goto loc_8266E708;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// beq cr6,0x8266e688
	if (cr6.eq) goto loc_8266E688;
loc_8266E680:
	// li r3,3
	ctx.r3.s64 = 3;
	// blr 
	return;
loc_8266E688:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x8266e698
	if (cr6.eq) goto loc_8266E698;
loc_8266E690:
	// li r3,5
	ctx.r3.s64 = 5;
	// blr 
	return;
loc_8266E698:
	// lis r11,12889
	r11.s64 = 844693504;
	// ori r11,r11,21849
	r11.u64 = r11.u64 | 21849;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x8266e6dc
	if (!cr6.eq) goto loc_8266E6DC;
	// lwz r11,14620(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14620);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8266e6c8
	if (!cr6.eq) goto loc_8266E6C8;
	// lis r11,-32153
	r11.s64 = -2107179008;
	// addi r11,r11,-10144
	r11.s64 = r11.s64 + -10144;
	// stw r11,14664(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14664, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8266E6C8:
	// lis r11,-32153
	r11.s64 = -2107179008;
	// addi r11,r11,-7656
	r11.s64 = r11.s64 + -7656;
	// stw r11,14664(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14664, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8266E6DC:
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// beq cr6,0x8266e6f4
	if (cr6.eq) goto loc_8266E6F4;
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// beq cr6,0x8266e6f4
	if (cr6.eq) goto loc_8266E6F4;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bne cr6,0x8266e690
	if (!cr6.eq) goto loc_8266E690;
loc_8266E6F4:
	// lis r11,-32153
	r11.s64 = -2107179008;
	// addi r11,r11,3448
	r11.s64 = r11.s64 + 3448;
	// stw r11,14660(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14660, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8266E708:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x8266e690
	if (!cr6.eq) goto loc_8266E690;
	// lis r11,12889
	r11.s64 = 844693504;
	// ori r11,r11,21849
	r11.u64 = r11.u64 | 21849;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x8266e734
	if (!cr6.eq) goto loc_8266E734;
	// lis r11,-32153
	r11.s64 = -2107179008;
	// addi r11,r11,-8952
	r11.s64 = r11.s64 + -8952;
	// stw r11,14664(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14664, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8266E734:
	// lis r11,12338
	r11.s64 = 808583168;
	// ori r11,r11,13385
	r11.u64 = r11.u64 | 13385;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x8266e75c
	if (cr6.eq) goto loc_8266E75C;
	// lis r11,22101
	r11.s64 = 1448411136;
	// ori r11,r11,22857
	r11.u64 = r11.u64 | 22857;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x8266e75c
	if (cr6.eq) goto loc_8266E75C;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bne cr6,0x8266e690
	if (!cr6.eq) goto loc_8266E690;
loc_8266E75C:
	// lis r11,-32153
	r11.s64 = -2107179008;
	// addi r11,r11,1696
	r11.s64 = r11.s64 + 1696;
	// stw r11,14660(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14660, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266E770"))) PPC_WEAK_FUNC(sub_8266E770);
PPC_FUNC_IMPL(__imp__sub_8266E770) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8266e8fc
	if (cr6.eq) goto loc_8266E8FC;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8266e8f4
	if (cr6.eq) goto loc_8266E8F4;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8266e8f4
	if (cr6.eq) goto loc_8266E8F4;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x8266e8f4
	if (cr6.eq) goto loc_8266E8F4;
	// li r29,0
	r29.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,1064
	ctx.r3.s64 = 1064;
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// bne cr6,0x8266e7e0
	if (!cr6.eq) goto loc_8266E7E0;
	// li r11,2
	r11.s64 = 2;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_8266E7E0:
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8266e800
	if (!cr6.eq) goto loc_8266E800;
	// lhz r10,14(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// bne cr6,0x8266e800
	if (!cr6.eq) goto loc_8266E800;
	// li r5,1064
	ctx.r5.s64 = 1064;
	// b 0x8266e810
	goto loc_8266E810;
loc_8266E800:
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// li r5,52
	ctx.r5.s64 = 52;
	// beq cr6,0x8266e810
	if (cr6.eq) goto loc_8266E810;
	// li r5,40
	ctx.r5.s64 = 40;
loc_8266E810:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,1064
	ctx.r3.s64 = 1064;
	// stw r29,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r29.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// bne cr6,0x8266e85c
	if (!cr6.eq) goto loc_8266E85C;
	// li r11,2
	r11.s64 = 2;
	// mr r31,r29
	r31.u64 = r29.u64;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8266e8fc
	if (cr6.eq) goto loc_8266E8FC;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_8266E85C:
	// lwz r11,16(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8266e87c
	if (!cr6.eq) goto loc_8266E87C;
	// lhz r10,14(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 14);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// bne cr6,0x8266e87c
	if (!cr6.eq) goto loc_8266E87C;
	// li r5,1064
	ctx.r5.s64 = 1064;
	// b 0x8266e88c
	goto loc_8266E88C;
loc_8266E87C:
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// li r5,52
	ctx.r5.s64 = 52;
	// beq cr6,0x8266e88c
	if (cr6.eq) goto loc_8266E88C;
	// li r5,40
	ctx.r5.s64 = 40;
loc_8266E88C:
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// stw r29,14552(r31)
	PPC_STORE_U32(r31.u32 + 14552, r29.u32);
	// stw r26,14620(r31)
	PPC_STORE_U32(r31.u32 + 14620, r26.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r29,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r29.u32);
	// stw r29,14580(r31)
	PPC_STORE_U32(r31.u32 + 14580, r29.u32);
	// stw r29,14468(r31)
	PPC_STORE_U32(r31.u32 + 14468, r29.u32);
	// bl 0x8266d578
	sub_8266D578(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// bne cr6,0x8266e8fc
	if (!cr6.eq) goto loc_8266E8FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82670c70
	sub_82670C70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// bne cr6,0x8266e8fc
	if (!cr6.eq) goto loc_8266E8FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8266e5a0
	sub_8266E5A0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// bne cr6,0x8266e8fc
	if (!cr6.eq) goto loc_8266E8FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82670b70
	sub_82670B70(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_8266E8F4:
	// li r11,1
	r11.s64 = 1;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
loc_8266E8FC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8266E904"))) PPC_WEAK_FUNC(sub_8266E904);
PPC_FUNC_IMPL(__imp__sub_8266E904) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266E908"))) PPC_WEAK_FUNC(sub_8266E908);
PPC_FUNC_IMPL(__imp__sub_8266E908) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r10,2
	ctx.r10.s64 = 2;
	// li r11,0
	r11.s64 = 0;
	// stw r10,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r10.u32);
	// stw r10,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r10.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stw r11,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, r11.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r11.u32);
	// stw r11,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, r11.u32);
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, r11.u32);
	// stw r11,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, r11.u32);
	// stw r11,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, r11.u32);
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, r11.u32);
	// stw r11,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, r11.u32);
	// stw r11,60(r3)
	PPC_STORE_U32(ctx.r3.u32 + 60, r11.u32);
	// stw r11,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, r11.u32);
	// stw r11,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, r11.u32);
	// stw r11,72(r3)
	PPC_STORE_U32(ctx.r3.u32 + 72, r11.u32);
	// stw r11,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, r11.u32);
	// stw r11,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, r11.u32);
	// stw r11,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, r11.u32);
	// stw r11,88(r3)
	PPC_STORE_U32(ctx.r3.u32 + 88, r11.u32);
	// stw r11,92(r3)
	PPC_STORE_U32(ctx.r3.u32 + 92, r11.u32);
	// stw r11,14464(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14464, r11.u32);
	// stw r11,14468(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14468, r11.u32);
	// stw r10,14472(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14472, ctx.r10.u32);
	// stw r10,14476(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14476, ctx.r10.u32);
	// stw r11,14480(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14480, r11.u32);
	// stw r11,14484(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14484, r11.u32);
	// stw r11,14488(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14488, r11.u32);
	// stw r11,14492(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14492, r11.u32);
	// stw r11,14496(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14496, r11.u32);
	// stw r11,14500(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14500, r11.u32);
	// stw r11,14504(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14504, r11.u32);
	// stw r11,14508(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14508, r11.u32);
	// stw r11,14512(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14512, r11.u32);
	// stw r11,14516(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14516, r11.u32);
	// stw r11,14520(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14520, r11.u32);
	// stw r11,14524(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14524, r11.u32);
	// stw r11,14528(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14528, r11.u32);
	// stw r11,14532(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14532, r11.u32);
	// stw r11,14536(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14536, r11.u32);
	// stw r11,14540(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14540, r11.u32);
	// stw r11,14544(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14544, r11.u32);
	// stw r11,14548(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14548, r11.u32);
	// stw r11,14552(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14552, r11.u32);
	// stw r11,14556(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14556, r11.u32);
	// stw r10,14560(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14560, ctx.r10.u32);
	// stw r11,14580(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14580, r11.u32);
	// stw r11,14584(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14584, r11.u32);
	// stw r11,14588(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14588, r11.u32);
	// stw r11,14592(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14592, r11.u32);
	// stw r11,14596(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14596, r11.u32);
	// stw r11,14600(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14600, r11.u32);
	// stw r11,14604(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14604, r11.u32);
	// stw r11,14608(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14608, r11.u32);
	// stw r11,14612(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14612, r11.u32);
	// stw r11,14616(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14616, r11.u32);
	// stw r11,14620(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14620, r11.u32);
	// stw r11,14624(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14624, r11.u32);
	// stw r11,14628(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14628, r11.u32);
	// stw r11,14632(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14632, r11.u32);
	// stw r11,14636(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14636, r11.u32);
	// stw r11,14640(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14640, r11.u32);
	// stw r11,14644(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14644, r11.u32);
	// stw r11,14648(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14648, r11.u32);
	// stw r11,14652(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14652, r11.u32);
	// stw r11,14656(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14656, r11.u32);
	// stw r11,14660(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14660, r11.u32);
	// stw r11,14664(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14664, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266EA34"))) PPC_WEAK_FUNC(sub_8266EA34);
PPC_FUNC_IMPL(__imp__sub_8266EA34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266EA38"))) PPC_WEAK_FUNC(sub_8266EA38);
PPC_FUNC_IMPL(__imp__sub_8266EA38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcec
	// lwz r11,14468(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14468);
	// lis r6,-19
	ctx.r6.s64 = -1245184;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8266eaf0
	if (!cr6.eq) goto loc_8266EAF0;
	// ori r29,r5,39061
	r29.u64 = ctx.r5.u64 | 39061;
	// lis r5,0
	ctx.r5.s64 = 0;
	// lis r7,-259
	ctx.r7.s64 = -16973824;
	// ori r30,r5,53279
	r30.u64 = ctx.r5.u64 | 53279;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// lis r8,-51
	ctx.r8.s64 = -3342336;
	// ori r31,r5,1129
	r31.u64 = ctx.r5.u64 | 1129;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// lis r9,-105
	ctx.r9.s64 = -6881280;
	// ori r4,r5,10773
	ctx.r4.u64 = ctx.r5.u64 | 10773;
	// lis r10,-205
	ctx.r10.s64 = -13434880;
	// lis r5,204
	ctx.r5.s64 = 13369344;
	// ori r6,r6,24240
	ctx.r6.u64 = ctx.r6.u64 | 24240;
	// ori r7,r7,52096
	ctx.r7.u64 = ctx.r7.u64 | 52096;
	// ori r8,r8,55936
	ctx.r8.u64 = ctx.r8.u64 | 55936;
	// ori r9,r9,61568
	ctx.r9.u64 = ctx.r9.u64 | 61568;
	// ori r10,r10,46464
	ctx.r10.u64 = ctx.r10.u64 | 46464;
	// addi r11,r3,10368
	r11.s64 = ctx.r3.s64 + 10368;
	// ori r5,r5,19072
	ctx.r5.u64 = ctx.r5.u64 | 19072;
loc_8266EAA0:
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// srawi r28,r28,16
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFFFF) != 0);
	r28.s64 = r28.s32 >> 16;
	// srawi r27,r9,16
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFF) != 0);
	r27.s64 = ctx.r9.s32 >> 16;
	// srawi r26,r8,16
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFF) != 0);
	r26.s64 = ctx.r8.s32 >> 16;
	// srawi r25,r7,16
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	r25.s64 = ctx.r7.s32 >> 16;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// stw r28,-1024(r11)
	PPC_STORE_U32(r11.u32 + -1024, r28.u32);
	// srawi r28,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	r28.s64 = ctx.r6.s32 >> 16;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// addi r8,r8,25675
	ctx.r8.s64 = ctx.r8.s64 + 25675;
	// stw r26,1024(r11)
	PPC_STORE_U32(r11.u32 + 1024, r26.u32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// stw r25,2048(r11)
	PPC_STORE_U32(r11.u32 + 2048, r25.u32);
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// cmpw cr6,r10,r5
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, xer);
	// stw r28,3072(r11)
	PPC_STORE_U32(r11.u32 + 3072, r28.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// blt cr6,0x8266eaa0
	if (cr6.lt) goto loc_8266EAA0;
	// b 0x8266eb88
	goto loc_8266EB88;
loc_8266EAF0:
	// ori r29,r5,52414
	r29.u64 = ctx.r5.u64 | 52414;
	// lis r5,0
	ctx.r5.s64 = 0;
	// lis r7,-272
	ctx.r7.s64 = -17825792;
	// ori r30,r5,35062
	r30.u64 = ctx.r5.u64 | 35062;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// lis r8,-69
	ctx.r8.s64 = -4521984;
	// ori r31,r5,7909
	r31.u64 = ctx.r5.u64 | 7909;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// lis r9,-28
	ctx.r9.s64 = -1835008;
	// ori r4,r5,11072
	ctx.r4.u64 = ctx.r5.u64 | 11072;
	// lis r10,-231
	ctx.r10.s64 = -15138816;
	// lis r5,230
	ctx.r5.s64 = 15073280;
	// ori r6,r6,19456
	ctx.r6.u64 = ctx.r6.u64 | 19456;
	// ori r7,r7,36224
	ctx.r7.u64 = ctx.r7.u64 | 36224;
	// ori r8,r8,34048
	ctx.r8.u64 = ctx.r8.u64 | 34048;
	// ori r9,r9,39168
	ctx.r9.u64 = ctx.r9.u64 | 39168;
	// ori r10,r10,41216
	ctx.r10.u64 = ctx.r10.u64 | 41216;
	// addi r11,r3,11392
	r11.s64 = ctx.r3.s64 + 11392;
	// ori r5,r5,24320
	ctx.r5.u64 = ctx.r5.u64 | 24320;
loc_8266EB3C:
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// srawi r28,r28,16
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFFFF) != 0);
	r28.s64 = r28.s32 >> 16;
	// srawi r27,r9,16
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFF) != 0);
	r27.s64 = ctx.r9.s32 >> 16;
	// srawi r26,r8,16
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFF) != 0);
	r26.s64 = ctx.r8.s32 >> 16;
	// srawi r25,r7,16
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	r25.s64 = ctx.r7.s32 >> 16;
	// addi r9,r9,14030
	ctx.r9.s64 = ctx.r9.s64 + 14030;
	// stw r28,-2048(r11)
	PPC_STORE_U32(r11.u32 + -2048, r28.u32);
	// srawi r28,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	r28.s64 = ctx.r6.s32 >> 16;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// stw r26,-1024(r11)
	PPC_STORE_U32(r11.u32 + -1024, r26.u32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// stw r25,1024(r11)
	PPC_STORE_U32(r11.u32 + 1024, r25.u32);
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// cmpw cr6,r10,r5
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, xer);
	// stw r28,2048(r11)
	PPC_STORE_U32(r11.u32 + 2048, r28.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// blt cr6,0x8266eb3c
	if (cr6.lt) goto loc_8266EB3C;
loc_8266EB88:
	// lis r11,-32126
	r11.s64 = -2105409536;
	// li r10,-510
	ctx.r10.s64 = -510;
	// addi r11,r11,-13056
	r11.s64 = r11.s64 + -13056;
	// li r9,-2040
	ctx.r9.s64 = -2040;
	// addi r11,r11,2040
	r11.s64 = r11.s64 + 2040;
	// stw r11,14464(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14464, r11.u32);
loc_8266EBA0:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x8266ebb0
	if (!cr6.lt) goto loc_8266EBB0;
	// li r11,0
	r11.s64 = 0;
	// b 0x8266ebc0
	goto loc_8266EBC0;
loc_8266EBB0:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// li r11,255
	r11.s64 = 255;
	// bgt cr6,0x8266ebc0
	if (cr6.gt) goto loc_8266EBC0;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8266EBC0:
	// lwz r8,14464(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14464);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r11,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, r11.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmpwi cr6,r9,2040
	cr6.compare<int32_t>(ctx.r9.s32, 2040, xer);
	// blt cr6,0x8266eba0
	if (cr6.lt) goto loc_8266EBA0;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_8266EBDC"))) PPC_WEAK_FUNC(sub_8266EBDC);
PPC_FUNC_IMPL(__imp__sub_8266EBDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266EBE0"))) PPC_WEAK_FUNC(sub_8266EBE0);
PPC_FUNC_IMPL(__imp__sub_8266EBE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// mflr r12
	// bl 0x8239bcf4
	// addi r12,r1,-48
	r12.s64 = ctx.r1.s64 + -48;
	// bl 0x8239d5e8
	// lwz r11,14468(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14468);
	// li r29,0
	r29.s64 = 0;
	// lis r27,1
	r27.s64 = 65536;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// addi r11,r3,3200
	r11.s64 = ctx.r3.s64 + 3200;
	// lis r28,256
	r28.s64 = 16777216;
	// bne cr6,0x8266ed14
	if (!cr6.eq) goto loc_8266ED14;
	// lis r30,-32244
	r30.s64 = -2113142784;
	// lis r31,-32243
	r31.s64 = -2113077248;
	// lis r3,-32243
	ctx.r3.s64 = -2113077248;
	// lis r4,-32244
	ctx.r4.s64 = -2113142784;
	// lis r5,-32243
	ctx.r5.s64 = -2113077248;
	// lis r6,-32243
	ctx.r6.s64 = -2113077248;
	// lfd f3,30784(r30)
	ctx.fpscr.disableFlushMode();
	ctx.f3.u64 = PPC_LOAD_U64(r30.u32 + 30784);
	// lis r7,-32243
	ctx.r7.s64 = -2113077248;
	// lfd f4,4112(r31)
	ctx.f4.u64 = PPC_LOAD_U64(r31.u32 + 4112);
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// lfd f12,4104(r3)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r3.u32 + 4104);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// lfd f5,30800(r4)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r4.u32 + 30800);
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lfd f6,4096(r5)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r5.u32 + 4096);
	// lfd f7,4088(r6)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r6.u32 + 4088);
	// lfd f8,4080(r7)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r7.u32 + 4080);
	// lfd f9,30776(r8)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r8.u32 + 30776);
	// lfd f10,30792(r9)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r9.u32 + 30792);
	// lfd f11,30816(r10)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r10.u32 + 30816);
loc_8266EC5C:
	// extsw r10,r29
	ctx.r10.s64 = r29.s32;
	// addi r9,r11,-3072
	ctx.r9.s64 = r11.s64 + -3072;
	// addi r8,r11,3072
	ctx.r8.s64 = r11.s64 + 3072;
	// addi r7,r11,1024
	ctx.r7.s64 = r11.s64 + 1024;
	// addi r6,r11,4096
	ctx.r6.s64 = r11.s64 + 4096;
	// std r10,-96(r1)
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.r10.u64);
	// addi r10,r11,-2048
	ctx.r10.s64 = r11.s64 + -2048;
	// addi r5,r11,-1024
	ctx.r5.s64 = r11.s64 + -1024;
	// addi r4,r11,2048
	ctx.r4.s64 = r11.s64 + 2048;
	// addi r3,r11,5120
	ctx.r3.s64 = r11.s64 + 5120;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// cmpw cr6,r29,r28
	cr6.compare<int32_t>(r29.s32, r28.s32, xer);
	// lfd f0,-96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fmul f2,f0,f11
	ctx.f2.f64 = f0.f64 * ctx.f11.f64;
	// fmul f1,f0,f10
	ctx.f1.f64 = f0.f64 * ctx.f10.f64;
	// fmul f30,f0,f6
	f30.f64 = f0.f64 * ctx.f6.f64;
	// fmadd f28,f0,f9,f8
	f28.f64 = f0.f64 * ctx.f9.f64 + ctx.f8.f64;
	// fmul f31,f0,f7
	f31.f64 = f0.f64 * ctx.f7.f64;
	// fmul f13,f0,f5
	ctx.f13.f64 = f0.f64 * ctx.f5.f64;
	// fmul f29,f0,f4
	f29.f64 = f0.f64 * ctx.f4.f64;
	// fnmsub f0,f0,f3,f12
	f0.f64 = -(f0.f64 * ctx.f3.f64 - ctx.f12.f64);
	// fctiwz f2,f2
	ctx.f2.s64 = (ctx.f2.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f2.f64));
	// stfiwx f2,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f2.u32);
	// fctiwz f2,f1
	ctx.f2.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f2,0,r11
	PPC_STORE_U32(r11.u32, ctx.f2.u32);
	// fctiwz f1,f30
	ctx.f1.s64 = (f30.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f30.f64));
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// fctiwz f30,f28
	f30.s64 = (f28.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f28.f64));
	// stfiwx f30,0,r8
	PPC_STORE_U32(ctx.r8.u32, f30.u32);
	// fctiwz f2,f31
	ctx.f2.s64 = (f31.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f31.f64));
	// stfiwx f2,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f2.u32);
	// fadd f2,f13,f12
	ctx.f2.f64 = ctx.f13.f64 + ctx.f12.f64;
	// stfiwx f1,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f1.u32);
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// fctiwz f31,f29
	f31.s64 = (f29.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f29.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// fctiwz f2,f2
	ctx.f2.s64 = (ctx.f2.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f2.f64));
	// stfiwx f2,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f2.u32);
	// stfiwx f13,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f13.u32);
	// stfiwx f31,0,r4
	PPC_STORE_U32(ctx.r4.u32, f31.u32);
	// stfiwx f0,0,r3
	PPC_STORE_U32(ctx.r3.u32, f0.u32);
	// blt cr6,0x8266ec5c
	if (cr6.lt) goto loc_8266EC5C;
	// addi r12,r1,-48
	r12.s64 = ctx.r1.s64 + -48;
	// bl 0x8239d634
	// b 0x8239bd44
	return;
loc_8266ED14:
	// lis r30,-32243
	r30.s64 = -2113077248;
	// lis r31,-32243
	r31.s64 = -2113077248;
	// lis r3,-32249
	ctx.r3.s64 = -2113470464;
	// lis r4,-32243
	ctx.r4.s64 = -2113077248;
	// lis r5,-32243
	ctx.r5.s64 = -2113077248;
	// lis r6,-32243
	ctx.r6.s64 = -2113077248;
	// lfd f3,4072(r30)
	ctx.fpscr.disableFlushMode();
	ctx.f3.u64 = PPC_LOAD_U64(r30.u32 + 4072);
	// lis r7,-32243
	ctx.r7.s64 = -2113077248;
	// lfd f4,4064(r31)
	ctx.f4.u64 = PPC_LOAD_U64(r31.u32 + 4064);
	// lis r8,-32243
	ctx.r8.s64 = -2113077248;
	// lfd f5,9696(r3)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r3.u32 + 9696);
	// lis r9,-32243
	ctx.r9.s64 = -2113077248;
	// lfd f6,4056(r4)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r4.u32 + 4056);
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// lfd f7,4048(r5)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r5.u32 + 4048);
	// lfd f8,4040(r6)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r6.u32 + 4040);
	// lfd f9,4032(r7)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r7.u32 + 4032);
	// lfd f10,4024(r8)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r8.u32 + 4024);
	// lfd f12,4104(r9)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r9.u32 + 4104);
	// lfd f11,4080(r10)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r10.u32 + 4080);
loc_8266ED64:
	// extsw r10,r29
	ctx.r10.s64 = r29.s32;
	// addi r9,r11,-3072
	ctx.r9.s64 = r11.s64 + -3072;
	// addi r8,r11,3072
	ctx.r8.s64 = r11.s64 + 3072;
	// addi r7,r11,1024
	ctx.r7.s64 = r11.s64 + 1024;
	// addi r6,r11,4096
	ctx.r6.s64 = r11.s64 + 4096;
	// std r10,-96(r1)
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.r10.u64);
	// addi r10,r11,-2048
	ctx.r10.s64 = r11.s64 + -2048;
	// addi r5,r11,-1024
	ctx.r5.s64 = r11.s64 + -1024;
	// addi r4,r11,2048
	ctx.r4.s64 = r11.s64 + 2048;
	// addi r3,r11,5120
	ctx.r3.s64 = r11.s64 + 5120;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// cmpw cr6,r29,r28
	cr6.compare<int32_t>(r29.s32, r28.s32, xer);
	// lfd f0,-96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fmul f2,f0,f10
	ctx.f2.f64 = f0.f64 * ctx.f10.f64;
	// fmul f1,f0,f9
	ctx.f1.f64 = f0.f64 * ctx.f9.f64;
	// fmul f30,f0,f6
	f30.f64 = f0.f64 * ctx.f6.f64;
	// fmadd f28,f0,f8,f11
	f28.f64 = f0.f64 * ctx.f8.f64 + ctx.f11.f64;
	// fmul f31,f0,f7
	f31.f64 = f0.f64 * ctx.f7.f64;
	// fmul f13,f0,f5
	ctx.f13.f64 = f0.f64 * ctx.f5.f64;
	// fmul f29,f0,f4
	f29.f64 = f0.f64 * ctx.f4.f64;
	// fnmsub f0,f0,f3,f12
	f0.f64 = -(f0.f64 * ctx.f3.f64 - ctx.f12.f64);
	// fctiwz f2,f2
	ctx.f2.s64 = (ctx.f2.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f2.f64));
	// stfiwx f2,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f2.u32);
	// fctiwz f2,f1
	ctx.f2.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f2,0,r11
	PPC_STORE_U32(r11.u32, ctx.f2.u32);
	// fctiwz f1,f30
	ctx.f1.s64 = (f30.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f30.f64));
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// fctiwz f30,f28
	f30.s64 = (f28.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f28.f64));
	// stfiwx f30,0,r8
	PPC_STORE_U32(ctx.r8.u32, f30.u32);
	// fctiwz f2,f31
	ctx.f2.s64 = (f31.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f31.f64));
	// stfiwx f2,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f2.u32);
	// fadd f2,f13,f12
	ctx.f2.f64 = ctx.f13.f64 + ctx.f12.f64;
	// stfiwx f1,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f1.u32);
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// fctiwz f31,f29
	f31.s64 = (f29.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f29.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// fctiwz f2,f2
	ctx.f2.s64 = (ctx.f2.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f2.f64));
	// stfiwx f2,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f2.u32);
	// stfiwx f13,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f13.u32);
	// stfiwx f31,0,r4
	PPC_STORE_U32(ctx.r4.u32, f31.u32);
	// stfiwx f0,0,r3
	PPC_STORE_U32(ctx.r3.u32, f0.u32);
	// blt cr6,0x8266ed64
	if (cr6.lt) goto loc_8266ED64;
	// addi r12,r1,-48
	r12.s64 = ctx.r1.s64 + -48;
	// bl 0x8239d634
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8266EE1C"))) PPC_WEAK_FUNC(sub_8266EE1C);
PPC_FUNC_IMPL(__imp__sub_8266EE1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266EE20"))) PPC_WEAK_FUNC(sub_8266EE20);
PPC_FUNC_IMPL(__imp__sub_8266EE20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf0
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// addi r11,r11,-8960
	r11.s64 = r11.s64 + -8960;
	// addi r26,r10,3960
	r26.s64 = ctx.r10.s64 + 3960;
	// addi r27,r11,512
	r27.s64 = r11.s64 + 512;
	// lis r11,-24416
	r11.s64 = -1600126976;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// ori r11,r11,41121
	r11.u64 = r11.u64 | 41121;
loc_8266EE48:
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
	// mr r29,r27
	r29.u64 = r27.u64;
	// li r28,4
	r28.s64 = 4;
loc_8266EE54:
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r31,2
	r31.s64 = 2;
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
loc_8266EE60:
	// addi r5,r31,-2
	ctx.r5.s64 = r31.s64 + -2;
	// rlwinm r10,r5,0,24,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xF8;
	// mulhw r6,r10,r11
	ctx.r6.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// mulhw r7,r10,r11
	ctx.r7.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// add r4,r7,r10
	ctx.r4.u64 = ctx.r7.u64 + ctx.r10.u64;
	// srawi r7,r6,5
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1F) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 5;
	// srawi r6,r4,5
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1F) != 0);
	ctx.r6.s64 = ctx.r4.s32 >> 5;
	// rlwinm r3,r7,1,31,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// rlwinm r4,r6,1,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
	// add r7,r7,r3
	ctx.r7.u64 = ctx.r7.u64 + ctx.r3.u64;
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// mulli r6,r6,51
	ctx.r6.s64 = ctx.r6.s64 * 51;
	// subf r10,r6,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r6.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bgt cr6,0x8266eea8
	if (cr6.gt) goto loc_8266EEA8;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8266EEA8:
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r6,-512(r8)
	PPC_STORE_U8(ctx.r8.u32 + -512, ctx.r6.u8);
	// li r6,1
	ctx.r6.s64 = 1;
	// bgt cr6,0x8266eec0
	if (cr6.gt) goto loc_8266EEC0;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8266EEC0:
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r10,r5,0,24,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFC;
	// rlwinm r6,r7,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mulhw r6,r10,r11
	ctx.r6.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r7,r7,10
	ctx.r7.s64 = ctx.r7.s64 + 10;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// mulhw r7,r10,r11
	ctx.r7.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// add r5,r7,r10
	ctx.r5.u64 = ctx.r7.u64 + ctx.r10.u64;
	// srawi r7,r6,5
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1F) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 5;
	// srawi r6,r5,5
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1F) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 5;
	// rlwinm r4,r7,1,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// rlwinm r5,r6,1,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mulli r6,r6,51
	ctx.r6.s64 = ctx.r6.s64 * 51;
	// subf r10,r6,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r6.s64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x8266ef1c
	if (cr6.gt) goto loc_8266EF1C;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8266EF1C:
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// addi r5,r31,-1
	ctx.r5.s64 = r31.s64 + -1;
	// rlwinm r6,r7,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r5,0,24,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xF8;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mulhw r6,r10,r11
	ctx.r6.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// stb r7,-256(r8)
	PPC_STORE_U8(ctx.r8.u32 + -256, ctx.r7.u8);
	// mulhw r7,r10,r11
	ctx.r7.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// add r4,r7,r10
	ctx.r4.u64 = ctx.r7.u64 + ctx.r10.u64;
	// srawi r7,r6,5
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1F) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 5;
	// srawi r6,r4,5
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1F) != 0);
	ctx.r6.s64 = ctx.r4.s32 >> 5;
	// rlwinm r3,r7,1,31,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// rlwinm r4,r6,1,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
	// add r7,r7,r3
	ctx.r7.u64 = ctx.r7.u64 + ctx.r3.u64;
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// mulli r6,r6,51
	ctx.r6.s64 = ctx.r6.s64 * 51;
	// subf r10,r6,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r6.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bgt cr6,0x8266ef78
	if (cr6.gt) goto loc_8266EF78;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8266EF78:
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r6,-511(r8)
	PPC_STORE_U8(ctx.r8.u32 + -511, ctx.r6.u8);
	// li r6,1
	ctx.r6.s64 = 1;
	// bgt cr6,0x8266ef90
	if (cr6.gt) goto loc_8266EF90;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8266EF90:
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r10,r5,0,24,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFC;
	// rlwinm r6,r7,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mulhw r6,r10,r11
	ctx.r6.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r7,r7,10
	ctx.r7.s64 = ctx.r7.s64 + 10;
	// stb r7,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, ctx.r7.u8);
	// mulhw r7,r10,r11
	ctx.r7.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// add r5,r7,r10
	ctx.r5.u64 = ctx.r7.u64 + ctx.r10.u64;
	// srawi r7,r6,5
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1F) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 5;
	// srawi r6,r5,5
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1F) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 5;
	// rlwinm r4,r7,1,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// rlwinm r5,r6,1,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mulli r6,r6,51
	ctx.r6.s64 = ctx.r6.s64 * 51;
	// subf r10,r6,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r6.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bgt cr6,0x8266efec
	if (cr6.gt) goto loc_8266EFEC;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8266EFEC:
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r10,r31,0,24,28
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xF8;
	// rlwinm r6,r7,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mulhw r6,r10,r11
	ctx.r6.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// stb r7,-255(r8)
	PPC_STORE_U8(ctx.r8.u32 + -255, ctx.r7.u8);
	// mulhw r7,r10,r11
	ctx.r7.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// add r5,r7,r10
	ctx.r5.u64 = ctx.r7.u64 + ctx.r10.u64;
	// srawi r7,r6,5
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1F) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 5;
	// srawi r6,r5,5
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1F) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 5;
	// rlwinm r4,r7,1,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// rlwinm r5,r6,1,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mulli r6,r6,51
	ctx.r6.s64 = ctx.r6.s64 * 51;
	// subf r10,r6,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r6.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bgt cr6,0x8266f044
	if (cr6.gt) goto loc_8266F044;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8266F044:
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r6,-510(r8)
	PPC_STORE_U8(ctx.r8.u32 + -510, ctx.r6.u8);
	// li r6,1
	ctx.r6.s64 = 1;
	// bgt cr6,0x8266f05c
	if (cr6.gt) goto loc_8266F05C;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8266F05C:
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r10,r31,0,24,29
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFC;
	// rlwinm r6,r7,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mulhw r6,r10,r11
	ctx.r6.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r7,r7,10
	ctx.r7.s64 = ctx.r7.s64 + 10;
	// stb r7,2(r8)
	PPC_STORE_U8(ctx.r8.u32 + 2, ctx.r7.u8);
	// mulhw r7,r10,r11
	ctx.r7.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// add r5,r7,r10
	ctx.r5.u64 = ctx.r7.u64 + ctx.r10.u64;
	// srawi r7,r6,5
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1F) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 5;
	// srawi r6,r5,5
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1F) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 5;
	// rlwinm r4,r7,1,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// rlwinm r5,r6,1,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mulli r6,r6,51
	ctx.r6.s64 = ctx.r6.s64 * 51;
	// subf r10,r6,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r6.s64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x8266f0b8
	if (cr6.gt) goto loc_8266F0B8;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8266F0B8:
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// addi r5,r31,1
	ctx.r5.s64 = r31.s64 + 1;
	// rlwinm r6,r7,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r5,0,24,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xF8;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mulhw r6,r10,r11
	ctx.r6.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// stb r7,-254(r8)
	PPC_STORE_U8(ctx.r8.u32 + -254, ctx.r7.u8);
	// mulhw r7,r10,r11
	ctx.r7.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// add r4,r7,r10
	ctx.r4.u64 = ctx.r7.u64 + ctx.r10.u64;
	// srawi r7,r6,5
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1F) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 5;
	// srawi r6,r4,5
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1F) != 0);
	ctx.r6.s64 = ctx.r4.s32 >> 5;
	// rlwinm r3,r7,1,31,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// rlwinm r4,r6,1,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
	// add r7,r7,r3
	ctx.r7.u64 = ctx.r7.u64 + ctx.r3.u64;
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// mulli r6,r6,51
	ctx.r6.s64 = ctx.r6.s64 * 51;
	// subf r10,r6,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r6.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bgt cr6,0x8266f114
	if (cr6.gt) goto loc_8266F114;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8266F114:
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r6,-509(r8)
	PPC_STORE_U8(ctx.r8.u32 + -509, ctx.r6.u8);
	// li r6,1
	ctx.r6.s64 = 1;
	// bgt cr6,0x8266f12c
	if (cr6.gt) goto loc_8266F12C;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8266F12C:
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r10,r5,0,24,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFC;
	// rlwinm r6,r7,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mulhw r6,r10,r11
	ctx.r6.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r7,r7,10
	ctx.r7.s64 = ctx.r7.s64 + 10;
	// stb r7,3(r8)
	PPC_STORE_U8(ctx.r8.u32 + 3, ctx.r7.u8);
	// mulhw r7,r10,r11
	ctx.r7.s64 = (int64_t(ctx.r10.s32) * int64_t(r11.s32)) >> 32;
	// add r5,r7,r10
	ctx.r5.u64 = ctx.r7.u64 + ctx.r10.u64;
	// srawi r7,r6,5
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1F) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 5;
	// srawi r6,r5,5
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1F) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 5;
	// rlwinm r4,r7,1,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// rlwinm r5,r6,1,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mulli r6,r6,51
	ctx.r6.s64 = ctx.r6.s64 * 51;
	// subf r10,r6,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r6.s64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x8266f188
	if (cr6.gt) goto loc_8266F188;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8266F188:
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r6,r31,-2
	ctx.r6.s64 = r31.s64 + -2;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmpwi cr6,r6,256
	cr6.compare<int32_t>(ctx.r6.s32, 256, xer);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stb r10,-253(r8)
	PPC_STORE_U8(ctx.r8.u32 + -253, ctx.r10.u8);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// blt cr6,0x8266ee60
	if (cr6.lt) goto loc_8266EE60;
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r29,r29,3072
	r29.s64 = r29.s64 + 3072;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x8266ee54
	if (!cr6.eq) goto loc_8266EE54;
	// addi r9,r26,64
	ctx.r9.s64 = r26.s64 + 64;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// addi r27,r27,768
	r27.s64 = r27.s64 + 768;
	// cmpw cr6,r30,r9
	cr6.compare<int32_t>(r30.s32, ctx.r9.s32, xer);
	// blt cr6,0x8266ee48
	if (cr6.lt) goto loc_8266EE48;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8266F1DC"))) PPC_WEAK_FUNC(sub_8266F1DC);
PPC_FUNC_IMPL(__imp__sub_8266F1DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266F1E0"))) PPC_WEAK_FUNC(sub_8266F1E0);
PPC_FUNC_IMPL(__imp__sub_8266F1E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// stw r11,116(r4)
	PPC_STORE_U32(ctx.r4.u32 + 116, r11.u32);
	// stw r11,124(r4)
	PPC_STORE_U32(ctx.r4.u32 + 124, r11.u32);
	// stw r11,112(r4)
	PPC_STORE_U32(ctx.r4.u32 + 112, r11.u32);
	// stw r11,120(r4)
	PPC_STORE_U32(ctx.r4.u32 + 120, r11.u32);
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8266f234
	if (!cr6.eq) goto loc_8266F234;
	// lhz r11,14(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bne cr6,0x8266f22c
	if (!cr6.eq) goto loc_8266F22C;
loc_8266F20C:
	// li r11,7
	r11.s64 = 7;
	// li r10,2
	ctx.r10.s64 = 2;
	// li r9,31744
	ctx.r9.s64 = 31744;
	// li r8,992
	ctx.r8.s64 = 992;
loc_8266F21C:
	// stw r11,116(r4)
	PPC_STORE_U32(ctx.r4.u32 + 116, r11.u32);
	// stw r10,124(r4)
	PPC_STORE_U32(ctx.r4.u32 + 124, ctx.r10.u32);
	// stw r9,112(r4)
	PPC_STORE_U32(ctx.r4.u32 + 112, ctx.r9.u32);
	// stw r8,120(r4)
	PPC_STORE_U32(ctx.r4.u32 + 120, ctx.r8.u32);
loc_8266F22C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8266F234:
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x8266f22c
	if (!cr6.eq) goto loc_8266F22C;
	// lhz r11,14(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bne cr6,0x8266f2a4
	if (!cr6.eq) goto loc_8266F2A4;
	// lwz r11,40(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// cmplwi cr6,r11,31744
	cr6.compare<uint32_t>(r11.u32, 31744, xer);
	// bne cr6,0x8266f26c
	if (!cr6.eq) goto loc_8266F26C;
	// lwz r10,44(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// cmplwi cr6,r10,992
	cr6.compare<uint32_t>(ctx.r10.u32, 992, xer);
	// bne cr6,0x8266f26c
	if (!cr6.eq) goto loc_8266F26C;
	// lwz r10,48(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// cmplwi cr6,r10,31
	cr6.compare<uint32_t>(ctx.r10.u32, 31, xer);
	// beq cr6,0x8266f20c
	if (cr6.eq) goto loc_8266F20C;
loc_8266F26C:
	// cmplwi cr6,r11,63488
	cr6.compare<uint32_t>(r11.u32, 63488, xer);
	// bne cr6,0x8266f2dc
	if (!cr6.eq) goto loc_8266F2DC;
	// lwz r11,44(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// cmplwi cr6,r11,2016
	cr6.compare<uint32_t>(r11.u32, 2016, xer);
	// bne cr6,0x8266f2dc
	if (!cr6.eq) goto loc_8266F2DC;
	// lwz r11,48(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// cmplwi cr6,r11,31
	cr6.compare<uint32_t>(r11.u32, 31, xer);
	// bne cr6,0x8266f2dc
	if (!cr6.eq) goto loc_8266F2DC;
	// lis r9,0
	ctx.r9.s64 = 0;
	// li r11,8
	r11.s64 = 8;
	// li r10,3
	ctx.r10.s64 = 3;
	// ori r9,r9,63488
	ctx.r9.u64 = ctx.r9.u64 | 63488;
	// li r8,2016
	ctx.r8.s64 = 2016;
	// b 0x8266f21c
	goto loc_8266F21C;
loc_8266F2A4:
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// beq cr6,0x8266f2b4
	if (cr6.eq) goto loc_8266F2B4;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// bne cr6,0x8266f2dc
	if (!cr6.eq) goto loc_8266F2DC;
loc_8266F2B4:
	// lwz r11,40(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// lis r10,255
	ctx.r10.s64 = 16711680;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x8266f2dc
	if (!cr6.eq) goto loc_8266F2DC;
	// lwz r11,44(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// cmplwi cr6,r11,65280
	cr6.compare<uint32_t>(r11.u32, 65280, xer);
	// bne cr6,0x8266f2dc
	if (!cr6.eq) goto loc_8266F2DC;
	// lwz r11,48(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// beq cr6,0x8266f22c
	if (cr6.eq) goto loc_8266F22C;
loc_8266F2DC:
	// li r3,3
	ctx.r3.s64 = 3;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266F2E4"))) PPC_WEAK_FUNC(sub_8266F2E4);
PPC_FUNC_IMPL(__imp__sub_8266F2E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266F2E8"))) PPC_WEAK_FUNC(sub_8266F2E8);
PPC_FUNC_IMPL(__imp__sub_8266F2E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcf8
	// lwz r28,0(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r31,4(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x8266f380
	if (!cr6.gt) goto loc_8266F380;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r29,4(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x8266f380
	if (!cr6.gt) goto loc_8266F380;
	// lwz r8,16(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x8266f324
	if (cr6.eq) goto loc_8266F324;
	// cmplwi cr6,r8,3
	cr6.compare<uint32_t>(ctx.r8.u32, 3, xer);
	// bne cr6,0x8266f338
	if (!cr6.eq) goto loc_8266F338;
loc_8266F324:
	// lwz r11,16(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8266f4e4
	if (cr6.eq) goto loc_8266F4E4;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// beq cr6,0x8266f4e4
	if (cr6.eq) goto loc_8266F4E4;
loc_8266F338:
	// lwz r11,14580(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14580);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8266f388
	if (cr6.eq) goto loc_8266F388;
	// lwz r11,14572(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14572);
	// lwz r9,14564(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14564);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// bne cr6,0x8266f380
	if (!cr6.eq) goto loc_8266F380;
	// lwz r9,14568(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14568);
	// lwz r11,14576(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14576);
	// lwz r30,8(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// beq cr6,0x8266f3bc
	if (cr6.eq) goto loc_8266F3BC;
loc_8266F380:
	// li r3,6
	ctx.r3.s64 = 6;
	// b 0x8239bd48
	return;
loc_8266F388:
	// cmpw cr6,r31,r29
	cr6.compare<int32_t>(r31.s32, r29.s32, xer);
	// bne cr6,0x8266f380
	if (!cr6.eq) goto loc_8266F380;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r30,8(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bne cr6,0x8266f380
	if (!cr6.eq) goto loc_8266F380;
loc_8266F3BC:
	// lis r11,12593
	r11.s64 = 825294848;
	// ori r11,r11,13392
	r11.u64 = r11.u64 | 13392;
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// bne cr6,0x8266f3e0
	if (!cr6.eq) goto loc_8266F3E0;
	// srawi r9,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	ctx.r9.s64 = r31.s32 >> 2;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf. r9,r9,r31
	ctx.r9.s64 = r31.s64 - ctx.r9.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x8266f380
	if (!cr0.eq) goto loc_8266F380;
loc_8266F3E0:
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x8266f400
	if (!cr6.eq) goto loc_8266F400;
	// srawi r11,r29,2
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3) != 0);
	r11.s64 = r29.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf. r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8266f380
	if (!cr0.eq) goto loc_8266F380;
loc_8266F400:
	// lis r11,12889
	r11.s64 = 844693504;
	// lis r9,22870
	ctx.r9.s64 = 1498808320;
	// ori r11,r11,21849
	r11.u64 = r11.u64 | 21849;
	// lis r7,21849
	ctx.r7.s64 = 1431896064;
	// lis r6,22101
	ctx.r6.s64 = 1448411136;
	// lis r5,12338
	ctx.r5.s64 = 808583168;
	// lis r4,12850
	ctx.r4.s64 = 842137600;
	// ori r9,r9,22869
	ctx.r9.u64 = ctx.r9.u64 | 22869;
	// ori r7,r7,22105
	ctx.r7.u64 = ctx.r7.u64 | 22105;
	// ori r6,r6,22857
	ctx.r6.u64 = ctx.r6.u64 | 22857;
	// ori r5,r5,13385
	ctx.r5.u64 = ctx.r5.u64 | 13385;
	// ori r4,r4,13392
	ctx.r4.u64 = ctx.r4.u64 | 13392;
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// beq cr6,0x8266f460
	if (cr6.eq) goto loc_8266F460;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// beq cr6,0x8266f460
	if (cr6.eq) goto loc_8266F460;
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// beq cr6,0x8266f460
	if (cr6.eq) goto loc_8266F460;
	// cmplw cr6,r8,r6
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r6.u32, xer);
	// beq cr6,0x8266f460
	if (cr6.eq) goto loc_8266F460;
	// cmplw cr6,r8,r5
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r5.u32, xer);
	// beq cr6,0x8266f460
	if (cr6.eq) goto loc_8266F460;
	// cmplw cr6,r8,r4
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r4.u32, xer);
	// bne cr6,0x8266f46c
	if (!cr6.eq) goto loc_8266F46C;
loc_8266F460:
	// clrlwi r3,r31,31
	ctx.r3.u64 = r31.u32 & 0x1;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8266f380
	if (!cr6.eq) goto loc_8266F380;
loc_8266F46C:
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x8266f49c
	if (cr6.eq) goto loc_8266F49C;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x8266f49c
	if (cr6.eq) goto loc_8266F49C;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// beq cr6,0x8266f49c
	if (cr6.eq) goto loc_8266F49C;
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// beq cr6,0x8266f49c
	if (cr6.eq) goto loc_8266F49C;
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// beq cr6,0x8266f49c
	if (cr6.eq) goto loc_8266F49C;
	// cmplw cr6,r10,r4
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r4.u32, xer);
	// bne cr6,0x8266f4a8
	if (!cr6.eq) goto loc_8266F4A8;
loc_8266F49C:
	// clrlwi r11,r29,31
	r11.u64 = r29.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8266f380
	if (!cr6.eq) goto loc_8266F380;
loc_8266F4A8:
	// cmplw cr6,r8,r6
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r6.u32, xer);
	// beq cr6,0x8266f4b8
	if (cr6.eq) goto loc_8266F4B8;
	// cmplw cr6,r8,r5
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r5.u32, xer);
	// bne cr6,0x8266f4c8
	if (!cr6.eq) goto loc_8266F4C8;
loc_8266F4B8:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8266f380
	if (!cr6.eq) goto loc_8266F380;
loc_8266F4C8:
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// beq cr6,0x8266f4d8
	if (cr6.eq) goto loc_8266F4D8;
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// bne cr6,0x8266f4e4
	if (!cr6.eq) goto loc_8266F4E4;
loc_8266F4D8:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8266f380
	if (!cr6.eq) goto loc_8266F380;
loc_8266F4E4:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8266F4EC"))) PPC_WEAK_FUNC(sub_8266F4EC);
PPC_FUNC_IMPL(__imp__sub_8266F4EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266F4F0"))) PPC_WEAK_FUNC(sub_8266F4F0);
PPC_FUNC_IMPL(__imp__sub_8266F4F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce0
	// lwz r10,14636(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14636);
	// li r23,1
	r23.s64 = 1;
	// stw r4,14588(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14588, ctx.r4.u32);
	// stw r5,14592(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14592, ctx.r5.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r6,14596(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14596, ctx.r6.u32);
	// stw r7,14600(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14600, ctx.r7.u32);
	// beq cr6,0x8266f524
	if (cr6.eq) goto loc_8266F524;
	// stw r10,14528(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14528, ctx.r10.u32);
	// stw r23,14472(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14472, r23.u32);
	// b 0x8266f54c
	goto loc_8266F54C;
loc_8266F524:
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,14472(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14472);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// mullw r11,r11,r4
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// stw r11,14528(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14528, r11.u32);
loc_8266F54C:
	// lwz r8,14528(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14528);
	// li r27,0
	r27.s64 = 0;
	// lwz r28,14472(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14472);
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// stw r11,14532(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14532, r11.u32);
	// bne cr6,0x8266f570
	if (!cr6.eq) goto loc_8266F570;
	// mr r30,r27
	r30.u64 = r27.u64;
	// b 0x8266f598
	goto loc_8266F598;
loc_8266F570:
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// srawi r31,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r11.s32 >> 31;
	// srawi r30,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = ctx.r9.s32 >> 31;
	// xor r11,r11,r31
	r11.u64 = r11.u64 ^ r31.u64;
	// xor r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 ^ r30.u64;
	// subf r11,r31,r11
	r11.s64 = r11.s64 - r31.s64;
	// subf r9,r30,r9
	ctx.r9.s64 = ctx.r9.s64 - r30.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mullw r30,r11,r9
	r30.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
loc_8266F598:
	// lwz r31,0(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r30,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, r30.u32);
	// lwz r11,14604(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14604);
	// lwz r29,14580(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14580);
	// lwz r9,14608(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14608);
	// lhz r30,14(r31)
	r30.u64 = PPC_LOAD_U16(r31.u32 + 14);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// mullw r29,r9,r8
	r29.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// mullw r30,r30,r11
	r30.s64 = int64_t(r30.s32) * int64_t(r11.s32);
	// srawi r30,r30,3
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7) != 0);
	r30.s64 = r30.s32 >> 3;
	// addze r30,r30
	temp.s64 = r30.s64 + xer.ca;
	xer.ca = temp.u32 < r30.u32;
	r30.s64 = temp.s64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// stw r30,14536(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14536, r30.u32);
	// beq cr6,0x8266f640
	if (cr6.eq) goto loc_8266F640;
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// bne cr6,0x8266f5f8
	if (!cr6.eq) goto loc_8266F5F8;
	// lwz r29,14564(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14564);
	// lhz r30,14(r31)
	r30.u64 = PPC_LOAD_U16(r31.u32 + 14);
	// lwz r28,14568(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14568);
	// mullw r30,r29,r30
	r30.s64 = int64_t(r29.s32) * int64_t(r30.s32);
	// srawi r29,r30,3
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7) != 0);
	r29.s64 = r30.s32 >> 3;
	// mullw r30,r28,r8
	r30.s64 = int64_t(r28.s32) * int64_t(ctx.r8.s32);
	// addze r8,r29
	temp.s64 = r29.s64 + xer.ca;
	xer.ca = temp.u32 < r29.u32;
	ctx.r8.s64 = temp.s64;
	// b 0x8266f638
	goto loc_8266F638;
loc_8266F5F8:
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lwz r29,14568(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14568);
	// lhz r28,14(r31)
	r28.u64 = PPC_LOAD_U16(r31.u32 + 14);
	// srawi r26,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	r26.s64 = r30.s32 >> 31;
	// lwz r25,14564(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14564);
	// srawi r24,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r24.s64 = ctx.r8.s32 >> 31;
	// xor r30,r30,r26
	r30.u64 = r30.u64 ^ r26.u64;
	// xor r22,r8,r24
	r22.u64 = ctx.r8.u64 ^ r24.u64;
	// subf r8,r26,r30
	ctx.r8.s64 = r30.s64 - r26.s64;
	// mullw r30,r25,r28
	r30.s64 = int64_t(r25.s32) * int64_t(r28.s32);
	// subf r8,r29,r8
	ctx.r8.s64 = ctx.r8.s64 - r29.s64;
	// subf r29,r24,r22
	r29.s64 = r22.s64 - r24.s64;
	// srawi r30,r30,3
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7) != 0);
	r30.s64 = r30.s32 >> 3;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addze r30,r30
	temp.s64 = r30.s64 + xer.ca;
	xer.ca = temp.u32 < r30.u32;
	r30.s64 = temp.s64;
	// mullw r8,r8,r29
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r29.s32);
loc_8266F638:
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// stw r8,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, ctx.r8.u32);
loc_8266F640:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8266f64c
	if (!cr6.eq) goto loc_8266F64C;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
loc_8266F64C:
	// lwz r8,16(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lis r31,12849
	r31.s64 = 842072064;
	// ori r29,r31,22105
	r29.u64 = r31.u64 | 22105;
	// lis r31,12850
	r31.s64 = 842137600;
	// cmplw cr6,r8,r29
	cr6.compare<uint32_t>(ctx.r8.u32, r29.u32, xer);
	// ori r28,r31,13392
	r28.u64 = r31.u64 | 13392;
	// lis r31,22101
	r31.s64 = 1448411136;
	// ori r26,r31,22857
	r26.u64 = r31.u64 | 22857;
	// lis r31,12338
	r31.s64 = 808583168;
	// ori r25,r31,13385
	r25.u64 = r31.u64 | 13385;
	// lis r31,12593
	r31.s64 = 825294848;
	// ori r24,r31,13392
	r24.u64 = r31.u64 | 13392;
	// bgt cr6,0x8266f734
	if (cr6.gt) goto loc_8266F734;
	// beq cr6,0x8266f6e8
	if (cr6.eq) goto loc_8266F6E8;
	// cmplw cr6,r8,r25
	cr6.compare<uint32_t>(ctx.r8.u32, r25.u32, xer);
	// beq cr6,0x8266f744
	if (cr6.eq) goto loc_8266F744;
	// cmplw cr6,r8,r24
	cr6.compare<uint32_t>(ctx.r8.u32, r24.u32, xer);
	// bne cr6,0x8266f804
	if (!cr6.eq) goto loc_8266F804;
	// lwz r8,14628(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14628);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x8266f6a8
	if (!cr6.eq) goto loc_8266F6A8;
	// srawi r8,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 2;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
loc_8266F6A8:
	// mullw r31,r9,r8
	r31.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// stw r8,14644(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14644, ctx.r8.u32);
	// mullw r9,r9,r4
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// srawi r30,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r30.s64 = r11.s32 >> 2;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// stw r11,14540(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14540, r11.u32);
	// stw r10,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r10.u32);
	// addze r30,r30
	temp.s64 = r30.s64 + xer.ca;
	xer.ca = temp.u32 < r30.u32;
	r30.s64 = temp.s64;
	// mullw r5,r8,r5
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r11,r5,r10
	r11.u64 = ctx.r5.u64 + ctx.r10.u64;
	// stw r31,14544(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14544, r31.u32);
	// stw r11,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, r11.u32);
	// stw r31,14548(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14548, r31.u32);
	// b 0x8266f800
	goto loc_8266F800;
loc_8266F6E8:
	// mullw r8,r9,r4
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// srawi r9,r5,31
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r5.s32 >> 31;
	// srawi r4,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r4.s64 = r11.s32 >> 1;
	// xor r31,r5,r9
	r31.u64 = ctx.r5.u64 ^ ctx.r9.u64;
	// addze r5,r4
	temp.s64 = ctx.r4.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r4.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r4,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r4.s64 = ctx.r8.s32 >> 2;
	// subf r9,r9,r31
	ctx.r9.s64 = r31.s64 - ctx.r9.s64;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// stw r9,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, ctx.r9.u32);
	// addze r11,r4
	temp.s64 = ctx.r4.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r4.u32;
	r11.s64 = temp.s64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// stw r10,14644(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14644, ctx.r10.u32);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// stw r10,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r10.u32);
	// b 0x8266f7f0
	goto loc_8266F7F0;
loc_8266F734:
	// cmplw cr6,r8,r28
	cr6.compare<uint32_t>(ctx.r8.u32, r28.u32, xer);
	// beq cr6,0x8266f7a8
	if (cr6.eq) goto loc_8266F7A8;
	// cmplw cr6,r8,r26
	cr6.compare<uint32_t>(ctx.r8.u32, r26.u32, xer);
	// bne cr6,0x8266f804
	if (!cr6.eq) goto loc_8266F804;
loc_8266F744:
	// lwz r8,14628(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14628);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x8266f758
	if (!cr6.eq) goto loc_8266F758;
	// srawi r8,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 1;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
loc_8266F758:
	// mullw r31,r8,r5
	r31.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// stw r8,14644(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14644, ctx.r8.u32);
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// stw r10,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r10.u32);
	// addze r5,r31
	temp.s64 = r31.s64 + xer.ca;
	xer.ca = temp.u32 < r31.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r31,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r31.s64 = ctx.r9.s32 >> 1;
	// mullw r4,r9,r4
	ctx.r4.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// addze r9,r31
	temp.s64 = r31.s64 + xer.ca;
	xer.ca = temp.u32 < r31.u32;
	ctx.r9.s64 = temp.s64;
	// srawi r31,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r31.s64 = r11.s32 >> 1;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// stw r11,14540(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14540, r11.u32);
	// addze r31,r31
	temp.s64 = r31.s64 + xer.ca;
	xer.ca = temp.u32 < r31.u32;
	r31.s64 = temp.s64;
	// add r11,r5,r10
	r11.u64 = ctx.r5.u64 + ctx.r10.u64;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// stw r11,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, r11.u32);
	// stw r9,14544(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14544, ctx.r9.u32);
	// stw r9,14548(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14548, ctx.r9.u32);
	// b 0x8266f800
	goto loc_8266F800;
loc_8266F7A8:
	// mullw r8,r9,r4
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// srawi r9,r5,31
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r5.s32 >> 31;
	// srawi r4,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r4.s64 = r11.s32 >> 1;
	// xor r31,r5,r9
	r31.u64 = ctx.r5.u64 ^ ctx.r9.u64;
	// addze r5,r4
	temp.s64 = ctx.r4.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r4.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r4,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r4.s64 = ctx.r8.s32 >> 2;
	// subf r9,r9,r31
	ctx.r9.s64 = r31.s64 - ctx.r9.s64;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// stw r9,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r9.u32);
	// addze r11,r4
	temp.s64 = ctx.r4.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r4.u32;
	r11.s64 = temp.s64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// stw r10,14644(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14644, ctx.r10.u32);
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r10,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, ctx.r10.u32);
loc_8266F7F0:
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// stw r8,14540(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14540, ctx.r8.u32);
	// stw r11,14548(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14548, r11.u32);
	// stw r11,14544(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14544, r11.u32);
loc_8266F800:
	// stw r27,60(r3)
	PPC_STORE_U32(ctx.r3.u32 + 60, r27.u32);
loc_8266F804:
	// lwz r11,14640(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14640);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8266f81c
	if (cr6.eq) goto loc_8266F81C;
	// stw r11,14492(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14492, r11.u32);
	// stw r23,14476(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14476, r23.u32);
	// b 0x8266f844
	goto loc_8266F844;
loc_8266F81C:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r9,14476(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14476);
	// lhz r10,14(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 14);
	// mullw r10,r10,r6
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// rlwinm r10,r10,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// stw r10,14492(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14492, ctx.r10.u32);
loc_8266F844:
	// lwz r10,14476(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14476);
	// lwz r5,14492(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14492);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,14496(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14496, ctx.r10.u32);
	// bne cr6,0x8266f864
	if (!cr6.eq) goto loc_8266F864;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// b 0x8266f88c
	goto loc_8266F88C;
loc_8266F864:
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// srawi r4,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = ctx.r9.s32 >> 31;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// xor r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r4.u64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// subf r9,r4,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r4.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mullw r8,r10,r9
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
loc_8266F88C:
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r8,72(r3)
	PPC_STORE_U32(ctx.r3.u32 + 72, ctx.r8.u32);
	// lwz r10,14612(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14612);
	// lwz r9,14616(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14616);
	// lhz r8,14(r4)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r4.u32 + 14);
	// mullw r5,r9,r5
	ctx.r5.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// srawi r8,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 3;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// stw r8,14500(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14500, ctx.r8.u32);
	// bne cr6,0x8266f8c4
	if (!cr6.eq) goto loc_8266F8C4;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
loc_8266F8C4:
	// lwz r8,16(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// cmplw cr6,r8,r29
	cr6.compare<uint32_t>(ctx.r8.u32, r29.u32, xer);
	// bgt cr6,0x8266f998
	if (cr6.gt) goto loc_8266F998;
	// beq cr6,0x8266f940
	if (cr6.eq) goto loc_8266F940;
	// cmplw cr6,r8,r25
	cr6.compare<uint32_t>(ctx.r8.u32, r25.u32, xer);
	// beq cr6,0x8266f9a8
	if (cr6.eq) goto loc_8266F9A8;
	// cmplw cr6,r8,r24
	cr6.compare<uint32_t>(ctx.r8.u32, r24.u32, xer);
	// bne cr6,0x8266fa50
	if (!cr6.eq) goto loc_8266FA50;
	// mullw r8,r9,r6
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// srawi r9,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 31;
	// srawi r6,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 2;
	// xor r5,r7,r9
	ctx.r5.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// addze r7,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r7.s64 = temp.s64;
	// srawi r6,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 2;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// subf r9,r9,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r9.s64;
	// addze r10,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r6,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r6.s64 = r11.s32 >> 2;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// stw r8,14504(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14504, ctx.r8.u32);
	// stw r9,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, ctx.r9.u32);
	// add r11,r7,r10
	r11.u64 = ctx.r7.u64 + ctx.r10.u64;
	// addze r10,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r10.s64 = temp.s64;
	// stw r11,14508(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14508, r11.u32);
	// stw r10,14648(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14648, ctx.r10.u32);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,14512(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14512, r11.u32);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// stw r10,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, ctx.r10.u32);
	// b 0x8266fa50
	goto loc_8266FA50;
loc_8266F940:
	// mullw r8,r9,r6
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// srawi r9,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 31;
	// srawi r6,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 1;
	// xor r5,r7,r9
	ctx.r5.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// addze r7,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r7.s64 = temp.s64;
	// srawi r6,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 2;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// subf r9,r9,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r9.s64;
	// addze r10,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r6,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r6.s64 = r11.s32 >> 1;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// stw r8,14504(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14504, ctx.r8.u32);
	// stw r9,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, ctx.r9.u32);
	// add r11,r7,r10
	r11.u64 = ctx.r7.u64 + ctx.r10.u64;
	// addze r10,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r10.s64 = temp.s64;
	// stw r11,14512(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14512, r11.u32);
	// stw r10,14648(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14648, ctx.r10.u32);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,14508(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14508, r11.u32);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// b 0x8266fa4c
	goto loc_8266FA4C;
loc_8266F998:
	// cmplw cr6,r8,r28
	cr6.compare<uint32_t>(ctx.r8.u32, r28.u32, xer);
	// beq cr6,0x8266fa04
	if (cr6.eq) goto loc_8266FA04;
	// cmplw cr6,r8,r26
	cr6.compare<uint32_t>(ctx.r8.u32, r26.u32, xer);
	// bne cr6,0x8266fa50
	if (!cr6.eq) goto loc_8266FA50;
loc_8266F9A8:
	// mullw r8,r9,r6
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// srawi r9,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 31;
	// srawi r6,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 1;
	// xor r5,r7,r9
	ctx.r5.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// addze r7,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r7.s64 = temp.s64;
	// srawi r6,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 2;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// subf r9,r9,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r9.s64;
	// addze r10,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r6,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r6.s64 = r11.s32 >> 1;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// stw r8,14504(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14504, ctx.r8.u32);
	// stw r9,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, ctx.r9.u32);
	// add r11,r7,r10
	r11.u64 = ctx.r7.u64 + ctx.r10.u64;
	// addze r10,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r10.s64 = temp.s64;
	// stw r11,14508(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14508, r11.u32);
	// stw r10,14648(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14648, ctx.r10.u32);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,14512(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14512, r11.u32);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// stw r10,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, ctx.r10.u32);
	// b 0x8266fa50
	goto loc_8266FA50;
loc_8266FA04:
	// mullw r9,r9,r6
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// srawi r10,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 31;
	// xor r8,r7,r10
	ctx.r8.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// stw r9,14504(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14504, ctx.r9.u32);
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r7,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r7.s64 = temp.s64;
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r8,14508(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14508, ctx.r8.u32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r8,14512(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14512, ctx.r8.u32);
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r7,14648(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14648, ctx.r7.u32);
	// stw r11,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, r11.u32);
loc_8266FA4C:
	// stw r10,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, ctx.r10.u32);
loc_8266FA50:
	// lwz r9,14560(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14560);
	// lwz r10,14484(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14484);
	// twllei r9,0
	// divwu r11,r10,r9
	r11.u32 = ctx.r10.u32 / ctx.r9.u32;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r8,r11,30
	ctx.r8.u64 = r11.u32 & 0x3;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stw r11,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, r11.u32);
	// beq cr6,0x8266fa7c
	if (cr6.eq) goto loc_8266FA7C;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// stw r11,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, r11.u32);
loc_8266FA7C:
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x8266fa88
	if (!cr6.eq) goto loc_8266FA88;
	// stw r10,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, ctx.r10.u32);
loc_8266FA88:
	// cmplwi cr6,r9,2
	cr6.compare<uint32_t>(ctx.r9.u32, 2, xer);
	// bne cr6,0x8266fa98
	if (!cr6.eq) goto loc_8266FA98;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// b 0x8266faa0
	goto loc_8266FAA0;
loc_8266FA98:
	// lwz r11,84(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_8266FAA0:
	// cmplwi cr6,r9,4
	cr6.compare<uint32_t>(ctx.r9.u32, 4, xer);
	// stw r11,88(r3)
	PPC_STORE_U32(ctx.r3.u32 + 88, r11.u32);
	// beq cr6,0x8266fab4
	if (cr6.eq) goto loc_8266FAB4;
	// stw r10,92(r3)
	PPC_STORE_U32(ctx.r3.u32 + 92, ctx.r10.u32);
	// b 0x8239bd30
	return;
loc_8266FAB4:
	// lwz r11,84(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,92(r3)
	PPC_STORE_U32(ctx.r3.u32 + 92, r11.u32);
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_8266FAC8"))) PPC_WEAK_FUNC(sub_8266FAC8);
PPC_FUNC_IMPL(__imp__sub_8266FAC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,14560(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14560);
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// ble cr6,0x8266fadc
	if (!cr6.gt) goto loc_8266FADC;
	// li r11,4
	r11.s64 = 4;
	// stw r11,14560(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14560, r11.u32);
loc_8266FADC:
	// lwz r9,14560(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14560);
	// lwz r10,14484(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14484);
	// twllei r9,0
	// divwu r11,r10,r9
	r11.u32 = ctx.r10.u32 / ctx.r9.u32;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r8,r11,30
	ctx.r8.u64 = r11.u32 & 0x3;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stw r11,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, r11.u32);
	// beq cr6,0x8266fb08
	if (cr6.eq) goto loc_8266FB08;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// stw r11,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, r11.u32);
loc_8266FB08:
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x8266fb14
	if (!cr6.eq) goto loc_8266FB14;
	// stw r10,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, ctx.r10.u32);
loc_8266FB14:
	// cmplwi cr6,r9,2
	cr6.compare<uint32_t>(ctx.r9.u32, 2, xer);
	// bne cr6,0x8266fb24
	if (!cr6.eq) goto loc_8266FB24;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// b 0x8266fb2c
	goto loc_8266FB2C;
loc_8266FB24:
	// lwz r11,84(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_8266FB2C:
	// cmplwi cr6,r9,4
	cr6.compare<uint32_t>(ctx.r9.u32, 4, xer);
	// stw r11,88(r3)
	PPC_STORE_U32(ctx.r3.u32 + 88, r11.u32);
	// beq cr6,0x8266fb40
	if (cr6.eq) goto loc_8266FB40;
	// stw r10,92(r3)
	PPC_STORE_U32(ctx.r3.u32 + 92, ctx.r10.u32);
	// blr 
	return;
loc_8266FB40:
	// lwz r11,84(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,92(r3)
	PPC_STORE_U32(ctx.r3.u32 + 92, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266FB54"))) PPC_WEAK_FUNC(sub_8266FB54);
PPC_FUNC_IMPL(__imp__sub_8266FB54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8266FB58"))) PPC_WEAK_FUNC(sub_8266FB58);
PPC_FUNC_IMPL(__imp__sub_8266FB58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// bne cr6,0x8266fbf0
	if (!cr6.eq) goto loc_8266FBF0;
	// lwz r3,20(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8266fbb8
	if (!cr6.eq) goto loc_8266FBB8;
	// lwz r6,84(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// lwz r4,36(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r11,14652(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 14652);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_8266FBB8:
	// lwz r8,84(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lwz r6,48(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r5,44(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// lwz r4,40(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// lwz r11,14656(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 14656);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_8266FBF0:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8266fc34
	if (!cr6.eq) goto loc_8266FC34;
	// lwz r8,84(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lwz r6,32(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r5,28(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// lwz r4,24(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// lwz r3,36(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// lwz r11,14664(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 14664);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_8266FC34:
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r8,48(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r7,44(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// lwz r6,40(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// lwz r5,32(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r4,28(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// lwz r3,24(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// lwz r31,14660(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 14660);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8266FC78"))) PPC_WEAK_FUNC(sub_8266FC78);
PPC_FUNC_IMPL(__imp__sub_8266FC78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,244(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// subf r24,r9,r10
	r24.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// lwz r11,14588(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14588);
	// lwz r10,14596(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14596);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// lwz r26,14540(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 14540);
	// lwz r27,14504(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 14504);
	// lwz r28,14544(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 14544);
	// lwz r29,14548(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 14548);
	// lwz r30,14512(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 14512);
	// lwz r25,14508(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 14508);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r9,r26,r11
	ctx.r9.u64 = r26.u64 + r11.u64;
	// srawi r23,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r23.s64 = r11.s32 >> 2;
	// add r26,r9,r3
	r26.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r9,r27,r10
	ctx.r9.u64 = r27.u64 + ctx.r10.u64;
	// addze r11,r23
	temp.s64 = r23.s64 + xer.ca;
	xer.ca = temp.u32 < r23.u32;
	r11.s64 = temp.s64;
	// add r27,r9,r6
	r27.u64 = ctx.r9.u64 + ctx.r6.u64;
	// srawi r3,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r10.s32 >> 2;
	// add r9,r28,r11
	ctx.r9.u64 = r28.u64 + r11.u64;
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// addze r10,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r10.s64 = temp.s64;
	// add r29,r9,r4
	r29.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r28,r11,r5
	r28.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r30,r10
	r11.u64 = r30.u64 + ctx.r10.u64;
	// add r9,r25,r10
	ctx.r9.u64 = r25.u64 + ctx.r10.u64;
	// add r23,r11,r8
	r23.u64 = r11.u64 + ctx.r8.u64;
	// add r30,r9,r7
	r30.u64 = ctx.r9.u64 + ctx.r7.u64;
	// ble cr6,0x8266fdb0
	if (!cr6.gt) goto loc_8266FDB0;
	// mr r25,r24
	r25.u64 = r24.u64;
loc_8266FD00:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r5,14480(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 14480);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,14596(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14596);
	// lwz r11,14588(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14588);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// add r27,r10,r27
	r27.u64 = ctx.r10.u64 + r27.u64;
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x8266fd00
	if (!cr6.eq) goto loc_8266FD00;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x8266fdb0
	if (!cr6.gt) goto loc_8266FDB0;
	// addi r11,r24,-1
	r11.s64 = r24.s64 + -1;
	// lwz r10,14488(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14488);
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r5,r11,1
	ctx.r5.s64 = r11.s64 + 1;
loc_8266FD44:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8266fd88
	if (!cr6.gt) goto loc_8266FD88;
	// subf r7,r30,r23
	ctx.r7.s64 = r23.s64 - r30.s64;
	// addi r6,r23,1
	ctx.r6.s64 = r23.s64 + 1;
loc_8266FD58:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// lbzx r8,r9,r29
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + r29.u32);
	// stb r8,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, ctx.r8.u8);
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// lbzx r9,r9,r28
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r28.u32);
	// stbx r9,r6,r11
	PPC_STORE_U8(ctx.r6.u32 + r11.u32, ctx.r9.u8);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stbx r9,r7,r10
	PPC_STORE_U8(ctx.r7.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r10,14488(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14488);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x8266fd58
	if (cr6.lt) goto loc_8266FD58;
loc_8266FD88:
	// lwz r9,14644(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14644);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lwz r11,14648(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14648);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// add r23,r11,r23
	r23.u64 = r11.u64 + r23.u64;
	// add r29,r9,r29
	r29.u64 = ctx.r9.u64 + r29.u64;
	// add r28,r9,r28
	r28.u64 = ctx.r9.u64 + r28.u64;
	// bne cr6,0x8266fd44
	if (!cr6.eq) goto loc_8266FD44;
loc_8266FDB0:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_8266FDB8"))) PPC_WEAK_FUNC(sub_8266FDB8);
PPC_FUNC_IMPL(__imp__sub_8266FDB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc0
	// mr r20,r9
	r20.u64 = ctx.r9.u64;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// mr r23,r8
	r23.u64 = ctx.r8.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// stw r20,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, r20.u32);
	// stw r22,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, r22.u32);
	// stw r23,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, r23.u32);
	// ble cr6,0x826700f0
	if (!cr6.gt) goto loc_826700F0;
	// rlwinm r9,r5,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r5,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r29,r9,r3
	r29.u64 = ctx.r9.u64 + ctx.r3.u64;
loc_8266FE08:
	// lbz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// lbz r27,0(r29)
	r27.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// mulli r6,r8,88
	ctx.r6.s64 = ctx.r8.s64 * 88;
	// mulli r9,r9,197
	ctx.r9.s64 = ctx.r9.s64 * 197;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// mulli r8,r27,29
	ctx.r8.s64 = r27.s64 * 29;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r9,0(r28)
	PPC_STORE_U8(r28.u32 + 0, ctx.r9.u8);
	// add r28,r28,r23
	r28.u64 = r28.u64 + r23.u64;
	// bne cr6,0x8266fe08
	if (!cr6.eq) goto loc_8266FE08;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x826700f0
	if (!cr6.gt) goto loc_826700F0;
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r5,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r29,r4,r22
	r29.u64 = ctx.r4.u64 + r22.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// mr r31,r20
	r31.u64 = r20.u64;
	// add r30,r9,r3
	r30.u64 = ctx.r9.u64 + ctx.r3.u64;
loc_8266FE74:
	// lbz r28,0(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// lbzx r9,r11,r5
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + ctx.r5.u32);
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// lbz r26,0(r30)
	r26.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// add r27,r9,r28
	r27.u64 = ctx.r9.u64 + r28.u64;
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// rlwinm r9,r27,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r9,r27,r9
	ctx.r9.s64 = ctx.r9.s64 - r27.s64;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// srawi r9,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// add r29,r29,r23
	r29.u64 = r29.u64 + r23.u64;
	// bne cr6,0x8266fe74
	if (!cr6.eq) goto loc_8266FE74;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x826700f0
	if (!cr6.gt) goto loc_826700F0;
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r5,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r5,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// add r27,r4,r22
	r27.u64 = ctx.r4.u64 + r22.u64;
	// add r28,r8,r3
	r28.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r29,r9,r3
	r29.u64 = ctx.r9.u64 + ctx.r3.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_8266FEE4:
	// lbz r8,0(r28)
	ctx.r8.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r26,0(r31)
	r26.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// add r28,r28,r7
	r28.u64 = r28.u64 + ctx.r7.u64;
	// lbz r24,0(r30)
	r24.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// mulli r25,r8,138
	r25.s64 = ctx.r8.s64 * 138;
	// lbz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// add r8,r24,r26
	ctx.r8.u64 = r24.u64 + r26.u64;
	// mulli r9,r9,29
	ctx.r9.s64 = ctx.r9.s64 * 29;
	// mulli r8,r8,88
	ctx.r8.s64 = ctx.r8.s64 * 88;
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// add r9,r9,r25
	ctx.r9.u64 = ctx.r9.u64 + r25.u64;
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r9,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r9.u8);
	// add r27,r27,r23
	r27.u64 = r27.u64 + r23.u64;
	// bne cr6,0x8266fee4
	if (!cr6.eq) goto loc_8266FEE4;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x826700f0
	if (!cr6.gt) goto loc_826700F0;
	// rlwinm r9,r22,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r5,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r28,r9,r4
	r28.u64 = ctx.r9.u64 + ctx.r4.u64;
	// rlwinm r31,r5,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r5,r8
	ctx.r6.u64 = ctx.r5.u64 + ctx.r8.u64;
	// subf r8,r5,r31
	ctx.r8.s64 = r31.s64 - ctx.r5.s64;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r31,r6,r3
	r31.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r29,r8,r3
	r29.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r30,r9,r3
	r30.u64 = ctx.r9.u64 + ctx.r3.u64;
loc_8266FF74:
	// lbz r27,0(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// lbz r25,0(r29)
	r25.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// add r26,r9,r27
	r26.u64 = ctx.r9.u64 + r27.u64;
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// rlwinm r9,r26,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// subf r9,r26,r9
	ctx.r9.s64 = ctx.r9.s64 - r26.s64;
	// add r9,r9,r25
	ctx.r9.u64 = ctx.r9.u64 + r25.u64;
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + r27.u64;
	// srawi r9,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// stb r9,0(r28)
	PPC_STORE_U8(r28.u32 + 0, ctx.r9.u8);
	// add r28,r28,r23
	r28.u64 = r28.u64 + r23.u64;
	// bne cr6,0x8266ff74
	if (!cr6.eq) goto loc_8266FF74;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x826700f0
	if (!cr6.gt) goto loc_826700F0;
	// rlwinm r6,r22,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r27,r6,r4
	r27.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r28,r9,r3
	r28.u64 = ctx.r9.u64 + ctx.r3.u64;
	// rlwinm r6,r5,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r5,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r5,r6
	ctx.r8.u64 = ctx.r5.u64 + ctx.r6.u64;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// rlwinm r11,r5,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r11,r3
	r31.u64 = r11.u64 + ctx.r3.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r29,r6,r3
	r29.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r30,r9,r3
	r30.u64 = ctx.r9.u64 + ctx.r3.u64;
loc_82670000:
	// lbz r8,0(r28)
	ctx.r8.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r26,0(r31)
	r26.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// add r28,r28,r7
	r28.u64 = r28.u64 + ctx.r7.u64;
	// lbz r24,0(r30)
	r24.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// mulli r25,r8,138
	r25.s64 = ctx.r8.s64 * 138;
	// lbz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// add r8,r24,r26
	ctx.r8.u64 = r24.u64 + r26.u64;
	// mulli r9,r9,29
	ctx.r9.s64 = ctx.r9.s64 * 29;
	// mulli r8,r8,88
	ctx.r8.s64 = ctx.r8.s64 * 88;
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// add r9,r9,r25
	ctx.r9.u64 = ctx.r9.u64 + r25.u64;
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r9,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r9.u8);
	// add r27,r27,r23
	r27.u64 = r27.u64 + r23.u64;
	// bne cr6,0x82670000
	if (!cr6.eq) goto loc_82670000;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x826700f0
	if (!cr6.gt) goto loc_826700F0;
	// rlwinm r30,r22,1,0,30
	r30.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r5,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r6,r5,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r29,r22,r30
	r29.u64 = r22.u64 + r30.u64;
	// rlwinm r8,r5,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r5,r31
	r30.u64 = ctx.r5.u64 + r31.u64;
	// add r31,r5,r6
	r31.u64 = ctx.r5.u64 + ctx.r6.u64;
	// subf r6,r5,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r5.s64;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r27,r29,r4
	r27.u64 = r29.u64 + ctx.r4.u64;
	// add r28,r30,r3
	r28.u64 = r30.u64 + ctx.r3.u64;
	// add r29,r31,r3
	r29.u64 = r31.u64 + ctx.r3.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r31,r6,r3
	r31.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r30,r9,r3
	r30.u64 = ctx.r9.u64 + ctx.r3.u64;
loc_826700A0:
	// lbz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r26,0(r30)
	r26.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// lbz r24,0(r28)
	r24.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// add r26,r9,r26
	r26.u64 = ctx.r9.u64 + r26.u64;
	// lbz r25,0(r29)
	r25.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// rlwinm r9,r26,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// add r28,r28,r7
	r28.u64 = r28.u64 + ctx.r7.u64;
	// subf r9,r26,r9
	ctx.r9.s64 = ctx.r9.s64 - r26.s64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// add r9,r9,r24
	ctx.r9.u64 = ctx.r9.u64 + r24.u64;
	// add r9,r9,r25
	ctx.r9.u64 = ctx.r9.u64 + r25.u64;
	// srawi r9,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// stb r9,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r9.u8);
	// add r27,r27,r23
	r27.u64 = r27.u64 + r23.u64;
	// bne cr6,0x826700a0
	if (!cr6.eq) goto loc_826700A0;
loc_826700F0:
	// srawi r11,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r11.s64 = ctx.r10.s32 >> 1;
	// li r8,3
	ctx.r8.s64 = 3;
	// addze r19,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r19.s64 = temp.s64;
	// addi r21,r19,-2
	r21.s64 = r19.s64 + -2;
	// cmpwi cr6,r21,3
	cr6.compare<int32_t>(r21.s32, 3, xer);
	// stw r19,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, r19.u32);
	// stw r21,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r21.u32);
	// ble cr6,0x82670304
	if (!cr6.gt) goto loc_82670304;
	// rlwinm r10,r5,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r17,r10,r3
	r17.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r29,r5,1,0,30
	r29.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r16,r9,r3
	r16.u64 = ctx.r9.u64 + ctx.r3.u64;
	// rlwinm r31,r5,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r5,r10
	ctx.r6.u64 = ctx.r5.u64 + ctx.r10.u64;
	// rlwinm r9,r5,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r30,r22,1,0,30
	r30.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r5,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r25,r5,r29
	r25.u64 = ctx.r5.u64 + r29.u64;
	// add r29,r5,r31
	r29.u64 = ctx.r5.u64 + r31.u64;
	// rlwinm r26,r5,3,0,28
	r26.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r5,r9
	r31.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r27,r22,r30
	r27.u64 = r22.u64 + r30.u64;
	// add r9,r5,r10
	ctx.r9.u64 = ctx.r5.u64 + ctx.r10.u64;
	// rlwinm r28,r6,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r30,r5,r26
	r30.s64 = r26.s64 - ctx.r5.s64;
	// rlwinm r10,r25,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// add r18,r27,r4
	r18.u64 = r27.u64 + ctx.r4.u64;
	// rlwinm r11,r5,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// add r21,r28,r3
	r21.u64 = r28.u64 + ctx.r3.u64;
	// add r4,r29,r3
	ctx.r4.u64 = r29.u64 + ctx.r3.u64;
	// add r14,r30,r3
	r14.u64 = r30.u64 + ctx.r3.u64;
	// add r15,r31,r3
	r15.u64 = r31.u64 + ctx.r3.u64;
	// add r6,r9,r3
	ctx.r6.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r20,r10,r3
	r20.u64 = ctx.r10.u64 + ctx.r3.u64;
loc_82670184:
	// lwz r10,68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82670210
	if (!cr6.gt) goto loc_82670210;
	// mr r26,r18
	r26.u64 = r18.u64;
	// mr r27,r21
	r27.u64 = r21.u64;
	// mr r28,r20
	r28.u64 = r20.u64;
	// mr r29,r19
	r29.u64 = r19.u64;
	// mr r30,r17
	r30.u64 = r17.u64;
	// mr r31,r16
	r31.u64 = r16.u64;
loc_826701A8:
	// lbz r23,0(r31)
	r23.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r24,0(r28)
	r24.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// lbz r22,0(r30)
	r22.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// add r28,r28,r7
	r28.u64 = r28.u64 + ctx.r7.u64;
	// lbz r25,0(r29)
	r25.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// add r9,r23,r22
	ctx.r9.u64 = r23.u64 + r22.u64;
	// lbz r23,0(r27)
	r23.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// add r25,r25,r24
	r25.u64 = r25.u64 + r24.u64;
	// mulli r9,r9,88
	ctx.r9.s64 = ctx.r9.s64 * 88;
	// mulli r25,r25,29
	r25.s64 = r25.s64 * 29;
	// subf r25,r25,r9
	r25.s64 = ctx.r9.s64 - r25.s64;
	// mulli r24,r23,138
	r24.s64 = r23.s64 * 138;
	// add r9,r25,r24
	ctx.r9.u64 = r25.u64 + r24.u64;
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// add r27,r27,r7
	r27.u64 = r27.u64 + ctx.r7.u64;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stb r9,0(r26)
	PPC_STORE_U8(r26.u32 + 0, ctx.r9.u8);
	// lwz r9,60(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// add r26,r26,r9
	r26.u64 = r26.u64 + ctx.r9.u64;
	// bne cr6,0x826701a8
	if (!cr6.eq) goto loc_826701A8;
	// lwz r22,44(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
loc_82670210:
	// lwz r10,68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// add r23,r18,r22
	r23.u64 = r18.u64 + r22.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r21,r21,r11
	r21.u64 = r21.u64 + r11.u64;
	// add r20,r20,r11
	r20.u64 = r20.u64 + r11.u64;
	// add r19,r19,r11
	r19.u64 = r19.u64 + r11.u64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// add r16,r16,r11
	r16.u64 = r16.u64 + r11.u64;
	// add r22,r6,r11
	r22.u64 = ctx.r6.u64 + r11.u64;
	// add r15,r15,r11
	r15.u64 = r15.u64 + r11.u64;
	// add r14,r14,r11
	r14.u64 = r14.u64 + r11.u64;
	// add r9,r4,r11
	ctx.r9.u64 = ctx.r4.u64 + r11.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826702b0
	if (!cr6.gt) goto loc_826702B0;
	// mr r27,r23
	r27.u64 = r23.u64;
	// mr r28,r22
	r28.u64 = r22.u64;
	// mr r29,r15
	r29.u64 = r15.u64;
	// mr r30,r14
	r30.u64 = r14.u64;
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
loc_8267025C:
	// lbz r25,0(r30)
	r25.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r26,0(r31)
	r26.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// lbz r24,0(r29)
	r24.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// add r26,r26,r25
	r26.u64 = r26.u64 + r25.u64;
	// lbz r25,0(r28)
	r25.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// rlwinm r6,r26,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// add r28,r28,r7
	r28.u64 = r28.u64 + ctx.r7.u64;
	// subf r26,r26,r6
	r26.s64 = ctx.r6.s64 - r26.s64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r26,r26,r24
	r26.u64 = r26.u64 + r24.u64;
	// add r6,r26,r25
	ctx.r6.u64 = r26.u64 + r25.u64;
	// srawi r6,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 4;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// stb r6,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r6.u8);
	// lwz r6,60(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// add r27,r27,r6
	r27.u64 = r27.u64 + ctx.r6.u64;
	// bne cr6,0x8267025c
	if (!cr6.eq) goto loc_8267025C;
loc_826702B0:
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r6,r22,r11
	ctx.r6.u64 = r22.u64 + r11.u64;
	// lwz r22,44(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// add r18,r23,r10
	r18.u64 = r23.u64 + ctx.r10.u64;
	// lwz r10,-160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// add r21,r21,r11
	r21.u64 = r21.u64 + r11.u64;
	// add r20,r20,r11
	r20.u64 = r20.u64 + r11.u64;
	// add r19,r19,r11
	r19.u64 = r19.u64 + r11.u64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// add r16,r16,r11
	r16.u64 = r16.u64 + r11.u64;
	// add r15,r15,r11
	r15.u64 = r15.u64 + r11.u64;
	// add r14,r14,r11
	r14.u64 = r14.u64 + r11.u64;
	// add r4,r9,r11
	ctx.r4.u64 = ctx.r9.u64 + r11.u64;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// blt cr6,0x82670184
	if (cr6.lt) goto loc_82670184;
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// rotlwi r21,r10,0
	r21.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r19,-156(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
	// lwz r23,60(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// lwz r20,68(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
loc_82670304:
	// addi r9,r19,-3
	ctx.r9.s64 = r19.s64 + -3;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x826703ac
	if (!cr6.gt) goto loc_826703AC;
	// mullw r8,r9,r5
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r9,-3
	r11.s64 = ctx.r9.s64 + -3;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r29,r6,r3
	r29.u64 = ctx.r6.u64 + ctx.r3.u64;
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// addi r6,r10,-2
	ctx.r6.s64 = ctx.r10.s64 + -2;
	// addi r8,r10,2
	ctx.r8.s64 = ctx.r10.s64 + 2;
	// mullw r31,r9,r22
	r31.s64 = int64_t(ctx.r9.s32) * int64_t(r22.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r10,r8,r5
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// mullw r6,r6,r5
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r5.s32);
	// add r28,r31,r4
	r28.u64 = r31.u64 + ctx.r4.u64;
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r31,r6,r3
	r31.u64 = ctx.r6.u64 + ctx.r3.u64;
loc_82670354:
	// lbz r8,0(r29)
	ctx.r8.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r27,0(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// lbz r24,0(r31)
	r24.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// mulli r26,r8,138
	r26.s64 = ctx.r8.s64 * 138;
	// lbz r25,0(r30)
	r25.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// add r8,r24,r27
	ctx.r8.u64 = r24.u64 + r27.u64;
	// add r6,r25,r27
	ctx.r6.u64 = r25.u64 + r27.u64;
	// mulli r8,r8,88
	ctx.r8.s64 = ctx.r8.s64 * 88;
	// mulli r6,r6,29
	ctx.r6.s64 = ctx.r6.s64 * 29;
	// subf r27,r6,r8
	r27.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r8,r27,r26
	ctx.r8.u64 = r27.u64 + r26.u64;
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// srawi r8,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 8;
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r8,0(r28)
	PPC_STORE_U8(r28.u32 + 0, ctx.r8.u8);
	// add r28,r28,r23
	r28.u64 = r28.u64 + r23.u64;
	// bne cr6,0x82670354
	if (!cr6.eq) goto loc_82670354;
loc_826703AC:
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x826704ec
	if (!cr6.gt) goto loc_826704EC;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r11,r11,r22
	r11.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// addi r9,r10,3
	ctx.r9.s64 = ctx.r10.s64 + 3;
	// addi r30,r10,1
	r30.s64 = ctx.r10.s64 + 1;
	// addi r8,r10,-3
	ctx.r8.s64 = ctx.r10.s64 + -3;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mullw r31,r9,r5
	r31.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// mullw r9,r30,r5
	ctx.r9.s64 = int64_t(r30.s32) * int64_t(ctx.r5.s32);
	// mullw r6,r8,r5
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// add r28,r11,r4
	r28.u64 = r11.u64 + ctx.r4.u64;
	// add r29,r31,r3
	r29.u64 = r31.u64 + ctx.r3.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r30,r6,r3
	r30.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r31,r9,r3
	r31.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
loc_826703F8:
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r27,0(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lbz r25,0(r30)
	r25.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// add r27,r9,r27
	r27.u64 = ctx.r9.u64 + r27.u64;
	// lbz r26,0(r29)
	r26.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// rlwinm r9,r27,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 3) & 0xFFFFFFF8;
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// subf r9,r27,r9
	ctx.r9.s64 = ctx.r9.s64 - r27.s64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// add r9,r9,r25
	ctx.r9.u64 = ctx.r9.u64 + r25.u64;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// srawi r9,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// stb r9,0(r28)
	PPC_STORE_U8(r28.u32 + 0, ctx.r9.u8);
	// add r28,r28,r23
	r28.u64 = r28.u64 + r23.u64;
	// bne cr6,0x826703f8
	if (!cr6.eq) goto loc_826703F8;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x826704ec
	if (!cr6.gt) goto loc_826704EC;
	// mullw r9,r21,r5
	ctx.r9.s64 = int64_t(r21.s32) * int64_t(ctx.r5.s32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r21,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r21,-3
	r11.s64 = r21.s64 + -3;
	// add r29,r9,r3
	r29.u64 = ctx.r9.u64 + ctx.r3.u64;
	// addi r9,r10,2
	ctx.r9.s64 = ctx.r10.s64 + 2;
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// addi r8,r10,-2
	ctx.r8.s64 = ctx.r10.s64 + -2;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r10,r9,r5
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// mullw r6,r21,r22
	ctx.r6.s64 = int64_t(r21.s32) * int64_t(r22.s32);
	// mullw r9,r8,r5
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
	// add r28,r6,r4
	r28.u64 = ctx.r6.u64 + ctx.r4.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r31,r9,r3
	r31.u64 = ctx.r9.u64 + ctx.r3.u64;
loc_82670494:
	// lbz r8,0(r29)
	ctx.r8.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r27,0(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// lbz r25,0(r31)
	r25.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// mulli r26,r8,138
	r26.s64 = ctx.r8.s64 * 138;
	// lbz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + r27.u64;
	// add r8,r25,r27
	ctx.r8.u64 = r25.u64 + r27.u64;
	// mulli r9,r9,29
	ctx.r9.s64 = ctx.r9.s64 * 29;
	// mulli r8,r8,88
	ctx.r8.s64 = ctx.r8.s64 * 88;
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r9,0(r28)
	PPC_STORE_U8(r28.u32 + 0, ctx.r9.u8);
	// add r28,r28,r23
	r28.u64 = r28.u64 + r23.u64;
	// bne cr6,0x82670494
	if (!cr6.eq) goto loc_82670494;
loc_826704EC:
	// addi r11,r21,1
	r11.s64 = r21.s64 + 1;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x82670588
	if (!cr6.gt) goto loc_82670588;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r11,r11,r22
	r11.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// addi r9,r10,3
	ctx.r9.s64 = ctx.r10.s64 + 3;
	// addi r30,r10,1
	r30.s64 = ctx.r10.s64 + 1;
	// addi r8,r10,-3
	ctx.r8.s64 = ctx.r10.s64 + -3;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mullw r31,r9,r5
	r31.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// mullw r9,r30,r5
	ctx.r9.s64 = int64_t(r30.s32) * int64_t(ctx.r5.s32);
	// mullw r6,r8,r5
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// add r28,r11,r4
	r28.u64 = r11.u64 + ctx.r4.u64;
	// add r29,r31,r3
	r29.u64 = r31.u64 + ctx.r3.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r30,r6,r3
	r30.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r31,r9,r3
	r31.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
loc_82670538:
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r27,0(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lbz r25,0(r30)
	r25.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// add r27,r9,r27
	r27.u64 = ctx.r9.u64 + r27.u64;
	// lbz r26,0(r29)
	r26.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// rlwinm r9,r27,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 3) & 0xFFFFFFF8;
	// add r29,r29,r7
	r29.u64 = r29.u64 + ctx.r7.u64;
	// subf r9,r27,r9
	ctx.r9.s64 = ctx.r9.s64 - r27.s64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// add r9,r9,r25
	ctx.r9.u64 = ctx.r9.u64 + r25.u64;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// srawi r9,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// stb r9,0(r28)
	PPC_STORE_U8(r28.u32 + 0, ctx.r9.u8);
	// add r28,r28,r23
	r28.u64 = r28.u64 + r23.u64;
	// bne cr6,0x82670538
	if (!cr6.eq) goto loc_82670538;
loc_82670588:
	// addi r9,r19,-1
	ctx.r9.s64 = r19.s64 + -1;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x82670614
	if (!cr6.gt) goto loc_82670614;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r9,-3
	ctx.r10.s64 = ctx.r9.s64 + -3;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// mullw r8,r9,r5
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// mullw r6,r9,r22
	ctx.r6.s64 = int64_t(ctx.r9.s32) * int64_t(r22.s32);
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r11,r3
	r31.u64 = r11.u64 + ctx.r3.u64;
	// add r29,r6,r4
	r29.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r30,r8,r3
	r30.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_826705CC:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r6,0(r30)
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lbz r28,0(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// mulli r8,r8,29
	ctx.r8.s64 = ctx.r8.s64 * 29;
	// mulli r6,r6,197
	ctx.r6.s64 = ctx.r6.s64 * 197;
	// subf r6,r8,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r8.s64;
	// mulli r28,r28,88
	r28.s64 = r28.s64 * 88;
	// add r8,r6,r28
	ctx.r8.u64 = ctx.r6.u64 + r28.u64;
	// add r31,r31,r7
	r31.u64 = r31.u64 + ctx.r7.u64;
	// srawi r8,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 8;
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r8,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r8.u8);
	// add r29,r29,r23
	r29.u64 = r29.u64 + r23.u64;
	// bne cr6,0x826705cc
	if (!cr6.eq) goto loc_826705CC;
loc_82670614:
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x8267069c
	if (!cr6.gt) goto loc_8267069C;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r11,r11,r22
	r11.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// addi r6,r10,-1
	ctx.r6.s64 = ctx.r10.s64 + -1;
	// addi r8,r10,-3
	ctx.r8.s64 = ctx.r10.s64 + -3;
	// mullw r10,r9,r5
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// mullw r9,r6,r5
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r5.s32);
	// mullw r8,r8,r5
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r6,r8,r3
	ctx.r6.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
loc_82670654:
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lbz r31,0(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// add r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 + ctx.r5.u64;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r8,r3,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// subf r3,r3,r8
	ctx.r3.s64 = ctx.r8.s64 - ctx.r3.s64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// add r8,r3,r5
	ctx.r8.u64 = ctx.r3.u64 + ctx.r5.u64;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// stb r8,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r8.u8);
	// add r4,r4,r23
	ctx.r4.u64 = ctx.r4.u64 + r23.u64;
	// bne cr6,0x82670654
	if (!cr6.eq) goto loc_82670654;
loc_8267069C:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826706A0"))) PPC_WEAK_FUNC(sub_826706A0);
PPC_FUNC_IMPL(__imp__sub_826706A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,260(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// subf r24,r9,r10
	r24.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r24,16
	cr6.compare<int32_t>(r24.s32, 16, xer);
	// lwz r11,14644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14644);
	// lwz r10,14588(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14588);
	// lwz r29,14540(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 14540);
	// rlwinm r23,r11,1,0,30
	r23.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,14596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14596);
	// lwz r30,14504(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 14504);
	// lwz r25,14544(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 14544);
	// lwz r26,14548(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 14548);
	// lwz r27,14508(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 14508);
	// lwz r28,14512(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 14512);
	// add r29,r29,r10
	r29.u64 = r29.u64 + ctx.r10.u64;
	// mullw r9,r11,r9
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// srawi r22,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r22.s64 = ctx.r10.s32 >> 2;
	// add r29,r29,r3
	r29.u64 = r29.u64 + ctx.r3.u64;
	// add r3,r30,r9
	ctx.r3.u64 = r30.u64 + ctx.r9.u64;
	// addze r10,r22
	temp.s64 = r22.s64 + xer.ca;
	xer.ca = temp.u32 < r22.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r22,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r22.s64 = ctx.r9.s32 >> 2;
	// add r30,r3,r6
	r30.u64 = ctx.r3.u64 + ctx.r6.u64;
	// add r6,r25,r10
	ctx.r6.u64 = r25.u64 + ctx.r10.u64;
	// add r10,r26,r10
	ctx.r10.u64 = r26.u64 + ctx.r10.u64;
	// addze r9,r22
	temp.s64 = r22.s64 + xer.ca;
	xer.ca = temp.u32 < r22.u32;
	ctx.r9.s64 = temp.s64;
	// add r26,r6,r4
	r26.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r25,r10,r5
	r25.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r6,r27,r9
	ctx.r6.u64 = r27.u64 + ctx.r9.u64;
	// add r10,r28,r9
	ctx.r10.u64 = r28.u64 + ctx.r9.u64;
	// add r27,r6,r7
	r27.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r22,r10,r8
	r22.u64 = ctx.r10.u64 + ctx.r8.u64;
	// ble cr6,0x826707e8
	if (!cr6.gt) goto loc_826707E8;
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82670774
	if (!cr6.gt) goto loc_82670774;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
loc_8267073C:
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82670768
	if (!cr6.gt) goto loc_82670768;
loc_82670748:
	// lbz r11,0(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// stb r11,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r11.u8);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lwz r11,14596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14596);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x82670748
	if (cr6.lt) goto loc_82670748;
loc_82670768:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8267073c
	if (!cr6.eq) goto loc_8267073C;
loc_82670774:
	// lwz r9,14480(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14480);
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// lwz r11,14596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14596);
	// li r8,1
	ctx.r8.s64 = 1;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// lwz r5,14528(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 14528);
	// li r7,1
	ctx.r7.s64 = 1;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addze r6,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r6.s64 = temp.s64;
	// bl 0x8266fdb8
	sub_8266FDB8(ctx, base);
	// lwz r11,14480(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14480);
	// lwz r8,14596(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 14596);
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r5,14528(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 14528);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// addze r9,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r9.s64 = temp.s64;
	// srawi r11,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r11.s64 = ctx.r8.s32 >> 1;
	// li r8,1
	ctx.r8.s64 = 1;
	// addze r6,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r6.s64 = temp.s64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8266fdb8
	sub_8266FDB8(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_826707E8:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82670898
	if (!cr6.gt) goto loc_82670898;
	// mr r28,r24
	r28.u64 = r24.u64;
loc_826707F4:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r5,14480(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 14480);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,14596(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14596);
	// lwz r11,14588(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14588);
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// add r30,r10,r30
	r30.u64 = ctx.r10.u64 + r30.u64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x826707f4
	if (!cr6.eq) goto loc_826707F4;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82670898
	if (!cr6.gt) goto loc_82670898;
	// addi r11,r24,-1
	r11.s64 = r24.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_82670834:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r5,14488(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 14488);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,14648(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14648);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r26,r23,r26
	r26.u64 = r23.u64 + r26.u64;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82670834
	if (!cr6.eq) goto loc_82670834;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82670898
	if (!cr6.gt) goto loc_82670898;
	// addi r11,r24,-1
	r11.s64 = r24.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_82670870:
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lwz r5,14488(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 14488);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,14648(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14648);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r25,r23,r25
	r25.u64 = r23.u64 + r25.u64;
	// add r22,r11,r22
	r22.u64 = r11.u64 + r22.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82670870
	if (!cr6.eq) goto loc_82670870;
loc_82670898:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_826708A0"))) PPC_WEAK_FUNC(sub_826708A0);
PPC_FUNC_IMPL(__imp__sub_826708A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,12889
	ctx.r10.s64 = 844693504;
	// lwz r28,0(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r7,12593
	ctx.r7.s64 = 825294848;
	// ori r9,r10,21849
	ctx.r9.u64 = ctx.r10.u64 | 21849;
	// ori r27,r7,13392
	r27.u64 = ctx.r7.u64 | 13392;
	// lis r11,12850
	r11.s64 = 842137600;
	// lis r10,21849
	ctx.r10.s64 = 1431896064;
	// lis r7,22101
	ctx.r7.s64 = 1448411136;
	// ori r5,r11,13392
	ctx.r5.u64 = r11.u64 | 13392;
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// ori r8,r10,22105
	ctx.r8.u64 = ctx.r10.u64 | 22105;
	// ori r31,r7,22857
	r31.u64 = ctx.r7.u64 | 22857;
	// lis r10,22870
	ctx.r10.s64 = 1498808320;
	// lis r7,12338
	ctx.r7.s64 = 808583168;
	// ori r10,r10,22869
	ctx.r10.u64 = ctx.r10.u64 | 22869;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r29,1
	r29.s64 = 1;
	// ori r30,r7,13385
	r30.u64 = ctx.r7.u64 | 13385;
	// cmplw cr6,r11,r5
	cr6.compare<uint32_t>(r11.u32, ctx.r5.u32, xer);
	// bgt cr6,0x82670930
	if (cr6.gt) goto loc_82670930;
	// beq cr6,0x82670928
	if (cr6.eq) goto loc_82670928;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bgt cr6,0x82670920
	if (cr6.gt) goto loc_82670920;
	// beq cr6,0x82670928
	if (cr6.eq) goto loc_82670928;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82670958
	if (cr6.eq) goto loc_82670958;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// beq cr6,0x82670958
	if (cr6.eq) goto loc_82670958;
	// b 0x8267095c
	goto loc_8267095C;
loc_82670920:
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bne cr6,0x8267095c
	if (!cr6.eq) goto loc_8267095C;
loc_82670928:
	// stw r29,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r29.u32);
	// b 0x8267095c
	goto loc_8267095C;
loc_82670930:
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bgt cr6,0x82670950
	if (cr6.gt) goto loc_82670950;
	// beq cr6,0x82670928
	if (cr6.eq) goto loc_82670928;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x82670958
	if (cr6.eq) goto loc_82670958;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// beq cr6,0x82670958
	if (cr6.eq) goto loc_82670958;
	// b 0x8267095c
	goto loc_8267095C;
loc_82670950:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x8267095c
	if (!cr6.eq) goto loc_8267095C;
loc_82670958:
	// stw r6,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r6.u32);
loc_8267095C:
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lis r7,12849
	ctx.r7.s64 = 842072064;
	// ori r7,r7,22105
	ctx.r7.u64 = ctx.r7.u64 | 22105;
	// lwz r11,16(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// cmplw cr6,r11,r5
	cr6.compare<uint32_t>(r11.u32, ctx.r5.u32, xer);
	// bgt cr6,0x826709b0
	if (cr6.gt) goto loc_826709B0;
	// beq cr6,0x826709a8
	if (cr6.eq) goto loc_826709A8;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bgt cr6,0x82670998
	if (cr6.gt) goto loc_82670998;
	// beq cr6,0x826709a8
	if (cr6.eq) goto loc_826709A8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826709d8
	if (cr6.eq) goto loc_826709D8;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// beq cr6,0x826709d8
	if (cr6.eq) goto loc_826709D8;
	// b 0x826709dc
	goto loc_826709DC;
loc_82670998:
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// beq cr6,0x826709a8
	if (cr6.eq) goto loc_826709A8;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bne cr6,0x826709dc
	if (!cr6.eq) goto loc_826709DC;
loc_826709A8:
	// stw r29,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r29.u32);
	// b 0x826709dc
	goto loc_826709DC;
loc_826709B0:
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bgt cr6,0x826709d0
	if (cr6.gt) goto loc_826709D0;
	// beq cr6,0x826709a8
	if (cr6.eq) goto loc_826709A8;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x826709d8
	if (cr6.eq) goto loc_826709D8;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// beq cr6,0x826709d8
	if (cr6.eq) goto loc_826709D8;
	// b 0x826709dc
	goto loc_826709DC;
loc_826709D0:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x826709dc
	if (!cr6.eq) goto loc_826709DC;
loc_826709D8:
	// stw r6,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r6.u32);
loc_826709DC:
	// stw r28,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r28.u32);
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826709fc
	if (cr6.eq) goto loc_826709FC;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// beq cr6,0x826709fc
	if (cr6.eq) goto loc_826709FC;
	// stw r29,14472(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14472, r29.u32);
	// b 0x82670a14
	goto loc_82670A14;
loc_826709FC:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,-1
	r11.s64 = -1;
	// bgt cr6,0x82670a10
	if (cr6.gt) goto loc_82670A10;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_82670A10:
	// stw r11,14472(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14472, r11.u32);
loc_82670A14:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// stw r11,14516(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14516, r11.u32);
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r11,14520(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14520, r11.u32);
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bgt cr6,0x82670a5c
	if (cr6.gt) goto loc_82670A5C;
	// beq cr6,0x82670a6c
	if (cr6.eq) goto loc_82670A6C;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// beq cr6,0x82670a6c
	if (cr6.eq) goto loc_82670A6C;
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bne cr6,0x82670a7c
	if (!cr6.eq) goto loc_82670A7C;
	// lwz r11,14516(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14516);
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// b 0x82670a74
	goto loc_82670A74;
loc_82670A5C:
	// cmplw cr6,r11,r5
	cr6.compare<uint32_t>(r11.u32, ctx.r5.u32, xer);
	// beq cr6,0x82670a6c
	if (cr6.eq) goto loc_82670A6C;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bne cr6,0x82670a7c
	if (!cr6.eq) goto loc_82670A7C;
loc_82670A6C:
	// lwz r11,14516(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14516);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
loc_82670A74:
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// stw r11,14524(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14524, r11.u32);
loc_82670A7C:
	// lwz r11,16(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82670a98
	if (cr6.eq) goto loc_82670A98;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// beq cr6,0x82670a98
	if (cr6.eq) goto loc_82670A98;
	// stw r29,14476(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14476, r29.u32);
	// b 0x82670ab0
	goto loc_82670AB0;
loc_82670A98:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,-1
	r11.s64 = -1;
	// bgt cr6,0x82670aac
	if (cr6.gt) goto loc_82670AAC;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_82670AAC:
	// stw r11,14476(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14476, r11.u32);
loc_82670AB0:
	// lwz r11,4(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// stw r11,14480(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14480, r11.u32);
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r11,14484(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14484, r11.u32);
	// lwz r11,16(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bgt cr6,0x82670af8
	if (cr6.gt) goto loc_82670AF8;
	// beq cr6,0x82670b08
	if (cr6.eq) goto loc_82670B08;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// beq cr6,0x82670b08
	if (cr6.eq) goto loc_82670B08;
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bne cr6,0x82670b18
	if (!cr6.eq) goto loc_82670B18;
	// lwz r11,14480(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14480);
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// b 0x82670b10
	goto loc_82670B10;
loc_82670AF8:
	// cmplw cr6,r11,r5
	cr6.compare<uint32_t>(r11.u32, ctx.r5.u32, xer);
	// beq cr6,0x82670b08
	if (cr6.eq) goto loc_82670B08;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bne cr6,0x82670b18
	if (!cr6.eq) goto loc_82670B18;
loc_82670B08:
	// lwz r11,14480(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14480);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
loc_82670B10:
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// stw r11,14488(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14488, r11.u32);
loc_82670B18:
	// lwz r11,14600(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14600);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// bne cr6,0x82670b2c
	if (!cr6.eq) goto loc_82670B2C;
	// lwz r7,8(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
loc_82670B2C:
	// lwz r11,14596(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14596);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// bne cr6,0x82670b40
	if (!cr6.eq) goto loc_82670B40;
	// lwz r6,4(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
loc_82670B40:
	// lwz r11,14592(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14592);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82670b50
	if (!cr6.eq) goto loc_82670B50;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
loc_82670B50:
	// lwz r4,14588(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14588);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x82670b60
	if (!cr6.eq) goto loc_82670B60;
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 4);
loc_82670B60:
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// bl 0x8266f4f0
	sub_8266F4F0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82670B70"))) PPC_WEAK_FUNC(sub_82670B70);
PPC_FUNC_IMPL(__imp__sub_82670B70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r11,1
	r11.s64 = 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,14560(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14560, r11.u32);
	// stw r10,14556(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14556, ctx.r10.u32);
	// stw r11,14552(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14552, r11.u32);
	// b 0x8266fac8
	sub_8266FAC8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82670B88"))) PPC_WEAK_FUNC(sub_82670B88);
PPC_FUNC_IMPL(__imp__sub_82670B88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,12(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82670bb0
	if (!cr6.eq) goto loc_82670BB0;
	// lwz r11,56(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 56);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,20(r5)
	PPC_STORE_U32(ctx.r5.u32 + 20, r11.u32);
	// b 0x82670bfc
	goto loc_82670BFC;
loc_82670BB0:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82670bfc
	if (!cr6.eq) goto loc_82670BFC;
	// lwz r11,60(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 60);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,24(r5)
	PPC_STORE_U32(ctx.r5.u32 + 24, r11.u32);
	// beq cr6,0x82670bd4
	if (cr6.eq) goto loc_82670BD4;
	// stw r6,28(r5)
	PPC_STORE_U32(ctx.r5.u32 + 28, ctx.r6.u32);
	// b 0x82670be0
	goto loc_82670BE0;
loc_82670BD4:
	// lwz r11,64(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,28(r5)
	PPC_STORE_U32(ctx.r5.u32 + 28, r11.u32);
loc_82670BE0:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82670bf0
	if (cr6.eq) goto loc_82670BF0;
	// stw r7,32(r5)
	PPC_STORE_U32(ctx.r5.u32 + 32, ctx.r7.u32);
	// b 0x82670bfc
	goto loc_82670BFC;
loc_82670BF0:
	// lwz r11,68(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,32(r5)
	PPC_STORE_U32(ctx.r5.u32 + 32, r11.u32);
loc_82670BFC:
	// lwz r11,16(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82670c18
	if (!cr6.eq) goto loc_82670C18;
	// lwz r11,72(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 72);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// stw r11,36(r5)
	PPC_STORE_U32(ctx.r5.u32 + 36, r11.u32);
	// b 0x82670c48
	goto loc_82670C48;
loc_82670C18:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82670c48
	if (!cr6.eq) goto loc_82670C48;
	// lwz r11,52(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 52);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x82670c48
	if (cr6.eq) goto loc_82670C48;
	// lwz r10,76(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 76);
	// lwz r11,80(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 80);
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// stw r4,40(r5)
	PPC_STORE_U32(ctx.r5.u32 + 40, ctx.r4.u32);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// stw r10,44(r5)
	PPC_STORE_U32(ctx.r5.u32 + 44, ctx.r10.u32);
	// stw r11,48(r5)
	PPC_STORE_U32(ctx.r5.u32 + 48, r11.u32);
loc_82670C48:
	// lwz r11,14560(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 14560);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x82670c5c
	if (!cr6.eq) goto loc_82670C5C;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// bl 0x8266fb58
	sub_8266FB58(ctx, base);
loc_82670C5C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82670C70"))) PPC_WEAK_FUNC(sub_82670C70);
PPC_FUNC_IMPL(__imp__sub_82670C70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r30,16(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// lwz r29,16(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// beq cr6,0x82670ca0
	if (cr6.eq) goto loc_82670CA0;
	// cmpwi cr6,r30,3
	cr6.compare<int32_t>(r30.s32, 3, xer);
	// bne cr6,0x82670ca8
	if (!cr6.eq) goto loc_82670CA8;
loc_82670CA0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8266ebe0
	sub_8266EBE0(ctx, base);
loc_82670CA8:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x82670cb8
	if (cr6.eq) goto loc_82670CB8;
	// cmpwi cr6,r29,3
	cr6.compare<int32_t>(r29.s32, 3, xer);
	// bne cr6,0x82670cc0
	if (!cr6.eq) goto loc_82670CC0;
loc_82670CB8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8266ea38
	sub_8266EA38(ctx, base);
loc_82670CC0:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x82670cd8
	if (!cr6.eq) goto loc_82670CD8;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// beq cr6,0x82670cf0
	if (cr6.eq) goto loc_82670CF0;
loc_82670CD8:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x82670cf8
	if (!cr6.eq) goto loc_82670CF8;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bne cr6,0x82670cf8
	if (!cr6.eq) goto loc_82670CF8;
loc_82670CF0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8266ee20
	sub_8266EE20(ctx, base);
loc_82670CF8:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8266f1e0
	sub_8266F1E0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82670d6c
	if (!cr6.eq) goto loc_82670D6C;
	// lwz r9,116(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r8,124(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 124);
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// lwz r10,120(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r9,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r9.u32);
	// rlwinm r9,r11,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// stw r8,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r8.u32);
	// rlwinm r8,r10,16,0,15
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
	// stw r10,104(r31)
	PPC_STORE_U32(r31.u32 + 104, ctx.r10.u32);
	// bl 0x8266f1e0
	sub_8266F1E0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82670d6c
	if (!cr6.eq) goto loc_82670D6C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8266f2e8
	sub_8266F2E8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x82670d68
	if (!cr6.eq) goto loc_82670D68;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826708a0
	sub_826708A0(ctx, base);
loc_82670D68:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_82670D6C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82670D74"))) PPC_WEAK_FUNC(sub_82670D74);
PPC_FUNC_IMPL(__imp__sub_82670D74) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82670D78"))) PPC_WEAK_FUNC(sub_82670D78);
PPC_FUNC_IMPL(__imp__sub_82670D78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bccc
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,292(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// subf r19,r9,r10
	r19.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r11,52(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// lwz r28,14596(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 14596);
	// lwz r10,14588(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14588);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r30,14504(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 14504);
	// mullw r11,r28,r9
	r11.s64 = int64_t(r28.s32) * int64_t(ctx.r9.s32);
	// lwz r29,14540(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 14540);
	// lwz r24,14544(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 14544);
	// lwz r25,14548(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 14548);
	// lwz r26,14508(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 14508);
	// lwz r27,14512(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 14512);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r9,r30,r11
	ctx.r9.u64 = r30.u64 + r11.u64;
	// add r30,r9,r6
	r30.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r29,r10
	ctx.r9.u64 = r29.u64 + ctx.r10.u64;
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// add r29,r9,r3
	r29.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r6,r24,r10
	ctx.r6.u64 = r24.u64 + ctx.r10.u64;
	// lwz r24,14480(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 14480);
	// add r9,r25,r10
	ctx.r9.u64 = r25.u64 + ctx.r10.u64;
	// add r10,r26,r11
	ctx.r10.u64 = r26.u64 + r11.u64;
	// add r11,r27,r11
	r11.u64 = r27.u64 + r11.u64;
	// add r25,r6,r4
	r25.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r22,r9,r5
	r22.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r26,r10,r7
	r26.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r23,r11,r8
	r23.u64 = r11.u64 + ctx.r8.u64;
	// beq cr6,0x82670e40
	if (cr6.eq) goto loc_82670E40;
	// lwz r28,14624(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 14624);
	// cmpw cr6,r24,r28
	cr6.compare<int32_t>(r24.s32, r28.s32, xer);
	// blt cr6,0x82670e0c
	if (cr6.lt) goto loc_82670E0C;
	// rotlwi r24,r28,0
	r24.u64 = __builtin_rotateleft32(r28.u32, 0);
loc_82670E0C:
	// lwz r11,14488(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14488);
	// lwz r20,14628(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 14628);
	// mr r21,r11
	r21.u64 = r11.u64;
	// cmpw cr6,r11,r20
	cr6.compare<int32_t>(r11.s32, r20.s32, xer);
	// blt cr6,0x82670e24
	if (cr6.lt) goto loc_82670E24;
	// rotlwi r21,r20,0
	r21.u64 = __builtin_rotateleft32(r20.u32, 0);
loc_82670E24:
	// lwz r17,14632(r31)
	r17.u64 = PPC_LOAD_U32(r31.u32 + 14632);
	// cmpw cr6,r11,r17
	cr6.compare<int32_t>(r11.s32, r17.s32, xer);
	// bge cr6,0x82670e38
	if (!cr6.lt) goto loc_82670E38;
	// mr r18,r11
	r18.u64 = r11.u64;
	// b 0x82670e58
	goto loc_82670E58;
loc_82670E38:
	// lwz r18,14632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + 14632);
	// b 0x82670e58
	goto loc_82670E58;
loc_82670E40:
	// lwz r11,14648(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14648);
	// lwz r10,14488(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14488);
	// mr r20,r11
	r20.u64 = r11.u64;
	// mr r21,r10
	r21.u64 = ctx.r10.u64;
	// mr r17,r11
	r17.u64 = r11.u64;
	// mr r18,r10
	r18.u64 = ctx.r10.u64;
loc_82670E58:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x823b4eb0
	sub_823B4EB0(ctx, base);
	// not r11,r3
	r11.u64 = ~ctx.r3.u64;
	// rlwinm r10,r11,0,21,21
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x400;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82670eb4
	if (cr6.eq) goto loc_82670EB4;
	// rlwinm r11,r11,0,22,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82670eb4
	if (cr6.eq) goto loc_82670EB4;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x82670ee8
	if (!cr6.gt) goto loc_82670EE8;
	// mr r27,r19
	r27.u64 = r19.u64;
loc_82670E88:
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x826577d0
	sub_826577D0(ctx, base);
	// lwz r11,14588(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14588);
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// add r30,r28,r30
	r30.u64 = r28.u64 + r30.u64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82670e88
	if (!cr6.eq) goto loc_82670E88;
	// b 0x82670ee8
	goto loc_82670EE8;
loc_82670EB4:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x82670ee8
	if (!cr6.gt) goto loc_82670EE8;
	// mr r27,r19
	r27.u64 = r19.u64;
loc_82670EC0:
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82657ad0
	sub_82657AD0(ctx, base);
	// lwz r11,14588(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14588);
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// add r30,r28,r30
	r30.u64 = r28.u64 + r30.u64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82670ec0
	if (!cr6.eq) goto loc_82670EC0;
loc_82670EE8:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x823b4eb0
	sub_823B4EB0(ctx, base);
	// not r11,r3
	r11.u64 = ~ctx.r3.u64;
	// rlwinm r10,r11,0,21,21
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x400;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82670f4c
	if (cr6.eq) goto loc_82670F4C;
	// rlwinm r11,r11,0,22,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82670f4c
	if (cr6.eq) goto loc_82670F4C;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x82670f88
	if (!cr6.gt) goto loc_82670F88;
	// addi r11,r19,-1
	r11.s64 = r19.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_82670F20:
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x826577d0
	sub_826577D0(ctx, base);
	// lwz r11,14644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14644);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r26,r20,r26
	r26.u64 = r20.u64 + r26.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82670f20
	if (!cr6.eq) goto loc_82670F20;
	// b 0x82670f88
	goto loc_82670F88;
loc_82670F4C:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x82670f88
	if (!cr6.gt) goto loc_82670F88;
	// addi r11,r19,-1
	r11.s64 = r19.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_82670F60:
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82657ad0
	sub_82657AD0(ctx, base);
	// lwz r11,14644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14644);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r26,r20,r26
	r26.u64 = r20.u64 + r26.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82670f60
	if (!cr6.eq) goto loc_82670F60;
loc_82670F88:
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x823b4eb0
	sub_823B4EB0(ctx, base);
	// not r11,r3
	r11.u64 = ~ctx.r3.u64;
	// rlwinm r10,r11,0,21,21
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x400;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82670ff0
	if (cr6.eq) goto loc_82670FF0;
	// rlwinm r11,r11,0,22,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82670ff0
	if (cr6.eq) goto loc_82670FF0;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x8267102c
	if (!cr6.gt) goto loc_8267102C;
	// addi r11,r19,-1
	r11.s64 = r19.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_82670FC0:
	// mr r5,r18
	ctx.r5.u64 = r18.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x826577d0
	sub_826577D0(ctx, base);
	// lwz r11,14644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14644);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r23,r17,r23
	r23.u64 = r17.u64 + r23.u64;
	// add r22,r11,r22
	r22.u64 = r11.u64 + r22.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82670fc0
	if (!cr6.eq) goto loc_82670FC0;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd1c
	return;
loc_82670FF0:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x8267102c
	if (!cr6.gt) goto loc_8267102C;
	// addi r11,r19,-1
	r11.s64 = r19.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_82671004:
	// mr r5,r18
	ctx.r5.u64 = r18.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82657ad0
	sub_82657AD0(ctx, base);
	// lwz r11,14644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14644);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r23,r17,r23
	r23.u64 = r17.u64 + r23.u64;
	// add r22,r11,r22
	r22.u64 = r11.u64 + r22.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82671004
	if (!cr6.eq) goto loc_82671004;
loc_8267102C:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_82671034"))) PPC_WEAK_FUNC(sub_82671034);
PPC_FUNC_IMPL(__imp__sub_82671034) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82671038"))) PPC_WEAK_FUNC(sub_82671038);
PPC_FUNC_IMPL(__imp__sub_82671038) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// cmpw cr6,r6,r28
	cr6.compare<int32_t>(ctx.r6.s32, r28.s32, xer);
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// blt cr6,0x82671068
	if (cr6.lt) goto loc_82671068;
	// mr r27,r28
	r27.u64 = r28.u64;
loc_82671068:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823b4eb0
	sub_823B4EB0(ctx, base);
	// not r11,r3
	r11.u64 = ~ctx.r3.u64;
	// rlwinm r10,r11,0,21,21
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x400;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826710c0
	if (cr6.eq) goto loc_826710C0;
	// rlwinm r11,r11,0,22,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826710c0
	if (cr6.eq) goto loc_826710C0;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x826710ec
	if (!cr6.gt) goto loc_826710EC;
loc_82671094:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826577d0
	sub_826577D0(ctx, base);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r31,r31,r28
	r31.u64 = r31.u64 + r28.u64;
	// add r30,r30,r26
	r30.u64 = r30.u64 + r26.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82671094
	if (!cr6.eq) goto loc_82671094;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_826710C0:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x826710ec
	if (!cr6.gt) goto loc_826710EC;
loc_826710C8:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82657ad0
	sub_82657AD0(ctx, base);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r31,r31,r28
	r31.u64 = r31.u64 + r28.u64;
	// add r30,r30,r26
	r30.u64 = r30.u64 + r26.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x826710c8
	if (!cr6.eq) goto loc_826710C8;
loc_826710EC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_826710F4"))) PPC_WEAK_FUNC(sub_826710F4);
PPC_FUNC_IMPL(__imp__sub_826710F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826710F8"))) PPC_WEAK_FUNC(sub_826710F8);
PPC_FUNC_IMPL(__imp__sub_826710F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5e0
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r9,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r9.u32);
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// lwz r9,412(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// mr r22,r8
	r22.u64 = ctx.r8.u64;
	// stw r10,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, ctx.r10.u32);
	// addi r11,r19,-1
	r11.s64 = r19.s64 + -1;
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// mr r15,r7
	r15.u64 = ctx.r7.u64;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f13,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// mr r17,r4
	r17.u64 = ctx.r4.u64;
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// srawi r24,r19,1
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x1) != 0);
	r24.s64 = r19.s32 >> 1;
	// mr r14,r5
	r14.u64 = ctx.r5.u64;
	// mr r16,r6
	r16.u64 = ctx.r6.u64;
	// srawi r18,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	r18.s64 = r17.s32 >> 1;
	// fsub f28,f13,f0
	f28.f64 = ctx.f13.f64 - f0.f64;
	// lfd f12,96(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f30,f12
	f30.f64 = double(ctx.f12.s64);
	// fsub f0,f28,f30
	f0.f64 = f28.f64 - f30.f64;
	// fadd f13,f28,f30
	ctx.f13.f64 = f28.f64 + f30.f64;
	// fctiwz f12,f0
	ctx.f12.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f12,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f12.u32);
	// lwz r23,80(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// extsw r11,r23
	r11.s64 = r23.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f26,-31368(r11)
	f26.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fsub f0,f0,f13
	f0.f64 = f0.f64 - ctx.f13.f64;
	// fcmpu cr6,f0,f26
	cr6.compare(f0.f64, f26.f64);
	// bne cr6,0x826711c0
	if (!cr6.eq) goto loc_826711C0;
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
loc_826711C0:
	// lwz r25,388(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// lwz r21,80(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// ble cr6,0x8267124c
	if (!cr6.gt) goto loc_8267124C;
	// subf r11,r21,r19
	r11.s64 = r19.s64 - r21.s64;
	// addi r28,r23,1
	r28.s64 = r23.s64 + 1;
	// addi r27,r11,-1
	r27.s64 = r11.s64 + -1;
	// mr r31,r25
	r31.u64 = r25.u64;
	// subf r26,r25,r14
	r26.s64 = r14.s64 - r25.s64;
	// subf r30,r25,r19
	r30.s64 = r19.s64 - r25.s64;
	// mr r29,r17
	r29.u64 = r17.u64;
loc_826711EC:
	// cmpw cr6,r28,r19
	cr6.compare<int32_t>(r28.s32, r19.s32, xer);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// blt cr6,0x826711fc
	if (cr6.lt) goto loc_826711FC;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
loc_826711FC:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82671210
	if (!cr6.gt) goto loc_82671210;
	// add r4,r26,r31
	ctx.r4.u64 = r26.u64 + r31.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82671210:
	// cmpw cr6,r27,r19
	cr6.compare<int32_t>(r27.s32, r19.s32, xer);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// blt cr6,0x82671220
	if (cr6.lt) goto loc_82671220;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
loc_82671220:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x8267123c
	if (!cr6.gt) goto loc_8267123C;
	// subf r11,r5,r30
	r11.s64 = r30.s64 - ctx.r5.s64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r4,r11,r22
	ctx.r4.u64 = r11.u64 + r22.u64;
	// add r3,r11,r25
	ctx.r3.u64 = r11.u64 + r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_8267123C:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r31,r31,r19
	r31.u64 = r31.u64 + r19.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x826711ec
	if (!cr6.eq) goto loc_826711EC;
loc_8267124C:
	// lwz r26,404(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// li r29,0
	r29.s64 = 0;
	// lwz r20,396(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// ble cr6,0x82671310
	if (!cr6.gt) goto loc_82671310;
	// srawi r11,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r11.s64 = r23.s32 >> 1;
	// srawi r10,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	ctx.r10.s64 = r21.s32 >> 1;
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
	// subf r11,r10,r24
	r11.s64 = r24.s64 - ctx.r10.s64;
	// addi r27,r11,-1
	r27.s64 = r11.s64 + -1;
loc_82671274:
	// cmpw cr6,r28,r24
	cr6.compare<int32_t>(r28.s32, r24.s32, xer);
	// mr r30,r28
	r30.u64 = r28.u64;
	// blt cr6,0x82671284
	if (cr6.lt) goto loc_82671284;
	// mr r30,r24
	r30.u64 = r24.u64;
loc_82671284:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x826712b4
	if (!cr6.gt) goto loc_826712B4;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mullw r31,r11,r24
	r31.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// add r4,r31,r16
	ctx.r4.u64 = r31.u64 + r16.u64;
	// add r3,r31,r20
	ctx.r3.u64 = r31.u64 + r20.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r15
	ctx.r4.u64 = r31.u64 + r15.u64;
	// add r3,r31,r26
	ctx.r3.u64 = r31.u64 + r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826712B4:
	// cmpw cr6,r27,r24
	cr6.compare<int32_t>(r27.s32, r24.s32, xer);
	// mr r30,r27
	r30.u64 = r27.u64;
	// blt cr6,0x826712c4
	if (cr6.lt) goto loc_826712C4;
	// mr r30,r24
	r30.u64 = r24.u64;
loc_826712C4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82671304
	if (!cr6.gt) goto loc_82671304;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r11,r11,r24
	r11.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// subf r31,r30,r11
	r31.s64 = r11.s64 - r30.s64;
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// add r3,r31,r20
	ctx.r3.u64 = r31.u64 + r20.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,380(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// add r3,r31,r26
	ctx.r3.u64 = r31.u64 + r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82671304:
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// cmpw cr6,r29,r17
	cr6.compare<int32_t>(r29.s32, r17.s32, xer);
	// blt cr6,0x82671274
	if (cr6.lt) goto loc_82671274;
loc_82671310:
	// addic. r11,r23,1
	xer.ca = r23.u32 > 4294967294;
	r11.s64 = r23.s64 + 1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r29,0
	r29.s64 = 0;
	// blt 0x82671320
	if (cr0.lt) goto loc_82671320;
	// mr r29,r11
	r29.u64 = r11.u64;
loc_82671320:
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// lwz r28,88(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// add r27,r29,r25
	r27.u64 = r29.u64 + r25.u64;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// subf r23,r22,r14
	r23.s64 = r14.s64 - r22.s64;
	// lfd f27,4120(r10)
	ctx.fpscr.disableFlushMode();
	f27.u64 = PPC_LOAD_U64(ctx.r10.u32 + 4120);
	// subf r25,r25,r22
	r25.s64 = r22.s64 - r25.s64;
	// lfd f29,-28592(r11)
	f29.u64 = PPC_LOAD_U64(r11.u32 + -28592);
loc_82671344:
	// cmpw cr6,r21,r19
	cr6.compare<int32_t>(r21.s32, r19.s32, xer);
	// mr r11,r21
	r11.u64 = r21.u64;
	// blt cr6,0x82671354
	if (cr6.lt) goto loc_82671354;
	// mr r11,r19
	r11.u64 = r19.u64;
loc_82671354:
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// bge cr6,0x826714a8
	if (!cr6.lt) goto loc_826714A8;
	// extsw r11,r29
	r11.s64 = r29.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f31,f0,f28
	f31.f64 = f0.f64 - f28.f64;
	// fdiv f1,f31,f30
	ctx.f1.f64 = f31.f64 / f30.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fsub f0,f27,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f27.f64 - ctx.f1.f64;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// fmsub f13,f1,f30,f31
	ctx.f13.f64 = ctx.f1.f64 * f30.f64 - f31.f64;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fmsub f0,f0,f30,f31
	f0.f64 = f0.f64 * f30.f64 - f31.f64;
	// fadd f13,f13,f29
	ctx.f13.f64 = ctx.f13.f64 + f29.f64;
	// fadd f0,f0,f29
	f0.f64 = f0.f64 + f29.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r10,r29
	r11.u64 = ctx.r10.u64 + r29.u64;
	// cmpw cr6,r11,r19
	cr6.compare<int32_t>(r11.s32, r19.s32, xer);
	// blt cr6,0x82671428
	if (cr6.lt) goto loc_82671428;
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// add r11,r10,r29
	r11.u64 = ctx.r10.u64 + r29.u64;
	// cmpw cr6,r11,r19
	cr6.compare<int32_t>(r11.s32, r19.s32, xer);
	// bge cr6,0x826713dc
	if (!cr6.lt) goto loc_826713DC;
	// fcmpu cr6,f31,f26
	cr6.compare(f31.f64, f26.f64);
	// bgt cr6,0x82671428
	if (cr6.gt) goto loc_82671428;
	// add r11,r25,r23
	r11.u64 = r25.u64 + r23.u64;
	// li r30,0
	r30.s64 = 0;
	// add r4,r11,r27
	ctx.r4.u64 = r11.u64 + r27.u64;
	// b 0x826713e4
	goto loc_826713E4;
loc_826713DC:
	// li r30,2
	r30.s64 = 2;
	// add r4,r25,r27
	ctx.r4.u64 = r25.u64 + r27.u64;
loc_826713E4:
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82671d00
	sub_82671D00(ctx, base);
	// clrlwi r11,r29,31
	r11.u64 = r29.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8267149c
	if (!cr6.eq) goto loc_8267149C;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// srawi r31,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r31.s64 = r29.s32 >> 1;
	// bne cr6,0x82671440
	if (!cr6.eq) goto loc_82671440;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// add r4,r31,r16
	ctx.r4.u64 = r31.u64 + r16.u64;
	// add r3,r31,r20
	ctx.r3.u64 = r31.u64 + r20.u64;
	// bl 0x82671d00
	sub_82671D00(ctx, base);
	// add r4,r31,r15
	ctx.r4.u64 = r31.u64 + r15.u64;
	// b 0x8267148c
	goto loc_8267148C;
loc_82671428:
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r9,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r9.s64 = r29.s32 >> 1;
	// li r30,1
	r30.s64 = 1;
	// add r28,r10,r9
	r28.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r4,r11,r14
	ctx.r4.u64 = r11.u64 + r14.u64;
	// b 0x826713e4
	goto loc_826713E4;
loc_82671440:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x82671464
	if (!cr6.eq) goto loc_82671464;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// add r4,r28,r16
	ctx.r4.u64 = r28.u64 + r16.u64;
	// add r3,r31,r20
	ctx.r3.u64 = r31.u64 + r20.u64;
	// bl 0x82671d00
	sub_82671D00(ctx, base);
	// add r4,r28,r15
	ctx.r4.u64 = r28.u64 + r15.u64;
	// b 0x8267148c
	goto loc_8267148C;
loc_82671464:
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// bne cr6,0x8267149c
	if (!cr6.eq) goto loc_8267149C;
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// add r3,r31,r20
	ctx.r3.u64 = r31.u64 + r20.u64;
	// bl 0x82671d00
	sub_82671D00(ctx, base);
	// lwz r11,380(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
loc_8267148C:
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// add r3,r31,r26
	ctx.r3.u64 = r31.u64 + r26.u64;
	// bl 0x82671d00
	sub_82671D00(ctx, base);
loc_8267149C:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// b 0x82671344
	goto loc_82671344;
loc_826714A8:
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d62c
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826714B8"))) PPC_WEAK_FUNC(sub_826714B8);
PPC_FUNC_IMPL(__imp__sub_826714B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5e0
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r10,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, ctx.r10.u32);
	// mr r22,r8
	r22.u64 = ctx.r8.u64;
	// lwz r10,420(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// lwz r11,412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stw r9,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r9.u32);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// mr r17,r4
	r17.u64 = ctx.r4.u64;
	// srawi r24,r19,1
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x1) != 0);
	r24.s64 = r19.s32 >> 1;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// mr r14,r5
	r14.u64 = ctx.r5.u64;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f30,f0
	f30.f64 = double(f0.s64);
	// mr r16,r6
	r16.u64 = ctx.r6.u64;
	// mr r15,r7
	r15.u64 = ctx.r7.u64;
	// srawi r18,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	r18.s64 = r17.s32 >> 1;
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f28,f13
	f28.f64 = double(ctx.f13.s64);
	// fsub f0,f28,f30
	f0.f64 = f28.f64 - f30.f64;
	// fadd f13,f28,f30
	ctx.f13.f64 = f28.f64 + f30.f64;
	// fctiwz f12,f0
	ctx.f12.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r23,88(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// extsw r11,r23
	r11.s64 = r23.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f26,-31368(r11)
	f26.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fsub f0,f0,f13
	f0.f64 = f0.f64 - ctx.f13.f64;
	// fcmpu cr6,f0,f26
	cr6.compare(f0.f64, f26.f64);
	// bne cr6,0x82671568
	if (!cr6.eq) goto loc_82671568;
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
loc_82671568:
	// lwz r25,388(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// lwz r20,88(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// ble cr6,0x826715f4
	if (!cr6.gt) goto loc_826715F4;
	// subf r11,r20,r19
	r11.s64 = r19.s64 - r20.s64;
	// addi r28,r23,1
	r28.s64 = r23.s64 + 1;
	// addi r27,r11,-1
	r27.s64 = r11.s64 + -1;
	// mr r31,r25
	r31.u64 = r25.u64;
	// subf r26,r25,r22
	r26.s64 = r22.s64 - r25.s64;
	// subf r30,r25,r19
	r30.s64 = r19.s64 - r25.s64;
	// mr r29,r17
	r29.u64 = r17.u64;
loc_82671594:
	// cmpw cr6,r28,r19
	cr6.compare<int32_t>(r28.s32, r19.s32, xer);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// blt cr6,0x826715a4
	if (cr6.lt) goto loc_826715A4;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
loc_826715A4:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826715b8
	if (!cr6.gt) goto loc_826715B8;
	// add r4,r26,r31
	ctx.r4.u64 = r26.u64 + r31.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826715B8:
	// cmpw cr6,r27,r19
	cr6.compare<int32_t>(r27.s32, r19.s32, xer);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// blt cr6,0x826715c8
	if (cr6.lt) goto loc_826715C8;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
loc_826715C8:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826715e4
	if (!cr6.gt) goto loc_826715E4;
	// subf r11,r5,r30
	r11.s64 = r30.s64 - ctx.r5.s64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r4,r11,r14
	ctx.r4.u64 = r11.u64 + r14.u64;
	// add r3,r11,r25
	ctx.r3.u64 = r11.u64 + r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826715E4:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r31,r31,r19
	r31.u64 = r31.u64 + r19.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82671594
	if (!cr6.eq) goto loc_82671594;
loc_826715F4:
	// lwz r26,404(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// li r29,0
	r29.s64 = 0;
	// lwz r21,396(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// ble cr6,0x826716b8
	if (!cr6.gt) goto loc_826716B8;
	// srawi r11,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r11.s64 = r23.s32 >> 1;
	// srawi r10,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	ctx.r10.s64 = r20.s32 >> 1;
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
	// subf r11,r10,r24
	r11.s64 = r24.s64 - ctx.r10.s64;
	// addi r27,r11,-1
	r27.s64 = r11.s64 + -1;
loc_8267161C:
	// cmpw cr6,r28,r24
	cr6.compare<int32_t>(r28.s32, r24.s32, xer);
	// mr r30,r28
	r30.u64 = r28.u64;
	// blt cr6,0x8267162c
	if (cr6.lt) goto loc_8267162C;
	// mr r30,r24
	r30.u64 = r24.u64;
loc_8267162C:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82671664
	if (!cr6.gt) goto loc_82671664;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mullw r31,r11,r24
	r31.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// add r3,r31,r21
	ctx.r3.u64 = r31.u64 + r21.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,380(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// add r3,r31,r26
	ctx.r3.u64 = r31.u64 + r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82671664:
	// cmpw cr6,r27,r24
	cr6.compare<int32_t>(r27.s32, r24.s32, xer);
	// mr r30,r27
	r30.u64 = r27.u64;
	// blt cr6,0x82671674
	if (cr6.lt) goto loc_82671674;
	// mr r30,r24
	r30.u64 = r24.u64;
loc_82671674:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x826716ac
	if (!cr6.gt) goto loc_826716AC;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r11,r11,r24
	r11.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// subf r31,r30,r11
	r31.s64 = r11.s64 - r30.s64;
	// add r4,r31,r16
	ctx.r4.u64 = r31.u64 + r16.u64;
	// add r3,r31,r21
	ctx.r3.u64 = r31.u64 + r21.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r15
	ctx.r4.u64 = r31.u64 + r15.u64;
	// add r3,r31,r26
	ctx.r3.u64 = r31.u64 + r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826716AC:
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// cmpw cr6,r29,r17
	cr6.compare<int32_t>(r29.s32, r17.s32, xer);
	// blt cr6,0x8267161c
	if (cr6.lt) goto loc_8267161C;
loc_826716B8:
	// addic. r11,r23,1
	xer.ca = r23.u32 > 4294967294;
	r11.s64 = r23.s64 + 1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r29,0
	r29.s64 = 0;
	// blt 0x826716c8
	if (cr0.lt) goto loc_826716C8;
	// mr r29,r11
	r29.u64 = r11.u64;
loc_826716C8:
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// lwz r28,88(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// add r27,r29,r25
	r27.u64 = r29.u64 + r25.u64;
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// subf r23,r22,r14
	r23.s64 = r14.s64 - r22.s64;
	// lfd f27,4120(r10)
	ctx.fpscr.disableFlushMode();
	f27.u64 = PPC_LOAD_U64(ctx.r10.u32 + 4120);
	// subf r25,r25,r22
	r25.s64 = r22.s64 - r25.s64;
	// lfd f29,-28592(r11)
	f29.u64 = PPC_LOAD_U64(r11.u32 + -28592);
loc_826716EC:
	// cmpw cr6,r20,r19
	cr6.compare<int32_t>(r20.s32, r19.s32, xer);
	// mr r11,r20
	r11.u64 = r20.u64;
	// blt cr6,0x826716fc
	if (cr6.lt) goto loc_826716FC;
	// mr r11,r19
	r11.u64 = r19.u64;
loc_826716FC:
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// bge cr6,0x82671868
	if (!cr6.lt) goto loc_82671868;
	// extsw r11,r29
	r11.s64 = r29.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f31,f28,f0
	f31.f64 = f28.f64 - f0.f64;
	// fdiv f1,f31,f30
	ctx.f1.f64 = f31.f64 / f30.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fmsub f13,f1,f30,f31
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64 * f30.f64 - f31.f64;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// fsub f0,f27,f1
	f0.f64 = f27.f64 - ctx.f1.f64;
	// fadd f13,f13,f29
	ctx.f13.f64 = ctx.f13.f64 + f29.f64;
	// fmsub f0,f0,f30,f31
	f0.f64 = f0.f64 * f30.f64 - f31.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// fadd f0,f0,f29
	f0.f64 = f0.f64 + f29.f64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// neg r9,r11
	ctx.r9.s64 = -r11.s64;
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// neg r10,r11
	ctx.r10.s64 = -r11.s64;
	// add. r11,r10,r29
	r11.u64 = ctx.r10.u64 + r29.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x8267177c
	if (cr0.lt) goto loc_8267177C;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r9,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r9.s64 = r29.s32 >> 1;
	// li r30,1
	r30.s64 = 1;
	// add r28,r10,r9
	r28.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r4,r11,r14
	ctx.r4.u64 = r11.u64 + r14.u64;
	// b 0x826717bc
	goto loc_826717BC;
loc_8267177C:
	// add. r11,r9,r29
	r11.u64 = ctx.r9.u64 + r29.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x826717b4
	if (cr0.lt) goto loc_826717B4;
	// fcmpu cr6,f31,f26
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, f26.f64);
	// ble cr6,0x826717a4
	if (!cr6.gt) goto loc_826717A4;
	// srawi r10,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// srawi r9,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r9.s64 = r29.s32 >> 1;
	// li r30,1
	r30.s64 = 1;
	// add r28,r10,r9
	r28.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r4,r11,r14
	ctx.r4.u64 = r11.u64 + r14.u64;
	// b 0x826717bc
	goto loc_826717BC;
loc_826717A4:
	// add r11,r23,r27
	r11.u64 = r23.u64 + r27.u64;
	// li r30,0
	r30.s64 = 0;
	// add r4,r11,r25
	ctx.r4.u64 = r11.u64 + r25.u64;
	// b 0x826717bc
	goto loc_826717BC;
loc_826717B4:
	// li r30,2
	r30.s64 = 2;
	// add r4,r27,r25
	ctx.r4.u64 = r27.u64 + r25.u64;
loc_826717BC:
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82671d00
	sub_82671D00(ctx, base);
	// clrlwi r11,r29,31
	r11.u64 = r29.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8267185c
	if (!cr6.eq) goto loc_8267185C;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// srawi r31,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r31.s64 = r29.s32 >> 1;
	// bne cr6,0x82671800
	if (!cr6.eq) goto loc_82671800;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// add r4,r31,r16
	ctx.r4.u64 = r31.u64 + r16.u64;
	// add r3,r31,r21
	ctx.r3.u64 = r31.u64 + r21.u64;
	// bl 0x82671d00
	sub_82671D00(ctx, base);
	// add r4,r31,r15
	ctx.r4.u64 = r31.u64 + r15.u64;
	// b 0x8267184c
	goto loc_8267184C;
loc_82671800:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x82671824
	if (!cr6.eq) goto loc_82671824;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// add r4,r28,r16
	ctx.r4.u64 = r28.u64 + r16.u64;
	// add r3,r31,r21
	ctx.r3.u64 = r31.u64 + r21.u64;
	// bl 0x82671d00
	sub_82671D00(ctx, base);
	// add r4,r28,r15
	ctx.r4.u64 = r28.u64 + r15.u64;
	// b 0x8267184c
	goto loc_8267184C;
loc_82671824:
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// bne cr6,0x8267185c
	if (!cr6.eq) goto loc_8267185C;
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// add r3,r31,r21
	ctx.r3.u64 = r31.u64 + r21.u64;
	// bl 0x82671d00
	sub_82671D00(ctx, base);
	// lwz r11,380(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
loc_8267184C:
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// add r3,r31,r26
	ctx.r3.u64 = r31.u64 + r26.u64;
	// bl 0x82671d00
	sub_82671D00(ctx, base);
loc_8267185C:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// b 0x826716ec
	goto loc_826716EC;
loc_82671868:
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d62c
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82671878"))) PPC_WEAK_FUNC(sub_82671878);
PPC_FUNC_IMPL(__imp__sub_82671878) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5dc
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r14,r9
	r14.u64 = ctx.r9.u64;
	// lwz r9,428(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// mr r21,r4
	r21.u64 = ctx.r4.u64;
	// stw r10,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, ctx.r10.u32);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// lwz r10,436(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// addi r11,r21,-1
	r11.s64 = r21.s64 + -1;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// mr r16,r5
	r16.u64 = ctx.r5.u64;
	// mr r19,r6
	r19.u64 = ctx.r6.u64;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// mr r18,r7
	r18.u64 = ctx.r7.u64;
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// li r27,0
	r27.s64 = 0;
	// srawi r29,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r29.s64 = r28.s32 >> 1;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// fsub f25,f13,f0
	f25.f64 = ctx.f13.f64 - f0.f64;
	// lfd f12,96(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f30,f12
	f30.f64 = double(ctx.f12.s64);
	// ble cr6,0x82671ab8
	if (!cr6.gt) goto loc_82671AB8;
	// lwz r11,404(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// lwz r23,420(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// mr r26,r11
	r26.u64 = r11.u64;
	// lwz r22,412(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// subf r17,r11,r8
	r17.s64 = ctx.r8.s64 - r11.s64;
	// lwz r20,88(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lwz r24,80(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lfd f27,-31368(r9)
	f27.u64 = PPC_LOAD_U64(ctx.r9.u32 + -31368);
	// fneg f26,f30
	f26.u64 = f30.u64 ^ 0x8000000000000000;
	// lfd f28,4120(r10)
	f28.u64 = PPC_LOAD_U64(ctx.r10.u32 + 4120);
	// li r25,0
	r25.s64 = 0;
	// subf r15,r8,r16
	r15.s64 = r16.s64 - ctx.r8.s64;
	// lfd f29,-28592(r11)
	f29.u64 = PPC_LOAD_U64(r11.u32 + -28592);
loc_82671938:
	// extsw r11,r27
	r11.s64 = r27.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f31,f0,f25
	f31.f64 = f0.f64 - f25.f64;
	// fcmpu cr6,f31,f26
	cr6.compare(f31.f64, f26.f64);
	// bge cr6,0x82671964
	if (!cr6.lt) goto loc_82671964;
loc_82671954:
	// add r11,r17,r15
	r11.u64 = r17.u64 + r15.u64;
	// li r30,0
	r30.s64 = 0;
	// add r4,r11,r26
	ctx.r4.u64 = r11.u64 + r26.u64;
	// b 0x82671974
	goto loc_82671974;
loc_82671964:
	// fcmpu cr6,f31,f30
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, f30.f64);
	// ble cr6,0x826719cc
	if (!cr6.gt) goto loc_826719CC;
loc_8267196C:
	// li r30,2
	r30.s64 = 2;
	// add r4,r17,r26
	ctx.r4.u64 = r17.u64 + r26.u64;
loc_82671974:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// clrlwi r11,r27,31
	r11.u64 = r27.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82671aa4
	if (!cr6.eq) goto loc_82671AA4;
	// srawi r11,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	r11.s64 = r27.s32 >> 1;
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// mullw r31,r11,r29
	r31.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// bne cr6,0x82671a58
	if (!cr6.eq) goto loc_82671A58;
	// add r11,r24,r27
	r11.u64 = r24.u64 + r27.u64;
	// cmpw cr6,r11,r21
	cr6.compare<int32_t>(r11.s32, r21.s32, xer);
	// bge cr6,0x82671a50
	if (!cr6.lt) goto loc_82671A50;
	// srawi r11,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	r11.s64 = r24.s32 >> 1;
loc_826719AC:
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r30,r11,r31
	r30.u64 = r11.u64 + r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r3,r31,r22
	ctx.r3.u64 = r31.u64 + r22.u64;
	// add r4,r30,r19
	ctx.r4.u64 = r30.u64 + r19.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r4,r30,r18
	ctx.r4.u64 = r30.u64 + r18.u64;
	// b 0x82671a98
	goto loc_82671A98;
loc_826719CC:
	// fdiv f1,f31,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64 / f30.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fsub f0,f28,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f28.f64 - ctx.f1.f64;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// fmsub f13,f1,f30,f31
	ctx.f13.f64 = ctx.f1.f64 * f30.f64 - f31.f64;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fmsub f0,f0,f30,f31
	f0.f64 = f0.f64 * f30.f64 - f31.f64;
	// fadd f13,f13,f29
	ctx.f13.f64 = ctx.f13.f64 + f29.f64;
	// fadd f0,f0,f29
	f0.f64 = f0.f64 + f29.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r20,88(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r24,80(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r24,r27
	r11.u64 = r24.u64 + r27.u64;
	// cmpw cr6,r11,r21
	cr6.compare<int32_t>(r11.s32, r21.s32, xer);
	// bge cr6,0x82671a28
	if (!cr6.lt) goto loc_82671A28;
	// mullw r11,r24,r28
	r11.s64 = int64_t(r24.s32) * int64_t(r28.s32);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// li r30,1
	r30.s64 = 1;
	// add r4,r11,r16
	ctx.r4.u64 = r11.u64 + r16.u64;
	// b 0x82671974
	goto loc_82671974;
loc_82671A28:
	// add r11,r20,r27
	r11.u64 = r20.u64 + r27.u64;
	// cmpw cr6,r11,r21
	cr6.compare<int32_t>(r11.s32, r21.s32, xer);
	// bge cr6,0x8267196c
	if (!cr6.lt) goto loc_8267196C;
	// fcmpu cr6,f31,f27
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, f27.f64);
	// ble cr6,0x82671954
	if (!cr6.gt) goto loc_82671954;
	// mullw r11,r20,r28
	r11.s64 = int64_t(r20.s32) * int64_t(r28.s32);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// li r30,1
	r30.s64 = 1;
	// add r4,r11,r16
	ctx.r4.u64 = r11.u64 + r16.u64;
	// b 0x82671974
	goto loc_82671974;
loc_82671A50:
	// srawi r11,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r11.s64 = r20.s32 >> 1;
	// b 0x826719ac
	goto loc_826719AC;
loc_82671A58:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x82671a78
	if (!cr6.eq) goto loc_82671A78;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r4,r31,r19
	ctx.r4.u64 = r31.u64 + r19.u64;
	// add r3,r31,r22
	ctx.r3.u64 = r31.u64 + r22.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r4,r31,r18
	ctx.r4.u64 = r31.u64 + r18.u64;
	// b 0x82671a98
	goto loc_82671A98;
loc_82671A78:
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// bne cr6,0x82671aa4
	if (!cr6.eq) goto loc_82671AA4;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r4,r31,r14
	ctx.r4.u64 = r31.u64 + r14.u64;
	// add r3,r31,r22
	ctx.r3.u64 = r31.u64 + r22.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,396(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
loc_82671A98:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r3,r31,r23
	ctx.r3.u64 = r31.u64 + r23.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82671AA4:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// add r25,r25,r28
	r25.u64 = r25.u64 + r28.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// cmpw cr6,r27,r21
	cr6.compare<int32_t>(r27.s32, r21.s32, xer);
	// blt cr6,0x82671938
	if (cr6.lt) goto loc_82671938;
loc_82671AB8:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d628
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82671AC8"))) PPC_WEAK_FUNC(sub_82671AC8);
PPC_FUNC_IMPL(__imp__sub_82671AC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5dc
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r10,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, ctx.r10.u32);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// mr r16,r4
	r16.u64 = ctx.r4.u64;
	// lwz r10,436(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// mr r17,r5
	r17.u64 = ctx.r5.u64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// mr r20,r6
	r20.u64 = ctx.r6.u64;
	// mr r19,r7
	r19.u64 = ctx.r7.u64;
	// mr r14,r9
	r14.u64 = ctx.r9.u64;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// li r26,0
	r26.s64 = 0;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// srawi r29,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r29.s64 = r28.s32 >> 1;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f30,f0
	f30.f64 = double(f0.s64);
	// fcfid f25,f13
	f25.f64 = double(ctx.f13.s64);
	// ble cr6,0x82671cec
	if (!cr6.gt) goto loc_82671CEC;
	// lwz r11,404(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// lwz r24,420(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// mr r27,r11
	r27.u64 = r11.u64;
	// lwz r23,412(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// subf r18,r11,r8
	r18.s64 = ctx.r8.s64 - r11.s64;
	// lwz r21,88(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lwz r22,88(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lfd f27,-31368(r9)
	f27.u64 = PPC_LOAD_U64(ctx.r9.u32 + -31368);
	// fneg f26,f30
	f26.u64 = f30.u64 ^ 0x8000000000000000;
	// lfd f28,4120(r10)
	f28.u64 = PPC_LOAD_U64(ctx.r10.u32 + 4120);
	// li r25,0
	r25.s64 = 0;
	// subf r15,r8,r17
	r15.s64 = r17.s64 - ctx.r8.s64;
	// lfd f29,-28592(r11)
	f29.u64 = PPC_LOAD_U64(r11.u32 + -28592);
loc_82671B70:
	// extsw r11,r26
	r11.s64 = r26.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f31,f25,f0
	f31.f64 = f25.f64 - f0.f64;
	// fcmpu cr6,f31,f26
	cr6.compare(f31.f64, f26.f64);
	// bge cr6,0x82671b9c
	if (!cr6.lt) goto loc_82671B9C;
loc_82671B8C:
	// add r11,r18,r15
	r11.u64 = r18.u64 + r15.u64;
	// li r30,0
	r30.s64 = 0;
	// add r4,r11,r27
	ctx.r4.u64 = r11.u64 + r27.u64;
	// b 0x82671bac
	goto loc_82671BAC;
loc_82671B9C:
	// fcmpu cr6,f31,f30
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, f30.f64);
	// ble cr6,0x82671c00
	if (!cr6.gt) goto loc_82671C00;
loc_82671BA4:
	// li r30,2
	r30.s64 = 2;
	// add r4,r18,r27
	ctx.r4.u64 = r18.u64 + r27.u64;
loc_82671BAC:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// clrlwi r11,r26,31
	r11.u64 = r26.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82671cd8
	if (!cr6.eq) goto loc_82671CD8;
	// srawi r11,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	r11.s64 = r26.s32 >> 1;
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// mullw r31,r11,r29
	r31.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// bne cr6,0x82671c8c
	if (!cr6.eq) goto loc_82671C8C;
	// add. r11,r22,r26
	r11.u64 = r22.u64 + r26.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x82671c84
	if (cr0.lt) goto loc_82671C84;
	// srawi r11,r22,1
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x1) != 0);
	r11.s64 = r22.s32 >> 1;
loc_82671BE0:
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r30,r11,r31
	r30.u64 = r11.u64 + r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r3,r31,r23
	ctx.r3.u64 = r31.u64 + r23.u64;
	// add r4,r30,r20
	ctx.r4.u64 = r30.u64 + r20.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r4,r30,r19
	ctx.r4.u64 = r30.u64 + r19.u64;
	// b 0x82671ccc
	goto loc_82671CCC;
loc_82671C00:
	// fdiv f1,f31,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64 / f30.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fmsub f13,f1,f30,f31
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64 * f30.f64 - f31.f64;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// fsub f0,f28,f1
	f0.f64 = f28.f64 - ctx.f1.f64;
	// fadd f13,f13,f29
	ctx.f13.f64 = ctx.f13.f64 + f29.f64;
	// fmsub f0,f0,f30,f31
	f0.f64 = f0.f64 * f30.f64 - f31.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// fadd f0,f0,f29
	f0.f64 = f0.f64 + f29.f64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// neg r21,r11
	r21.s64 = -r11.s64;
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// neg r22,r11
	r22.s64 = -r11.s64;
	// add. r11,r22,r26
	r11.u64 = r22.u64 + r26.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x82671c60
	if (cr0.lt) goto loc_82671C60;
	// mullw r11,r22,r28
	r11.s64 = int64_t(r22.s32) * int64_t(r28.s32);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// li r30,1
	r30.s64 = 1;
	// add r4,r11,r17
	ctx.r4.u64 = r11.u64 + r17.u64;
	// b 0x82671bac
	goto loc_82671BAC;
loc_82671C60:
	// add. r11,r21,r26
	r11.u64 = r21.u64 + r26.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x82671ba4
	if (cr0.lt) goto loc_82671BA4;
	// fcmpu cr6,f31,f27
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, f27.f64);
	// ble cr6,0x82671b8c
	if (!cr6.gt) goto loc_82671B8C;
	// mullw r11,r21,r28
	r11.s64 = int64_t(r21.s32) * int64_t(r28.s32);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// li r30,1
	r30.s64 = 1;
	// add r4,r11,r17
	ctx.r4.u64 = r11.u64 + r17.u64;
	// b 0x82671bac
	goto loc_82671BAC;
loc_82671C84:
	// srawi r11,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	r11.s64 = r21.s32 >> 1;
	// b 0x82671be0
	goto loc_82671BE0;
loc_82671C8C:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x82671cac
	if (!cr6.eq) goto loc_82671CAC;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r4,r31,r20
	ctx.r4.u64 = r31.u64 + r20.u64;
	// add r3,r31,r23
	ctx.r3.u64 = r31.u64 + r23.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r4,r31,r19
	ctx.r4.u64 = r31.u64 + r19.u64;
	// b 0x82671ccc
	goto loc_82671CCC;
loc_82671CAC:
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// bne cr6,0x82671cd8
	if (!cr6.eq) goto loc_82671CD8;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r4,r31,r14
	ctx.r4.u64 = r31.u64 + r14.u64;
	// add r3,r31,r23
	ctx.r3.u64 = r31.u64 + r23.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,396(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
loc_82671CCC:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r3,r31,r24
	ctx.r3.u64 = r31.u64 + r24.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82671CD8:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// add r25,r25,r28
	r25.u64 = r25.u64 + r28.u64;
	// add r27,r27,r28
	r27.u64 = r27.u64 + r28.u64;
	// cmpw cr6,r26,r16
	cr6.compare<int32_t>(r26.s32, r16.s32, xer);
	// blt cr6,0x82671b70
	if (cr6.lt) goto loc_82671B70;
loc_82671CEC:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d628
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82671CFC"))) PPC_WEAK_FUNC(sub_82671CFC);
PPC_FUNC_IMPL(__imp__sub_82671CFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82671D00"))) PPC_WEAK_FUNC(sub_82671D00);
PPC_FUNC_IMPL(__imp__sub_82671D00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mullw r3,r5,r6
	ctx.r3.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r6.s32);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// subf r9,r11,r4
	ctx.r9.s64 = ctx.r4.s64 - r11.s64;
loc_82671D18:
	// lbzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// cmpw cr6,r10,r3
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r3.s32, xer);
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// blt cr6,0x82671d18
	if (cr6.lt) goto loc_82671D18;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82671D34"))) PPC_WEAK_FUNC(sub_82671D34);
PPC_FUNC_IMPL(__imp__sub_82671D34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82671D38"))) PPC_WEAK_FUNC(sub_82671D38);
PPC_FUNC_IMPL(__imp__sub_82671D38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	// mflr r12
	// bl 0x8239bce4
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// li r31,0
	r31.s64 = 0;
	// lwz r24,108(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// cmpw cr6,r27,r24
	cr6.compare<int32_t>(r27.s32, r24.s32, xer);
	// blt cr6,0x82671d58
	if (cr6.lt) goto loc_82671D58;
	// mr r27,r24
	r27.u64 = r24.u64;
loc_82671D58:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x82671d64
	if (!cr6.lt) goto loc_82671D64;
	// li r10,0
	ctx.r10.s64 = 0;
loc_82671D64:
	// lwz r28,92(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bge cr6,0x82671d74
	if (!cr6.lt) goto loc_82671D74;
	// li r28,0
	r28.s64 = 0;
loc_82671D74:
	// li r11,0
	r11.s64 = 0;
	// cmpw cr6,r28,r27
	cr6.compare<int32_t>(r28.s32, r27.s32, xer);
	// bge cr6,0x8267203c
	if (!cr6.lt) goto loc_8267203C;
	// lwz r25,84(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpw cr6,r25,r24
	cr6.compare<int32_t>(r25.s32, r24.s32, xer);
	// blt cr6,0x82671d90
	if (cr6.lt) goto loc_82671D90;
	// mr r25,r24
	r25.u64 = r24.u64;
loc_82671D90:
	// cmpw cr6,r10,r25
	cr6.compare<int32_t>(ctx.r10.s32, r25.s32, xer);
	// bge cr6,0x82671fb4
	if (!cr6.lt) goto loc_82671FB4;
	// cmpw cr6,r28,r25
	cr6.compare<int32_t>(r28.s32, r25.s32, xer);
	// mr r29,r28
	r29.u64 = r28.u64;
	// blt cr6,0x82671da8
	if (cr6.lt) goto loc_82671DA8;
	// mr r29,r25
	r29.u64 = r25.u64;
loc_82671DA8:
	// cmpw cr6,r27,r10
	cr6.compare<int32_t>(r27.s32, ctx.r10.s32, xer);
	// mr r26,r27
	r26.u64 = r27.u64;
	// bgt cr6,0x82671db8
	if (cr6.gt) goto loc_82671DB8;
	// mr r26,r10
	r26.u64 = ctx.r10.u64;
loc_82671DB8:
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// cmpw cr6,r10,r28
	cr6.compare<int32_t>(ctx.r10.s32, r28.s32, xer);
	// lfd f0,-31368(r9)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -31368);
	// bge cr6,0x82671e94
	if (!cr6.lt) goto loc_82671E94;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82671e04
	if (!cr6.gt) goto loc_82671E04;
	// mullw r31,r8,r10
	r31.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// li r9,0
	ctx.r9.s64 = 0;
	// ble cr6,0x82671e00
	if (!cr6.gt) goto loc_82671E00;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// subf r30,r3,r7
	r30.s64 = ctx.r7.s64 - ctx.r3.s64;
loc_82671DE8:
	// lbzx r23,r30,r11
	r23.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpw cr6,r9,r31
	cr6.compare<int32_t>(ctx.r9.s32, r31.s32, xer);
	// stb r23,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r23.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// blt cr6,0x82671de8
	if (cr6.lt) goto loc_82671DE8;
loc_82671E00:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82671E04:
	// cmpw cr6,r11,r29
	cr6.compare<int32_t>(r11.s32, r29.s32, xer);
	// bge cr6,0x82671e50
	if (!cr6.lt) goto loc_82671E50;
	// fcmpu cr6,f1,f0
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, f0.f64);
	// add r30,r31,r4
	r30.u64 = r31.u64 + ctx.r4.u64;
	// bge cr6,0x82671e1c
	if (!cr6.lt) goto loc_82671E1C;
	// add r30,r31,r6
	r30.u64 = r31.u64 + ctx.r6.u64;
loc_82671E1C:
	// subf r10,r11,r29
	ctx.r10.s64 = r29.s64 - r11.s64;
	// li r11,0
	r11.s64 = 0;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82671e48
	if (!cr6.gt) goto loc_82671E48;
	// add r9,r31,r3
	ctx.r9.u64 = r31.u64 + ctx.r3.u64;
loc_82671E34:
	// lbzx r23,r30,r11
	r23.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// stbx r23,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, r23.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82671e34
	if (cr6.lt) goto loc_82671E34;
loc_82671E48:
	// add r31,r10,r31
	r31.u64 = ctx.r10.u64 + r31.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_82671E50:
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// bge cr6,0x82671ed4
	if (!cr6.lt) goto loc_82671ED4;
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	// li r10,0
	ctx.r10.s64 = 0;
	// mullw r9,r11,r8
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82671e8c
	if (!cr6.gt) goto loc_82671E8C;
	// add r11,r31,r3
	r11.u64 = r31.u64 + ctx.r3.u64;
	// subf r30,r3,r7
	r30.s64 = ctx.r7.s64 - ctx.r3.s64;
loc_82671E74:
	// lbzx r29,r11,r30
	r29.u64 = PPC_LOAD_U8(r11.u32 + r30.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r29,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r29.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// blt cr6,0x82671e74
	if (cr6.lt) goto loc_82671E74;
loc_82671E8C:
	// add r31,r9,r31
	r31.u64 = ctx.r9.u64 + r31.u64;
	// b 0x82671ed0
	goto loc_82671ED0;
loc_82671E94:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x82671ed4
	if (!cr6.gt) goto loc_82671ED4;
	// mullw r9,r8,r28
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(r28.s32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// li r10,0
	ctx.r10.s64 = 0;
	// ble cr6,0x82671ecc
	if (!cr6.gt) goto loc_82671ECC;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// subf r31,r3,r7
	r31.s64 = ctx.r7.s64 - ctx.r3.s64;
loc_82671EB4:
	// lbzx r30,r11,r31
	r30.u64 = PPC_LOAD_U8(r11.u32 + r31.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r30,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r30.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// blt cr6,0x82671eb4
	if (cr6.lt) goto loc_82671EB4;
loc_82671ECC:
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
loc_82671ED0:
	// mr r11,r28
	r11.u64 = r28.u64;
loc_82671ED4:
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// bge cr6,0x82671f18
	if (!cr6.lt) goto loc_82671F18;
	// subf r11,r11,r27
	r11.s64 = r27.s64 - r11.s64;
	// li r10,0
	ctx.r10.s64 = 0;
	// mullw r9,r11,r8
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82671f10
	if (!cr6.gt) goto loc_82671F10;
	// add r11,r31,r3
	r11.u64 = r31.u64 + ctx.r3.u64;
	// subf r30,r3,r5
	r30.s64 = ctx.r5.s64 - ctx.r3.s64;
loc_82671EF8:
	// lbzx r5,r30,r11
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r5,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r5.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// blt cr6,0x82671ef8
	if (cr6.lt) goto loc_82671EF8;
loc_82671F10:
	// add r31,r9,r31
	r31.u64 = ctx.r9.u64 + r31.u64;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_82671F18:
	// cmpw cr6,r25,r27
	cr6.compare<int32_t>(r25.s32, r27.s32, xer);
	// ble cr6,0x826720ec
	if (!cr6.gt) goto loc_826720EC;
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// bge cr6,0x82671f64
	if (!cr6.lt) goto loc_82671F64;
	// subf r11,r11,r26
	r11.s64 = r26.s64 - r11.s64;
	// li r10,0
	ctx.r10.s64 = 0;
	// mullw r9,r11,r8
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82671f5c
	if (!cr6.gt) goto loc_82671F5C;
	// add r11,r31,r3
	r11.u64 = r31.u64 + ctx.r3.u64;
	// subf r30,r3,r7
	r30.s64 = ctx.r7.s64 - ctx.r3.s64;
loc_82671F44:
	// lbzx r5,r11,r30
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + r30.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r5,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r5.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// blt cr6,0x82671f44
	if (cr6.lt) goto loc_82671F44;
loc_82671F5C:
	// add r31,r9,r31
	r31.u64 = ctx.r9.u64 + r31.u64;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_82671F64:
	// cmpw cr6,r11,r25
	cr6.compare<int32_t>(r11.s32, r25.s32, xer);
	// bge cr6,0x826720ec
	if (!cr6.lt) goto loc_826720EC;
	// fcmpu cr6,f1,f0
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, f0.f64);
	// blt cr6,0x82671f7c
	if (cr6.lt) goto loc_82671F7C;
	// add r6,r31,r4
	ctx.r6.u64 = r31.u64 + ctx.r4.u64;
	// b 0x82671f80
	goto loc_82671F80;
loc_82671F7C:
	// add r6,r31,r6
	ctx.r6.u64 = r31.u64 + ctx.r6.u64;
loc_82671F80:
	// subf r10,r11,r25
	ctx.r10.s64 = r25.s64 - r11.s64;
	// li r11,0
	r11.s64 = 0;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82671fac
	if (!cr6.gt) goto loc_82671FAC;
	// add r9,r31,r3
	ctx.r9.u64 = r31.u64 + ctx.r3.u64;
loc_82671F98:
	// lbzx r5,r11,r6
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// stbx r5,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r5.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82671f98
	if (cr6.lt) goto loc_82671F98;
loc_82671FAC:
	// mr r11,r25
	r11.u64 = r25.u64;
	// b 0x826720e8
	goto loc_826720E8;
loc_82671FB4:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x82671ff4
	if (!cr6.gt) goto loc_82671FF4;
	// mullw r9,r8,r28
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(r28.s32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// li r10,0
	ctx.r10.s64 = 0;
	// ble cr6,0x82671fec
	if (!cr6.gt) goto loc_82671FEC;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// subf r6,r3,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r3.s64;
loc_82671FD4:
	// lbzx r4,r11,r6
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r4,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r4.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// blt cr6,0x82671fd4
	if (cr6.lt) goto loc_82671FD4;
loc_82671FEC:
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_82671FF4:
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// bge cr6,0x826720ec
	if (!cr6.lt) goto loc_826720EC;
	// subf r11,r11,r27
	r11.s64 = r27.s64 - r11.s64;
	// li r10,0
	ctx.r10.s64 = 0;
	// mullw r9,r11,r8
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82672030
	if (!cr6.gt) goto loc_82672030;
	// add r11,r31,r3
	r11.u64 = r31.u64 + ctx.r3.u64;
	// subf r6,r3,r5
	ctx.r6.s64 = ctx.r5.s64 - ctx.r3.s64;
loc_82672018:
	// lbzx r5,r11,r6
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r5,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r5.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// blt cr6,0x82672018
	if (cr6.lt) goto loc_82672018;
loc_82672030:
	// add r31,r9,r31
	r31.u64 = ctx.r9.u64 + r31.u64;
	// mr r11,r27
	r11.u64 = r27.u64;
	// b 0x826720ec
	goto loc_826720EC;
loc_8267203C:
	// lwz r30,84(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpw cr6,r30,r24
	cr6.compare<int32_t>(r30.s32, r24.s32, xer);
	// blt cr6,0x8267204c
	if (cr6.lt) goto loc_8267204C;
	// mr r30,r24
	r30.u64 = r24.u64;
loc_8267204C:
	// cmpw cr6,r10,r30
	cr6.compare<int32_t>(ctx.r10.s32, r30.s32, xer);
	// bge cr6,0x826720ec
	if (!cr6.lt) goto loc_826720EC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82672094
	if (!cr6.gt) goto loc_82672094;
	// mullw r5,r8,r10
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// li r9,0
	ctx.r9.s64 = 0;
	// ble cr6,0x8267208c
	if (!cr6.gt) goto loc_8267208C;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// subf r31,r3,r7
	r31.s64 = ctx.r7.s64 - ctx.r3.s64;
loc_82672074:
	// lbzx r29,r11,r31
	r29.u64 = PPC_LOAD_U8(r11.u32 + r31.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpw cr6,r9,r5
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r5.s32, xer);
	// stb r29,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r29.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// blt cr6,0x82672074
	if (cr6.lt) goto loc_82672074;
loc_8267208C:
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82672094:
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x826720ec
	if (!cr6.lt) goto loc_826720EC;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// lfd f0,-31368(r10)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + -31368);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// blt cr6,0x826720b4
	if (cr6.lt) goto loc_826720B4;
	// add r6,r31,r4
	ctx.r6.u64 = r31.u64 + ctx.r4.u64;
	// b 0x826720b8
	goto loc_826720B8;
loc_826720B4:
	// add r6,r31,r6
	ctx.r6.u64 = r31.u64 + ctx.r6.u64;
loc_826720B8:
	// subf r10,r11,r30
	ctx.r10.s64 = r30.s64 - r11.s64;
	// li r11,0
	r11.s64 = 0;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826720e4
	if (!cr6.gt) goto loc_826720E4;
	// add r9,r31,r3
	ctx.r9.u64 = r31.u64 + ctx.r3.u64;
loc_826720D0:
	// lbzx r5,r11,r6
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// stbx r5,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r5.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x826720d0
	if (cr6.lt) goto loc_826720D0;
loc_826720E4:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_826720E8:
	// add r31,r10,r31
	r31.u64 = ctx.r10.u64 + r31.u64;
loc_826720EC:
	// cmpw cr6,r11,r24
	cr6.compare<int32_t>(r11.s32, r24.s32, xer);
	// bge cr6,0x82672128
	if (!cr6.lt) goto loc_82672128;
	// subf r11,r11,r24
	r11.s64 = r24.s64 - r11.s64;
	// li r10,0
	ctx.r10.s64 = 0;
	// mullw r9,r11,r8
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82672128
	if (!cr6.gt) goto loc_82672128;
	// add r11,r31,r3
	r11.u64 = r31.u64 + ctx.r3.u64;
	// subf r7,r3,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r3.s64;
loc_82672110:
	// lbzx r6,r7,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// blt cr6,0x82672110
	if (cr6.lt) goto loc_82672110;
loc_82672128:
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_8267212C"))) PPC_WEAK_FUNC(sub_8267212C);
PPC_FUNC_IMPL(__imp__sub_8267212C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82672130"))) PPC_WEAK_FUNC(sub_82672130);
PPC_FUNC_IMPL(__imp__sub_82672130) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5dc
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// lwz r25,452(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// mr r16,r9
	r16.u64 = ctx.r9.u64;
	// addi r9,r23,-1
	ctx.r9.s64 = r23.s64 + -1;
	// mr r17,r5
	r17.u64 = ctx.r5.u64;
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// lwz r8,476(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// mr r19,r6
	r19.u64 = ctx.r6.u64;
	// stw r23,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, r23.u32);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// stw r16,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, r16.u32);
	// stw r17,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, r17.u32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r9,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r9.u64);
	// mr r14,r3
	r14.u64 = ctx.r3.u64;
	// mr r21,r10
	r21.u64 = ctx.r10.u64;
	// stw r19,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, r19.u32);
	// addi r10,r14,-1
	ctx.r10.s64 = r14.s64 + -1;
	// lfd f31,4128(r11)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(r11.u32 + 4128);
	// lwz r11,484(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	// std r8,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r8.u64);
	// mr r18,r7
	r18.u64 = ctx.r7.u64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// addi r7,r1,120
	ctx.r7.s64 = ctx.r1.s64 + 120;
	// stw r21,444(r1)
	PPC_STORE_U32(ctx.r1.u32 + 444, r21.u32);
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// srawi r22,r14,1
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x1) != 0);
	r22.s64 = r14.s32 >> 1;
	// stw r18,420(r1)
	PPC_STORE_U32(ctx.r1.u32 + 420, r18.u32);
	// srawi r20,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r20.s64 = r23.s32 >> 1;
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// extsw r11,r10
	r11.s64 = ctx.r10.s32;
	// lfd f0,144(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lfd f13,128(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f11,f13
	ctx.f11.f64 = double(ctx.f13.s64);
	// lfd f13,144(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f0,136(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f30,f0
	f30.f64 = double(f0.s64);
	// lfd f0,4136(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 4136);
	// fnmsub f27,f12,f31,f13
	f27.f64 = -(ctx.f12.f64 * f31.f64 - ctx.f13.f64);
	// fmul f0,f30,f0
	f0.f64 = f30.f64 * f0.f64;
	// fsub f13,f27,f13
	ctx.f13.f64 = f27.f64 - ctx.f13.f64;
	// fadd f26,f13,f11
	f26.f64 = ctx.f13.f64 + ctx.f11.f64;
	// fadd f13,f26,f27
	ctx.f13.f64 = f26.f64 + f27.f64;
	// fsub f12,f13,f0
	ctx.f12.f64 = ctx.f13.f64 - f0.f64;
	// fadd f0,f0,f13
	f0.f64 = f0.f64 + ctx.f13.f64;
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r6
	PPC_STORE_U32(ctx.r6.u32, f0.u32);
	// lwz r15,144(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// ble cr6,0x826722ac
	if (!cr6.gt) goto loc_826722AC;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// subf r29,r15,r14
	r29.s64 = r14.s64 - r15.s64;
	// mr r31,r25
	r31.u64 = r25.u64;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// subf r26,r25,r17
	r26.s64 = r17.s64 - r25.s64;
	// subf r27,r25,r14
	r27.s64 = r14.s64 - r25.s64;
	// mr r28,r23
	r28.u64 = r23.u64;
loc_82672244:
	// cmpw cr6,r30,r14
	cr6.compare<int32_t>(r30.s32, r14.s32, xer);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// blt cr6,0x82672254
	if (cr6.lt) goto loc_82672254;
	// mr r5,r14
	ctx.r5.u64 = r14.u64;
loc_82672254:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82672268
	if (!cr6.gt) goto loc_82672268;
	// add r4,r26,r31
	ctx.r4.u64 = r26.u64 + r31.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82672268:
	// cmpw cr6,r29,r14
	cr6.compare<int32_t>(r29.s32, r14.s32, xer);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// blt cr6,0x82672278
	if (cr6.lt) goto loc_82672278;
	// mr r5,r14
	ctx.r5.u64 = r14.u64;
loc_82672278:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82672294
	if (!cr6.gt) goto loc_82672294;
	// subf r11,r5,r27
	r11.s64 = r27.s64 - ctx.r5.s64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r4,r11,r24
	ctx.r4.u64 = r11.u64 + r24.u64;
	// add r3,r11,r25
	ctx.r3.u64 = r11.u64 + r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82672294:
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r31,r31,r14
	r31.u64 = r31.u64 + r14.u64;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82672244
	if (!cr6.eq) goto loc_82672244;
loc_826722AC:
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// ble cr6,0x82672378
	if (!cr6.gt) goto loc_82672378;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// mr r28,r15
	r28.u64 = r15.u64;
	// subf r27,r15,r11
	r27.s64 = r11.s64 - r15.s64;
loc_826722C4:
	// add r11,r27,r28
	r11.u64 = r27.u64 + r28.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// cmpw cr6,r30,r22
	cr6.compare<int32_t>(r30.s32, r22.s32, xer);
	// blt cr6,0x826722dc
	if (cr6.lt) goto loc_826722DC;
	// mr r30,r22
	r30.u64 = r22.u64;
loc_826722DC:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82672314
	if (!cr6.gt) goto loc_82672314;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mullw r31,r11,r22
	r31.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// lwz r11,460(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// add r4,r31,r19
	ctx.r4.u64 = r31.u64 + r19.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r18
	ctx.r4.u64 = r31.u64 + r18.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82672314:
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// subf r30,r11,r22
	r30.s64 = r22.s64 - r11.s64;
	// cmpw cr6,r30,r22
	cr6.compare<int32_t>(r30.s32, r22.s32, xer);
	// blt cr6,0x82672328
	if (cr6.lt) goto loc_82672328;
	// mr r30,r22
	r30.u64 = r22.u64;
loc_82672328:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82672368
	if (!cr6.gt) goto loc_82672368;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r11,r11,r22
	r11.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// subf r31,r30,r11
	r31.s64 = r11.s64 - r30.s64;
	// lwz r11,460(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// add r4,r31,r16
	ctx.r4.u64 = r31.u64 + r16.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r21
	ctx.r4.u64 = r31.u64 + r21.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82672368:
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// addi r28,r28,-2
	r28.s64 = r28.s64 + -2;
	// cmpw cr6,r29,r23
	cr6.compare<int32_t>(r29.s32, r23.s32, xer);
	// blt cr6,0x826722c4
	if (cr6.lt) goto loc_826722C4;
loc_82672378:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addic. r27,r11,1
	xer.ca = r11.u32 > 4294967294;
	r27.s64 = r11.s64 + 1;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// addi r11,r22,-1
	r11.s64 = r22.s64 + -1;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// bge 0x82672390
	if (!cr0.lt) goto loc_82672390;
	// li r27,0
	r27.s64 = 0;
loc_82672390:
	// subf r11,r17,r25
	r11.s64 = r25.s64 - r17.s64;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// add r21,r27,r17
	r21.u64 = r27.u64 + r17.u64;
	// subfic r18,r17,1
	xer.ca = r17.u32 <= 1;
	r18.s64 = 1 - r17.s64;
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r11.u32);
	// subf r11,r17,r24
	r11.s64 = r24.s64 - r17.s64;
	// lfd f25,4120(r10)
	ctx.fpscr.disableFlushMode();
	f25.u64 = PPC_LOAD_U64(ctx.r10.u32 + 4120);
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f28,-28592(r11)
	f28.u64 = PPC_LOAD_U64(r11.u32 + -28592);
loc_826723B8:
	// cmpw cr6,r15,r14
	cr6.compare<int32_t>(r15.s32, r14.s32, xer);
	// mr r11,r15
	r11.u64 = r15.u64;
	// blt cr6,0x826723c8
	if (cr6.lt) goto loc_826723C8;
	// mr r11,r14
	r11.u64 = r14.u64;
loc_826723C8:
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// bge cr6,0x826725a8
	if (!cr6.lt) goto loc_826725A8;
	// extsw r11,r27
	r11.s64 = r27.s32;
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// lfd f0,152(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f0,f0,f27
	f0.f64 = f0.f64 - f27.f64;
	// fsub f0,f0,f26
	f0.f64 = f0.f64 - f26.f64;
	// fmul f29,f0,f31
	f29.f64 = f0.f64 * f31.f64;
	// fdiv f1,f29,f30
	ctx.f1.f64 = f29.f64 / f30.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fsub f0,f25,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f25.f64 - ctx.f1.f64;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// fmsub f13,f1,f30,f29
	ctx.f13.f64 = ctx.f1.f64 * f30.f64 - f29.f64;
	// addi r8,r1,116
	ctx.r8.s64 = ctx.r1.s64 + 116;
	// addi r11,r14,1
	r11.s64 = r14.s64 + 1;
	// add r10,r18,r21
	ctx.r10.u64 = r18.u64 + r21.u64;
	// cmpw cr6,r10,r23
	cr6.compare<int32_t>(ctx.r10.s32, r23.s32, xer);
	// fmsub f0,f0,f30,f29
	f0.f64 = f0.f64 * f30.f64 - f29.f64;
	// fmadd f13,f13,f31,f28
	ctx.f13.f64 = ctx.f13.f64 * f31.f64 + f28.f64;
	// fmadd f0,f0,f31,f28
	f0.f64 = f0.f64 * f31.f64 + f28.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r30,112(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mullw r9,r11,r30
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// lwz r29,116(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + r27.u64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// add r4,r9,r17
	ctx.r4.u64 = ctx.r9.u64 + r17.u64;
	// add r5,r11,r17
	ctx.r5.u64 = r11.u64 + r17.u64;
	// blt cr6,0x82672454
	if (cr6.lt) goto loc_82672454;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
loc_82672454:
	// subf r9,r14,r29
	ctx.r9.s64 = r29.s64 - r14.s64;
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// subf r11,r14,r30
	r11.s64 = r30.s64 - r14.s64;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// add r10,r9,r27
	ctx.r10.u64 = ctx.r9.u64 + r27.u64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// subf r3,r30,r23
	ctx.r3.s64 = r23.s64 - r30.s64;
	// subf r9,r29,r23
	ctx.r9.s64 = r23.s64 - r29.s64;
	// add r7,r11,r21
	ctx.r7.u64 = r11.u64 + r21.u64;
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// addi r8,r14,-1
	ctx.r8.s64 = r14.s64 + -1;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// add r3,r11,r21
	ctx.r3.u64 = r11.u64 + r21.u64;
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// clrlwi r11,r27,31
	r11.u64 = r27.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8267259c
	if (!cr6.eq) goto loc_8267259C;
	// srawi r31,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	r31.s64 = r27.s32 >> 1;
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// addi r9,r22,1
	ctx.r9.s64 = r22.s64 + 1;
	// srawi r10,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r10.s64 = r29.s32 >> 1;
	// mullw r8,r9,r11
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r28,r9,r31
	r28.u64 = ctx.r9.u64 + r31.u64;
	// lwz r9,460(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// add r29,r8,r31
	r29.u64 = ctx.r8.u64 + r31.u64;
	// addi r30,r31,1
	r30.s64 = r31.s64 + 1;
	// add r3,r31,r9
	ctx.r3.u64 = r31.u64 + ctx.r9.u64;
	// add r6,r31,r19
	ctx.r6.u64 = r31.u64 + r19.u64;
	// add r7,r31,r16
	ctx.r7.u64 = r31.u64 + r16.u64;
	// add r4,r29,r19
	ctx.r4.u64 = r29.u64 + r19.u64;
	// add r5,r28,r19
	ctx.r5.u64 = r28.u64 + r19.u64;
	// cmpw cr6,r30,r20
	cr6.compare<int32_t>(r30.s32, r20.s32, xer);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// blt cr6,0x82672504
	if (cr6.lt) goto loc_82672504;
	// mr r8,r20
	ctx.r8.u64 = r20.u64;
loc_82672504:
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// subf r8,r22,r10
	ctx.r8.s64 = ctx.r10.s64 - r22.s64;
	// subf r9,r22,r11
	ctx.r9.s64 = r11.s64 - r22.s64;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// subf r25,r10,r20
	r25.s64 = r20.s64 - ctx.r10.s64;
	// addi r24,r8,1
	r24.s64 = ctx.r8.s64 + 1;
	// lwz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// subf r23,r11,r20
	r23.s64 = r20.s64 - r11.s64;
	// addi r26,r9,1
	r26.s64 = ctx.r9.s64 + 1;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r25.u32);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// cmpw cr6,r30,r20
	cr6.compare<int32_t>(r30.s32, r20.s32, xer);
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// add r4,r29,r11
	ctx.r4.u64 = r29.u64 + r11.u64;
	// add r5,r28,r11
	ctx.r5.u64 = r28.u64 + r11.u64;
	// add r6,r31,r11
	ctx.r6.u64 = r31.u64 + r11.u64;
	// lwz r11,444(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// add r7,r31,r11
	ctx.r7.u64 = r31.u64 + r11.u64;
	// blt cr6,0x82672570
	if (cr6.lt) goto loc_82672570;
	// mr r30,r20
	r30.u64 = r20.u64;
loc_82672570:
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// stw r30,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r30.u32);
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r25.u32);
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// lwz r23,396(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// lwz r16,436(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// lwz r17,404(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// lwz r19,412(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
loc_8267259C:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// b 0x826723b8
	goto loc_826723B8;
loc_826725A8:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// li r18,1
	r18.s64 = 1;
	// subf r11,r14,r11
	r11.s64 = r11.s64 - r14.s64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// blt cr6,0x826725c4
	if (cr6.lt) goto loc_826725C4;
	// mr r18,r11
	r18.u64 = r11.u64;
loc_826725C4:
	// subf r10,r14,r15
	ctx.r10.s64 = r15.s64 - r14.s64;
	// addi r9,r14,-1
	ctx.r9.s64 = r14.s64 + -1;
	// mullw r11,r18,r14
	r11.s64 = int64_t(r18.s32) * int64_t(r14.s32);
	// addi r28,r10,2
	r28.s64 = ctx.r10.s64 + 2;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r21,r18,1
	r21.s64 = r18.s64 + 1;
	// subf r15,r18,r23
	r15.s64 = r23.s64 - r18.s64;
	// addi r19,r9,-1
	r19.s64 = ctx.r9.s64 + -1;
	// mr r16,r11
	r16.u64 = r11.u64;
	// stw r28,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r28.u32);
	// add r17,r11,r17
	r17.u64 = r11.u64 + r17.u64;
loc_826725F0:
	// cmpw cr6,r23,r28
	cr6.compare<int32_t>(r23.s32, r28.s32, xer);
	// mr r11,r23
	r11.u64 = r23.u64;
	// blt cr6,0x82672600
	if (cr6.lt) goto loc_82672600;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_82672600:
	// cmpw cr6,r18,r11
	cr6.compare<int32_t>(r18.s32, r11.s32, xer);
	// bge cr6,0x826727f8
	if (!cr6.lt) goto loc_826727F8;
	// add r30,r21,r19
	r30.u64 = r21.u64 + r19.u64;
	// extsw r11,r30
	r11.s64 = r30.s32;
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// lfd f0,152(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f0,f0,f27
	f0.f64 = f0.f64 - f27.f64;
	// fsub f0,f0,f26
	f0.f64 = f0.f64 - f26.f64;
	// fmul f29,f0,f31
	f29.f64 = f0.f64 * f31.f64;
	// fdiv f1,f29,f30
	ctx.f1.f64 = f29.f64 / f30.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fsub f0,f25,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f25.f64 - ctx.f1.f64;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// fmsub f13,f1,f30,f29
	ctx.f13.f64 = ctx.f1.f64 * f30.f64 - f29.f64;
	// addi r9,r1,116
	ctx.r9.s64 = ctx.r1.s64 + 116;
	// addi r10,r14,1
	ctx.r10.s64 = r14.s64 + 1;
	// addi r8,r14,1
	ctx.r8.s64 = r14.s64 + 1;
	// cmpw cr6,r14,r15
	cr6.compare<int32_t>(r14.s32, r15.s32, xer);
	// fmsub f0,f0,f30,f29
	f0.f64 = f0.f64 * f30.f64 - f29.f64;
	// fmadd f13,f13,f31,f28
	ctx.f13.f64 = ctx.f13.f64 * f31.f64 + f28.f64;
	// fmadd f0,f0,f31,f28
	f0.f64 = f0.f64 * f31.f64 + f28.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r31,112(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mullw r11,r10,r31
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r31.s32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r29,116(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r9,404(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// mullw r10,r8,r29
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(r29.s32);
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// add r10,r10,r16
	ctx.r10.u64 = ctx.r10.u64 + r16.u64;
	// add r4,r11,r9
	ctx.r4.u64 = r11.u64 + ctx.r9.u64;
	// mr r11,r14
	r11.u64 = r14.u64;
	// add r5,r10,r9
	ctx.r5.u64 = ctx.r10.u64 + ctx.r9.u64;
	// blt cr6,0x82672698
	if (cr6.lt) goto loc_82672698;
	// mr r11,r15
	r11.u64 = r15.u64;
loc_82672698:
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// subf r11,r31,r23
	r11.s64 = r23.s64 - r31.s64;
	// subf r10,r29,r23
	ctx.r10.s64 = r23.s64 - r29.s64;
	// lwz r7,136(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// subf r11,r18,r11
	r11.s64 = r11.s64 - r18.s64;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// subf r9,r18,r10
	ctx.r9.s64 = ctx.r10.s64 - r18.s64;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// addi r8,r14,-1
	ctx.r8.s64 = r14.s64 + -1;
	// add r7,r17,r7
	ctx.r7.u64 = r17.u64 + ctx.r7.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// add r3,r17,r11
	ctx.r3.u64 = r17.u64 + r11.u64;
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826727e0
	if (!cr6.eq) goto loc_826727E0;
	// srawi r30,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r30.s64 = r31.s32 >> 1;
	// srawi r29,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r29.s64 = r29.s32 >> 1;
	// srawi r11,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	r11.s64 = r21.s32 >> 1;
	// srawi r7,r19,1
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x1) != 0);
	ctx.r7.s64 = r19.s32 >> 1;
	// srawi r10,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 1;
	// mullw r8,r11,r22
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// stw r29,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r29.u32);
	// add r31,r8,r7
	r31.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mullw r8,r29,r22
	ctx.r8.s64 = int64_t(r29.s32) * int64_t(r22.s32);
	// addi r28,r10,1
	r28.s64 = ctx.r10.s64 + 1;
	// add r10,r8,r31
	ctx.r10.u64 = ctx.r8.u64 + r31.u64;
	// mullw r9,r30,r22
	ctx.r9.s64 = int64_t(r30.s32) * int64_t(r22.s32);
	// add r25,r10,r29
	r25.u64 = ctx.r10.u64 + r29.u64;
	// lwz r10,460(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// add r3,r31,r10
	ctx.r3.u64 = r31.u64 + ctx.r10.u64;
	// lwz r10,412(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// add r26,r9,r30
	r26.u64 = ctx.r9.u64 + r30.u64;
	// lwz r9,436(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// subf r27,r11,r20
	r27.s64 = r20.s64 - r11.s64;
	// add r6,r31,r10
	ctx.r6.u64 = r31.u64 + ctx.r10.u64;
	// add r4,r26,r10
	ctx.r4.u64 = r26.u64 + ctx.r10.u64;
	// add r5,r25,r10
	ctx.r5.u64 = r25.u64 + ctx.r10.u64;
	// add r7,r31,r9
	ctx.r7.u64 = r31.u64 + ctx.r9.u64;
	// cmpw cr6,r28,r27
	cr6.compare<int32_t>(r28.s32, r27.s32, xer);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// blt cr6,0x8267275c
	if (cr6.lt) goto loc_8267275C;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_8267275C:
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// subf r9,r29,r20
	ctx.r9.s64 = r20.s64 - r29.s64;
	// subf r10,r30,r20
	ctx.r10.s64 = r20.s64 - r30.s64;
	// lwz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// subf r24,r11,r9
	r24.s64 = ctx.r9.s64 - r11.s64;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// subf r23,r11,r10
	r23.s64 = ctx.r10.s64 - r11.s64;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// stw r24,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r24.u32);
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// cmpw cr6,r28,r27
	cr6.compare<int32_t>(r28.s32, r27.s32, xer);
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// add r4,r26,r11
	ctx.r4.u64 = r26.u64 + r11.u64;
	// add r5,r25,r11
	ctx.r5.u64 = r25.u64 + r11.u64;
	// add r6,r31,r11
	ctx.r6.u64 = r31.u64 + r11.u64;
	// lwz r11,444(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// add r7,r31,r11
	ctx.r7.u64 = r31.u64 + r11.u64;
	// blt cr6,0x826727b8
	if (cr6.lt) goto loc_826727B8;
	// mr r28,r27
	r28.u64 = r27.u64;
loc_826727B8:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// lwz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// stw r28,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r28.u32);
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// stw r24,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r24.u32);
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// lwz r23,396(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// lwz r28,120(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
loc_826727E0:
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// add r16,r16,r14
	r16.u64 = r16.u64 + r14.u64;
	// add r17,r17,r14
	r17.u64 = r17.u64 + r14.u64;
	// addi r15,r15,-1
	r15.s64 = r15.s64 + -1;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// b 0x826725f0
	goto loc_826725F0;
loc_826727F8:
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d628
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82672808"))) PPC_WEAK_FUNC(sub_82672808);
PPC_FUNC_IMPL(__imp__sub_82672808) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5d8
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r9,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, ctx.r9.u32);
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// lwz r9,476(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// mr r23,r8
	r23.u64 = ctx.r8.u64;
	// stw r5,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, ctx.r5.u32);
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// lwz r11,484(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	// stw r10,444(r1)
	PPC_STORE_U32(ctx.r1.u32 + 444, ctx.r10.u32);
	// addi r10,r22,-1
	ctx.r10.s64 = r22.s64 + -1;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// stw r4,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, ctx.r4.u32);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stw r6,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, ctx.r6.u32);
	// mr r19,r7
	r19.u64 = ctx.r7.u64;
	// std r9,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r9.u64);
	// addi r7,r1,136
	ctx.r7.s64 = ctx.r1.s64 + 136;
	// srawi r16,r22,1
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x1) != 0);
	r16.s64 = r22.s32 >> 1;
	// std r11,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, r11.u64);
	// lfd f0,136(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// std r10,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r10.u64);
	// lfd f12,120(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// srawi r18,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	r18.s64 = ctx.r4.s32 >> 1;
	// lfd f30,4128(r11)
	f30.u64 = PPC_LOAD_U64(r11.u32 + 4128);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// fmul f0,f0,f30
	f0.f64 = f0.f64 * f30.f64;
	// fsub f27,f12,f0
	f27.f64 = ctx.f12.f64 - f0.f64;
	// fmr f26,f0
	f26.f64 = f0.f64;
	// fsub f0,f27,f0
	f0.f64 = f27.f64 - f0.f64;
	// lfd f13,128(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f31,f13
	f31.f64 = double(ctx.f13.s64);
	// lfd f13,4136(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 4136);
	// fmul f13,f31,f13
	ctx.f13.f64 = f31.f64 * ctx.f13.f64;
	// fsub f12,f0,f13
	ctx.f12.f64 = f0.f64 - ctx.f13.f64;
	// fadd f0,f0,f13
	f0.f64 = f0.f64 + ctx.f13.f64;
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r7
	PPC_STORE_U32(ctx.r7.u32, f0.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x826728d8
	if (!cr6.lt) goto loc_826728D8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
loc_826728D8:
	// lwz r14,136(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// bge cr6,0x826728e8
	if (!cr6.lt) goto loc_826728E8;
	// addi r14,r14,-1
	r14.s64 = r14.s64 + -1;
loc_826728E8:
	// lwz r24,452(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82672998
	if (!cr6.gt) goto loc_82672998;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// li r31,0
	r31.s64 = 0;
	// mr r26,r14
	r26.u64 = r14.u64;
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// mr r30,r24
	r30.u64 = r24.u64;
	// subf r28,r14,r22
	r28.s64 = r22.s64 - r14.s64;
	// subf r25,r24,r5
	r25.s64 = ctx.r5.s64 - r24.s64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
loc_82672914:
	// cmpw cr6,r22,r29
	cr6.compare<int32_t>(r22.s32, r29.s32, xer);
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// blt cr6,0x82672924
	if (cr6.lt) goto loc_82672924;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
loc_82672924:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82672938
	if (!cr6.gt) goto loc_82672938;
	// add r4,r25,r30
	ctx.r4.u64 = r25.u64 + r30.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82672938:
	// cmpw cr6,r22,r28
	cr6.compare<int32_t>(r22.s32, r28.s32, xer);
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// blt cr6,0x82672948
	if (cr6.lt) goto loc_82672948;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
loc_82672948:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82672970
	if (!cr6.gt) goto loc_82672970;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// mr r11,r26
	r11.u64 = r26.u64;
	// bgt cr6,0x82672960
	if (cr6.gt) goto loc_82672960;
	// li r11,0
	r11.s64 = 0;
loc_82672960:
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r4,r11,r23
	ctx.r4.u64 = r11.u64 + r23.u64;
	// add r3,r11,r24
	ctx.r3.u64 = r11.u64 + r24.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82672970:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// add r31,r31,r22
	r31.u64 = r31.u64 + r22.u64;
	// add r30,r30,r22
	r30.u64 = r30.u64 + r22.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82672914
	if (!cr6.eq) goto loc_82672914;
	// lwz r5,404(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// lwz r4,396(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
loc_82672998:
	// lwz r17,460(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82672a7c
	if (!cr6.gt) goto loc_82672A7C;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r28,r14
	r28.u64 = r14.u64;
	// subf r27,r14,r11
	r27.s64 = r11.s64 - r14.s64;
loc_826729B4:
	// add r11,r27,r28
	r11.u64 = r27.u64 + r28.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// cmpw cr6,r30,r16
	cr6.compare<int32_t>(r30.s32, r16.s32, xer);
	// blt cr6,0x826729cc
	if (cr6.lt) goto loc_826729CC;
	// mr r30,r16
	r30.u64 = r16.u64;
loc_826729CC:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82672a04
	if (!cr6.gt) goto loc_82672A04;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mullw r31,r11,r16
	r31.s64 = int64_t(r11.s32) * int64_t(r16.s32);
	// lwz r11,412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// add r3,r31,r17
	ctx.r3.u64 = r31.u64 + r17.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r19
	ctx.r4.u64 = r31.u64 + r19.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82672A04:
	// srawi r10,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	ctx.r10.s64 = r28.s32 >> 1;
	// subf r30,r10,r16
	r30.s64 = r16.s64 - ctx.r10.s64;
	// cmpw cr6,r30,r16
	cr6.compare<int32_t>(r30.s32, r16.s32, xer);
	// blt cr6,0x82672a18
	if (cr6.lt) goto loc_82672A18;
	// mr r30,r16
	r30.u64 = r16.u64;
loc_82672A18:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82672a64
	if (!cr6.gt) goto loc_82672A64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bgt cr6,0x82672a2c
	if (cr6.gt) goto loc_82672A2C;
	// li r10,0
	ctx.r10.s64 = 0;
loc_82672A2C:
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mullw r11,r11,r16
	r11.s64 = int64_t(r11.s32) * int64_t(r16.s32);
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// add r3,r31,r17
	ctx.r3.u64 = r31.u64 + r17.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,444(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82672A64:
	// lwz r11,396(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x826729b4
	if (cr6.lt) goto loc_826729B4;
	// lwz r5,404(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
loc_82672A7C:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r15,r16,1
	r15.s64 = r16.s64 + 1;
	// addic. r25,r11,1
	xer.ca = r11.u32 > 4294967294;
	r25.s64 = r11.s64 + 1;
	cr0.compare<int32_t>(r25.s32, 0, xer);
	// bge 0x82672a90
	if (!cr0.lt) goto loc_82672A90;
	// li r25,0
	r25.s64 = 0;
loc_82672A90:
	// subf r11,r5,r24
	r11.s64 = r24.s64 - ctx.r5.s64;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// add r21,r25,r5
	r21.u64 = r25.u64 + ctx.r5.u64;
	// subf r20,r25,r22
	r20.s64 = r22.s64 - r25.s64;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// subf r11,r5,r23
	r11.s64 = r23.s64 - ctx.r5.s64;
	// lfd f25,4120(r10)
	ctx.fpscr.disableFlushMode();
	f25.u64 = PPC_LOAD_U64(ctx.r10.u32 + 4120);
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f28,-28592(r11)
	f28.u64 = PPC_LOAD_U64(r11.u32 + -28592);
loc_82672AB8:
	// cmpw cr6,r14,r22
	cr6.compare<int32_t>(r14.s32, r22.s32, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// blt cr6,0x82672ac8
	if (cr6.lt) goto loc_82672AC8;
	// mr r11,r22
	r11.u64 = r22.u64;
loc_82672AC8:
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// bge cr6,0x82672c8c
	if (!cr6.lt) goto loc_82672C8C;
	// extsw r11,r25
	r11.s64 = r25.s32;
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lfd f0,144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f0,f0,f27
	f0.f64 = f0.f64 - f27.f64;
	// fadd f0,f0,f26
	f0.f64 = f0.f64 + f26.f64;
	// fmul f29,f0,f30
	f29.f64 = f0.f64 * f30.f64;
	// fdiv f1,f29,f31
	ctx.f1.f64 = f29.f64 / f31.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fsub f0,f25,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f25.f64 - ctx.f1.f64;
	// addi r10,r1,116
	ctx.r10.s64 = ctx.r1.s64 + 116;
	// fmsub f13,f1,f31,f29
	ctx.f13.f64 = ctx.f1.f64 * f31.f64 - f29.f64;
	// addi r9,r1,120
	ctx.r9.s64 = ctx.r1.s64 + 120;
	// subfic r11,r22,1
	xer.ca = r22.u32 <= 1;
	r11.s64 = 1 - r22.s64;
	// lwz r8,396(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// cmpw cr6,r20,r8
	cr6.compare<int32_t>(r20.s32, ctx.r8.s32, xer);
	// fmsub f0,f0,f31,f29
	f0.f64 = f0.f64 * f31.f64 - f29.f64;
	// fmadd f13,f13,f30,f28
	ctx.f13.f64 = ctx.f13.f64 * f30.f64 + f28.f64;
	// fmadd f0,f0,f30,f28
	f0.f64 = f0.f64 * f30.f64 + f28.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r30,116(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mullw r10,r11,r30
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r29,120(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r9,404(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// add r5,r11,r9
	ctx.r5.u64 = r11.u64 + ctx.r9.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r4,r10,r9
	ctx.r4.u64 = ctx.r10.u64 + ctx.r9.u64;
	// blt cr6,0x82672b5c
	if (cr6.lt) goto loc_82672B5C;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
loc_82672B5C:
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// subf r11,r30,r22
	r11.s64 = r22.s64 - r30.s64;
	// subf r10,r29,r22
	ctx.r10.s64 = r22.s64 - r29.s64;
	// lwz r7,128(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// subf r9,r25,r10
	ctx.r9.s64 = ctx.r10.s64 - r25.s64;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// addi r8,r22,1
	ctx.r8.s64 = r22.s64 + 1;
	// add r7,r7,r21
	ctx.r7.u64 = ctx.r7.u64 + r21.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// add r3,r11,r21
	ctx.r3.u64 = r11.u64 + r21.u64;
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// clrlwi r11,r25,31
	r11.u64 = r25.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82672c7c
	if (!cr6.eq) goto loc_82672C7C;
	// srawi r31,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	r31.s64 = r25.s32 >> 1;
	// lwz r9,412(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// srawi r30,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r30.s64 = r30.s32 >> 1;
	// srawi r29,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r29.s64 = r29.s32 >> 1;
	// subfic r11,r16,1
	xer.ca = r16.u32 <= 1;
	r11.s64 = 1 - r16.s64;
	// subf r28,r31,r16
	r28.s64 = r16.s64 - r31.s64;
	// mullw r10,r11,r30
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// stw r30,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r30.u32);
	// stw r29,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r29.u32);
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r26,r11,r31
	r26.u64 = r11.u64 + r31.u64;
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// add r27,r10,r31
	r27.u64 = ctx.r10.u64 + r31.u64;
	// add r7,r31,r11
	ctx.r7.u64 = r31.u64 + r11.u64;
	// add r3,r31,r17
	ctx.r3.u64 = r31.u64 + r17.u64;
	// add r6,r31,r9
	ctx.r6.u64 = r31.u64 + ctx.r9.u64;
	// cmpw cr6,r28,r18
	cr6.compare<int32_t>(r28.s32, r18.s32, xer);
	// add r4,r27,r9
	ctx.r4.u64 = r27.u64 + ctx.r9.u64;
	// add r5,r26,r9
	ctx.r5.u64 = r26.u64 + ctx.r9.u64;
	// mr r11,r28
	r11.u64 = r28.u64;
	// blt cr6,0x82672c04
	if (cr6.lt) goto loc_82672C04;
	// mr r11,r18
	r11.u64 = r18.u64;
loc_82672C04:
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// subf r10,r29,r16
	ctx.r10.s64 = r16.s64 - r29.s64;
	// subf r11,r30,r16
	r11.s64 = r16.s64 - r30.s64;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// subf r24,r31,r10
	r24.s64 = ctx.r10.s64 - r31.s64;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// subf r23,r31,r11
	r23.s64 = r11.s64 - r31.s64;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// stw r24,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r24.u32);
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// add r4,r27,r19
	ctx.r4.u64 = r27.u64 + r19.u64;
	// add r5,r26,r19
	ctx.r5.u64 = r26.u64 + r19.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// lwz r11,444(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// add r6,r31,r19
	ctx.r6.u64 = r31.u64 + r19.u64;
	// add r7,r31,r11
	ctx.r7.u64 = r31.u64 + r11.u64;
	// cmpw cr6,r28,r18
	cr6.compare<int32_t>(r28.s32, r18.s32, xer);
	// blt cr6,0x82672c5c
	if (cr6.lt) goto loc_82672C5C;
	// mr r28,r18
	r28.u64 = r18.u64;
loc_82672C5C:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// stw r28,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r28.u32);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// stw r24,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r24.u32);
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
loc_82672C7C:
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// addi r20,r20,-1
	r20.s64 = r20.s64 + -1;
	// b 0x82672ab8
	goto loc_82672AB8;
loc_82672C8C:
	// subfic r28,r14,1
	xer.ca = r14.u32 <= 1;
	r28.s64 = 1 - r14.s64;
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// bgt cr6,0x82672c9c
	if (cr6.gt) goto loc_82672C9C;
	// li r28,1
	r28.s64 = 1;
loc_82672C9C:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mullw r11,r28,r22
	r11.s64 = int64_t(r28.s32) * int64_t(r22.s32);
	// neg r20,r10
	r20.s64 = -ctx.r10.s64;
	// lwz r10,396(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// mr r23,r11
	r23.u64 = r11.u64;
	// subf r21,r28,r10
	r21.s64 = ctx.r10.s64 - r28.s64;
	// lwz r10,404(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// add r24,r11,r10
	r24.u64 = r11.u64 + ctx.r10.u64;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f24,-31368(r11)
	ctx.fpscr.disableFlushMode();
	f24.u64 = PPC_LOAD_U64(r11.u32 + -31368);
loc_82672CC4:
	// lwz r11,396(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// cmpw cr6,r20,r11
	cr6.compare<int32_t>(r20.s32, r11.s32, xer);
	// bge cr6,0x82672cd4
	if (!cr6.lt) goto loc_82672CD4;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_82672CD4:
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// bge cr6,0x82672ee8
	if (!cr6.lt) goto loc_82672EE8;
	// extsw r11,r28
	r11.s64 = r28.s32;
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lfd f0,144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fadd f0,f0,f27
	f0.f64 = f0.f64 + f27.f64;
	// fsub f0,f26,f0
	f0.f64 = f26.f64 - f0.f64;
	// fmul f29,f0,f30
	f29.f64 = f0.f64 * f30.f64;
	// fdiv f1,f29,f31
	ctx.f1.f64 = f29.f64 / f31.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fsub f0,f25,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f25.f64 - ctx.f1.f64;
	// addi r11,r1,116
	r11.s64 = ctx.r1.s64 + 116;
	// fmsub f13,f1,f31,f29
	ctx.f13.f64 = ctx.f1.f64 * f31.f64 - f29.f64;
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// lwz r9,404(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// cmpw cr6,r22,r21
	cr6.compare<int32_t>(r22.s32, r21.s32, xer);
	// fmsub f0,f0,f31,f29
	f0.f64 = f0.f64 * f31.f64 - f29.f64;
	// fmadd f13,f13,f30,f28
	ctx.f13.f64 = ctx.f13.f64 * f30.f64 + f28.f64;
	// fmadd f0,f0,f30,f28
	f0.f64 = f0.f64 * f30.f64 + f28.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r31,116(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// neg r30,r31
	r30.s64 = -r31.s64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r29,120(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// mullw r10,r30,r22
	ctx.r10.s64 = int64_t(r30.s32) * int64_t(r22.s32);
	// neg r27,r29
	r27.s64 = -r29.s64;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// mullw r11,r27,r22
	r11.s64 = int64_t(r27.s32) * int64_t(r22.s32);
	// add r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r4,r10,r9
	ctx.r4.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r5,r11,r9
	ctx.r5.u64 = r11.u64 + ctx.r9.u64;
	// mr r11,r22
	r11.u64 = r22.u64;
	// blt cr6,0x82672d70
	if (cr6.lt) goto loc_82672D70;
	// mr r11,r21
	r11.u64 = r21.u64;
loc_82672D70:
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// subf r11,r28,r29
	r11.s64 = r29.s64 - r28.s64;
	// add r3,r30,r22
	ctx.r3.u64 = r30.u64 + r22.u64;
	// lwz r7,128(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// add r9,r27,r22
	ctx.r9.u64 = r27.u64 + r22.u64;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// subf r10,r28,r31
	ctx.r10.s64 = r31.s64 - r28.s64;
	// addi r8,r22,1
	ctx.r8.s64 = r22.s64 + 1;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// add r7,r24,r7
	ctx.r7.u64 = r24.u64 + ctx.r7.u64;
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// add r3,r24,r11
	ctx.r3.u64 = r24.u64 + r11.u64;
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// clrlwi r11,r28,31
	r11.u64 = r28.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82672ed4
	if (!cr6.eq) goto loc_82672ED4;
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// subfic r10,r16,1
	xer.ca = r16.u32 <= 1;
	ctx.r10.s64 = 1 - r16.s64;
	// srawi r9,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	ctx.r9.s64 = r31.s32 >> 1;
	// srawi r7,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r7.s64 = r29.s32 >> 1;
	// mullw r8,r9,r10
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// mullw r11,r11,r16
	r11.s64 = int64_t(r11.s32) * int64_t(r16.s32);
	// mullw r9,r7,r10
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// li r26,0
	r26.s64 = 0;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// add r10,r8,r11
	ctx.r10.u64 = ctx.r8.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// ble cr6,0x82672ed4
	if (!cr6.gt) goto loc_82672ED4;
	// add r7,r9,r19
	ctx.r7.u64 = ctx.r9.u64 + r19.u64;
	// lwz r9,412(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// subf r25,r30,r27
	r25.s64 = r27.s64 - r30.s64;
	// subf r4,r19,r9
	ctx.r4.s64 = ctx.r9.s64 - r19.s64;
	// lwz r9,436(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// add r8,r10,r19
	ctx.r8.u64 = ctx.r10.u64 + r19.u64;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// subf r29,r31,r29
	r29.s64 = r29.s64 - r31.s64;
	// subf r27,r31,r28
	r27.s64 = r28.s64 - r31.s64;
	// subf r31,r19,r9
	r31.s64 = ctx.r9.s64 - r19.s64;
	// lwz r9,444(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// add r5,r30,r28
	ctx.r5.u64 = r30.u64 + r28.u64;
	// subf r30,r17,r9
	r30.s64 = ctx.r9.s64 - r17.s64;
	// lwz r9,468(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// add r11,r11,r17
	r11.u64 = r11.u64 + r17.u64;
	// subf r3,r17,r19
	ctx.r3.s64 = r19.s64 - r17.s64;
	// subf r6,r17,r9
	ctx.r6.s64 = ctx.r9.s64 - r17.s64;
loc_82672E30:
	// lwz r18,396(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// add r9,r27,r10
	ctx.r9.u64 = r27.u64 + ctx.r10.u64;
	// cmpw cr6,r9,r18
	cr6.compare<int32_t>(ctx.r9.s32, r18.s32, xer);
	// bge cr6,0x82672ed4
	if (!cr6.lt) goto loc_82672ED4;
	// add r9,r29,r10
	ctx.r9.u64 = r29.u64 + ctx.r10.u64;
	// cmpw cr6,r9,r22
	cr6.compare<int32_t>(ctx.r9.s32, r22.s32, xer);
	// bge cr6,0x82672e64
	if (!cr6.lt) goto loc_82672E64;
	// add. r9,r25,r5
	ctx.r9.u64 = r25.u64 + ctx.r5.u64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt 0x82672e64
	if (cr0.lt) goto loc_82672E64;
	// lbzx r9,r4,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r7.u32);
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// lbz r9,0(r7)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// b 0x82672eb0
	goto loc_82672EB0;
loc_82672E64:
	// cmpw cr6,r10,r22
	cr6.compare<int32_t>(ctx.r10.s32, r22.s32, xer);
	// bge cr6,0x82672ea0
	if (!cr6.lt) goto loc_82672EA0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// blt cr6,0x82672ea0
	if (cr6.lt) goto loc_82672EA0;
	// fcmpu cr6,f29,f24
	ctx.fpscr.disableFlushMode();
	cr6.compare(f29.f64, f24.f64);
	// blt cr6,0x82672e8c
	if (cr6.lt) goto loc_82672E8C;
	// lbzx r9,r4,r8
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r8.u32);
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// lbz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// b 0x82672eb0
	goto loc_82672EB0;
loc_82672E8C:
	// add r9,r3,r11
	ctx.r9.u64 = ctx.r3.u64 + r11.u64;
	// lbzx r18,r9,r4
	r18.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stb r18,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r18.u8);
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// b 0x82672eb0
	goto loc_82672EB0;
loc_82672EA0:
	// add r9,r3,r31
	ctx.r9.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// lbzx r9,r30,r11
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
loc_82672EB0:
	// addi r26,r26,2
	r26.s64 = r26.s64 + 2;
	// stbx r9,r6,r11
	PPC_STORE_U8(ctx.r6.u32 + r11.u32, ctx.r9.u8);
	// add r11,r11,r15
	r11.u64 = r11.u64 + r15.u64;
	// add r8,r8,r15
	ctx.r8.u64 = ctx.r8.u64 + r15.u64;
	// add r7,r7,r15
	ctx.r7.u64 = ctx.r7.u64 + r15.u64;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// cmpw cr6,r26,r22
	cr6.compare<int32_t>(r26.s32, r22.s32, xer);
	// blt cr6,0x82672e30
	if (cr6.lt) goto loc_82672E30;
loc_82672ED4:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// add r23,r23,r22
	r23.u64 = r23.u64 + r22.u64;
	// add r24,r24,r22
	r24.u64 = r24.u64 + r22.u64;
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// b 0x82672cc4
	goto loc_82672CC4;
loc_82672EE8:
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d624
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82672EF8"))) PPC_WEAK_FUNC(sub_82672EF8);
PPC_FUNC_IMPL(__imp__sub_82672EF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5dc
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// lwz r10,476(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// lwz r11,484(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stw r9,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, ctx.r9.u32);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lwz r19,452(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// stw r8,428(r1)
	PPC_STORE_U32(ctx.r1.u32 + 428, ctx.r8.u32);
	// mr r17,r7
	r17.u64 = ctx.r7.u64;
	// stw r23,444(r1)
	PPC_STORE_U32(ctx.r1.u32 + 444, r23.u32);
	// stw r24,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, r24.u32);
	// srawi r22,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r22.s64 = r20.s32 >> 1;
	// std r10,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r10.u64);
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// srawi r11,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	r11.s64 = r24.s32 >> 1;
	// lfd f0,128(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r14,r6
	r14.u64 = ctx.r6.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// stw r25,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, r25.u32);
	// lfd f29,4128(r11)
	f29.u64 = PPC_LOAD_U64(r11.u32 + 4128);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// fmul f28,f0,f29
	f28.f64 = f0.f64 * f29.f64;
	// lfd f0,4136(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 4136);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// lfd f13,120(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f31,f13
	f31.f64 = double(ctx.f13.s64);
	// lfd f13,264(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 264);
	// fmul f13,f28,f13
	ctx.f13.f64 = f28.f64 * ctx.f13.f64;
	// fmul f0,f31,f0
	f0.f64 = f31.f64 * f0.f64;
	// fsub f12,f13,f0
	ctx.f12.f64 = ctx.f13.f64 - f0.f64;
	// fadd f0,f13,f0
	f0.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r7
	PPC_STORE_U32(ctx.r7.u32, f0.u32);
	// lwz r21,112(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r18,128(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// ble cr6,0x82673044
	if (!cr6.gt) goto loc_82673044;
	// subf r29,r21,r20
	r29.s64 = r20.s64 - r21.s64;
	// mr r31,r19
	r31.u64 = r19.u64;
	// addi r30,r18,1
	r30.s64 = r18.s64 + 1;
	// subf r26,r19,r8
	r26.s64 = ctx.r8.s64 - r19.s64;
	// subf r27,r19,r20
	r27.s64 = r20.s64 - r19.s64;
	// mr r28,r24
	r28.u64 = r24.u64;
loc_82672FDC:
	// cmpw cr6,r30,r20
	cr6.compare<int32_t>(r30.s32, r20.s32, xer);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// blt cr6,0x82672fec
	if (cr6.lt) goto loc_82672FEC;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
loc_82672FEC:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82673000
	if (!cr6.gt) goto loc_82673000;
	// add r4,r31,r26
	ctx.r4.u64 = r31.u64 + r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82673000:
	// cmpw cr6,r29,r20
	cr6.compare<int32_t>(r29.s32, r20.s32, xer);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// blt cr6,0x82673010
	if (cr6.lt) goto loc_82673010;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
loc_82673010:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x8267302c
	if (!cr6.gt) goto loc_8267302C;
	// subf r11,r5,r31
	r11.s64 = r31.s64 - ctx.r5.s64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// add r4,r11,r25
	ctx.r4.u64 = r11.u64 + r25.u64;
	// add r3,r11,r19
	ctx.r3.u64 = r11.u64 + r19.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_8267302C:
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r31,r31,r20
	r31.u64 = r31.u64 + r20.u64;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82672fdc
	if (!cr6.eq) goto loc_82672FDC;
loc_82673044:
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82673110
	if (!cr6.gt) goto loc_82673110;
	// mr r28,r21
	r28.u64 = r21.u64;
	// subf r27,r21,r18
	r27.s64 = r18.s64 - r21.s64;
loc_82673058:
	// add r11,r27,r28
	r11.u64 = r27.u64 + r28.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// cmpw cr6,r30,r22
	cr6.compare<int32_t>(r30.s32, r22.s32, xer);
	// blt cr6,0x82673070
	if (cr6.lt) goto loc_82673070;
	// mr r30,r22
	r30.u64 = r22.u64;
loc_82673070:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x826730ac
	if (!cr6.gt) goto loc_826730AC;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mullw r31,r11,r22
	r31.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// lwz r11,460(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r23
	ctx.r4.u64 = r31.u64 + r23.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826730AC:
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// subf r30,r11,r22
	r30.s64 = r22.s64 - r11.s64;
	// cmpw cr6,r30,r22
	cr6.compare<int32_t>(r30.s32, r22.s32, xer);
	// blt cr6,0x826730c0
	if (cr6.lt) goto loc_826730C0;
	// mr r30,r22
	r30.u64 = r22.u64;
loc_826730C0:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82673100
	if (!cr6.gt) goto loc_82673100;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r11,r11,r22
	r11.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// subf r31,r30,r11
	r31.s64 = r11.s64 - r30.s64;
	// lwz r11,460(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// add r4,r31,r14
	ctx.r4.u64 = r31.u64 + r14.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r17
	ctx.r4.u64 = r31.u64 + r17.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82673100:
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// addi r28,r28,-2
	r28.s64 = r28.s64 + -2;
	// cmpw cr6,r29,r24
	cr6.compare<int32_t>(r29.s32, r24.s32, xer);
	// blt cr6,0x82673058
	if (cr6.lt) goto loc_82673058;
loc_82673110:
	// addic. r23,r18,1
	xer.ca = r18.u32 > 4294967294;
	r23.s64 = r18.s64 + 1;
	cr0.compare<int32_t>(r23.s32, 0, xer);
	// addi r15,r22,-1
	r15.s64 = r22.s64 + -1;
	// bge 0x82673120
	if (!cr0.lt) goto loc_82673120;
	// li r23,0
	r23.s64 = 0;
loc_82673120:
	// lwz r11,428(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lis r9,-32243
	ctx.r9.s64 = -2113077248;
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// subf r16,r25,r11
	r16.s64 = r11.s64 - r25.s64;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// subf r18,r25,r19
	r18.s64 = r19.s64 - r25.s64;
	// lfd f25,4120(r9)
	ctx.fpscr.disableFlushMode();
	f25.u64 = PPC_LOAD_U64(ctx.r9.u32 + 4120);
	// add r21,r23,r25
	r21.u64 = r23.u64 + r25.u64;
	// lfd f27,-28592(r10)
	f27.u64 = PPC_LOAD_U64(ctx.r10.u32 + -28592);
	// subfic r19,r25,1
	xer.ca = r25.u32 <= 1;
	r19.s64 = 1 - r25.s64;
	// lfd f26,4144(r11)
	f26.u64 = PPC_LOAD_U64(r11.u32 + 4144);
loc_8267314C:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpw cr6,r10,r20
	cr6.compare<int32_t>(ctx.r10.s32, r20.s32, xer);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// blt cr6,0x82673160
	if (cr6.lt) goto loc_82673160;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_82673160:
	// cmpw cr6,r23,r11
	cr6.compare<int32_t>(r23.s32, r11.s32, xer);
	// bge cr6,0x82673328
	if (!cr6.lt) goto loc_82673328;
	// extsw r11,r23
	r11.s64 = r23.s32;
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lfd f0,144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f0,f0,f28
	f0.f64 = f0.f64 - f28.f64;
	// fsub f0,f0,f28
	f0.f64 = f0.f64 - f28.f64;
	// fmul f30,f0,f26
	f30.f64 = f0.f64 * f26.f64;
	// fdiv f1,f30,f31
	ctx.f1.f64 = f30.f64 / f31.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fmsub f13,f1,f31,f30
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64 * f31.f64 - f30.f64;
	// addi r9,r1,116
	ctx.r9.s64 = ctx.r1.s64 + 116;
	// fsub f0,f25,f1
	f0.f64 = f25.f64 - ctx.f1.f64;
	// addi r11,r20,1
	r11.s64 = r20.s64 + 1;
	// add r10,r19,r21
	ctx.r10.u64 = r19.u64 + r21.u64;
	// cmpw cr6,r10,r24
	cr6.compare<int32_t>(ctx.r10.s32, r24.s32, xer);
	// fmadd f13,f13,f29,f27
	ctx.f13.f64 = ctx.f13.f64 * f29.f64 + f27.f64;
	// fmsub f0,f0,f31,f30
	f0.f64 = f0.f64 * f31.f64 - f30.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// fmadd f0,f0,f29,f27
	f0.f64 = f0.f64 * f29.f64 + f27.f64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lwz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// neg r30,r9
	r30.s64 = -ctx.r9.s64;
	// mullw r9,r11,r30
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// add r4,r9,r25
	ctx.r4.u64 = ctx.r9.u64 + r25.u64;
	// lwz r8,120(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// neg r29,r8
	r29.s64 = -ctx.r8.s64;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	// add r5,r11,r25
	ctx.r5.u64 = r11.u64 + r25.u64;
	// blt cr6,0x826731f4
	if (cr6.lt) goto loc_826731F4;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_826731F4:
	// neg r3,r29
	ctx.r3.s64 = -r29.s64;
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// add r9,r29,r23
	ctx.r9.u64 = r29.u64 + r23.u64;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// add r11,r30,r23
	r11.u64 = r30.u64 + r23.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r3,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r3.u32);
	// neg r10,r30
	ctx.r10.s64 = -r30.s64;
	// addi r8,r20,-1
	ctx.r8.s64 = r20.s64 + -1;
	// add r7,r16,r21
	ctx.r7.u64 = r16.u64 + r21.u64;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// add r3,r18,r21
	ctx.r3.u64 = r18.u64 + r21.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// clrlwi r11,r23,31
	r11.u64 = r23.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8267331c
	if (!cr6.eq) goto loc_8267331C;
	// srawi r31,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r31.s64 = r23.s32 >> 1;
	// lwz r8,460(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// addi r9,r22,1
	ctx.r9.s64 = r22.s64 + 1;
	// srawi r10,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r10.s64 = r29.s32 >> 1;
	// add r3,r31,r8
	ctx.r3.u64 = r31.u64 + ctx.r8.u64;
	// mullw r8,r9,r11
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r29,r8,r31
	r29.u64 = ctx.r8.u64 + r31.u64;
	// add r28,r9,r31
	r28.u64 = ctx.r9.u64 + r31.u64;
	// lwz r8,136(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r9,436(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// addi r30,r31,1
	r30.s64 = r31.s64 + 1;
	// add r6,r31,r14
	ctx.r6.u64 = r31.u64 + r14.u64;
	// add r7,r31,r9
	ctx.r7.u64 = r31.u64 + ctx.r9.u64;
	// cmpw cr6,r30,r8
	cr6.compare<int32_t>(r30.s32, ctx.r8.s32, xer);
	// add r4,r29,r14
	ctx.r4.u64 = r29.u64 + r14.u64;
	// add r5,r28,r14
	ctx.r5.u64 = r28.u64 + r14.u64;
	// bge cr6,0x82673290
	if (!cr6.lt) goto loc_82673290;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
loc_82673290:
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// add r9,r10,r31
	ctx.r9.u64 = ctx.r10.u64 + r31.u64;
	// add r8,r11,r31
	ctx.r8.u64 = r11.u64 + r31.u64;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// neg r25,r10
	r25.s64 = -ctx.r10.s64;
	// addi r26,r9,1
	r26.s64 = ctx.r9.s64 + 1;
	// addi r24,r8,1
	r24.s64 = ctx.r8.s64 + 1;
	// neg r27,r11
	r27.s64 = -r11.s64;
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// stw r25,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r25.u32);
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// stw r24,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r24.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// add r4,r29,r17
	ctx.r4.u64 = r29.u64 + r17.u64;
	// add r5,r28,r17
	ctx.r5.u64 = r28.u64 + r17.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// lwz r11,444(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// add r6,r31,r17
	ctx.r6.u64 = r31.u64 + r17.u64;
	// add r7,r31,r11
	ctx.r7.u64 = r31.u64 + r11.u64;
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x826732f4
	if (cr6.lt) goto loc_826732F4;
	// mr r30,r11
	r30.u64 = r11.u64;
loc_826732F4:
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// stw r30,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r30.u32);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// stw r25,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r25.u32);
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// stw r24,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r24.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// lwz r25,404(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// lwz r24,396(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
loc_8267331C:
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// b 0x8267314c
	goto loc_8267314C;
loc_82673328:
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// li r23,1
	r23.s64 = 1;
	// subf r11,r20,r11
	r11.s64 = r11.s64 - r20.s64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// blt cr6,0x82673344
	if (cr6.lt) goto loc_82673344;
	// mr r23,r11
	r23.u64 = r11.u64;
loc_82673344:
	// subf r11,r20,r10
	r11.s64 = ctx.r10.s64 - r20.s64;
	// addi r21,r23,1
	r21.s64 = r23.s64 + 1;
	// addi r16,r11,2
	r16.s64 = r11.s64 + 2;
	// mullw r18,r23,r20
	r18.s64 = int64_t(r23.s32) * int64_t(r20.s32);
	// subf r19,r23,r24
	r19.s64 = r24.s64 - r23.s64;
loc_82673358:
	// cmpw cr6,r24,r16
	cr6.compare<int32_t>(r24.s32, r16.s32, xer);
	// mr r11,r24
	r11.u64 = r24.u64;
	// blt cr6,0x82673368
	if (cr6.lt) goto loc_82673368;
	// mr r11,r16
	r11.u64 = r16.u64;
loc_82673368:
	// cmpw cr6,r23,r11
	cr6.compare<int32_t>(r23.s32, r11.s32, xer);
	// bge cr6,0x8267355c
	if (!cr6.lt) goto loc_8267355C;
	// addi r28,r20,-1
	r28.s64 = r20.s64 + -1;
	// add r11,r28,r23
	r11.u64 = r28.u64 + r23.u64;
	// add r31,r18,r28
	r31.u64 = r18.u64 + r28.u64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lfd f0,144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f0,f0,f28
	f0.f64 = f0.f64 - f28.f64;
	// fsub f0,f0,f28
	f0.f64 = f0.f64 - f28.f64;
	// fmul f30,f0,f26
	f30.f64 = f0.f64 * f26.f64;
	// fdiv f1,f30,f31
	ctx.f1.f64 = f30.f64 / f31.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fsub f0,f25,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f25.f64 - ctx.f1.f64;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// fmsub f13,f1,f31,f30
	ctx.f13.f64 = ctx.f1.f64 * f31.f64 - f30.f64;
	// lwz r9,452(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// addi r11,r20,1
	r11.s64 = r20.s64 + 1;
	// add r3,r31,r9
	ctx.r3.u64 = r31.u64 + ctx.r9.u64;
	// lwz r9,428(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// add r6,r31,r25
	ctx.r6.u64 = r31.u64 + r25.u64;
	// add r7,r31,r9
	ctx.r7.u64 = r31.u64 + ctx.r9.u64;
	// cmpw cr6,r20,r19
	cr6.compare<int32_t>(r20.s32, r19.s32, xer);
	// fmsub f0,f0,f31,f30
	f0.f64 = f0.f64 * f31.f64 - f30.f64;
	// fmadd f13,f13,f29,f27
	ctx.f13.f64 = ctx.f13.f64 * f29.f64 + f27.f64;
	// fmadd f0,f0,f29,f27
	f0.f64 = f0.f64 * f29.f64 + f27.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r10,128(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// addi r9,r1,120
	ctx.r9.s64 = ctx.r1.s64 + 120;
	// neg r30,r10
	r30.s64 = -ctx.r10.s64;
	// mullw r10,r11,r30
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r4,r10,r25
	ctx.r4.u64 = ctx.r10.u64 + r25.u64;
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// neg r29,r9
	r29.s64 = -ctx.r9.s64;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r5,r11,r25
	ctx.r5.u64 = r11.u64 + r25.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// blt cr6,0x8267341c
	if (cr6.lt) goto loc_8267341C;
	// mr r11,r19
	r11.u64 = r19.u64;
loc_8267341C:
	// add r10,r29,r23
	ctx.r10.u64 = r29.u64 + r23.u64;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// add r11,r29,r20
	r11.u64 = r29.u64 + r20.u64;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// add r31,r30,r20
	r31.u64 = r30.u64 + r20.u64;
	// add r10,r30,r23
	ctx.r10.u64 = r30.u64 + r23.u64;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// clrlwi r11,r23,31
	r11.u64 = r23.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82673548
	if (cr6.eq) goto loc_82673548;
	// srawi r10,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r10.s64 = r30.s32 >> 1;
	// srawi r9,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r9.s64 = r29.s32 >> 1;
	// addi r7,r20,-2
	ctx.r7.s64 = r20.s64 + -2;
	// srawi r11,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	r11.s64 = r21.s32 >> 1;
	// srawi r6,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 1;
	// mullw r7,r11,r22
	ctx.r7.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// addi r8,r22,1
	ctx.r8.s64 = r22.s64 + 1;
	// add r31,r7,r6
	r31.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r7,r8,r10
	ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwz r6,136(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// mullw r8,r8,r9
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// add r28,r8,r31
	r28.u64 = ctx.r8.u64 + r31.u64;
	// lwz r8,460(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// subf r30,r11,r6
	r30.s64 = ctx.r6.s64 - r11.s64;
	// add r3,r31,r8
	ctx.r3.u64 = r31.u64 + ctx.r8.u64;
	// lwz r8,436(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// add r29,r7,r31
	r29.u64 = ctx.r7.u64 + r31.u64;
	// add r7,r31,r8
	ctx.r7.u64 = r31.u64 + ctx.r8.u64;
	// add r6,r31,r14
	ctx.r6.u64 = r31.u64 + r14.u64;
	// cmpw cr6,r22,r30
	cr6.compare<int32_t>(r22.s32, r30.s32, xer);
	// add r4,r29,r14
	ctx.r4.u64 = r29.u64 + r14.u64;
	// add r5,r28,r14
	ctx.r5.u64 = r28.u64 + r14.u64;
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// blt cr6,0x826734c0
	if (cr6.lt) goto loc_826734C0;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
loc_826734C0:
	// add r27,r10,r11
	r27.u64 = ctx.r10.u64 + r11.u64;
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// add r24,r10,r22
	r24.u64 = ctx.r10.u64 + r22.u64;
	// add r26,r9,r22
	r26.u64 = ctx.r9.u64 + r22.u64;
	// neg r25,r11
	r25.s64 = -r11.s64;
	// neg r27,r27
	r27.s64 = -r27.s64;
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// stw r24,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r24.u32);
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// stw r25,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r25.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// add r4,r29,r17
	ctx.r4.u64 = r29.u64 + r17.u64;
	// add r5,r28,r17
	ctx.r5.u64 = r28.u64 + r17.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// lwz r11,444(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// add r6,r31,r17
	ctx.r6.u64 = r31.u64 + r17.u64;
	// add r7,r31,r11
	ctx.r7.u64 = r31.u64 + r11.u64;
	// cmpw cr6,r22,r30
	cr6.compare<int32_t>(r22.s32, r30.s32, xer);
	// bge cr6,0x82673520
	if (!cr6.lt) goto loc_82673520;
	// mr r30,r22
	r30.u64 = r22.u64;
loc_82673520:
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// stw r30,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r30.u32);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// stw r25,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r25.u32);
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// stw r24,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r24.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// lwz r25,404(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// lwz r24,396(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
loc_82673548:
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// add r18,r18,r20
	r18.u64 = r18.u64 + r20.u64;
	// addi r19,r19,-1
	r19.s64 = r19.s64 + -1;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// b 0x82673358
	goto loc_82673358;
loc_8267355C:
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d628
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8267356C"))) PPC_WEAK_FUNC(sub_8267356C);
PPC_FUNC_IMPL(__imp__sub_8267356C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82673570"))) PPC_WEAK_FUNC(sub_82673570);
PPC_FUNC_IMPL(__imp__sub_82673570) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5d8
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r9,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, ctx.r9.u32);
	// mr r23,r8
	r23.u64 = ctx.r8.u64;
	// lwz r9,476(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// stw r5,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, ctx.r5.u32);
	// mr r18,r7
	r18.u64 = ctx.r7.u64;
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// lwz r11,484(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	// stw r10,444(r1)
	PPC_STORE_U32(ctx.r1.u32 + 444, ctx.r10.u32);
	// addi r10,r4,-1
	ctx.r10.s64 = ctx.r4.s64 + -1;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// stw r4,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, ctx.r4.u32);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stw r6,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, ctx.r6.u32);
	// addi r7,r1,136
	ctx.r7.s64 = ctx.r1.s64 + 136;
	// std r9,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r9.u64);
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// std r11,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, r11.u64);
	// lfd f0,136(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// std r10,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r10.u64);
	// lfd f12,120(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// srawi r15,r19,1
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x1) != 0);
	r15.s64 = r19.s32 >> 1;
	// srawi r22,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	r22.s64 = ctx.r4.s32 >> 1;
	// lfd f30,4128(r11)
	f30.u64 = PPC_LOAD_U64(r11.u32 + 4128);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// fmul f0,f0,f30
	f0.f64 = f0.f64 * f30.f64;
	// fsub f27,f12,f0
	f27.f64 = ctx.f12.f64 - f0.f64;
	// fmr f26,f0
	f26.f64 = f0.f64;
	// fsub f0,f0,f27
	f0.f64 = f0.f64 - f27.f64;
	// lfd f13,128(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f31,f13
	f31.f64 = double(ctx.f13.s64);
	// lfd f13,4136(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 4136);
	// fmul f13,f31,f13
	ctx.f13.f64 = f31.f64 * ctx.f13.f64;
	// fsub f12,f0,f13
	ctx.f12.f64 = f0.f64 - ctx.f13.f64;
	// fadd f0,f0,f13
	f0.f64 = f0.f64 + ctx.f13.f64;
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r7
	PPC_STORE_U32(ctx.r7.u32, f0.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82673640
	if (!cr6.lt) goto loc_82673640;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
loc_82673640:
	// lwz r14,136(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// bge cr6,0x82673650
	if (!cr6.lt) goto loc_82673650;
	// addi r14,r14,-1
	r14.s64 = r14.s64 + -1;
loc_82673650:
	// lwz r24,452(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82673704
	if (!cr6.gt) goto loc_82673704;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// li r31,0
	r31.s64 = 0;
	// mr r26,r14
	r26.u64 = r14.u64;
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// mr r30,r24
	r30.u64 = r24.u64;
	// subf r28,r14,r19
	r28.s64 = r19.s64 - r14.s64;
	// subf r25,r24,r23
	r25.s64 = r23.s64 - r24.s64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
loc_8267367C:
	// cmpw cr6,r19,r29
	cr6.compare<int32_t>(r19.s32, r29.s32, xer);
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// blt cr6,0x8267368c
	if (cr6.lt) goto loc_8267368C;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
loc_8267368C:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826736a0
	if (!cr6.gt) goto loc_826736A0;
	// add r4,r25,r30
	ctx.r4.u64 = r25.u64 + r30.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826736A0:
	// cmpw cr6,r19,r28
	cr6.compare<int32_t>(r19.s32, r28.s32, xer);
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// blt cr6,0x826736b0
	if (cr6.lt) goto loc_826736B0;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
loc_826736B0:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826736dc
	if (!cr6.gt) goto loc_826736DC;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// mr r11,r26
	r11.u64 = r26.u64;
	// bgt cr6,0x826736c8
	if (cr6.gt) goto loc_826736C8;
	// li r11,0
	r11.s64 = 0;
loc_826736C8:
	// lwz r10,404(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// add r3,r11,r24
	ctx.r3.u64 = r11.u64 + r24.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826736DC:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// add r31,r31,r19
	r31.u64 = r31.u64 + r19.u64;
	// add r30,r30,r19
	r30.u64 = r30.u64 + r19.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x8267367c
	if (!cr6.eq) goto loc_8267367C;
	// lwz r4,396(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// lwz r5,404(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
loc_82673704:
	// lwz r17,460(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x826737e8
	if (!cr6.gt) goto loc_826737E8;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r28,r14
	r28.u64 = r14.u64;
	// subf r27,r14,r11
	r27.s64 = r11.s64 - r14.s64;
loc_82673720:
	// add r11,r27,r28
	r11.u64 = r27.u64 + r28.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// cmpw cr6,r30,r15
	cr6.compare<int32_t>(r30.s32, r15.s32, xer);
	// blt cr6,0x82673738
	if (cr6.lt) goto loc_82673738;
	// mr r30,r15
	r30.u64 = r15.u64;
loc_82673738:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82673774
	if (!cr6.gt) goto loc_82673774;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mullw r31,r11,r15
	r31.s64 = int64_t(r11.s32) * int64_t(r15.s32);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// add r3,r31,r17
	ctx.r3.u64 = r31.u64 + r17.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,444(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82673774:
	// srawi r10,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	ctx.r10.s64 = r28.s32 >> 1;
	// subf r30,r10,r15
	r30.s64 = r15.s64 - ctx.r10.s64;
	// cmpw cr6,r30,r15
	cr6.compare<int32_t>(r30.s32, r15.s32, xer);
	// blt cr6,0x82673788
	if (cr6.lt) goto loc_82673788;
	// mr r30,r15
	r30.u64 = r15.u64;
loc_82673788:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x826737d0
	if (!cr6.gt) goto loc_826737D0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bgt cr6,0x8267379c
	if (cr6.gt) goto loc_8267379C;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8267379C:
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mullw r11,r11,r15
	r11.s64 = int64_t(r11.s32) * int64_t(r15.s32);
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// add r4,r31,r11
	ctx.r4.u64 = r31.u64 + r11.u64;
	// add r3,r31,r17
	ctx.r3.u64 = r31.u64 + r17.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r18
	ctx.r4.u64 = r31.u64 + r18.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826737D0:
	// lwz r11,396(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x82673720
	if (cr6.lt) goto loc_82673720;
	// lwz r5,404(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
loc_826737E8:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r16,r15,1
	r16.s64 = r15.s64 + 1;
	// addic. r28,r11,1
	xer.ca = r11.u32 > 4294967294;
	r28.s64 = r11.s64 + 1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// bge 0x826737fc
	if (!cr0.lt) goto loc_826737FC;
	// li r28,0
	r28.s64 = 0;
loc_826737FC:
	// subf r11,r5,r24
	r11.s64 = r24.s64 - ctx.r5.s64;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// add r21,r28,r5
	r21.u64 = r28.u64 + ctx.r5.u64;
	// subf r20,r28,r19
	r20.s64 = r19.s64 - r28.s64;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// subf r11,r5,r23
	r11.s64 = r23.s64 - ctx.r5.s64;
	// lfd f25,4120(r10)
	ctx.fpscr.disableFlushMode();
	f25.u64 = PPC_LOAD_U64(ctx.r10.u32 + 4120);
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f28,-28592(r11)
	f28.u64 = PPC_LOAD_U64(r11.u32 + -28592);
loc_82673824:
	// cmpw cr6,r14,r19
	cr6.compare<int32_t>(r14.s32, r19.s32, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// blt cr6,0x82673834
	if (cr6.lt) goto loc_82673834;
	// mr r11,r19
	r11.u64 = r19.u64;
loc_82673834:
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// bge cr6,0x82673a08
	if (!cr6.lt) goto loc_82673A08;
	// extsw r11,r28
	r11.s64 = r28.s32;
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lfd f0,144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f0,f26,f0
	f0.f64 = f26.f64 - f0.f64;
	// fsub f0,f0,f27
	f0.f64 = f0.f64 - f27.f64;
	// fmul f29,f0,f30
	f29.f64 = f0.f64 * f30.f64;
	// fdiv f1,f29,f31
	ctx.f1.f64 = f29.f64 / f31.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fsub f0,f25,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f25.f64 - ctx.f1.f64;
	// addi r11,r1,116
	r11.s64 = ctx.r1.s64 + 116;
	// fmsub f13,f1,f31,f29
	ctx.f13.f64 = ctx.f1.f64 * f31.f64 - f29.f64;
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// lwz r8,396(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// cmpw cr6,r20,r8
	cr6.compare<int32_t>(r20.s32, ctx.r8.s32, xer);
	// fmsub f0,f0,f31,f29
	f0.f64 = f0.f64 * f31.f64 - f29.f64;
	// fmadd f13,f13,f30,f28
	ctx.f13.f64 = ctx.f13.f64 * f30.f64 + f28.f64;
	// fmadd f0,f0,f30,f28
	f0.f64 = f0.f64 * f30.f64 + f28.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mullw r9,r11,r19
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(r19.s32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// neg r30,r11
	r30.s64 = -r11.s64;
	// neg r29,r10
	r29.s64 = -ctx.r10.s64;
	// mullw r11,r10,r19
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r19.s32);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r10,r9,r30
	ctx.r10.u64 = ctx.r9.u64 + r30.u64;
	// lwz r9,404(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r5,r11,r9
	ctx.r5.u64 = r11.u64 + ctx.r9.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// add r4,r10,r9
	ctx.r4.u64 = ctx.r10.u64 + ctx.r9.u64;
	// blt cr6,0x826738d4
	if (cr6.lt) goto loc_826738D4;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
loc_826738D4:
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// add r11,r29,r8
	r11.u64 = r29.u64 + ctx.r8.u64;
	// add r3,r30,r8
	ctx.r3.u64 = r30.u64 + ctx.r8.u64;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// add r10,r29,r28
	ctx.r10.u64 = r29.u64 + r28.u64;
	// addi r8,r19,1
	ctx.r8.s64 = r19.s64 + 1;
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// add r10,r30,r28
	ctx.r10.u64 = r30.u64 + r28.u64;
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// add r7,r11,r21
	ctx.r7.u64 = r11.u64 + r21.u64;
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// add r3,r11,r21
	ctx.r3.u64 = r11.u64 + r21.u64;
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// clrlwi r11,r28,31
	r11.u64 = r28.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826739f8
	if (!cr6.eq) goto loc_826739F8;
	// srawi r31,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r31.s64 = r28.s32 >> 1;
	// lwz r5,412(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// srawi r10,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r10.s64 = r29.s32 >> 1;
	// subfic r9,r15,1
	xer.ca = r15.u32 <= 1;
	ctx.r9.s64 = 1 - r15.s64;
	// subf r30,r31,r15
	r30.s64 = r15.s64 - r31.s64;
	// mullw r8,r9,r11
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r27,r9,r31
	r27.u64 = ctx.r9.u64 + r31.u64;
	// lwz r9,436(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// add r29,r8,r31
	r29.u64 = ctx.r8.u64 + r31.u64;
	// add r6,r31,r5
	ctx.r6.u64 = r31.u64 + ctx.r5.u64;
	// add r7,r31,r9
	ctx.r7.u64 = r31.u64 + ctx.r9.u64;
	// add r4,r29,r5
	ctx.r4.u64 = r29.u64 + ctx.r5.u64;
	// add r3,r31,r17
	ctx.r3.u64 = r31.u64 + r17.u64;
	// cmpw cr6,r30,r22
	cr6.compare<int32_t>(r30.s32, r22.s32, xer);
	// add r5,r27,r5
	ctx.r5.u64 = r27.u64 + ctx.r5.u64;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// blt cr6,0x82673978
	if (cr6.lt) goto loc_82673978;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
loc_82673978:
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// add r9,r10,r31
	ctx.r9.u64 = ctx.r10.u64 + r31.u64;
	// add r8,r11,r31
	ctx.r8.u64 = r11.u64 + r31.u64;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// add r25,r10,r22
	r25.u64 = ctx.r10.u64 + r22.u64;
	// neg r24,r9
	r24.s64 = -ctx.r9.s64;
	// add r23,r11,r22
	r23.u64 = r11.u64 + r22.u64;
	// neg r26,r8
	r26.s64 = -ctx.r8.s64;
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r25.u32);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// add r4,r29,r18
	ctx.r4.u64 = r29.u64 + r18.u64;
	// add r5,r27,r18
	ctx.r5.u64 = r27.u64 + r18.u64;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// lwz r11,444(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// add r6,r31,r18
	ctx.r6.u64 = r31.u64 + r18.u64;
	// add r7,r31,r11
	ctx.r7.u64 = r31.u64 + r11.u64;
	// cmpw cr6,r30,r22
	cr6.compare<int32_t>(r30.s32, r22.s32, xer);
	// blt cr6,0x826739d8
	if (cr6.lt) goto loc_826739D8;
	// mr r30,r22
	r30.u64 = r22.u64;
loc_826739D8:
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// stw r30,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r30.u32);
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r25.u32);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// bl 0x82671d38
	sub_82671D38(ctx, base);
loc_826739F8:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// addi r20,r20,-1
	r20.s64 = r20.s64 + -1;
	// b 0x82673824
	goto loc_82673824;
loc_82673A08:
	// subfic r28,r14,1
	xer.ca = r14.u32 <= 1;
	r28.s64 = 1 - r14.s64;
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// bgt cr6,0x82673a18
	if (cr6.gt) goto loc_82673A18;
	// li r28,1
	r28.s64 = 1;
loc_82673A18:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mullw r11,r28,r19
	r11.s64 = int64_t(r28.s32) * int64_t(r19.s32);
	// neg r21,r10
	r21.s64 = -ctx.r10.s64;
	// lwz r10,396(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// mr r23,r11
	r23.u64 = r11.u64;
	// subf r22,r28,r10
	r22.s64 = ctx.r10.s64 - r28.s64;
	// lwz r10,404(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// add r24,r11,r10
	r24.u64 = r11.u64 + ctx.r10.u64;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f24,-31368(r11)
	ctx.fpscr.disableFlushMode();
	f24.u64 = PPC_LOAD_U64(r11.u32 + -31368);
loc_82673A40:
	// lwz r11,396(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// cmpw cr6,r21,r11
	cr6.compare<int32_t>(r21.s32, r11.s32, xer);
	// bge cr6,0x82673a50
	if (!cr6.lt) goto loc_82673A50;
	// mr r11,r21
	r11.u64 = r21.u64;
loc_82673A50:
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// bge cr6,0x82673c78
	if (!cr6.lt) goto loc_82673C78;
	// extsw r11,r28
	r11.s64 = r28.s32;
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lfd f0,144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fadd f0,f0,f26
	f0.f64 = f0.f64 + f26.f64;
	// fsub f0,f0,f27
	f0.f64 = f0.f64 - f27.f64;
	// fmul f29,f0,f30
	f29.f64 = f0.f64 * f30.f64;
	// fdiv f1,f29,f31
	ctx.f1.f64 = f29.f64 / f31.f64;
	// bl 0x8239dcf0
	sub_8239DCF0(ctx, base);
	// fsub f0,f25,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f25.f64 - ctx.f1.f64;
	// addi r11,r1,116
	r11.s64 = ctx.r1.s64 + 116;
	// fmsub f13,f1,f31,f29
	ctx.f13.f64 = ctx.f1.f64 * f31.f64 - f29.f64;
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// lwz r9,404(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// cmpw cr6,r19,r22
	cr6.compare<int32_t>(r19.s32, r22.s32, xer);
	// fmsub f0,f0,f31,f29
	f0.f64 = f0.f64 * f31.f64 - f29.f64;
	// fmadd f13,f13,f30,f28
	ctx.f13.f64 = ctx.f13.f64 * f30.f64 + f28.f64;
	// fmadd f0,f0,f30,f28
	f0.f64 = f0.f64 * f30.f64 + f28.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r29,116(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// neg r31,r29
	r31.s64 = -r29.s64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r27,120(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// mullw r10,r29,r19
	ctx.r10.s64 = int64_t(r29.s32) * int64_t(r19.s32);
	// mullw r11,r27,r19
	r11.s64 = int64_t(r27.s32) * int64_t(r19.s32);
	// add r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	// neg r30,r27
	r30.s64 = -r27.s64;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r5,r11,r9
	ctx.r5.u64 = r11.u64 + ctx.r9.u64;
	// mr r11,r19
	r11.u64 = r19.u64;
	// add r4,r10,r9
	ctx.r4.u64 = ctx.r10.u64 + ctx.r9.u64;
	// blt cr6,0x82673aec
	if (cr6.lt) goto loc_82673AEC;
	// mr r11,r22
	r11.u64 = r22.u64;
loc_82673AEC:
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// subf r11,r28,r31
	r11.s64 = r31.s64 - r28.s64;
	// lwz r20,396(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// subf r10,r28,r30
	ctx.r10.s64 = r30.s64 - r28.s64;
	// neg r3,r30
	ctx.r3.s64 = -r30.s64;
	// lwz r7,128(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// add r11,r11,r20
	r11.u64 = r11.u64 + r20.u64;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// add r9,r10,r20
	ctx.r9.u64 = ctx.r10.u64 + r20.u64;
	// neg r10,r31
	ctx.r10.s64 = -r31.s64;
	// addi r8,r19,1
	ctx.r8.s64 = r19.s64 + 1;
	// stw r3,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r3.u32);
	// add r7,r24,r7
	ctx.r7.u64 = r24.u64 + ctx.r7.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// add r3,r24,r11
	ctx.r3.u64 = r24.u64 + r11.u64;
	// bl 0x82671d38
	sub_82671D38(ctx, base);
	// clrlwi r11,r28,31
	r11.u64 = r28.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82673c64
	if (!cr6.eq) goto loc_82673C64;
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// srawi r10,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r10.s64 = r29.s32 >> 1;
	// srawi r7,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	ctx.r7.s64 = r31.s32 >> 1;
	// srawi r9,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r9.s64 = r27.s32 >> 1;
	// mullw r10,r10,r15
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r15.s32);
	// mullw r9,r9,r15
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r15.s32);
	// srawi r8,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r8.s64 = r30.s32 >> 1;
	// mullw r11,r11,r15
	r11.s64 = int64_t(r11.s32) * int64_t(r15.s32);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// li r26,0
	r26.s64 = 0;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// ble cr6,0x82673c64
	if (!cr6.gt) goto loc_82673C64;
	// add r8,r10,r18
	ctx.r8.u64 = ctx.r10.u64 + r18.u64;
	// lwz r10,412(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// add r5,r29,r28
	ctx.r5.u64 = r29.u64 + r28.u64;
	// subf r4,r18,r10
	ctx.r4.s64 = ctx.r10.s64 - r18.s64;
	// lwz r10,436(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// subf r29,r29,r27
	r29.s64 = r27.s64 - r29.s64;
	// subf r25,r31,r30
	r25.s64 = r30.s64 - r31.s64;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// subf r27,r31,r28
	r27.s64 = r28.s64 - r31.s64;
	// subf r31,r18,r10
	r31.s64 = ctx.r10.s64 - r18.s64;
	// lwz r10,444(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// add r11,r11,r17
	r11.u64 = r11.u64 + r17.u64;
	// subf r30,r17,r10
	r30.s64 = ctx.r10.s64 - r17.s64;
	// lwz r10,468(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + r18.u64;
	// subf r3,r17,r18
	ctx.r3.s64 = r18.s64 - r17.s64;
	// subf r6,r17,r10
	ctx.r6.s64 = ctx.r10.s64 - r17.s64;
loc_82673BC4:
	// add r10,r27,r7
	ctx.r10.u64 = r27.u64 + ctx.r7.u64;
	// cmpw cr6,r10,r20
	cr6.compare<int32_t>(ctx.r10.s32, r20.s32, xer);
	// bge cr6,0x82673c64
	if (!cr6.lt) goto loc_82673C64;
	// add. r10,r25,r7
	ctx.r10.u64 = r25.u64 + ctx.r7.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt 0x82673bf4
	if (cr0.lt) goto loc_82673BF4;
	// add r10,r29,r5
	ctx.r10.u64 = r29.u64 + ctx.r5.u64;
	// cmpw cr6,r10,r20
	cr6.compare<int32_t>(ctx.r10.s32, r20.s32, xer);
	// bge cr6,0x82673bf4
	if (!cr6.lt) goto loc_82673BF4;
	// lbzx r10,r4,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r9.u32);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// lbz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// b 0x82673c40
	goto loc_82673C40;
loc_82673BF4:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// blt cr6,0x82673c30
	if (cr6.lt) goto loc_82673C30;
	// cmpw cr6,r5,r20
	cr6.compare<int32_t>(ctx.r5.s32, r20.s32, xer);
	// bge cr6,0x82673c30
	if (!cr6.lt) goto loc_82673C30;
	// fcmpu cr6,f29,f24
	ctx.fpscr.disableFlushMode();
	cr6.compare(f29.f64, f24.f64);
	// blt cr6,0x82673c1c
	if (cr6.lt) goto loc_82673C1C;
	// lbzx r10,r4,r8
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r8.u32);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// lbz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// b 0x82673c40
	goto loc_82673C40;
loc_82673C1C:
	// add r10,r3,r11
	ctx.r10.u64 = ctx.r3.u64 + r11.u64;
	// lbzx r14,r10,r4
	r14.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// stb r14,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r14.u8);
	// lbz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// b 0x82673c40
	goto loc_82673C40;
loc_82673C30:
	// add r10,r3,r31
	ctx.r10.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// lbzx r10,r30,r11
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
loc_82673C40:
	// addi r26,r26,2
	r26.s64 = r26.s64 + 2;
	// stbx r10,r6,r11
	PPC_STORE_U8(ctx.r6.u32 + r11.u32, ctx.r10.u8);
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// add r8,r8,r16
	ctx.r8.u64 = ctx.r8.u64 + r16.u64;
	// add r9,r9,r16
	ctx.r9.u64 = ctx.r9.u64 + r16.u64;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// cmpw cr6,r26,r19
	cr6.compare<int32_t>(r26.s32, r19.s32, xer);
	// blt cr6,0x82673bc4
	if (cr6.lt) goto loc_82673BC4;
loc_82673C64:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// add r23,r23,r19
	r23.u64 = r23.u64 + r19.u64;
	// add r24,r24,r19
	r24.u64 = r24.u64 + r19.u64;
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// b 0x82673a40
	goto loc_82673A40;
loc_82673C78:
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d624
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82673C88"))) PPC_WEAK_FUNC(sub_82673C88);
PPC_FUNC_IMPL(__imp__sub_82673C88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r8,8
	ctx.r8.s64 = 8;
loc_82673C8C:
	// lhz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmplwi cr6,r10,255
	cr6.compare<uint32_t>(ctx.r10.u32, 255, xer);
	// ble cr6,0x82673cbc
	if (!cr6.gt) goto loc_82673CBC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// blt cr6,0x82673cbc
	if (cr6.lt) goto loc_82673CBC;
	// li r11,255
	r11.s64 = 255;
loc_82673CBC:
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r11.u8);
	// lbz r10,1(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// lhz r11,2(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmplwi cr6,r10,255
	cr6.compare<uint32_t>(ctx.r10.u32, 255, xer);
	// ble cr6,0x82673cf0
	if (!cr6.gt) goto loc_82673CF0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// blt cr6,0x82673cf0
	if (cr6.lt) goto loc_82673CF0;
	// li r11,255
	r11.s64 = 255;
loc_82673CF0:
	// stb r11,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, r11.u8);
	// lbz r10,2(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// lhz r11,4(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 4);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmplwi cr6,r10,255
	cr6.compare<uint32_t>(ctx.r10.u32, 255, xer);
	// ble cr6,0x82673d24
	if (!cr6.gt) goto loc_82673D24;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// blt cr6,0x82673d24
	if (cr6.lt) goto loc_82673D24;
	// li r11,255
	r11.s64 = 255;
loc_82673D24:
	// addi r9,r4,3
	ctx.r9.s64 = ctx.r4.s64 + 3;
	// stb r11,2(r4)
	PPC_STORE_U8(ctx.r4.u32 + 2, r11.u8);
	// lhz r11,6(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 6);
	// lbz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmplwi cr6,r10,255
	cr6.compare<uint32_t>(ctx.r10.u32, 255, xer);
	// ble cr6,0x82673d5c
	if (!cr6.gt) goto loc_82673D5C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// blt cr6,0x82673d5c
	if (cr6.lt) goto loc_82673D5C;
	// li r11,255
	r11.s64 = 255;
loc_82673D5C:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// add r4,r4,r5
	ctx.r4.u64 = ctx.r4.u64 + ctx.r5.u64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82673c8c
	if (!cr6.eq) goto loc_82673C8C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82673D78"))) PPC_WEAK_FUNC(sub_82673D78);
PPC_FUNC_IMPL(__imp__sub_82673D78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
loc_82673D88:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x82673d88
	if (!cr6.eq) goto loc_82673D88;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82673DA8"))) PPC_WEAK_FUNC(sub_82673DA8);
PPC_FUNC_IMPL(__imp__sub_82673DA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82673df4
	if (!cr6.gt) goto loc_82673DF4;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
loc_82673DD0:
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r31,r31,r27
	r31.u64 = r31.u64 + r27.u64;
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82673dd0
	if (!cr6.eq) goto loc_82673DD0;
loc_82673DF4:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82673DFC"))) PPC_WEAK_FUNC(sub_82673DFC);
PPC_FUNC_IMPL(__imp__sub_82673DFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82673E00"))) PPC_WEAK_FUNC(sub_82673E00);
PPC_FUNC_IMPL(__imp__sub_82673E00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-1264(r1)
	ea = -1264 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r8,r8,30
	ctx.r8.u64 = ctx.r8.u32 & 0x3;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// clrlwi r3,r9,30
	ctx.r3.u64 = ctx.r9.u32 & 0x3;
	// addi r11,r11,30952
	r11.s64 = r11.s64 + 30952;
	// rlwinm r10,r8,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r3,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x826740fc
	if (!cr6.eq) goto loc_826740FC;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82673e90
	if (!cr6.eq) goto loc_82673E90;
	// lwz r8,1356(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1356);
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x82674678
	if (!cr6.gt) goto loc_82674678;
loc_82673E4C:
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r4,r4,r5
	ctx.r4.u64 = ctx.r4.u64 + ctx.r5.u64;
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stb r6,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r6.u8);
	// lbz r6,1(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// stb r6,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, ctx.r6.u8);
	// lbz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// stb r6,2(r10)
	PPC_STORE_U8(ctx.r10.u32 + 2, ctx.r6.u8);
	// lbz r11,3(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// stb r11,3(r10)
	PPC_STORE_U8(ctx.r10.u32 + 3, r11.u8);
	// bne cr6,0x82673e4c
	if (!cr6.eq) goto loc_82673E4C;
	// addi r1,r1,1264
	ctx.r1.s64 = ctx.r1.s64 + 1264;
	// b 0x8239bd10
	return;
loc_82673E90:
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// li r31,4
	r31.s64 = 4;
	// beq cr6,0x82673ea0
	if (cr6.eq) goto loc_82673EA0;
	// li r31,6
	r31.s64 = 6;
loc_82673EA0:
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r10,1356(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1356);
	// addi r9,r31,-1
	ctx.r9.s64 = r31.s64 + -1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// slw r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r9.u8 & 0x3F));
	// lwz r8,1348(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1348);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r30,r9,-1
	r30.s64 = ctx.r9.s64 + -1;
	// ble cr6,0x82674678
	if (!cr6.gt) goto loc_82674678;
	// rlwinm r8,r5,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r3,r5,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// add r3,r5,r3
	ctx.r3.u64 = ctx.r5.u64 + ctx.r3.u64;
	// subf r10,r5,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r5.s64;
	// subfic r25,r8,1
	xer.ca = ctx.r8.u32 <= 1;
	r25.s64 = 1 - ctx.r8.s64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// subf r29,r5,r3
	r29.s64 = ctx.r3.s64 - ctx.r5.s64;
	// subfic r24,r8,2
	xer.ca = ctx.r8.u32 <= 2;
	r24.s64 = 2 - ctx.r8.s64;
	// subfic r22,r8,-1
	xer.ca = ctx.r8.u32 <= 4294967295;
	r22.s64 = -1 - ctx.r8.s64;
	// subf r26,r8,r3
	r26.s64 = ctx.r3.s64 - ctx.r8.s64;
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// addi r9,r4,2
	ctx.r9.s64 = ctx.r4.s64 + 2;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// subf r28,r8,r5
	r28.s64 = ctx.r5.s64 - ctx.r8.s64;
	// subfic r27,r5,-1
	xer.ca = ctx.r5.u32 <= 4294967295;
	r27.s64 = -1 - ctx.r5.s64;
	// add r3,r29,r4
	ctx.r3.u64 = r29.u64 + ctx.r4.u64;
loc_82673F08:
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lbzx r21,r22,r10
	r21.u64 = PPC_LOAD_U8(r22.u32 + ctx.r10.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbz r20,-1(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhz r19,2(r11)
	r19.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// mullw r8,r21,r8
	ctx.r8.s64 = int64_t(r21.s32) * int64_t(ctx.r8.s32);
	// lbz r21,-2(r9)
	r21.u64 = PPC_LOAD_U8(ctx.r9.u32 + -2);
	// lhz r18,6(r11)
	r18.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lbz r17,0(r3)
	r17.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// mullw r4,r20,r4
	ctx.r4.s64 = int64_t(r20.s32) * int64_t(ctx.r4.s32);
	// extsh r20,r19
	r20.s64 = r19.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r21,r20
	ctx.r4.s64 = int64_t(r21.s32) * int64_t(r20.s32);
	// extsh r19,r18
	r19.s64 = r18.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r19,r17
	ctx.r4.s64 = int64_t(r19.s32) * int64_t(r17.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// sraw r8,r8,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	ctx.r8.s64 = ctx.r8.s32 >> temp.u32;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x82673f6c
	if (!cr6.lt) goto loc_82673F6C;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x82673f78
	goto loc_82673F78;
loc_82673F6C:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x82673f78
	if (!cr6.gt) goto loc_82673F78;
	// li r8,255
	ctx.r8.s64 = 255;
loc_82673F78:
	// stb r8,-2(r6)
	PPC_STORE_U8(ctx.r6.u32 + -2, ctx.r8.u8);
	// lhz r8,2(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r4,6(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lbzx r21,r28,r10
	r21.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r20,r26,r10
	r20.u64 = PPC_LOAD_U8(r26.u32 + ctx.r10.u32);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhz r19,0(r11)
	r19.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// mullw r8,r21,r8
	ctx.r8.s64 = int64_t(r21.s32) * int64_t(ctx.r8.s32);
	// lbzx r21,r27,r9
	r21.u64 = PPC_LOAD_U8(r27.u32 + ctx.r9.u32);
	// lhz r18,4(r11)
	r18.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lbz r17,0(r10)
	r17.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// mullw r4,r20,r4
	ctx.r4.s64 = int64_t(r20.s32) * int64_t(ctx.r4.s32);
	// extsh r20,r19
	r20.s64 = r19.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r21,r20
	ctx.r4.s64 = int64_t(r21.s32) * int64_t(r20.s32);
	// extsh r19,r18
	r19.s64 = r18.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r19,r17
	ctx.r4.s64 = int64_t(r19.s32) * int64_t(r17.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// sraw r8,r8,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	ctx.r8.s64 = ctx.r8.s32 >> temp.u32;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x82673fe0
	if (!cr6.lt) goto loc_82673FE0;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x82673fec
	goto loc_82673FEC;
loc_82673FE0:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x82673fec
	if (!cr6.gt) goto loc_82673FEC;
	// li r8,255
	ctx.r8.s64 = 255;
loc_82673FEC:
	// stb r8,-1(r6)
	PPC_STORE_U8(ctx.r6.u32 + -1, ctx.r8.u8);
	// lhz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lbzx r21,r29,r9
	r21.u64 = PPC_LOAD_U8(r29.u32 + ctx.r9.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r20,r25,r10
	r20.u64 = PPC_LOAD_U8(r25.u32 + ctx.r10.u32);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhz r19,4(r11)
	r19.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// mullw r8,r21,r8
	ctx.r8.s64 = int64_t(r21.s32) * int64_t(ctx.r8.s32);
	// lbz r21,1(r10)
	r21.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lhz r18,2(r11)
	r18.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lbz r17,0(r9)
	r17.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// mullw r4,r20,r4
	ctx.r4.s64 = int64_t(r20.s32) * int64_t(ctx.r4.s32);
	// extsh r20,r19
	r20.s64 = r19.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r21,r20
	ctx.r4.s64 = int64_t(r21.s32) * int64_t(r20.s32);
	// extsh r19,r18
	r19.s64 = r18.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r19,r17
	ctx.r4.s64 = int64_t(r19.s32) * int64_t(r17.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// sraw r8,r8,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	ctx.r8.s64 = ctx.r8.s32 >> temp.u32;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x82674054
	if (!cr6.lt) goto loc_82674054;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x82674060
	goto loc_82674060;
loc_82674054:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x82674060
	if (!cr6.gt) goto loc_82674060;
	// li r8,255
	ctx.r8.s64 = 255;
loc_82674060:
	// stb r8,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, ctx.r8.u8);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lbzx r21,r24,r10
	r21.u64 = PPC_LOAD_U8(r24.u32 + ctx.r10.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbz r20,2(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhz r19,2(r11)
	r19.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// mullw r8,r21,r8
	ctx.r8.s64 = int64_t(r21.s32) * int64_t(ctx.r8.s32);
	// lbz r21,1(r9)
	r21.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lhz r18,6(r11)
	r18.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lbz r17,3(r3)
	r17.u64 = PPC_LOAD_U8(ctx.r3.u32 + 3);
	// mullw r4,r20,r4
	ctx.r4.s64 = int64_t(r20.s32) * int64_t(ctx.r4.s32);
	// extsh r20,r19
	r20.s64 = r19.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r21,r20
	ctx.r4.s64 = int64_t(r21.s32) * int64_t(r20.s32);
	// extsh r19,r18
	r19.s64 = r18.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r17,r19
	ctx.r4.s64 = int64_t(r17.s32) * int64_t(r19.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// sraw r8,r8,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	ctx.r8.s64 = ctx.r8.s32 >> temp.u32;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x826740c8
	if (!cr6.lt) goto loc_826740C8;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x826740d4
	goto loc_826740D4;
loc_826740C8:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x826740d4
	if (!cr6.gt) goto loc_826740D4;
	// li r8,255
	ctx.r8.s64 = 255;
loc_826740D4:
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// stb r8,1(r6)
	PPC_STORE_U8(ctx.r6.u32 + 1, ctx.r8.u8);
	// add r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 + ctx.r5.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x82673f08
	if (!cr6.eq) goto loc_82673F08;
	// addi r1,r1,1264
	ctx.r1.s64 = ctx.r1.s64 + 1264;
	// b 0x8239bd10
	return;
loc_826740FC:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8267432c
	if (!cr6.eq) goto loc_8267432C;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// li r3,4
	ctx.r3.s64 = 4;
	// beq cr6,0x82674114
	if (cr6.eq) goto loc_82674114;
	// li r3,6
	ctx.r3.s64 = 6;
loc_82674114:
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r9,1356(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1356);
	// addi r11,r3,-1
	r11.s64 = ctx.r3.s64 + -1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// slw r11,r8,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// lwz r8,1348(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1348);
	// subf r31,r8,r11
	r31.s64 = r11.s64 - ctx.r8.s64;
	// ble cr6,0x82674678
	if (!cr6.gt) goto loc_82674678;
	// addi r8,r6,2
	ctx.r8.s64 = ctx.r6.s64 + 2;
	// addi r11,r4,1
	r11.s64 = ctx.r4.s64 + 1;
	// mr r30,r9
	r30.u64 = ctx.r9.u64;
loc_82674140:
	// lhz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lbz r4,-2(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lbz r29,0(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// mullw r9,r4,r9
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// lhz r28,6(r10)
	r28.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// lhz r4,2(r10)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// mullw r6,r6,r29
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r29.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbz r6,1(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r29,r28
	r29.s64 = r28.s16;
	// lbz r28,-1(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// mullw r6,r29,r6
	ctx.r6.s64 = int64_t(r29.s32) * int64_t(ctx.r6.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// mullw r6,r28,r4
	ctx.r6.s64 = int64_t(r28.s32) * int64_t(ctx.r4.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// sraw r9,r9,r3
	temp.u32 = ctx.r3.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x826741a4
	if (!cr6.lt) goto loc_826741A4;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x826741b0
	goto loc_826741B0;
loc_826741A4:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x826741b0
	if (!cr6.gt) goto loc_826741B0;
	// li r9,255
	ctx.r9.s64 = 255;
loc_826741B0:
	// stb r9,-2(r8)
	PPC_STORE_U8(ctx.r8.u32 + -2, ctx.r9.u8);
	// lhz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lhz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lbz r4,1(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lbz r29,-1(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// mullw r9,r9,r4
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// lhz r4,2(r10)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r28,6(r10)
	r28.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// lbz r27,0(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mullw r6,r6,r29
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r29.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// lbz r4,2(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// extsh r29,r28
	r29.s64 = r28.s16;
	// mullw r6,r27,r6
	ctx.r6.s64 = int64_t(r27.s32) * int64_t(ctx.r6.s32);
	// mullw r4,r29,r4
	ctx.r4.s64 = int64_t(r29.s32) * int64_t(ctx.r4.s32);
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// sraw r9,r9,r3
	temp.u32 = ctx.r3.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674218
	if (!cr6.lt) goto loc_82674218;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674224
	goto loc_82674224;
loc_82674218:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674224
	if (!cr6.gt) goto loc_82674224;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674224:
	// stb r9,-1(r8)
	PPC_STORE_U8(ctx.r8.u32 + -1, ctx.r9.u8);
	// lhz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lhz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lbz r4,2(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lbz r29,0(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// mullw r9,r9,r4
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// lhz r4,2(r10)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r28,6(r10)
	r28.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// lbz r27,1(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// mullw r6,r6,r29
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r29.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// lbz r4,3(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// extsh r29,r28
	r29.s64 = r28.s16;
	// mullw r6,r27,r6
	ctx.r6.s64 = int64_t(r27.s32) * int64_t(ctx.r6.s32);
	// mullw r4,r29,r4
	ctx.r4.s64 = int64_t(r29.s32) * int64_t(ctx.r4.s32);
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// sraw r9,r9,r3
	temp.u32 = ctx.r3.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x8267428c
	if (!cr6.lt) goto loc_8267428C;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674298
	goto loc_82674298;
loc_8267428C:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674298
	if (!cr6.gt) goto loc_82674298;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674298:
	// stb r9,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r9.u8);
	// lhz r9,6(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// lhz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lbz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lbz r29,3(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r28,0(r10)
	r28.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// mullw r9,r4,r9
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// lbz r4,1(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lhz r27,2(r10)
	r27.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lbz r26,2(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mullw r6,r6,r29
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r29.s32);
	// extsh r29,r28
	r29.s64 = r28.s16;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// mullw r6,r29,r4
	ctx.r6.s64 = int64_t(r29.s32) * int64_t(ctx.r4.s32);
	// extsh r28,r27
	r28.s64 = r27.s16;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// mullw r6,r26,r28
	ctx.r6.s64 = int64_t(r26.s32) * int64_t(r28.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// sraw r9,r9,r3
	temp.u32 = ctx.r3.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674300
	if (!cr6.lt) goto loc_82674300;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x8267430c
	goto loc_8267430C;
loc_82674300:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x8267430c
	if (!cr6.gt) goto loc_8267430C;
	// li r9,255
	ctx.r9.s64 = 255;
loc_8267430C:
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// stb r9,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, ctx.r9.u8);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82674140
	if (!cr6.eq) goto loc_82674140;
	// addi r1,r1,1264
	ctx.r1.s64 = ctx.r1.s64 + 1264;
	// b 0x8239bd10
	return;
loc_8267432C:
	// addi r9,r1,47
	ctx.r9.s64 = ctx.r1.s64 + 47;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// rlwinm r19,r9,0,0,26
	r19.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFE0;
	// li r8,4
	ctx.r8.s64 = 4;
	// mr r21,r19
	r21.u64 = r19.u64;
	// beq cr6,0x82674348
	if (cr6.eq) goto loc_82674348;
	// li r8,6
	ctx.r8.s64 = 6;
loc_82674348:
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// li r9,4
	ctx.r9.s64 = 4;
	// beq cr6,0x82674358
	if (cr6.eq) goto loc_82674358;
	// li r9,6
	ctx.r9.s64 = 6;
loc_82674358:
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r20,1356(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1356);
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r27,r9,-7
	r27.s64 = ctx.r9.s64 + -7;
	// lwz r9,1348(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1348);
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// addi r3,r27,-1
	ctx.r3.s64 = r27.s64 + -1;
	// subfic r22,r9,64
	xer.ca = ctx.r9.u32 <= 64;
	r22.s64 = 64 - ctx.r9.s64;
	// slw r8,r8,r3
	ctx.r8.u64 = ctx.r3.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r3.u8 & 0x3F));
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// addi r26,r9,-1
	r26.s64 = ctx.r9.s64 + -1;
	// ble cr6,0x82674448
	if (!cr6.gt) goto loc_82674448;
	// add r8,r4,r5
	ctx.r8.u64 = ctx.r4.u64 + ctx.r5.u64;
	// rlwinm r9,r5,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r24,r8,-1
	r24.s64 = ctx.r8.s64 + -1;
	// rlwinm r8,r5,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r4,r5,r4
	ctx.r4.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
	// addi r25,r4,-1
	r25.s64 = ctx.r4.s64 + -1;
	// mr r23,r20
	r23.u64 = r20.u64;
	// subf r29,r9,r5
	r29.s64 = ctx.r5.s64 - ctx.r9.s64;
	// subf r28,r9,r8
	r28.s64 = ctx.r8.s64 - ctx.r9.s64;
loc_826743B0:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// li r8,7
	ctx.r8.s64 = 7;
loc_826743C0:
	// lhz r31,2(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lhz r30,6(r11)
	r30.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lbzx r18,r9,r29
	r18.u64 = PPC_LOAD_U8(ctx.r9.u32 + r29.u32);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// lbzx r17,r9,r28
	r17.u64 = PPC_LOAD_U8(ctx.r9.u32 + r28.u32);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// lhz r16,4(r11)
	r16.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// mullw r31,r18,r31
	r31.s64 = int64_t(r18.s32) * int64_t(r31.s32);
	// lbz r18,0(r9)
	r18.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lhz r15,0(r11)
	r15.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lbz r14,0(r4)
	r14.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// mullw r30,r17,r30
	r30.s64 = int64_t(r17.s32) * int64_t(r30.s32);
	// extsh r17,r16
	r17.s64 = r16.s16;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// mullw r30,r18,r17
	r30.s64 = int64_t(r18.s32) * int64_t(r17.s32);
	// extsh r16,r15
	r16.s64 = r15.s16;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// mullw r30,r16,r14
	r30.s64 = int64_t(r16.s32) * int64_t(r14.s32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// add r31,r31,r26
	r31.u64 = r31.u64 + r26.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// sraw r31,r31,r27
	temp.u32 = r27.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r31.s32 < 0) & (((r31.s32 >> temp.u32) << temp.u32) != r31.s32);
	r31.s64 = r31.s32 >> temp.u32;
	// sth r31,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, r31.u16);
	// addi r3,r3,2
	ctx.r3.s64 = ctx.r3.s64 + 2;
	// bne cr6,0x826743c0
	if (!cr6.eq) goto loc_826743C0;
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// add r25,r25,r5
	r25.u64 = r25.u64 + ctx.r5.u64;
	// add r24,r24,r5
	r24.u64 = r24.u64 + ctx.r5.u64;
	// addi r21,r21,64
	r21.s64 = r21.s64 + 64;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x826743b0
	if (!cr6.eq) goto loc_826743B0;
loc_82674448:
	// mr r11,r19
	r11.u64 = r19.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x82674678
	if (!cr6.gt) goto loc_82674678;
loc_82674454:
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r3,6(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// lhz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r4,4(r10)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// mullw r8,r8,r5
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// lhz r31,4(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lhz r30,6(r11)
	r30.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r5,r3
	ctx.r5.s64 = ctx.r3.s16;
	// lhz r3,2(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// mullw r9,r3,r9
	ctx.r9.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r9.s32);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mullw r8,r31,r4
	ctx.r8.s64 = int64_t(r31.s32) * int64_t(ctx.r4.s32);
	// extsh r4,r30
	ctx.r4.s64 = r30.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mullw r8,r4,r5
	ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + r22.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x826744c8
	if (!cr6.lt) goto loc_826744C8;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x826744d4
	goto loc_826744D4;
loc_826744C8:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x826744d4
	if (!cr6.gt) goto loc_826744D4;
	// li r9,255
	ctx.r9.s64 = 255;
loc_826744D4:
	// stb r9,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, ctx.r9.u8);
	// lhz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lhz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lhz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// lhz r3,2(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r31,2(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r30,r4
	r30.s64 = ctx.r4.s16;
	// lhz r29,0(r10)
	r29.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// mullw r4,r8,r5
	ctx.r4.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// lhz r9,6(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// extsh r8,r3
	ctx.r8.s64 = ctx.r3.s16;
	// extsh r3,r31
	ctx.r3.s64 = r31.s16;
	// extsh r31,r29
	r31.s64 = r29.s16;
	// mullw r5,r30,r8
	ctx.r5.s64 = int64_t(r30.s32) * int64_t(ctx.r8.s32);
	// mullw r8,r3,r31
	ctx.r8.s64 = int64_t(ctx.r3.s32) * int64_t(r31.s32);
	// lhz r3,8(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// mullw r9,r3,r9
	ctx.r9.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r9.s32);
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + r22.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x8267454c
	if (!cr6.lt) goto loc_8267454C;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674558
	goto loc_82674558;
loc_8267454C:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674558
	if (!cr6.gt) goto loc_82674558;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674558:
	// stb r9,1(r6)
	PPC_STORE_U8(ctx.r6.u32 + 1, ctx.r9.u8);
	// lhz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lhz r5,2(r10)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// lhz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhz r31,4(r10)
	r31.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// extsh r30,r4
	r30.s64 = ctx.r4.s16;
	// lhz r29,8(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// mullw r4,r8,r5
	ctx.r4.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// lhz r9,6(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// extsh r8,r3
	ctx.r8.s64 = ctx.r3.s16;
	// extsh r3,r31
	ctx.r3.s64 = r31.s16;
	// extsh r31,r29
	r31.s64 = r29.s16;
	// mullw r5,r30,r8
	ctx.r5.s64 = int64_t(r30.s32) * int64_t(ctx.r8.s32);
	// mullw r8,r3,r31
	ctx.r8.s64 = int64_t(ctx.r3.s32) * int64_t(r31.s32);
	// lhz r3,10(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// mullw r9,r3,r9
	ctx.r9.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r9.s32);
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + r22.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x826745d0
	if (!cr6.lt) goto loc_826745D0;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x826745dc
	goto loc_826745DC;
loc_826745D0:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x826745dc
	if (!cr6.gt) goto loc_826745DC;
	// li r9,255
	ctx.r9.s64 = 255;
loc_826745DC:
	// stb r9,2(r6)
	PPC_STORE_U8(ctx.r6.u32 + 2, ctx.r9.u8);
	// lhz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 12);
	// lhz r8,6(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// lhz r5,6(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// lhz r3,8(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhz r31,2(r10)
	r31.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// lhz r30,4(r10)
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lhz r29,10(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// mullw r8,r5,r4
	ctx.r8.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// extsh r5,r3
	ctx.r5.s64 = ctx.r3.s16;
	// extsh r4,r31
	ctx.r4.s64 = r31.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mullw r8,r5,r4
	ctx.r8.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// extsh r3,r30
	ctx.r3.s64 = r30.s16;
	// extsh r5,r29
	ctx.r5.s64 = r29.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mullw r8,r3,r5
	ctx.r8.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r5.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + r22.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674654
	if (!cr6.lt) goto loc_82674654;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674660
	goto loc_82674660;
loc_82674654:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674660
	if (!cr6.gt) goto loc_82674660;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674660:
	// addi r20,r20,-1
	r20.s64 = r20.s64 + -1;
	// stb r9,3(r6)
	PPC_STORE_U8(ctx.r6.u32 + 3, ctx.r9.u8);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x82674454
	if (!cr6.eq) goto loc_82674454;
loc_82674678:
	// addi r1,r1,1264
	ctx.r1.s64 = ctx.r1.s64 + 1264;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82674680"))) PPC_WEAK_FUNC(sub_82674680);
PPC_FUNC_IMPL(__imp__sub_82674680) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-1280(r1)
	ea = -1280 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// stw r7,1332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1332, ctx.r7.u32);
	// clrlwi r8,r8,30
	ctx.r8.u64 = ctx.r8.u32 & 0x3;
	// clrlwi r3,r9,30
	ctx.r3.u64 = ctx.r9.u32 & 0x3;
	// addi r10,r11,30952
	ctx.r10.s64 = r11.s64 + 30952;
	// rlwinm r11,r8,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r3,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x826749c0
	if (!cr6.eq) goto loc_826749C0;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8267470c
	if (!cr6.eq) goto loc_8267470C;
	// lwz r8,1372(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1372);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x82674f80
	if (!cr6.gt) goto loc_82674F80;
loc_826746CC:
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_826746DC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x826746dc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826746DC;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r4,r4,r5
	ctx.r4.u64 = ctx.r4.u64 + ctx.r5.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x826746cc
	if (!cr6.eq) goto loc_826746CC;
	// addi r1,r1,1280
	ctx.r1.s64 = ctx.r1.s64 + 1280;
	// b 0x8239bd10
	return;
loc_8267470C:
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// li r3,4
	ctx.r3.s64 = 4;
	// beq cr6,0x8267471c
	if (cr6.eq) goto loc_8267471C;
	// li r3,6
	ctx.r3.s64 = 6;
loc_8267471C:
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r9,1372(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1372);
	// addi r11,r3,-1
	r11.s64 = ctx.r3.s64 + -1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// slw r11,r8,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// lwz r8,1364(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1364);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// addi r31,r11,-1
	r31.s64 = r11.s64 + -1;
	// ble cr6,0x82674f80
	if (!cr6.gt) goto loc_82674F80;
	// mr r19,r9
	r19.u64 = ctx.r9.u64;
	// rlwinm r11,r5,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r5,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r29,r11,1
	xer.ca = r11.u32 <= 1;
	r29.s64 = 1 - r11.s64;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// subfic r28,r11,2
	xer.ca = r11.u32 <= 2;
	r28.s64 = 2 - r11.s64;
	// subfic r27,r11,3
	xer.ca = r11.u32 <= 3;
	r27.s64 = 3 - r11.s64;
	// addi r22,r4,1
	r22.s64 = ctx.r4.s64 + 1;
	// addi r21,r6,1
	r21.s64 = ctx.r6.s64 + 1;
	// addi r18,r5,-1
	r18.s64 = ctx.r5.s64 + -1;
	// subfic r24,r5,-1
	xer.ca = ctx.r5.u32 <= 4294967295;
	r24.s64 = -1 - ctx.r5.s64;
	// addi r20,r8,3
	r20.s64 = ctx.r8.s64 + 3;
	// subf r23,r11,r9
	r23.s64 = ctx.r9.s64 - r11.s64;
	// subf r26,r5,r9
	r26.s64 = ctx.r9.s64 - ctx.r5.s64;
loc_8267477C:
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// mr r30,r20
	r30.u64 = r20.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// add r11,r18,r22
	r11.u64 = r18.u64 + r22.u64;
	// li r25,2
	r25.s64 = 2;
loc_82674790:
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lbzx r4,r9,r24
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + r24.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r7,6(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// lbzx r17,r23,r11
	r17.u64 = PPC_LOAD_U8(r23.u32 + r11.u32);
	// mullw r8,r4,r8
	ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// lbz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lhz r16,2(r10)
	r16.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lbz r15,-1(r9)
	r15.u64 = PPC_LOAD_U8(ctx.r9.u32 + -1);
	// lhz r14,4(r10)
	r14.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// stw r4,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r4.u32);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// mullw r4,r17,r7
	ctx.r4.s64 = int64_t(r17.s32) * int64_t(ctx.r7.s32);
	// extsh r7,r16
	ctx.r7.s64 = r16.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r15,r7
	ctx.r4.s64 = int64_t(r15.s32) * int64_t(ctx.r7.s32);
	// extsh r17,r14
	r17.s64 = r14.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// lwz r7,16(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// mullw r4,r7,r17
	ctx.r4.s64 = int64_t(ctx.r7.s32) * int64_t(r17.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// sraw r8,r8,r3
	temp.u32 = ctx.r3.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	ctx.r8.s64 = ctx.r8.s32 >> temp.u32;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x826747fc
	if (!cr6.lt) goto loc_826747FC;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x82674808
	goto loc_82674808;
loc_826747FC:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x82674808
	if (!cr6.gt) goto loc_82674808;
	// li r8,255
	ctx.r8.s64 = 255;
loc_82674808:
	// stb r8,-1(r6)
	PPC_STORE_U8(ctx.r6.u32 + -1, ctx.r8.u8);
	// lhz r8,6(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// lbzx r4,r26,r9
	ctx.r4.u64 = PPC_LOAD_U8(r26.u32 + ctx.r9.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lbzx r17,r29,r11
	r17.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// mullw r8,r4,r8
	ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// lbz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lhz r16,4(r10)
	r16.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lbz r15,1(r11)
	r15.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lhz r14,2(r10)
	r14.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// stw r4,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r4.u32);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// mullw r4,r17,r7
	ctx.r4.s64 = int64_t(r17.s32) * int64_t(ctx.r7.s32);
	// extsh r7,r16
	ctx.r7.s64 = r16.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r15,r7
	ctx.r4.s64 = int64_t(r15.s32) * int64_t(ctx.r7.s32);
	// extsh r17,r14
	r17.s64 = r14.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// lwz r7,16(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// mullw r4,r17,r7
	ctx.r4.s64 = int64_t(r17.s32) * int64_t(ctx.r7.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// sraw r8,r8,r3
	temp.u32 = ctx.r3.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	ctx.r8.s64 = ctx.r8.s32 >> temp.u32;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x82674878
	if (!cr6.lt) goto loc_82674878;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x82674884
	goto loc_82674884;
loc_82674878:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x82674884
	if (!cr6.gt) goto loc_82674884;
	// li r8,255
	ctx.r8.s64 = 255;
loc_82674884:
	// stb r8,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, ctx.r8.u8);
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lbzx r4,r28,r11
	ctx.r4.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r7,4(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lbz r17,2(r11)
	r17.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mullw r8,r4,r8
	ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// lbz r4,-1(r30)
	ctx.r4.u64 = PPC_LOAD_U8(r30.u32 + -1);
	// lhz r16,2(r10)
	r16.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lbz r15,1(r9)
	r15.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lhz r14,6(r10)
	r14.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// stw r4,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r4.u32);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// mullw r4,r17,r7
	ctx.r4.s64 = int64_t(r17.s32) * int64_t(ctx.r7.s32);
	// extsh r7,r16
	ctx.r7.s64 = r16.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r15,r7
	ctx.r4.s64 = int64_t(r15.s32) * int64_t(ctx.r7.s32);
	// extsh r17,r14
	r17.s64 = r14.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// lwz r7,16(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// mullw r4,r7,r17
	ctx.r4.s64 = int64_t(ctx.r7.s32) * int64_t(r17.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// sraw r8,r8,r3
	temp.u32 = ctx.r3.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	ctx.r8.s64 = ctx.r8.s32 >> temp.u32;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x826748f4
	if (!cr6.lt) goto loc_826748F4;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x82674900
	goto loc_82674900;
loc_826748F4:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x82674900
	if (!cr6.gt) goto loc_82674900;
	// li r8,255
	ctx.r8.s64 = 255;
loc_82674900:
	// stb r8,1(r6)
	PPC_STORE_U8(ctx.r6.u32 + 1, ctx.r8.u8);
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lbzx r4,r27,r11
	ctx.r4.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r7,4(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lbz r17,3(r11)
	r17.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// mullw r8,r4,r8
	ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// lbz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lhz r16,2(r10)
	r16.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lbz r15,2(r9)
	r15.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lhz r14,6(r10)
	r14.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// stw r4,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r4.u32);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// mullw r4,r17,r7
	ctx.r4.s64 = int64_t(r17.s32) * int64_t(ctx.r7.s32);
	// extsh r7,r16
	ctx.r7.s64 = r16.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r15,r7
	ctx.r4.s64 = int64_t(r15.s32) * int64_t(ctx.r7.s32);
	// extsh r17,r14
	r17.s64 = r14.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// lwz r7,16(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// mullw r4,r17,r7
	ctx.r4.s64 = int64_t(r17.s32) * int64_t(ctx.r7.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// sraw r8,r8,r3
	temp.u32 = ctx.r3.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	ctx.r8.s64 = ctx.r8.s32 >> temp.u32;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x82674970
	if (!cr6.lt) goto loc_82674970;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x8267497c
	goto loc_8267497C;
loc_82674970:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x8267497c
	if (!cr6.gt) goto loc_8267497C;
	// li r8,255
	ctx.r8.s64 = 255;
loc_8267497C:
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// stb r8,2(r6)
	PPC_STORE_U8(ctx.r6.u32 + 2, ctx.r8.u8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x82674790
	if (!cr6.eq) goto loc_82674790;
	// lwz r11,1332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1332);
	// addi r19,r19,-1
	r19.s64 = r19.s64 + -1;
	// add r22,r22,r5
	r22.u64 = r22.u64 + ctx.r5.u64;
	// add r21,r21,r11
	r21.u64 = r21.u64 + r11.u64;
	// add r20,r20,r5
	r20.u64 = r20.u64 + ctx.r5.u64;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// bne cr6,0x8267477c
	if (!cr6.eq) goto loc_8267477C;
	// addi r1,r1,1280
	ctx.r1.s64 = ctx.r1.s64 + 1280;
	// b 0x8239bd10
	return;
loc_826749C0:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82674c10
	if (!cr6.eq) goto loc_82674C10;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// li r31,4
	r31.s64 = 4;
	// beq cr6,0x826749d8
	if (cr6.eq) goto loc_826749D8;
	// li r31,6
	r31.s64 = 6;
loc_826749D8:
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r10,1372(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1372);
	// addi r9,r31,-1
	ctx.r9.s64 = r31.s64 + -1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// slw r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r9.u8 & 0x3F));
	// lwz r8,1364(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1364);
	// subf r30,r8,r9
	r30.s64 = ctx.r9.s64 - ctx.r8.s64;
	// ble cr6,0x82674f80
	if (!cr6.gt) goto loc_82674F80;
	// addi r27,r4,1
	r27.s64 = ctx.r4.s64 + 1;
	// addi r28,r6,1
	r28.s64 = ctx.r6.s64 + 1;
	// mr r26,r10
	r26.u64 = ctx.r10.u64;
loc_82674A04:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// li r29,2
	r29.s64 = 2;
loc_82674A10:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lbz r6,-2(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lbz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// mullw r9,r6,r9
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r9.s32);
	// lhz r25,6(r11)
	r25.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lhz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// mullw r8,r8,r4
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,1(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsh r4,r25
	ctx.r4.s64 = r25.s16;
	// lbz r25,-1(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// mullw r8,r8,r4
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mullw r8,r25,r6
	ctx.r8.s64 = int64_t(r25.s32) * int64_t(ctx.r6.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// sraw r9,r9,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674a74
	if (!cr6.lt) goto loc_82674A74;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674a80
	goto loc_82674A80;
loc_82674A74:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674a80
	if (!cr6.gt) goto loc_82674A80;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674A80:
	// stb r9,-1(r3)
	PPC_STORE_U8(ctx.r3.u32 + -1, ctx.r9.u8);
	// lhz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lbz r6,1(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// mullw r9,r9,r6
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// lhz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lbz r24,0(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsh r25,r8
	r25.s64 = ctx.r8.s16;
	// lbz r8,-1(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r23,r6
	r23.s64 = ctx.r6.s16;
	// mullw r6,r8,r4
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// lbz r4,2(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// mullw r4,r4,r25
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r25.s32);
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// mullw r8,r24,r23
	ctx.r8.s64 = int64_t(r24.s32) * int64_t(r23.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// sraw r9,r9,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674ae8
	if (!cr6.lt) goto loc_82674AE8;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674af4
	goto loc_82674AF4;
loc_82674AE8:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674af4
	if (!cr6.gt) goto loc_82674AF4;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674AF4:
	// stb r9,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r9.u8);
	// lhz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lbz r6,2(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// mullw r9,r9,r6
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// lhz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lbz r24,1(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsh r25,r8
	r25.s64 = ctx.r8.s16;
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r23,r6
	r23.s64 = ctx.r6.s16;
	// mullw r6,r8,r4
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// lbz r4,3(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// mullw r4,r25,r4
	ctx.r4.s64 = int64_t(r25.s32) * int64_t(ctx.r4.s32);
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// mullw r8,r24,r23
	ctx.r8.s64 = int64_t(r24.s32) * int64_t(r23.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// sraw r9,r9,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674b5c
	if (!cr6.lt) goto loc_82674B5C;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674b68
	goto loc_82674B68;
loc_82674B5C:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674b68
	if (!cr6.gt) goto loc_82674B68;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674B68:
	// stb r9,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r9.u8);
	// lhz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lbz r6,3(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// mullw r9,r9,r6
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// lhz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lbz r24,2(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsh r25,r8
	r25.s64 = ctx.r8.s16;
	// lbz r8,1(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r23,r6
	r23.s64 = ctx.r6.s16;
	// mullw r6,r8,r4
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// lbz r4,4(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// mullw r4,r4,r25
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r25.s32);
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// mullw r8,r24,r23
	ctx.r8.s64 = int64_t(r24.s32) * int64_t(r23.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// sraw r9,r9,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674bd0
	if (!cr6.lt) goto loc_82674BD0;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674bdc
	goto loc_82674BDC;
loc_82674BD0:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674bdc
	if (!cr6.gt) goto loc_82674BDC;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674BDC:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// stb r9,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, ctx.r9.u8);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82674a10
	if (!cr6.eq) goto loc_82674A10;
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// add r28,r28,r7
	r28.u64 = r28.u64 + ctx.r7.u64;
	// add r27,r27,r5
	r27.u64 = r27.u64 + ctx.r5.u64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// bne cr6,0x82674a04
	if (!cr6.eq) goto loc_82674A04;
	// addi r1,r1,1280
	ctx.r1.s64 = ctx.r1.s64 + 1280;
	// b 0x8239bd10
	return;
loc_82674C10:
	// addi r9,r1,63
	ctx.r9.s64 = ctx.r1.s64 + 63;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// rlwinm r19,r9,0,0,26
	r19.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFE0;
	// li r8,4
	ctx.r8.s64 = 4;
	// mr r21,r19
	r21.u64 = r19.u64;
	// beq cr6,0x82674c2c
	if (cr6.eq) goto loc_82674C2C;
	// li r8,6
	ctx.r8.s64 = 6;
loc_82674C2C:
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// li r9,4
	ctx.r9.s64 = 4;
	// beq cr6,0x82674c3c
	if (cr6.eq) goto loc_82674C3C;
	// li r9,6
	ctx.r9.s64 = 6;
loc_82674C3C:
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r20,1372(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1372);
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r27,r9,-7
	r27.s64 = ctx.r9.s64 + -7;
	// lwz r9,1364(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1364);
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// addi r3,r27,-1
	ctx.r3.s64 = r27.s64 + -1;
	// subfic r23,r9,64
	xer.ca = ctx.r9.u32 <= 64;
	r23.s64 = 64 - ctx.r9.s64;
	// slw r8,r8,r3
	ctx.r8.u64 = ctx.r3.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r3.u8 & 0x3F));
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// addi r26,r9,-1
	r26.s64 = ctx.r9.s64 + -1;
	// ble cr6,0x82674f80
	if (!cr6.gt) goto loc_82674F80;
	// add r8,r4,r5
	ctx.r8.u64 = ctx.r4.u64 + ctx.r5.u64;
	// rlwinm r9,r5,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r24,r8,-1
	r24.s64 = ctx.r8.s64 + -1;
	// rlwinm r8,r5,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r4,r5,r4
	ctx.r4.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
	// addi r25,r4,-1
	r25.s64 = ctx.r4.s64 + -1;
	// mr r22,r20
	r22.u64 = r20.u64;
	// subf r29,r9,r5
	r29.s64 = ctx.r5.s64 - ctx.r9.s64;
	// subf r28,r9,r8
	r28.s64 = ctx.r8.s64 - ctx.r9.s64;
loc_82674C94:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// li r8,11
	ctx.r8.s64 = 11;
loc_82674CA4:
	// lhz r31,6(r10)
	r31.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lhz r30,2(r10)
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lbzx r18,r28,r9
	r18.u64 = PPC_LOAD_U8(r28.u32 + ctx.r9.u32);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// lbzx r17,r29,r9
	r17.u64 = PPC_LOAD_U8(r29.u32 + ctx.r9.u32);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// lhz r16,4(r10)
	r16.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// mullw r31,r18,r31
	r31.s64 = int64_t(r18.s32) * int64_t(r31.s32);
	// lbz r18,0(r9)
	r18.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lhz r15,0(r10)
	r15.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lbz r14,0(r4)
	r14.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// mullw r30,r17,r30
	r30.s64 = int64_t(r17.s32) * int64_t(r30.s32);
	// extsh r17,r16
	r17.s64 = r16.s16;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// mullw r30,r18,r17
	r30.s64 = int64_t(r18.s32) * int64_t(r17.s32);
	// extsh r16,r15
	r16.s64 = r15.s16;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// mullw r30,r14,r16
	r30.s64 = int64_t(r14.s32) * int64_t(r16.s32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// add r31,r31,r26
	r31.u64 = r31.u64 + r26.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// sraw r31,r31,r27
	temp.u32 = r27.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r31.s32 < 0) & (((r31.s32 >> temp.u32) << temp.u32) != r31.s32);
	r31.s64 = r31.s32 >> temp.u32;
	// sth r31,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, r31.u16);
	// addi r3,r3,2
	ctx.r3.s64 = ctx.r3.s64 + 2;
	// bne cr6,0x82674ca4
	if (!cr6.eq) goto loc_82674CA4;
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// add r25,r25,r5
	r25.u64 = r25.u64 + ctx.r5.u64;
	// add r24,r24,r5
	r24.u64 = r24.u64 + ctx.r5.u64;
	// addi r21,r21,64
	r21.s64 = r21.s64 + 64;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x82674c94
	if (!cr6.eq) goto loc_82674C94;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x82674f80
	if (!cr6.gt) goto loc_82674F80;
	// addi r31,r19,4
	r31.s64 = r19.s64 + 4;
	// addi r30,r6,1
	r30.s64 = ctx.r6.s64 + 1;
loc_82674D3C:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// li r3,2
	ctx.r3.s64 = 2;
loc_82674D48:
	// lhz r9,-4(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + -4);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r6,6(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhz r29,4(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r28,r6
	r28.s64 = ctx.r6.s16;
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// lhz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r8,r5
	ctx.r8.s64 = ctx.r5.s16;
	// extsh r5,r29
	ctx.r5.s64 = r29.s16;
	// extsh r29,r6
	r29.s64 = ctx.r6.s16;
	// mullw r6,r8,r5
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// lhz r8,2(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r5,-2(r10)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// mullw r8,r8,r28
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r28.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mullw r8,r5,r29
	ctx.r8.s64 = int64_t(ctx.r5.s32) * int64_t(r29.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674dbc
	if (!cr6.lt) goto loc_82674DBC;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674dc8
	goto loc_82674DC8;
loc_82674DBC:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674dc8
	if (!cr6.gt) goto loc_82674DC8;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674DC8:
	// stb r9,-1(r4)
	PPC_STORE_U8(ctx.r4.u32 + -1, ctx.r9.u8);
	// lhz r8,-2(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r5,2(r10)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r29,4(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lhz r28,0(r10)
	r28.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// extsh r27,r5
	r27.s64 = ctx.r5.s16;
	// lhz r26,2(r11)
	r26.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// mullw r5,r8,r6
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// lhz r9,6(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r8,r29
	ctx.r8.s64 = r29.s16;
	// extsh r29,r28
	r29.s64 = r28.s16;
	// extsh r28,r26
	r28.s64 = r26.s16;
	// mullw r6,r27,r8
	ctx.r6.s64 = int64_t(r27.s32) * int64_t(ctx.r8.s32);
	// mullw r8,r29,r28
	ctx.r8.s64 = int64_t(r29.s32) * int64_t(r28.s32);
	// lhz r29,4(r10)
	r29.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r29,r29
	r29.s64 = r29.s16;
	// mullw r9,r29,r9
	ctx.r9.s64 = int64_t(r29.s32) * int64_t(ctx.r9.s32);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674e40
	if (!cr6.lt) goto loc_82674E40;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674e4c
	goto loc_82674E4C;
loc_82674E40:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674e4c
	if (!cr6.gt) goto loc_82674E4C;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674E4C:
	// stb r9,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r9.u8);
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r29,4(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lhz r28,2(r10)
	r28.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// extsh r27,r5
	r27.s64 = ctx.r5.s16;
	// lhz r26,2(r11)
	r26.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// mullw r5,r8,r6
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// lhz r9,6(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r8,r29
	ctx.r8.s64 = r29.s16;
	// extsh r29,r28
	r29.s64 = r28.s16;
	// extsh r28,r26
	r28.s64 = r26.s16;
	// mullw r6,r27,r8
	ctx.r6.s64 = int64_t(r27.s32) * int64_t(ctx.r8.s32);
	// mullw r8,r29,r28
	ctx.r8.s64 = int64_t(r29.s32) * int64_t(r28.s32);
	// lhz r29,6(r10)
	r29.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r29,r29
	r29.s64 = r29.s16;
	// mullw r9,r29,r9
	ctx.r9.s64 = int64_t(r29.s32) * int64_t(ctx.r9.s32);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674ec4
	if (!cr6.lt) goto loc_82674EC4;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674ed0
	goto loc_82674ED0;
loc_82674EC4:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674ed0
	if (!cr6.gt) goto loc_82674ED0;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674ED0:
	// stb r9,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r9.u8);
	// lhz r8,2(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r5,6(r10)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r29,4(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lhz r28,4(r10)
	r28.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// extsh r27,r5
	r27.s64 = ctx.r5.s16;
	// lhz r26,2(r11)
	r26.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// mullw r5,r8,r6
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// lhz r9,6(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r8,r29
	ctx.r8.s64 = r29.s16;
	// extsh r29,r28
	r29.s64 = r28.s16;
	// extsh r28,r26
	r28.s64 = r26.s16;
	// mullw r6,r27,r8
	ctx.r6.s64 = int64_t(r27.s32) * int64_t(ctx.r8.s32);
	// mullw r8,r29,r28
	ctx.r8.s64 = int64_t(r29.s32) * int64_t(r28.s32);
	// lhz r29,8(r10)
	r29.u64 = PPC_LOAD_U16(ctx.r10.u32 + 8);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r29,r29
	r29.s64 = r29.s16;
	// mullw r9,r29,r9
	ctx.r9.s64 = int64_t(r29.s32) * int64_t(ctx.r9.s32);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82674f48
	if (!cr6.lt) goto loc_82674F48;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82674f54
	goto loc_82674F54;
loc_82674F48:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82674f54
	if (!cr6.gt) goto loc_82674F54;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82674F54:
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// stb r9,2(r4)
	PPC_STORE_U8(ctx.r4.u32 + 2, ctx.r9.u8);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82674d48
	if (!cr6.eq) goto loc_82674D48;
	// addi r20,r20,-1
	r20.s64 = r20.s64 + -1;
	// addi r31,r31,64
	r31.s64 = r31.s64 + 64;
	// add r30,r30,r7
	r30.u64 = r30.u64 + ctx.r7.u64;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x82674d3c
	if (!cr6.eq) goto loc_82674D3C;
loc_82674F80:
	// addi r1,r1,1280
	ctx.r1.s64 = ctx.r1.s64 + 1280;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82674F88"))) PPC_WEAK_FUNC(sub_82674F88);
PPC_FUNC_IMPL(__imp__sub_82674F88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce8
	// lis r11,-32137
	r11.s64 = -2106130432;
	// lwz r24,92(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// rlwinm r8,r8,2,28,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xC;
	// lwz r27,84(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r11,r11,-15736
	r11.s64 = r11.s64 + -15736;
	// rlwinm r10,r9,2,28,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xC;
	// add r28,r8,r11
	r28.u64 = ctx.r8.u64 + r11.u64;
	// add r29,r10,r11
	r29.u64 = ctx.r10.u64 + r11.u64;
	// addi r26,r24,1
	r26.s64 = r24.s64 + 1;
	// li r25,8
	r25.s64 = 8;
loc_82674FB8:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x82675008
	if (!cr6.gt) goto loc_82675008;
	// lhz r11,2(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 2);
	// addi r9,r1,-208
	ctx.r9.s64 = ctx.r1.s64 + -208;
	// lhz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 0);
	// extsh r31,r11
	r31.s64 = r11.s16;
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_82674FDC:
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// mullw r8,r8,r31
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r31.s32);
	// mullw r3,r3,r30
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r30.s32);
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x82674fdc
	if (!cr6.eq) goto loc_82674FDC;
loc_82675008:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82675080
	if (!cr6.gt) goto loc_82675080;
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// addi r10,r1,-208
	ctx.r10.s64 = ctx.r1.s64 + -208;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
loc_8267501C:
	// lhz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U16(r29.u32 + 0);
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhz r11,2(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 2);
	// mullw r9,r9,r31
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r31.s32);
	// lwz r31,4(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mullw r11,r11,r31
	r11.s64 = int64_t(r11.s32) * int64_t(r31.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// subf r11,r27,r11
	r11.s64 = r11.s64 - r27.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8267505c
	if (!cr6.lt) goto loc_8267505C;
	// li r11,0
	r11.s64 = 0;
	// b 0x82675068
	goto loc_82675068;
loc_8267505C:
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82675068
	if (!cr6.gt) goto loc_82675068;
	// li r11,255
	r11.s64 = 255;
loc_82675068:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// add r3,r3,r7
	ctx.r3.u64 = ctx.r3.u64 + ctx.r7.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x8267501c
	if (!cr6.eq) goto loc_8267501C;
loc_82675080:
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x82674fb8
	if (!cr6.eq) goto loc_82674FB8;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_82675098"))) PPC_WEAK_FUNC(sub_82675098);
PPC_FUNC_IMPL(__imp__sub_82675098) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce8
	// lis r11,-32137
	r11.s64 = -2106130432;
	// lwz r24,92(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// rlwinm r8,r8,2,28,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xC;
	// lwz r27,84(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r11,r11,-15736
	r11.s64 = r11.s64 + -15736;
	// rlwinm r10,r9,2,28,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xC;
	// add r28,r8,r11
	r28.u64 = ctx.r8.u64 + r11.u64;
	// add r29,r10,r11
	r29.u64 = ctx.r10.u64 + r11.u64;
	// addi r26,r24,1
	r26.s64 = r24.s64 + 1;
	// li r25,4
	r25.s64 = 4;
loc_826750C8:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x82675118
	if (!cr6.gt) goto loc_82675118;
	// lhz r11,2(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 2);
	// addi r9,r1,-208
	ctx.r9.s64 = ctx.r1.s64 + -208;
	// lhz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 0);
	// extsh r31,r11
	r31.s64 = r11.s16;
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_826750EC:
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// mullw r8,r8,r31
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r31.s32);
	// mullw r3,r3,r30
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r30.s32);
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x826750ec
	if (!cr6.eq) goto loc_826750EC;
loc_82675118:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82675190
	if (!cr6.gt) goto loc_82675190;
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// addi r10,r1,-208
	ctx.r10.s64 = ctx.r1.s64 + -208;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
loc_8267512C:
	// lhz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U16(r29.u32 + 0);
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhz r11,2(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 2);
	// mullw r9,r9,r31
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r31.s32);
	// lwz r31,4(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mullw r11,r11,r31
	r11.s64 = int64_t(r11.s32) * int64_t(r31.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// subf r11,r27,r11
	r11.s64 = r11.s64 - r27.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8267516c
	if (!cr6.lt) goto loc_8267516C;
	// li r11,0
	r11.s64 = 0;
	// b 0x82675178
	goto loc_82675178;
loc_8267516C:
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82675178
	if (!cr6.gt) goto loc_82675178;
	// li r11,255
	r11.s64 = 255;
loc_82675178:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// add r3,r3,r7
	ctx.r3.u64 = ctx.r3.u64 + ctx.r7.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x8267512c
	if (!cr6.eq) goto loc_8267512C;
loc_82675190:
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x826750c8
	if (!cr6.eq) goto loc_826750C8;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_826751A8"))) PPC_WEAK_FUNC(sub_826751A8);
PPC_FUNC_IMPL(__imp__sub_826751A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lis r30,-32153
	r30.s64 = -2107179008;
	// lis r31,-32153
	r31.s64 = -2107179008;
	// lis r4,-32153
	ctx.r4.s64 = -2107179008;
	// lis r5,-32153
	ctx.r5.s64 = -2107179008;
	// lis r6,-32153
	ctx.r6.s64 = -2107179008;
	// lis r7,-32153
	ctx.r7.s64 = -2107179008;
	// lis r8,-32142
	ctx.r8.s64 = -2106458112;
	// lis r9,-32142
	ctx.r9.s64 = -2106458112;
	// lis r10,-32153
	ctx.r10.s64 = -2107179008;
	// lis r11,-32152
	r11.s64 = -2107113472;
	// addi r30,r30,15872
	r30.s64 = r30.s64 + 15872;
	// addi r31,r31,15784
	r31.s64 = r31.s64 + 15784;
	// addi r4,r4,18048
	ctx.r4.s64 = ctx.r4.s64 + 18048;
	// addi r5,r5,20360
	ctx.r5.s64 = ctx.r5.s64 + 20360;
	// addi r6,r6,20632
	ctx.r6.s64 = ctx.r6.s64 + 20632;
	// addi r7,r7,15736
	ctx.r7.s64 = ctx.r7.s64 + 15736;
	// stw r30,15848(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15848, r30.u32);
	// addi r8,r8,-10976
	ctx.r8.s64 = ctx.r8.s64 + -10976;
	// stw r31,3136(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3136, r31.u32);
	// addi r9,r9,-11544
	ctx.r9.s64 = ctx.r9.s64 + -11544;
	// stw r4,20016(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20016, ctx.r4.u32);
	// addi r10,r10,15496
	ctx.r10.s64 = ctx.r10.s64 + 15496;
	// stw r5,20008(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20008, ctx.r5.u32);
	// addi r11,r11,-28424
	r11.s64 = r11.s64 + -28424;
	// stw r6,20012(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20012, ctx.r6.u32);
	// stw r7,15832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15832, ctx.r7.u32);
	// stw r8,15836(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15836, ctx.r8.u32);
	// stw r9,15840(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15840, ctx.r9.u32);
	// stw r10,15844(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15844, ctx.r10.u32);
	// stw r11,3132(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3132, r11.u32);
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82675234"))) PPC_WEAK_FUNC(sub_82675234);
PPC_FUNC_IMPL(__imp__sub_82675234) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82675238"))) PPC_WEAK_FUNC(sub_82675238);
PPC_FUNC_IMPL(__imp__sub_82675238) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r7,356(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// mr r20,r6
	r20.u64 = ctx.r6.u64;
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// vspltish v13,1
	// rlwinm r19,r20,0,0,26
	r19.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0xFFFFFFE0;
	// mullw r7,r7,r9
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// add r5,r7,r3
	ctx.r5.u64 = ctx.r7.u64 + ctx.r3.u64;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// add r22,r4,r8
	r22.u64 = ctx.r4.u64 + ctx.r8.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r29,1
	r29.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// mr r18,r11
	r18.u64 = r11.u64;
	// add r27,r11,r4
	r27.u64 = r11.u64 + ctx.r4.u64;
	// ble cr6,0x82675330
	if (!cr6.gt) goto loc_82675330;
	// addi r11,r19,-1
	r11.s64 = r19.s64 + -1;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// rlwinm r7,r11,27,5,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r5,1
	r11.s64 = ctx.r5.s64 + 1;
	// addi r6,r7,1
	ctx.r6.s64 = ctx.r7.s64 + 1;
	// rlwinm r7,r6,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r3,r6,5,0,26
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r29,r7,1
	r29.s64 = ctx.r7.s64 + 1;
loc_826752AC:
	// li r26,16
	r26.s64 = 16;
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r31,r11,-1
	r31.s64 = r11.s64 + -1;
	// li r28,16
	r28.s64 = 16;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// lvrx v11,r11,r26
	temp.u32 = r11.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r30,r8,16
	r30.s64 = ctx.r8.s64 + 16;
	// vor v11,v12,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvlx v10,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r31,r28
	temp.u32 = r31.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r21,16
	r21.s64 = 16;
	// vor v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// vmrghb v10,v0,v11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r8,r8,32
	ctx.r8.s64 = ctx.r8.s64 + 32;
	// vmrglb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// vmrghb v9,v0,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v11,v8,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v10,v10,v13
	// vsrah v11,v11,v13
	// vpkshus v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vmrghb v10,v12,v11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvlx v10,0,r25
	ea = r25.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// stvrx v10,r24,r26
	ea = r24.u32 + r26.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// stvlx v12,0,r30
	ea = r30.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// stvrx v12,r30,r21
	ea = r30.u32 + r21.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// bne cr6,0x826752ac
	if (!cr6.eq) goto loc_826752AC;
loc_82675330:
	// cmpw cr6,r3,r20
	cr6.compare<int32_t>(ctx.r3.s32, r20.s32, xer);
	// bge cr6,0x8267538c
	if (!cr6.lt) goto loc_8267538C;
	// subf r8,r3,r20
	ctx.r8.s64 = r20.s64 - ctx.r3.s64;
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// add r6,r29,r5
	ctx.r6.u64 = r29.u64 + ctx.r5.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// rlwinm r31,r8,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
loc_8267535C:
	// lbzx r31,r7,r5
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r5.u32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbz r29,0(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// add r30,r31,r29
	r30.u64 = r31.u64 + r29.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// rlwinm r30,r30,31,1,31
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 31) & 0x7FFFFFFF;
	// stb r31,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, r31.u8);
	// stb r30,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r30.u8);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x8267535c
	if (!cr6.eq) goto loc_8267535C;
loc_8267538C:
	// lbzx r8,r7,r5
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r5.u32);
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lwz r7,340(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// add r31,r5,r9
	r31.u64 = ctx.r5.u64 + ctx.r9.u64;
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// stb r8,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r8.u8);
	// bge cr6,0x826755e8
	if (!cr6.lt) goto loc_826755E8;
	// subf r11,r7,r10
	r11.s64 = ctx.r10.s64 - ctx.r7.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r21,r11,1
	r21.s64 = r11.s64 + 1;
loc_826753BC:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r25,1
	r25.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x82675508
	if (!cr6.gt) goto loc_82675508;
	// addi r11,r19,-1
	r11.s64 = r19.s64 + -1;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r10,r31,1
	ctx.r10.s64 = r31.s64 + 1;
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// addi r11,r22,16
	r11.s64 = r22.s64 + 16;
	// rlwinm r4,r7,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r29,r22,r23
	r29.s64 = r23.s64 - r22.s64;
	// subf r28,r22,r27
	r28.s64 = r27.s64 - r22.s64;
	// subf r26,r27,r23
	r26.s64 = r23.s64 - r27.s64;
	// rlwinm r3,r7,5,0,26
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r25,r4,1
	r25.s64 = ctx.r4.s64 + 1;
loc_82675400:
	// li r17,16
	r17.s64 = 16;
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r10,-1
	ctx.r6.s64 = ctx.r10.s64 + -1;
	// li r24,16
	r24.s64 = 16;
	// add r5,r26,r8
	ctx.r5.u64 = r26.u64 + ctx.r8.u64;
	// li r16,16
	r16.s64 = 16;
	// lvrx v12,r10,r17
	temp.u32 = ctx.r10.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r30,r29,r11
	r30.u64 = r29.u64 + r11.u64;
	// vor v11,v11,v12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvlx v10,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r6,r24
	temp.u32 = ctx.r6.u32 + r24.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r6,r11,-16
	ctx.r6.s64 = r11.s64 + -16;
	// vor v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvlx v9,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v10,r5,r16
	temp.u32 = ctx.r5.u32 + r16.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r5,16
	ctx.r5.s64 = 16;
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v8,r30,r17
	temp.u32 = r30.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r30,16
	r30.s64 = 16;
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v8,v0,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vmrghb v7,v0,v12
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// vmrglb v6,v0,v12
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// vmrghb v3,v0,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v11,v6,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vmrghb v6,v0,v10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsrah v8,v8,v13
	// vsrah v11,v11,v13
	// vpkshus v8,v8,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vmrghb v11,v12,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v7,v0,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v12
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v4,v0,v12
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v10,v10,v7
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v7,v3,v5
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsrah v10,v10,v13
	// vsrah v8,v8,v13
	// vsrah v7,v7,v13
	// vsrah v9,v9,v13
	// vpkshus v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// stvlx v10,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// stvrx v10,r6,r5
	ea = ctx.r6.u32 + ctx.r5.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// mr r5,r8
	ctx.r5.u64 = ctx.r8.u64;
	// add r6,r28,r11
	ctx.r6.u64 = r28.u64 + r11.u64;
	// stvlx v9,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// stvrx v9,r11,r30
	ea = r11.u32 + r30.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// stvlx v11,0,r8
	ea = ctx.r8.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// addi r8,r8,32
	ctx.r8.s64 = ctx.r8.s64 + 32;
	// stvrx v11,r5,r30
	ea = ctx.r5.u32 + r30.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// stvlx v12,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// stvrx v12,r6,r24
	ea = ctx.r6.u32 + r24.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// bne cr6,0x82675400
	if (!cr6.eq) goto loc_82675400;
loc_82675508:
	// cmpw cr6,r3,r20
	cr6.compare<int32_t>(ctx.r3.s32, r20.s32, xer);
	// bge cr6,0x826755a0
	if (!cr6.lt) goto loc_826755A0;
	// subf r10,r3,r20
	ctx.r10.s64 = r20.s64 - ctx.r3.s64;
	// add r11,r3,r23
	r11.u64 = ctx.r3.u64 + r23.u64;
	// addi r8,r10,-1
	ctx.r8.s64 = ctx.r10.s64 + -1;
	// add r10,r3,r27
	ctx.r10.u64 = ctx.r3.u64 + r27.u64;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// add r7,r25,r31
	ctx.r7.u64 = r25.u64 + r31.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r26,r23,r22
	r26.s64 = r22.s64 - r23.s64;
	// subf r25,r23,r27
	r25.s64 = r27.s64 - r23.s64;
	// subf r24,r27,r22
	r24.s64 = r22.s64 - r27.s64;
	// add r3,r6,r3
	ctx.r3.u64 = ctx.r6.u64 + ctx.r3.u64;
loc_82675544:
	// lbzx r6,r4,r31
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r4.u32 + r31.u32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbz r28,0(r7)
	r28.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// lbz r30,-1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// lbz r29,0(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// add r28,r28,r5
	r28.u64 = r28.u64 + ctx.r5.u64;
	// add r5,r30,r5
	ctx.r5.u64 = r30.u64 + ctx.r5.u64;
	// rlwinm r30,r28,31,1,31
	r30.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r17,r5,31,1,31
	r17.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 31) & 0x7FFFFFFF;
	// clrlwi r5,r30,24
	ctx.r5.u64 = r30.u32 & 0xFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// add r29,r29,r5
	r29.u64 = r29.u64 + ctx.r5.u64;
	// rlwinm r29,r29,31,1,31
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// stbx r17,r24,r10
	PPC_STORE_U8(r24.u32 + ctx.r10.u32, r17.u8);
	// stbx r29,r26,r11
	PPC_STORE_U8(r26.u32 + r11.u32, r29.u8);
	// stb r6,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r6.u8);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// stbx r5,r25,r11
	PPC_STORE_U8(r25.u32 + r11.u32, ctx.r5.u8);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x82675544
	if (!cr6.eq) goto loc_82675544;
loc_826755A0:
	// lbzx r11,r4,r31
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + r31.u32);
	// add r10,r3,r22
	ctx.r10.u64 = ctx.r3.u64 + r22.u64;
	// lbzx r7,r3,r23
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + r23.u32);
	// add r8,r3,r27
	ctx.r8.u64 = ctx.r3.u64 + r27.u64;
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// add r23,r23,r18
	r23.u64 = r23.u64 + r18.u64;
	// rlwinm r7,r7,31,1,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// add r22,r22,r18
	r22.u64 = r22.u64 + r18.u64;
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// add r27,r27,r18
	r27.u64 = r27.u64 + r18.u64;
	// add r31,r31,r9
	r31.u64 = r31.u64 + ctx.r9.u64;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// stb r7,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r7.u8);
	// stb r7,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, ctx.r7.u8);
	// stb r11,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r11.u8);
	// stb r11,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, r11.u8);
	// bne cr6,0x826753bc
	if (!cr6.eq) goto loc_826753BC;
loc_826755E8:
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lbzx r10,r23,r20
	ctx.r10.u64 = PPC_LOAD_U8(r23.u32 + r20.u32);
	// add r11,r22,r20
	r11.u64 = r22.u64 + r20.u64;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// stb r10,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r10.u8);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_82675610"))) PPC_WEAK_FUNC(sub_82675610);
PPC_FUNC_IMPL(__imp__sub_82675610) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r14,r6
	r14.u64 = ctx.r6.u64;
	// lwz r6,372(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// stw r8,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r8.u32);
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// stw r9,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, ctx.r9.u32);
	// rlwinm r11,r14,0,0,26
	r11.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 0) & 0xFFFFFFE0;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// mullw r6,r6,r9
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r9.s32);
	// vspltish v3,1
	// stw r14,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r14.u32);
	// vspltish v13,2
	// add r3,r6,r3
	ctx.r3.u64 = ctx.r6.u64 + ctx.r3.u64;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// add r16,r5,r7
	r16.u64 = ctx.r5.u64 + ctx.r7.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r27,1
	r27.s64 = 1;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82675710
	if (!cr6.gt) goto loc_82675710;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// rlwinm r6,r11,27,5,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r3,1
	r11.s64 = ctx.r3.s64 + 1;
	// addi r4,r6,1
	ctx.r4.s64 = ctx.r6.s64 + 1;
	// rlwinm r6,r4,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r31,r4,5,0,26
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r27,r6,1
	r27.s64 = ctx.r6.s64 + 1;
loc_8267568C:
	// li r25,16
	r25.s64 = 16;
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r29,r11,-1
	r29.s64 = r11.s64 + -1;
	// li r26,16
	r26.s64 = 16;
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// mr r23,r7
	r23.u64 = ctx.r7.u64;
	// lvrx v11,r11,r25
	temp.u32 = r11.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r28,r7,16
	r28.s64 = ctx.r7.s64 + 16;
	// vor v11,v12,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvlx v10,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r29,r26
	temp.u32 = r29.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r21,16
	r21.s64 = 16;
	// vor v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// vmrghb v10,v0,v11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r7,r7,32
	ctx.r7.s64 = ctx.r7.s64 + 32;
	// vmrglb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// vmrghb v9,v0,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v11,v8,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v10,v10,v3
	// vsrah v11,v11,v3
	// vpkshus v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vmrghb v10,v12,v11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvlx v10,0,r24
	ea = r24.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// stvrx v10,r23,r25
	ea = r23.u32 + r25.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// stvlx v12,0,r28
	ea = r28.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// stvrx v12,r28,r21
	ea = r28.u32 + r21.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// bne cr6,0x8267568c
	if (!cr6.eq) goto loc_8267568C;
loc_82675710:
	// cmpw cr6,r31,r14
	cr6.compare<int32_t>(r31.s32, r14.s32, xer);
	// bge cr6,0x8267576c
	if (!cr6.lt) goto loc_8267576C;
	// subf r7,r31,r14
	ctx.r7.s64 = r14.s64 - r31.s64;
	// add r11,r31,r5
	r11.u64 = r31.u64 + ctx.r5.u64;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r7,r7,31,1,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// add r4,r27,r3
	ctx.r4.u64 = r27.u64 + ctx.r3.u64;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// rlwinm r29,r7,1,0,30
	r29.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r29,r31
	r31.u64 = r29.u64 + r31.u64;
loc_8267573C:
	// lbzx r29,r6,r3
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r3.u32);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lbz r28,0(r4)
	r28.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// stb r29,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, r29.u8);
	// stb r28,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r28.u8);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x8267573c
	if (!cr6.eq) goto loc_8267573C;
loc_8267576C:
	// lbzx r7,r6,r3
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r3.u32);
	// add r11,r31,r5
	r11.u64 = r31.u64 + ctx.r5.u64;
	// lwz r6,356(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// add r26,r3,r9
	r26.u64 = ctx.r3.u64 + ctx.r9.u64;
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// stb r7,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r7.u8);
	// bge cr6,0x82675b08
	if (!cr6.lt) goto loc_82675B08;
	// subf r11,r6,r10
	r11.s64 = ctx.r10.s64 - ctx.r6.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r15,r11,1
	r15.s64 = r11.s64 + 1;
	// stw r15,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r15.u32);
loc_826757A0:
	// rlwinm r11,r14,0,0,26
	r11.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 0) & 0xFFFFFFE0;
	// li r28,0
	r28.s64 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826759a8
	if (!cr6.gt) goto loc_826759A8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r5,r30,r8
	ctx.r5.u64 = r30.u64 + ctx.r8.u64;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// addi r6,r11,1
	ctx.r6.s64 = r11.s64 + 1;
	// addi r7,r26,1
	ctx.r7.s64 = r26.s64 + 1;
	// rlwinm r28,r6,4,0,27
	r28.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r11,r16,16
	r11.s64 = r16.s64 + 16;
	// addi r9,r28,1
	ctx.r9.s64 = r28.s64 + 1;
	// subf r29,r16,r22
	r29.s64 = r22.s64 - r16.s64;
	// subf r25,r16,r30
	r25.s64 = r30.s64 - r16.s64;
	// subf r24,r30,r22
	r24.s64 = r22.s64 - r30.s64;
	// subf r23,r16,r5
	r23.s64 = ctx.r5.s64 - r16.s64;
	// subf r21,r30,r5
	r21.s64 = ctx.r5.s64 - r30.s64;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// rlwinm r27,r6,5,0,26
	r27.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
loc_826757F8:
	// li r8,16
	ctx.r8.s64 = 16;
	// lvlx v11,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r5,r7,-1
	ctx.r5.s64 = ctx.r7.s64 + -1;
	// li r9,16
	ctx.r9.s64 = 16;
	// add r4,r24,r10
	ctx.r4.u64 = r24.u64 + ctx.r10.u64;
	// add r3,r29,r11
	ctx.r3.u64 = r29.u64 + r11.u64;
	// lvrx v12,r7,r8
	temp.u32 = ctx.r7.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r20,16
	r20.s64 = 16;
	// vor v11,v11,v12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvlx v10,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r31,r11,-16
	r31.s64 = r11.s64 + -16;
	// vor v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v10,r4,r8
	temp.u32 = ctx.r4.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r5,r25,r11
	ctx.r5.u64 = r25.u64 + r11.u64;
	// vor v6,v9,v10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v8,r3,r20
	temp.u32 = ctx.r3.u32 + r20.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r4,r21,r10
	ctx.r4.u64 = r21.u64 + ctx.r10.u64;
	// vor v4,v10,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v10,v0,v11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// add r3,r23,r11
	ctx.r3.u64 = r23.u64 + r11.u64;
	// vmrglb v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// vmrglb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vmrglb v4,v0,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v11,v8,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v28,v5,v5
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsrah v10,v10,v3
	// vaddshs v27,v4,v4
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsrah v11,v11,v3
	// vaddshs v28,v28,v5
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v27,v27,v4
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vpkshus v10,v10,v11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vmrghb v11,v12,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v7,v0,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v10,v0,v11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvlx v11,0,r31
	ea = r31.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// vmrglb v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvrx v11,r31,r9
	ea = r31.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// vor v11,v7,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vmrghb v7,v0,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvlx v12,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// vmrglb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvrx v12,r11,r8
	ea = r11.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// vaddshs v2,v10,v10
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// vaddshs v12,v11,v11
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// vaddshs v30,v7,v7
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v29,v6,v6
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v1,v9,v9
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v31,v8,v8
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v30,v30,v7
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v2,v2,v10
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v29,v29,v6
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v1,v1,v9
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v31,v31,v8
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v12,v7,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v11,v30,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v7,v6,v2
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v10,v29,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v6,v5,v1
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v9,v28,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v8,v27,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v5,v4,v31
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsrah v4,v12,v13
	// vsrah v12,v11,v13
	// vsrah v11,v10,v13
	// vsrah v10,v9,v13
	// vsrah v9,v8,v13
	// vsrah v8,v6,v13
	// vsrah v7,v7,v13
	// vsrah v6,v5,v13
	// vpkshus v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// li r31,16
	r31.s64 = 16;
	// vpkshus v11,v10,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// li r19,16
	r19.s64 = 16;
	// vpkshus v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// li r18,16
	r18.s64 = 16;
	// vpkshus v10,v4,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// vor v8,v12,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// stvlx v12,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// vor v12,v9,v9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// stvrx v8,r8,r31
	ea = ctx.r8.u32 + r31.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// stvlx v11,0,r5
	ea = ctx.r5.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// stvrx v11,r5,r20
	ea = ctx.r5.u32 + r20.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// stvlx v10,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// stvrx v10,r4,r19
	ea = ctx.r4.u32 + r19.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// stvlx v12,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// stvrx v12,r3,r18
	ea = ctx.r3.u32 + r18.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// bne cr6,0x826757f8
	if (!cr6.eq) goto loc_826757F8;
	// lwz r9,340(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// lwz r8,332(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// lwz r14,316(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r15,80(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_826759A8:
	// cmpw cr6,r27,r14
	cr6.compare<int32_t>(r27.s32, r14.s32, xer);
	// bge cr6,0x82675a88
	if (!cr6.lt) goto loc_82675A88;
	// subf r11,r27,r14
	r11.s64 = r14.s64 - r27.s64;
	// add r7,r30,r8
	ctx.r7.u64 = r30.u64 + ctx.r8.u64;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// subf r18,r22,r7
	r18.s64 = ctx.r7.s64 - r22.s64;
	// rlwinm r6,r10,31,1,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// subf r17,r30,r7
	r17.s64 = ctx.r7.s64 - r30.s64;
	// addi r3,r6,1
	ctx.r3.s64 = ctx.r6.s64 + 1;
	// add r11,r27,r22
	r11.u64 = r27.u64 + r22.u64;
	// rlwinm r7,r3,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r27,r30
	ctx.r10.u64 = r27.u64 + r30.u64;
	// add r31,r5,r26
	r31.u64 = ctx.r5.u64 + r26.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// subf r21,r22,r16
	r21.s64 = r16.s64 - r22.s64;
	// subf r20,r22,r30
	r20.s64 = r30.s64 - r22.s64;
	// subf r19,r30,r16
	r19.s64 = r16.s64 - r30.s64;
	// add r27,r7,r27
	r27.u64 = ctx.r7.u64 + r27.u64;
loc_826759F0:
	// lbzx r4,r28,r26
	ctx.r4.u64 = PPC_LOAD_U8(r28.u32 + r26.u32);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// lbz r6,-1(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// lbz r29,0(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// lbz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r23,r6,1
	r23.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// rlwinm r24,r7,1,0,30
	r24.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// stbx r4,r19,r10
	PPC_STORE_U8(r19.u32 + ctx.r10.u32, ctx.r4.u8);
	// add r4,r29,r7
	ctx.r4.u64 = r29.u64 + ctx.r7.u64;
	// add r23,r6,r23
	r23.u64 = ctx.r6.u64 + r23.u64;
	// add r24,r7,r24
	r24.u64 = ctx.r7.u64 + r24.u64;
	// srawi r4,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 1;
	// add r7,r23,r7
	ctx.r7.u64 = r23.u64 + ctx.r7.u64;
	// clrlwi r29,r4,16
	r29.u64 = ctx.r4.u32 & 0xFFFF;
	// srawi r7,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 2;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// rotlwi r25,r5,1
	r25.u64 = __builtin_rotateleft32(ctx.r5.u32, 1);
	// add r6,r24,r6
	ctx.r6.u64 = r24.u64 + ctx.r6.u64;
	// stbx r29,r21,r11
	PPC_STORE_U8(r21.u32 + r11.u32, r29.u8);
	// add r25,r5,r25
	r25.u64 = ctx.r5.u64 + r25.u64;
	// stb r7,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r7.u8);
	// rlwinm r7,r4,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r29,r25,r4
	r29.u64 = r25.u64 + ctx.r4.u64;
	// add r7,r4,r7
	ctx.r7.u64 = ctx.r4.u64 + ctx.r7.u64;
	// srawi r4,r29,2
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3) != 0);
	ctx.r4.s64 = r29.s32 >> 2;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// srawi r6,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 2;
	// srawi r7,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 2;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stbx r4,r20,r11
	PPC_STORE_U8(r20.u32 + r11.u32, ctx.r4.u8);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stbx r6,r17,r10
	PPC_STORE_U8(r17.u32 + ctx.r10.u32, ctx.r6.u8);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// stbx r7,r18,r11
	PPC_STORE_U8(r18.u32 + r11.u32, ctx.r7.u8);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x826759f0
	if (!cr6.eq) goto loc_826759F0;
loc_82675A88:
	// lbzx r7,r28,r26
	ctx.r7.u64 = PPC_LOAD_U8(r28.u32 + r26.u32);
	// rlwinm r4,r8,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r11,r27,r22
	r11.u64 = PPC_LOAD_U8(r27.u32 + r22.u32);
	// add r6,r27,r30
	ctx.r6.u64 = r27.u64 + r30.u64;
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// add r5,r27,r30
	ctx.r5.u64 = r27.u64 + r30.u64;
	// add r30,r30,r4
	r30.u64 = r30.u64 + ctx.r4.u64;
	// stbx r3,r27,r16
	PPC_STORE_U8(r27.u32 + r16.u32, ctx.r3.u8);
	// mr r16,r22
	r16.u64 = r22.u64;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// rotlwi r4,r11,1
	ctx.r4.u64 = __builtin_rotateleft32(r11.u32, 1);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// srawi r7,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r7.s64 = r11.s32 >> 2;
	// clrlwi r11,r10,24
	r11.u64 = ctx.r10.u32 & 0xFF;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// addi r15,r15,-1
	r15.s64 = r15.s64 + -1;
	// clrlwi r10,r7,24
	ctx.r10.u64 = ctx.r7.u32 & 0xFF;
	// add r26,r26,r9
	r26.u64 = r26.u64 + ctx.r9.u64;
	// stb r11,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, r11.u8);
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// stb r11,1(r6)
	PPC_STORE_U8(ctx.r6.u32 + 1, r11.u8);
	// stw r15,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r15.u32);
	// stb r10,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r10.u8);
	// stb r10,1(r5)
	PPC_STORE_U8(ctx.r5.u32 + 1, ctx.r10.u8);
	// bne cr6,0x826757a0
	if (!cr6.eq) goto loc_826757A0;
loc_82675B08:
	// mr r5,r14
	ctx.r5.u64 = r14.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lbzx r10,r22,r14
	ctx.r10.u64 = PPC_LOAD_U8(r22.u32 + r14.u32);
	// add r11,r30,r14
	r11.u64 = r30.u64 + r14.u64;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// stb r10,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r10.u8);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82675B30"))) PPC_WEAK_FUNC(sub_82675B30);
PPC_FUNC_IMPL(__imp__sub_82675B30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-640(r1)
	ea = -640 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r6,684(r1)
	PPC_STORE_U32(ctx.r1.u32 + 684, ctx.r6.u32);
	// mr r21,r9
	r21.u64 = ctx.r9.u64;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// add r26,r7,r4
	r26.u64 = ctx.r7.u64 + ctx.r4.u64;
	// vspltish v0,2
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r8,2,0,29
	r25.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r4,r8
	ctx.r9.u64 = ctx.r4.u64 + ctx.r8.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r21,708(r1)
	PPC_STORE_U32(ctx.r1.u32 + 708, r21.u32);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// stw r26,64(r1)
	PPC_STORE_U32(ctx.r1.u32 + 64, r26.u32);
	// add r5,r25,r4
	ctx.r5.u64 = r25.u64 + ctx.r4.u64;
	// add r30,r8,r4
	r30.u64 = ctx.r8.u64 + ctx.r4.u64;
	// stw r25,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r25.u32);
	// rlwinm r11,r6,0,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFC0;
	// stw r9,72(r1)
	PPC_STORE_U32(ctx.r1.u32 + 72, ctx.r9.u32);
	// li r28,0
	r28.s64 = 0;
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r31.u32);
	// li r24,1
	r24.s64 = 1;
	// li r23,0
	r23.s64 = 0;
	// stw r5,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r5.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// ble cr6,0x82675ccc
	if (!cr6.gt) goto loc_82675CCC;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r8,r3,1
	ctx.r8.s64 = ctx.r3.s64 + 1;
	// rlwinm r7,r11,26,6,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// addi r11,r4,32
	r11.s64 = ctx.r4.s64 + 32;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// rlwinm r28,r7,4,0,27
	r28.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r23,r7,6,0,25
	r23.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 6) & 0xFFFFFFC0;
	// addi r24,r28,1
	r24.s64 = r28.s64 + 1;
loc_82675BC4:
	// li r22,16
	r22.s64 = 16;
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r29,r8,-1
	r29.s64 = ctx.r8.s64 + -1;
	// li r27,16
	r27.s64 = 16;
	// li r19,16
	r19.s64 = 16;
	// mr r18,r11
	r18.u64 = r11.u64;
	// lvrx v12,r8,r22
	temp.u32 = ctx.r8.u32 + r22.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r17,16
	r17.s64 = 16;
	// vor v10,v11,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvlx v11,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r29,r27
	temp.u32 = r29.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r29,r11,-32
	r29.s64 = r11.s64 + -32;
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// addi r27,r11,16
	r27.s64 = r11.s64 + 16;
	// mr r20,r29
	r20.u64 = r29.u64;
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// vmrghb v9,v13,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// vmrglb v8,v13,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v7,v11,v11
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v6,v10,v10
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v5,v9,v9
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v4,v8,v8
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v3,v7,v11
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v2,v6,v10
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v1,v5,v9
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v7,v5,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v5,v4,v8
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v6,v4,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v11,v1,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v9,v9,v3
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v10,v5,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v8,v8,v2
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsrah v11,v11,v0
	// vsrah v7,v7,v0
	// vsrah v10,v10,v0
	// vsrah v6,v6,v0
	// vsrah v8,v8,v0
	// vsrah v9,v9,v0
	// vpkshus v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vpkshus v10,v7,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vpkshus v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vmrghb v8,v12,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v10,v11,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrglb v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrghb v9,v8,v10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrglb v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v8,v12,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvlx v9,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// addi r29,r11,-16
	r29.s64 = r11.s64 + -16;
	// stvrx v9,r20,r22
	ea = r20.u32 + r22.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// mr r22,r11
	r22.u64 = r11.u64;
	// li r20,16
	r20.s64 = 16;
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stvlx v10,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// stvrx v10,r29,r19
	ea = r29.u32 + r19.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// stvlx v8,0,r18
	ea = r18.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// stvrx v8,r22,r20
	ea = r22.u32 + r20.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// stvlx v12,0,r27
	ea = r27.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// stvrx v12,r27,r17
	ea = r27.u32 + r17.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// bne cr6,0x82675bc4
	if (!cr6.eq) goto loc_82675BC4;
loc_82675CCC:
	// cmpw cr6,r23,r6
	cr6.compare<int32_t>(r23.s32, ctx.r6.s32, xer);
	// bge cr6,0x82675d54
	if (!cr6.lt) goto loc_82675D54;
	// subf r8,r23,r6
	ctx.r8.s64 = ctx.r6.s64 - r23.s64;
	// add r11,r23,r4
	r11.u64 = r23.u64 + ctx.r4.u64;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r8,r8,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// add r27,r24,r3
	r27.u64 = r24.u64 + ctx.r3.u64;
	// addi r29,r8,1
	r29.s64 = ctx.r8.s64 + 1;
	// rlwinm r8,r29,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r23,r8,r23
	r23.u64 = ctx.r8.u64 + r23.u64;
loc_82675CF8:
	// lbzx r24,r28,r3
	r24.u64 = PPC_LOAD_U8(r28.u32 + ctx.r3.u32);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// lbz r7,0(r27)
	ctx.r7.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// rlwinm r22,r8,1,0,30
	r22.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// stb r24,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, r24.u8);
	// rotlwi r24,r7,1
	r24.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// add r22,r8,r22
	r22.u64 = ctx.r8.u64 + r22.u64;
	// add r24,r7,r24
	r24.u64 = ctx.r7.u64 + r24.u64;
	// add r22,r22,r7
	r22.u64 = r22.u64 + ctx.r7.u64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r8,r24,r8
	ctx.r8.u64 = r24.u64 + ctx.r8.u64;
	// srawi r24,r22,2
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3) != 0);
	r24.s64 = r22.s32 >> 2;
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// srawi r8,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 2;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// stb r24,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r24.u8);
	// stb r7,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r7.u8);
	// stb r8,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r8.u8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x82675cf8
	if (!cr6.eq) goto loc_82675CF8;
loc_82675D54:
	// lbzx r7,r28,r3
	ctx.r7.u64 = PPC_LOAD_U8(r28.u32 + ctx.r3.u32);
	// add r11,r23,r4
	r11.u64 = r23.u64 + ctx.r4.u64;
	// add r8,r3,r21
	ctx.r8.u64 = ctx.r3.u64 + r21.u64;
	// lwz r4,724(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 724);
	// cmpw cr6,r4,r10
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r10.s32, xer);
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// stw r8,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r8.u32);
	// stb r7,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r7.u8);
	// stb r7,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r7.u8);
	// stb r7,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r7.u8);
	// bge cr6,0x826766cc
	if (!cr6.lt) goto loc_826766CC;
	// subf r11,r4,r10
	r11.s64 = ctx.r10.s64 - ctx.r4.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,24(r1)
	PPC_STORE_U32(ctx.r1.u32 + 24, r11.u32);
loc_82675D94:
	// rlwinm r11,r6,0,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFC0;
	// li r22,0
	r22.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// li r21,0
	r21.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826763cc
	if (!cr6.gt) goto loc_826763CC;
	// subf r6,r31,r9
	ctx.r6.s64 = ctx.r9.s64 - r31.s64;
	// addi r10,r9,48
	ctx.r10.s64 = ctx.r9.s64 + 48;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// subf r29,r31,r5
	r29.s64 = ctx.r5.s64 - r31.s64;
	// rlwinm r11,r11,26,6,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// stw r6,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r6.u32);
	// subf r6,r31,r26
	ctx.r6.s64 = r26.s64 - r31.s64;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// addi r7,r8,1
	ctx.r7.s64 = ctx.r8.s64 + 1;
	// rlwinm r22,r4,4,0,27
	r22.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r29,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r29.u32);
	// addi r8,r30,16
	ctx.r8.s64 = r30.s64 + 16;
	// stw r6,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r6.u32);
	// subf r6,r31,r30
	ctx.r6.s64 = r30.s64 - r31.s64;
	// addi r11,r31,32
	r11.s64 = r31.s64 + 32;
	// rlwinm r21,r4,6,0,25
	r21.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 6) & 0xFFFFFFC0;
	// stw r6,76(r1)
	PPC_STORE_U32(ctx.r1.u32 + 76, ctx.r6.u32);
	// subf r6,r9,r26
	ctx.r6.s64 = r26.s64 - ctx.r9.s64;
	// stw r6,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, ctx.r6.u32);
	// subf r6,r9,r30
	ctx.r6.s64 = r30.s64 - ctx.r9.s64;
	// subf r9,r9,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r9.s64;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// stw r9,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r9.u32);
	// subf r9,r30,r26
	ctx.r9.s64 = r26.s64 - r30.s64;
	// stw r9,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r9.u32);
	// subf r9,r30,r5
	ctx.r9.s64 = ctx.r5.s64 - r30.s64;
	// stw r9,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r9.u32);
	// subf r9,r5,r26
	ctx.r9.s64 = r26.s64 - ctx.r5.s64;
	// stw r9,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r9.u32);
	// addi r9,r22,1
	ctx.r9.s64 = r22.s64 + 1;
	// stw r9,32(r1)
	PPC_STORE_U32(ctx.r1.u32 + 32, ctx.r9.u32);
loc_82675E28:
	// li r28,16
	r28.s64 = 16;
	// lvlx v11,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r26,16
	r26.s64 = 16;
	// addi r9,r7,-1
	ctx.r9.s64 = ctx.r7.s64 + -1;
	// li r30,16
	r30.s64 = 16;
	// addi r6,r11,-32
	ctx.r6.s64 = r11.s64 + -32;
	// lvrx v12,r7,r28
	temp.u32 = ctx.r7.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r3,r11,-16
	ctx.r3.s64 = r11.s64 + -16;
	// vor v10,v11,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v12,r11,r26
	temp.u32 = r11.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r27,16
	r27.s64 = 16;
	// vor v7,v11,v12
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvlx v11,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v12,r9,r30
	temp.u32 = ctx.r9.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r31,r11,16
	r31.s64 = r11.s64 + 16;
	// li r25,16
	r25.s64 = 16;
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v11,r6,r28
	temp.u32 = ctx.r6.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v6,v9,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v8,r3,r27
	temp.u32 = ctx.r3.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r31,r25
	temp.u32 = r31.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v5,v11,v8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v4,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v24,v4,v9
	_mm_store_si128((__m128i*)v24.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v9,v13,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v4,v11,v11
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v3,v10,v10
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v2,v9,v9
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v1,v8,v8
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v29,v4,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v28,v3,v10
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v31,v2,v9
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v30,v1,v8
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v4,v2,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v3,v1,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v11,v31,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v10,v30,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v9,v9,v29
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v8,v8,v28
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vsrah v2,v11,v0
	// vsrah v4,v4,v0
	// vsrah v3,v3,v0
	// vsrah v10,v10,v0
	// vsrah v9,v9,v0
	// vsrah v8,v8,v0
	// vpkshus v11,v4,v3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vpkshus v10,v2,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vpkshus v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vmrghb v4,v12,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v3,v12,v11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v12,v13,v6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v2,v10,v9
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vmrglb v1,v10,v9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vmrglb v11,v13,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v10,v13,v5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v9,v13,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v5,v4,v2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrghb v6,v4,v2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrghb v4,v3,v1
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrglb v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v31,v13,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v2,v13,v6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v1,v13,v6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v30,v13,v5
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v29,v13,v4
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v21,v31,v31
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vmrghb v8,v13,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v23,v2,v2
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vmrglb v7,v13,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v22,v1,v1
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vmrglb v28,v13,v4
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v20,v30,v30
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vmrghb v27,v13,v3
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v19,v29,v29
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vmrglb v26,v13,v3
	_mm_store_si128((__m128i*)v26.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v25,v13,v24
	_mm_store_si128((__m128i*)v25.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r12,48
	r12.s64 = 48;
	// stvx128 v13,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v24,v13,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// addi r9,r1,352
	ctx.r9.s64 = ctx.r1.s64 + 352;
	// vaddshs v13,v23,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v18,v28,v28
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v17,v27,v27
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v16,v26,v26
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,240
	ctx.r9.s64 = ctx.r1.s64 + 240;
	// vaddshs v13,v22,v1
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v15,v12,v12
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v14,v11,v11
	_mm_store_si128((__m128i*)v14.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,272
	ctx.r9.s64 = ctx.r1.s64 + 272;
	// vaddshs v13,v21,v31
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)v31.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,304
	ctx.r9.s64 = ctx.r1.s64 + 304;
	// vaddshs v13,v20,v30
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v30.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,336
	ctx.r9.s64 = ctx.r1.s64 + 336;
	// vaddshs v13,v19,v29
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)v29.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,368
	ctx.r9.s64 = ctx.r1.s64 + 368;
	// vaddshs v13,v18,v28
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)v28.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,400
	ctx.r9.s64 = ctx.r1.s64 + 400;
	// vaddshs v13,v17,v27
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v27.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,432
	ctx.r9.s64 = ctx.r1.s64 + 432;
	// vaddshs v13,v16,v26
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v26.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,192
	ctx.r9.s64 = ctx.r1.s64 + 192;
	// vaddshs v13,v10,v10
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// vaddshs v13,v9,v9
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// vaddshs v13,v8,v8
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// vaddshs v13,v7,v7
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// vaddshs v13,v25,v25
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v25.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// vaddshs v13,v24,v24
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v24.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,384
	ctx.r9.s64 = ctx.r1.s64 + 384;
	// vaddshs v13,v15,v12
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,464
	ctx.r9.s64 = ctx.r1.s64 + 464;
	// vaddshs v13,v14,v11
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v14.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,192
	ctx.r9.s64 = ctx.r1.s64 + 192;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,256
	ctx.r9.s64 = ctx.r1.s64 + 256;
	// vaddshs v13,v13,v10
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,416
	ctx.r9.s64 = ctx.r1.s64 + 416;
	// vaddshs v13,v13,v9
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,288
	ctx.r9.s64 = ctx.r1.s64 + 288;
	// vaddshs v13,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,224
	ctx.r9.s64 = ctx.r1.s64 + 224;
	// vaddshs v13,v13,v7
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,320
	ctx.r9.s64 = ctx.r1.s64 + 320;
	// vaddshs v13,v13,v25
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)v25.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// lwz r3,28(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// li r31,16
	r31.s64 = 16;
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,448
	ctx.r9.s64 = ctx.r1.s64 + 448;
	// vaddshs v13,v13,v24
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)v24.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,384
	ctx.r9.s64 = ctx.r1.s64 + 384;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,464
	ctx.r9.s64 = ctx.r1.s64 + 464;
	// vaddshs v2,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,256
	ctx.r9.s64 = ctx.r1.s64 + 256;
	// vaddshs v1,v13,v1
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsrah v2,v2,v0
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,416
	ctx.r9.s64 = ctx.r1.s64 + 416;
	// vaddshs v31,v13,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsrah v1,v1,v0
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,288
	ctx.r9.s64 = ctx.r1.s64 + 288;
	// vaddshs v30,v13,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vpkshus v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsrah v31,v31,v0
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,224
	ctx.r9.s64 = ctx.r1.s64 + 224;
	// vaddshs v29,v13,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vsrah v30,v30,v0
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,320
	ctx.r9.s64 = ctx.r1.s64 + 320;
	// vaddshs v28,v13,v28
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vpkshus v1,v31,v30
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsrah v29,v29,v0
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v28,v28,v0
	// vaddshs v27,v13,v27
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)v27.s16)));
	// addi r9,r10,-48
	ctx.r9.s64 = ctx.r10.s64 + -48;
	// vpkshus v31,v29,v28
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vsrah v27,v27,v0
	// addi r28,r1,448
	r28.s64 = ctx.r1.s64 + 448;
	// lvx128 v13,r0,r28
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r28,16
	r28.s64 = 16;
	// vaddshs v26,v13,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)v26.s16)));
	// stvlx v2,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v2.u8[15 - i]);
	// stvrx v2,r9,r31
	ea = ctx.r9.u32 + r31.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v2.u8[i]);
	// li r9,16
	ctx.r9.s64 = 16;
	// stvlx v1,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v1.u8[15 - i]);
	// vaddshs v2,v15,v23
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v23.s16)));
	// stvrx v1,r6,r30
	ea = ctx.r6.u32 + r30.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v1.u8[i]);
	// addi r6,r1,192
	ctx.r6.s64 = ctx.r1.s64 + 192;
	// vsrah v26,v26,v0
	// stvlx v31,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v31.u8[15 - i]);
	// stvrx v31,r3,r28
	ea = ctx.r3.u32 + r28.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v31.u8[i]);
	// vaddshs v1,v14,v22
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v14.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vsrah v2,v2,v0
	// vpkshus v30,v27,v26
	_mm_store_si128((__m128i*)v30.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vsrah v1,v1,v0
	// stvlx v30,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v30.u8[15 - i]);
	// lvx128 v31,r0,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// vaddshs v31,v31,v21
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v21.s16)));
	// lvx128 v29,r0,r6
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// vaddshs v29,v29,v20
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v20.s16)));
	// vsrah v31,v31,v0
	// lvx128 v28,r0,r6
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// vaddshs v28,v28,v19
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v19.s16)));
	// lvx128 v27,r0,r6
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// vaddshs v27,v27,v18
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v18.s16)));
	// lvx128 v26,r0,r6
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,176
	ctx.r6.s64 = ctx.r1.s64 + 176;
	// vaddshs v26,v26,v17
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v17.s16)));
	// lvx128 v23,r0,r6
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvrx v30,r10,r9
	ea = ctx.r10.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v30.u8[i]);
	// vsrah v30,v29,v0
	// vaddshs v23,v23,v16
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vsrah v29,v28,v0
	// addi r28,r1,352
	r28.s64 = ctx.r1.s64 + 352;
	// vsrah v28,v27,v0
	// vpkshus v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// addi r27,r1,240
	r27.s64 = ctx.r1.s64 + 240;
	// vpkshus v1,v31,v30
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v31.s16)));
	// addi r25,r1,272
	r25.s64 = ctx.r1.s64 + 272;
	// vsrah v27,v26,v0
	// lwz r9,208(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// vpkshus v31,v29,v28
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vsrah v26,v23,v0
	// lvx128 v29,r0,r28
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r6,20(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// vaddshs v12,v12,v29
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)v29.s16)));
	// lvx128 v29,r0,r27
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r27.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v11,v11,v29
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v29.s16)));
	// lvx128 v29,r0,r25
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r25,r1,304
	r25.s64 = ctx.r1.s64 + 304;
	// vaddshs v10,v10,v29
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vor v23,v1,v1
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// vsrah v11,v11,v0
	// li r28,16
	r28.s64 = 16;
	// vsrah v12,v12,v0
	// lwz r31,68(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// lvx128 v29,r0,r25
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r25,r1,336
	r25.s64 = ctx.r1.s64 + 336;
	// vaddshs v9,v9,v29
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v29.s16)));
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// vor v29,v1,v1
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vpkshus v30,v27,v26
	_mm_store_si128((__m128i*)v30.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vor v22,v31,v31
	_mm_store_si128((__m128i*)v22.u8, _mm_load_si128((__m128i*)v31.u8));
	// li r27,16
	r27.s64 = 16;
	// vor v21,v31,v31
	_mm_store_si128((__m128i*)v21.u8, _mm_load_si128((__m128i*)v31.u8));
	// add r31,r31,r10
	r31.u64 = r31.u64 + ctx.r10.u64;
	// lvx128 v1,r0,r25
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r25,r1,368
	r25.s64 = ctx.r1.s64 + 368;
	// vaddshs v8,v8,v1
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vpkshus v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsrah v10,v10,v0
	// li r24,16
	r24.s64 = 16;
	// vsrah v9,v9,v0
	// lvx128 v1,r0,r25
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r25,r1,400
	r25.s64 = ctx.r1.s64 + 400;
	// vaddshs v7,v7,v1
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsrah v8,v8,v0
	// vpkshus v11,v10,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// lvx128 v1,r0,r25
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r25,r1,432
	r25.s64 = ctx.r1.s64 + 432;
	// vsrah v7,v7,v0
	// vaddshs v1,v25,v1
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// lvx128 v31,r0,r25
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v10,v8,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvlx v2,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v2.u8[15 - i]);
	// vaddshs v31,v24,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v31.s16)));
	// stvrx v2,r9,r30
	ea = ctx.r9.u32 + r30.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v2.u8[i]);
	// addi r9,r8,-16
	ctx.r9.s64 = ctx.r8.s64 + -16;
	// stvlx v29,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v29.u8[15 - i]);
	// add r30,r29,r11
	r30.u64 = r29.u64 + r11.u64;
	// stvrx v23,r6,r28
	ea = ctx.r6.u32 + r28.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v23.u8[i]);
	// lwz r6,76(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 76);
	// li r29,16
	r29.s64 = 16;
	// stvlx v22,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v22.u8[15 - i]);
	// stvrx v21,r3,r27
	ea = ctx.r3.u32 + r27.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v21.u8[i]);
	// vsrah v2,v1,v0
	// stvlx v30,0,r31
	ea = r31.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v30.u8[15 - i]);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// stvrx v30,r31,r26
	ea = r31.u32 + r26.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v30.u8[i]);
	// vsrah v1,v31,v0
	// stvlx v12,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r31,16(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// stvrx v12,r9,r29
	ea = ctx.r9.u32 + r29.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// stvlx v11,0,r8
	ea = ctx.r8.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// add r3,r3,r10
	ctx.r3.u64 = ctx.r3.u64 + ctx.r10.u64;
	// stvrx v11,r8,r28
	ea = ctx.r8.u32 + r28.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// add r31,r31,r8
	r31.u64 = r31.u64 + ctx.r8.u64;
	// li r25,16
	r25.s64 = 16;
	// vpkshus v9,v2,v1
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// stvlx v10,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// stvrx v10,r6,r27
	ea = ctx.r6.u32 + r27.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// lwz r29,96(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stvlx v9,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stvrx v9,r3,r26
	ea = ctx.r3.u32 + r26.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// add r6,r29,r11
	ctx.r6.u64 = r29.u64 + r11.u64;
	// stvlx v6,0,r5
	ea = ctx.r5.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v6.u8[15 - i]);
	// li r3,16
	ctx.r3.s64 = 16;
	// stvrx v6,r5,r25
	ea = ctx.r5.u32 + r25.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v6.u8[i]);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvlx v5,0,r31
	ea = r31.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v5.u8[15 - i]);
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// stvrx v5,r31,r24
	ea = r31.u32 + r24.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v5.u8[i]);
	// li r31,16
	r31.s64 = 16;
	// stvlx v4,0,r30
	ea = r30.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v4.u8[15 - i]);
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// stvrx v4,r6,r3
	ea = ctx.r6.u32 + ctx.r3.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v4.u8[i]);
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// stvlx v3,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v3.u8[15 - i]);
	// addi r8,r8,64
	ctx.r8.s64 = ctx.r8.s64 + 64;
	// li r12,48
	r12.s64 = 48;
	// lvx128 v13,r1,r12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// stvrx v3,r9,r31
	ea = ctx.r9.u32 + r31.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v3.u8[i]);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x82675e28
	if (!cr6.eq) goto loc_82675E28;
	// lwz r4,32(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r26,64(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 64);
	// lwz r8,36(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r9,72(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 72);
	// lwz r25,108(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r6,684(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	// lwz r31,92(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_826763CC:
	// cmpw cr6,r21,r6
	cr6.compare<int32_t>(r21.s32, ctx.r6.s32, xer);
	// bge cr6,0x826765f4
	if (!cr6.lt) goto loc_826765F4;
	// subf r10,r21,r6
	ctx.r10.s64 = ctx.r6.s64 - r21.s64;
	// add r11,r21,r26
	r11.u64 = r21.u64 + r26.u64;
	// addi r7,r10,-1
	ctx.r7.s64 = ctx.r10.s64 + -1;
	// add r6,r21,r31
	ctx.r6.u64 = r21.u64 + r31.u64;
	// rlwinm r7,r7,30,2,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// add r10,r21,r9
	ctx.r10.u64 = r21.u64 + ctx.r9.u64;
	// addi r28,r7,1
	r28.s64 = ctx.r7.s64 + 1;
	// addi r7,r11,3
	ctx.r7.s64 = r11.s64 + 3;
	// addi r11,r6,1
	r11.s64 = ctx.r6.s64 + 1;
	// subf r6,r31,r26
	ctx.r6.s64 = r26.s64 - r31.s64;
	// subf r19,r31,r9
	r19.s64 = ctx.r9.s64 - r31.s64;
	// add r29,r21,r5
	r29.u64 = r21.u64 + ctx.r5.u64;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// add r27,r4,r8
	r27.u64 = ctx.r4.u64 + ctx.r8.u64;
	// stw r6,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r6.u32);
	// subf r6,r31,r30
	ctx.r6.s64 = r30.s64 - r31.s64;
	// stw r6,76(r1)
	PPC_STORE_U32(ctx.r1.u32 + 76, ctx.r6.u32);
	// subf r6,r31,r5
	ctx.r6.s64 = ctx.r5.s64 - r31.s64;
	// stw r6,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r6.u32);
	// subf r6,r9,r26
	ctx.r6.s64 = r26.s64 - ctx.r9.s64;
	// stw r6,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, ctx.r6.u32);
	// subf r6,r9,r30
	ctx.r6.s64 = r30.s64 - ctx.r9.s64;
	// subf r9,r9,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r9.s64;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// stw r9,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r9.u32);
	// subf r9,r26,r30
	ctx.r9.s64 = r30.s64 - r26.s64;
	// stw r9,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r9.u32);
	// subf r9,r26,r5
	ctx.r9.s64 = ctx.r5.s64 - r26.s64;
	// stw r9,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r9.u32);
	// subf r9,r5,r30
	ctx.r9.s64 = r30.s64 - ctx.r5.s64;
	// stw r9,32(r1)
	PPC_STORE_U32(ctx.r1.u32 + 32, ctx.r9.u32);
	// rlwinm r9,r28,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r21,r9,r21
	r21.u64 = ctx.r9.u64 + r21.u64;
loc_82676458:
	// lbzx r26,r22,r8
	r26.u64 = PPC_LOAD_U8(r22.u32 + ctx.r8.u32);
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// lbz r5,0(r27)
	ctx.r5.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// lbz r4,-1(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r31,1(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// add r15,r4,r8
	r15.u64 = ctx.r4.u64 + ctx.r8.u64;
	// lbz r30,2(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// add r25,r8,r9
	r25.u64 = ctx.r8.u64 + ctx.r9.u64;
	// rotlwi r9,r5,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r5.u32, 1);
	// add r18,r5,r8
	r18.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r14,r4,r25
	r14.u64 = ctx.r4.u64 + r25.u64;
	// add r17,r9,r8
	r17.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rotlwi r9,r4,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 1);
	// rotlwi r23,r3,1
	r23.u64 = __builtin_rotateleft32(ctx.r3.u32, 1);
	// add r24,r4,r9
	r24.u64 = ctx.r4.u64 + ctx.r9.u64;
	// rotlwi r6,r31,1
	ctx.r6.u64 = __builtin_rotateleft32(r31.u32, 1);
	// add r16,r24,r8
	r16.u64 = r24.u64 + ctx.r8.u64;
	// add r8,r5,r25
	ctx.r8.u64 = ctx.r5.u64 + r25.u64;
	// add r20,r3,r23
	r20.u64 = ctx.r3.u64 + r23.u64;
	// srawi r8,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 2;
	// srawi r5,r18,1
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x1) != 0);
	ctx.r5.s64 = r18.s32 >> 1;
	// srawi r4,r17,2
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x3) != 0);
	ctx.r4.s64 = r17.s32 >> 2;
	// clrlwi r25,r8,16
	r25.u64 = ctx.r8.u32 & 0xFFFF;
	// clrlwi r24,r5,16
	r24.u64 = ctx.r5.u32 & 0xFFFF;
	// rotlwi r9,r30,1
	ctx.r9.u64 = __builtin_rotateleft32(r30.u32, 1);
	// clrlwi r23,r4,16
	r23.u64 = ctx.r4.u32 & 0xFFFF;
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// add r6,r31,r6
	ctx.r6.u64 = r31.u64 + ctx.r6.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// add r9,r30,r9
	ctx.r9.u64 = r30.u64 + ctx.r9.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// add r20,r20,r8
	r20.u64 = r20.u64 + ctx.r8.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// srawi r18,r16,2
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x3) != 0);
	r18.s64 = r16.s32 >> 2;
	// srawi r20,r20,2
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x3) != 0);
	r20.s64 = r20.s32 >> 2;
	// srawi r6,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 2;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	// srawi r16,r15,1
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x1) != 0);
	r16.s64 = r15.s32 >> 1;
	// stb r18,-2(r10)
	PPC_STORE_U8(ctx.r10.u32 + -2, r18.u8);
	// add r18,r8,r3
	r18.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r17,r5,r31
	r17.u64 = ctx.r5.u64 + r31.u64;
	// stbx r20,r19,r11
	PPC_STORE_U8(r19.u32 + r11.u32, r20.u8);
	// srawi r18,r18,1
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x1) != 0);
	r18.s64 = r18.s32 >> 1;
	// stb r6,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r6.u8);
	// stb r9,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, ctx.r9.u8);
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// srawi r17,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	r17.s64 = r17.s32 >> 1;
	// rlwinm r20,r5,1,0,30
	r20.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// stb r16,-3(r7)
	PPC_STORE_U8(ctx.r7.u32 + -3, r16.u8);
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r15,r4,r30
	r15.u64 = ctx.r4.u64 + r30.u64;
	// add r8,r5,r20
	ctx.r8.u64 = ctx.r5.u64 + r20.u64;
	// stbx r18,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, r18.u8);
	// srawi r15,r15,1
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x1) != 0);
	r15.s64 = r15.s32 >> 1;
	// lwz r9,68(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// srawi r14,r14,2
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x3) != 0);
	r14.s64 = r14.s32 >> 2;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// lwz r31,76(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 76);
	// srawi r6,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 2;
	// stb r15,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r15.u8);
	// srawi r8,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 2;
	// stbx r17,r9,r10
	PPC_STORE_U8(ctx.r9.u32 + ctx.r10.u32, r17.u8);
	// lwz r9,32(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32);
	// stbx r6,r31,r11
	PPC_STORE_U8(r31.u32 + r11.u32, ctx.r6.u8);
	// lwz r31,84(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stbx r14,r9,r29
	PPC_STORE_U8(ctx.r9.u32 + r29.u32, r14.u8);
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r4,r9
	ctx.r9.u64 = ctx.r4.u64 + ctx.r9.u64;
	// stbx r8,r31,r10
	PPC_STORE_U8(r31.u32 + ctx.r10.u32, ctx.r8.u8);
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// lwz r8,16(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	// stbx r9,r8,r7
	PPC_STORE_U8(ctx.r8.u32 + ctx.r7.u32, ctx.r9.u8);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stb r26,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r26.u8);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// lwz r8,36(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stbx r25,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, r25.u8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stbx r24,r9,r10
	PPC_STORE_U8(ctx.r9.u32 + ctx.r10.u32, r24.u8);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// stbx r23,r9,r7
	PPC_STORE_U8(ctx.r9.u32 + ctx.r7.u32, r23.u8);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// bne cr6,0x82676458
	if (!cr6.eq) goto loc_82676458;
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r26,64(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 64);
	// lwz r9,72(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 72);
	// lwz r25,108(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r6,684(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	// lwz r31,92(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_826765F4:
	// lwz r27,24(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// add r4,r21,r9
	ctx.r4.u64 = r21.u64 + ctx.r9.u64;
	// lbzx r7,r22,r8
	ctx.r7.u64 = PPC_LOAD_U8(r22.u32 + ctx.r8.u32);
	// add r3,r21,r26
	ctx.r3.u64 = r21.u64 + r26.u64;
	// lbzx r11,r21,r31
	r11.u64 = PPC_LOAD_U8(r21.u32 + r31.u32);
	// addi r23,r27,-1
	r23.s64 = r27.s64 + -1;
	// lwz r27,708(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 708);
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// rotlwi r24,r11,1
	r24.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r8,r8,r27
	ctx.r8.u64 = ctx.r8.u64 + r27.u64;
	// rlwinm r27,r10,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r24,r11,r24
	r24.u64 = r11.u64 + r24.u64;
	// stw r23,24(r1)
	PPC_STORE_U32(ctx.r1.u32 + 24, r23.u32);
	// add r27,r10,r27
	r27.u64 = ctx.r10.u64 + r27.u64;
	// add r24,r24,r10
	r24.u64 = r24.u64 + ctx.r10.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r8,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r8.u32);
	// add r11,r27,r11
	r11.u64 = r27.u64 + r11.u64;
	// srawi r27,r24,2
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3) != 0);
	r27.s64 = r24.s32 >> 2;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r24,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r24.s64 = r11.s32 >> 2;
	// clrlwi r11,r27,24
	r11.u64 = r27.u32 & 0xFF;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// add r29,r21,r30
	r29.u64 = r21.u64 + r30.u64;
	// add r28,r21,r5
	r28.u64 = r21.u64 + ctx.r5.u64;
	// clrlwi r27,r24,24
	r27.u64 = r24.u32 & 0xFF;
	// add r31,r25,r31
	r31.u64 = r25.u64 + r31.u64;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r11.u8);
	// add r9,r25,r9
	ctx.r9.u64 = r25.u64 + ctx.r9.u64;
	// stb r11,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, r11.u8);
	// add r26,r25,r26
	r26.u64 = r25.u64 + r26.u64;
	// stb r11,2(r4)
	PPC_STORE_U8(ctx.r4.u32 + 2, r11.u8);
	// add r30,r25,r30
	r30.u64 = r25.u64 + r30.u64;
	// stb r11,3(r4)
	PPC_STORE_U8(ctx.r4.u32 + 3, r11.u8);
	// add r5,r25,r5
	ctx.r5.u64 = r25.u64 + ctx.r5.u64;
	// stb r10,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r10.u8);
	// stb r10,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r10.u8);
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// stb r10,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, ctx.r10.u8);
	// stb r10,3(r3)
	PPC_STORE_U8(ctx.r3.u32 + 3, ctx.r10.u8);
	// stb r27,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r27.u8);
	// stb r27,1(r29)
	PPC_STORE_U8(r29.u32 + 1, r27.u8);
	// stb r27,2(r29)
	PPC_STORE_U8(r29.u32 + 2, r27.u8);
	// stb r27,3(r29)
	PPC_STORE_U8(r29.u32 + 3, r27.u8);
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r31.u32);
	// stw r9,72(r1)
	PPC_STORE_U32(ctx.r1.u32 + 72, ctx.r9.u32);
	// stw r26,64(r1)
	PPC_STORE_U32(ctx.r1.u32 + 64, r26.u32);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// stw r5,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r5.u32);
	// stb r7,0(r28)
	PPC_STORE_U8(r28.u32 + 0, ctx.r7.u8);
	// stb r7,1(r28)
	PPC_STORE_U8(r28.u32 + 1, ctx.r7.u8);
	// stb r7,2(r28)
	PPC_STORE_U8(r28.u32 + 2, ctx.r7.u8);
	// stb r7,3(r28)
	PPC_STORE_U8(r28.u32 + 3, ctx.r7.u8);
	// bne cr6,0x82675d94
	if (!cr6.eq) goto loc_82675D94;
loc_826766CC:
	// rlwinm r11,r6,0,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFF0;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8267675c
	if (!cr6.gt) goto loc_8267675C;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// rlwinm r10,r10,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0xFFFFFFF;
	// subf r3,r9,r31
	ctx.r3.s64 = r31.s64 - ctx.r9.s64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// subf r29,r9,r26
	r29.s64 = r26.s64 - ctx.r9.s64;
	// rlwinm r8,r10,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r28,r9,r30
	r28.s64 = r30.s64 - ctx.r9.s64;
	// mr r27,r8
	r27.u64 = ctx.r8.u64;
loc_82676704:
	// add r7,r3,r11
	ctx.r7.u64 = ctx.r3.u64 + r11.u64;
	// li r25,16
	r25.s64 = 16;
	// mr r24,r11
	r24.u64 = r11.u64;
	// mr r23,r11
	r23.u64 = r11.u64;
	// li r22,16
	r22.s64 = 16;
	// lvlx v13,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r5,r29,r11
	ctx.r5.u64 = r29.u64 + r11.u64;
	// lvrx v0,r7,r25
	temp.u32 = ctx.r7.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r21,16
	r21.s64 = 16;
	// vor v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// add r4,r28,r11
	ctx.r4.u64 = r28.u64 + r11.u64;
	// li r20,16
	r20.s64 = 16;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stvlx v0,0,r24
	ea = r24.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stvrx v0,r23,r22
	ea = r23.u32 + r22.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// stvlx v0,0,r5
	ea = ctx.r5.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,r5,r21
	ea = ctx.r5.u32 + r21.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// stvlx v0,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,r4,r20
	ea = ctx.r4.u32 + r20.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// bne cr6,0x82676704
	if (!cr6.eq) goto loc_82676704;
loc_8267675C:
	// cmpw cr6,r27,r6
	cr6.compare<int32_t>(r27.s32, ctx.r6.s32, xer);
	// bge cr6,0x8267679c
	if (!cr6.lt) goto loc_8267679C;
	// subf r10,r27,r6
	ctx.r10.s64 = ctx.r6.s64 - r27.s64;
	// add r11,r8,r9
	r11.u64 = ctx.r8.u64 + ctx.r9.u64;
	// subf r6,r9,r31
	ctx.r6.s64 = r31.s64 - ctx.r9.s64;
	// subf r5,r9,r26
	ctx.r5.s64 = r26.s64 - ctx.r9.s64;
	// subf r4,r9,r30
	ctx.r4.s64 = r30.s64 - ctx.r9.s64;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
loc_8267677C:
	// lbzx r7,r11,r6
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// stbx r7,r5,r11
	PPC_STORE_U8(ctx.r5.u32 + r11.u32, ctx.r7.u8);
	// stbx r7,r4,r11
	PPC_STORE_U8(ctx.r4.u32 + r11.u32, ctx.r7.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8267677c
	if (!cr6.eq) goto loc_8267677C;
loc_8267679C:
	// lbzx r11,r8,r31
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + r31.u32);
	// stbx r11,r8,r9
	PPC_STORE_U8(ctx.r8.u32 + ctx.r9.u32, r11.u8);
	// stbx r11,r8,r26
	PPC_STORE_U8(ctx.r8.u32 + r26.u32, r11.u8);
	// stbx r11,r8,r30
	PPC_STORE_U8(ctx.r8.u32 + r30.u32, r11.u8);
	// addi r1,r1,640
	ctx.r1.s64 = ctx.r1.s64 + 640;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826767B4"))) PPC_WEAK_FUNC(sub_826767B4);
PPC_FUNC_IMPL(__imp__sub_826767B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826767B8"))) PPC_WEAK_FUNC(sub_826767B8);
PPC_FUNC_IMPL(__imp__sub_826767B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-864(r1)
	ea = -864 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r22,r9
	r22.u64 = ctx.r9.u64;
	// lwz r9,964(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r6,908(r1)
	PPC_STORE_U32(ctx.r1.u32 + 908, ctx.r6.u32);
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// vspltish v12,2
	// mullw r9,r9,r22
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r22.s32);
	// vspltish v0,3
	// stw r22,932(r1)
	PPC_STORE_U32(ctx.r1.u32 + 932, r22.u32);
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// add r30,r9,r3
	r30.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r19,r4,r8
	r19.u64 = ctx.r4.u64 + ctx.r8.u64;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// add r31,r5,r7
	r31.u64 = ctx.r5.u64 + ctx.r7.u64;
	// rlwinm r28,r8,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r20,r11,r4
	r20.u64 = r11.u64 + ctx.r4.u64;
	// rlwinm r11,r6,0,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFC0;
	// stw r19,56(r1)
	PPC_STORE_U32(ctx.r1.u32 + 56, r19.u32);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r24,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r24.u32);
	// stw r31,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r31.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// li r29,1
	r29.s64 = 1;
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// add r25,r9,r4
	r25.u64 = ctx.r9.u64 + ctx.r4.u64;
	// ble cr6,0x82676968
	if (!cr6.gt) goto loc_82676968;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r30,1
	ctx.r9.s64 = r30.s64 + 1;
	// rlwinm r8,r11,26,6,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// addi r11,r5,32
	r11.s64 = ctx.r5.s64 + 32;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// rlwinm r3,r8,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r27,r8,6,0,25
	r27.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 6) & 0xFFFFFFC0;
	// addi r29,r3,1
	r29.s64 = ctx.r3.s64 + 1;
loc_82676860:
	// li r26,16
	r26.s64 = 16;
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r9,-1
	ctx.r7.s64 = ctx.r9.s64 + -1;
	// li r4,16
	ctx.r4.s64 = 16;
	// li r18,16
	r18.s64 = 16;
	// mr r17,r11
	r17.u64 = r11.u64;
	// lvrx v11,r9,r26
	temp.u32 = ctx.r9.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r16,16
	r16.s64 = 16;
	// vor v9,v10,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvlx v10,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v11,r7,r4
	temp.u32 = ctx.r7.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r7,r11,-32
	ctx.r7.s64 = r11.s64 + -32;
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// addi r4,r11,16
	ctx.r4.s64 = r11.s64 + 16;
	// mr r21,r7
	r21.u64 = ctx.r7.u64;
	// vmrghb v10,v13,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// vmrglb v9,v13,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// vmrghb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// vmrglb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v6,v10,v10
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v5,v9,v9
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v4,v8,v8
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v3,v7,v7
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v2,v6,v10
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v1,v5,v9
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v31,v4,v8
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v6,v4,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v4,v3,v7
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v5,v3,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v10,v31,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v8,v8,v2
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v9,v4,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v7,v7,v1
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsrah v10,v10,v12
	// vsrah v6,v6,v12
	// vsrah v9,v9,v12
	// vsrah v5,v5,v12
	// vsrah v7,v7,v12
	// vsrah v8,v8,v12
	// vpkshus v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vpkshus v9,v6,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vpkshus v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vmrghb v7,v11,v9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrglb v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrghb v9,v10,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vmrglb v10,v10,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vmrghb v8,v7,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrglb v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v7,v11,v10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrglb v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stvlx v8,0,r7
	ea = ctx.r7.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// addi r7,r11,-16
	ctx.r7.s64 = r11.s64 + -16;
	// stvrx v8,r21,r26
	ea = r21.u32 + r26.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// mr r26,r11
	r26.u64 = r11.u64;
	// li r21,16
	r21.s64 = 16;
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stvlx v9,0,r7
	ea = ctx.r7.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// stvrx v9,r7,r18
	ea = ctx.r7.u32 + r18.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// stvlx v7,0,r17
	ea = r17.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v7.u8[15 - i]);
	// stvrx v7,r26,r21
	ea = r26.u32 + r21.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v7.u8[i]);
	// stvlx v11,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// stvrx v11,r4,r16
	ea = ctx.r4.u32 + r16.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// bne cr6,0x82676860
	if (!cr6.eq) goto loc_82676860;
loc_82676968:
	// cmpw cr6,r27,r6
	cr6.compare<int32_t>(r27.s32, ctx.r6.s32, xer);
	// bge cr6,0x826769f0
	if (!cr6.lt) goto loc_826769F0;
	// subf r9,r27,r6
	ctx.r9.s64 = ctx.r6.s64 - r27.s64;
	// add r11,r27,r5
	r11.u64 = r27.u64 + ctx.r5.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// add r4,r29,r30
	ctx.r4.u64 = r29.u64 + r30.u64;
	// addi r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 1;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r27,r9,r27
	r27.u64 = ctx.r9.u64 + r27.u64;
loc_82676994:
	// lbzx r29,r3,r30
	r29.u64 = PPC_LOAD_U8(ctx.r3.u32 + r30.u32);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lbz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// rlwinm r26,r9,1,0,30
	r26.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stb r29,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, r29.u8);
	// rotlwi r29,r8,1
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// add r26,r9,r26
	r26.u64 = ctx.r9.u64 + r26.u64;
	// add r29,r8,r29
	r29.u64 = ctx.r8.u64 + r29.u64;
	// add r26,r26,r8
	r26.u64 = r26.u64 + ctx.r8.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r9,r29,r9
	ctx.r9.u64 = r29.u64 + ctx.r9.u64;
	// srawi r29,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r29.s64 = r26.s32 >> 2;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stb r29,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r29.u8);
	// stb r8,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r8.u8);
	// stb r9,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r9.u8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x82676994
	if (!cr6.eq) goto loc_82676994;
loc_826769F0:
	// lbzx r9,r3,r30
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + r30.u32);
	// add r11,r27,r5
	r11.u64 = r27.u64 + ctx.r5.u64;
	// add r30,r30,r22
	r30.u64 = r30.u64 + r22.u64;
	// lwz r8,948(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// stb r9,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r9.u8);
	// stb r9,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r9.u8);
	// stb r9,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r9.u8);
	// bge cr6,0x826776fc
	if (!cr6.lt) goto loc_826776FC;
	// subf r11,r8,r10
	r11.s64 = ctx.r10.s64 - ctx.r8.s64;
	// subf r9,r20,r25
	ctx.r9.s64 = r25.s64 - r20.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
loc_82676A3C:
	// rlwinm r11,r6,0,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFC0;
	// li r21,0
	r21.s64 = 0;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826772c8
	if (!cr6.gt) goto loc_826772C8;
	// add r3,r9,r20
	ctx.r3.u64 = ctx.r9.u64 + r20.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// subf r9,r24,r3
	ctx.r9.s64 = ctx.r3.s64 - r24.s64;
	// rlwinm r11,r11,26,6,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// addi r5,r30,1
	ctx.r5.s64 = r30.s64 + 1;
	// stw r9,76(r1)
	PPC_STORE_U32(ctx.r1.u32 + 76, ctx.r9.u32);
	// subf r9,r31,r3
	ctx.r9.s64 = ctx.r3.s64 - r31.s64;
	// rlwinm r21,r4,4,0,27
	r21.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r8,r19,16
	ctx.r8.s64 = r19.s64 + 16;
	// addi r10,r31,48
	ctx.r10.s64 = r31.s64 + 48;
	// addi r11,r24,32
	r11.s64 = r24.s64 + 32;
	// stw r9,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r9.u32);
	// subf r9,r19,r3
	ctx.r9.s64 = ctx.r3.s64 - r19.s64;
	// subf r25,r24,r31
	r25.s64 = r31.s64 - r24.s64;
	// stw r9,48(r1)
	PPC_STORE_U32(ctx.r1.u32 + 48, ctx.r9.u32);
	// subf r9,r20,r3
	ctx.r9.s64 = ctx.r3.s64 - r20.s64;
	// rlwinm r3,r4,6,0,25
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 6) & 0xFFFFFFC0;
	// stw r9,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r9.u32);
	// addi r9,r21,1
	ctx.r9.s64 = r21.s64 + 1;
	// stw r9,24(r1)
	PPC_STORE_U32(ctx.r1.u32 + 24, ctx.r9.u32);
loc_82676AAC:
	// li r6,16
	ctx.r6.s64 = 16;
	// lvlx v10,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r18,16
	r18.s64 = 16;
	// addi r29,r5,-1
	r29.s64 = ctx.r5.s64 + -1;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r28,r11,-32
	r28.s64 = r11.s64 + -32;
	// lvrx v11,r5,r6
	temp.u32 = ctx.r5.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r27,r11,-16
	r27.s64 = r11.s64 + -16;
	// vor v9,v10,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v11,r11,r18
	temp.u32 = r11.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r22,16
	r22.s64 = 16;
	// vor v29,v10,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvlx v10,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v11,r29,r9
	temp.u32 = r29.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r26,r11,16
	r26.s64 = r11.s64 + 16;
	// li r17,16
	r17.s64 = 16;
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r28,r6
	temp.u32 = r28.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r29,r10,-48
	r29.s64 = ctx.r10.s64 + -48;
	// lvlx v8,0,r28
	temp.u32 = r28.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r28,r10,-32
	r28.s64 = ctx.r10.s64 + -32;
	// vor v2,v8,v10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v7,r27,r22
	temp.u32 = r27.u32 + r22.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r27
	temp.u32 = r27.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r27,r25,r11
	r27.u64 = r25.u64 + r11.u64;
	// lvrx v8,r26,r17
	temp.u32 = r26.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v31,v10,v7
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v6,0,r26
	temp.u32 = r26.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v10,v13,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v27,v6,v8
	_mm_store_si128((__m128i*)v27.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrglb v9,v13,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r26,16
	r26.s64 = 16;
	// vmrglb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v6,v10,v10
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v5,v9,v9
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v4,v8,v8
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v3,v7,v7
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v28,v6,v10
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v26,v5,v9
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v1,v4,v8
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v30,v3,v7
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v6,v4,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v5,v3,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v10,v1,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v7,v7,v26
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v9,v30,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v8,v8,v28
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vsrah v6,v6,v12
	// vsrah v5,v5,v12
	// vsrah v4,v10,v12
	// vsrah v3,v8,v12
	// vsrah v7,v7,v12
	// vsrah v9,v9,v12
	// vpkshus v10,v6,v5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vpkshus v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vpkshus v9,v4,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vmrglb v6,v11,v10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrghb v8,v11,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrghb v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrglb v7,v9,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v11,v8,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrglb v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v9,v6,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrglb v8,v6,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrglb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v5,v13,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvlx v11,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// vmrglb v4,v13,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvrx v11,r29,r9
	ea = r29.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// vmrghb v3,v13,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvlx v10,0,r28
	ea = r28.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// vmrglb v1,v13,v9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvrx v10,r28,r6
	ea = r28.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// vor v11,v7,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// stvlx v9,0,r27
	ea = r27.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// vor v10,v6,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// stvrx v9,r27,r26
	ea = r27.u32 + r26.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// stvlx v8,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// vor v6,v3,v3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// addi r9,r1,688
	ctx.r9.s64 = ctx.r1.s64 + 688;
	// vor v9,v5,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// stvrx v8,r10,r22
	ea = ctx.r10.u32 + r22.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// vor v5,v1,v1
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vmrghb v3,v13,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v4,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmrghb v4,v13,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v22,v6,v6
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vmrghb v1,v13,v31
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v30,v13,v29
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r12,32
	r12.s64 = 32;
	// stvx128 v13,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v21,v5,v5
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vmrghb v28,v13,v27
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v20,v4,v4
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v18,v22,v6
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vmrglb v2,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v31,v13,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v26,v11,v11
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vmrglb v29,v13,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v25,v10,v10
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vmrglb v27,v13,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v13,v21,v5
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvx v18,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,272
	ctx.r9.s64 = ctx.r1.s64 + 272;
	// vaddshs v19,v8,v8
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v17,v26,v11
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v16,v25,v10
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v24,v9,v9
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// vaddshs v13,v20,v4
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v23,v7,v7
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v15,v24,v9
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v18,v22,v18
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v18.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,448
	ctx.r9.s64 = ctx.r1.s64 + 448;
	// vaddshs v13,v19,v8
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v14,v23,v7
	_mm_store_si128((__m128i*)v14.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,512
	ctx.r9.s64 = ctx.r1.s64 + 512;
	// vaddshs v13,v26,v17
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v17.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,384
	ctx.r9.s64 = ctx.r1.s64 + 384;
	// vaddshs v13,v25,v16
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,416
	ctx.r9.s64 = ctx.r1.s64 + 416;
	// vaddshs v13,v24,v15
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v15.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,304
	ctx.r9.s64 = ctx.r1.s64 + 304;
	// vaddshs v13,v23,v14
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v14.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,672
	ctx.r9.s64 = ctx.r1.s64 + 672;
	// stvx v18,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,272
	ctx.r9.s64 = ctx.r1.s64 + 272;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,224
	ctx.r9.s64 = ctx.r1.s64 + 224;
	// vaddshs v13,v21,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,288
	ctx.r9.s64 = ctx.r1.s64 + 288;
	// vaddshs v13,v20,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,448
	ctx.r9.s64 = ctx.r1.s64 + 448;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,528
	ctx.r9.s64 = ctx.r1.s64 + 528;
	// vaddshs v13,v19,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,512
	ctx.r9.s64 = ctx.r1.s64 + 512;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,608
	ctx.r9.s64 = ctx.r1.s64 + 608;
	// vaddshs v26,v26,v13
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v26,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,384
	ctx.r9.s64 = ctx.r1.s64 + 384;
	// lvx128 v26,r0,r9
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,624
	ctx.r9.s64 = ctx.r1.s64 + 624;
	// vaddshs v26,v25,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v26.s16)));
	// stvx v26,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,416
	ctx.r9.s64 = ctx.r1.s64 + 416;
	// lvx128 v26,r0,r9
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,656
	ctx.r9.s64 = ctx.r1.s64 + 656;
	// vaddshs v26,v24,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v25,v2,v2
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v24,v1,v1
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// stvx v26,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,304
	ctx.r9.s64 = ctx.r1.s64 + 304;
	// lvx128 v26,r0,r9
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,592
	ctx.r9.s64 = ctx.r1.s64 + 592;
	// vaddshs v26,v23,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v23,v31,v31
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// stvx v26,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,640
	ctx.r9.s64 = ctx.r1.s64 + 640;
	// vaddshs v26,v22,v18
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vaddshs v22,v30,v30
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v18,v27,v27
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// stvx v26,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,224
	ctx.r9.s64 = ctx.r1.s64 + 224;
	// lvx128 v26,r0,r9
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// vaddshs v26,v21,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v21,v29,v29
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// stvx v26,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,288
	ctx.r9.s64 = ctx.r1.s64 + 288;
	// lvx128 v26,r0,r9
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// vaddshs v26,v20,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v20,v28,v28
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// stvx v26,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,528
	ctx.r9.s64 = ctx.r1.s64 + 528;
	// vaddshs v26,v3,v3
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// vaddshs v19,v19,v13
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v13,v25,v2
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// stvx v19,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,480
	ctx.r9.s64 = ctx.r1.s64 + 480;
	// vaddshs v19,v26,v3
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,544
	ctx.r9.s64 = ctx.r1.s64 + 544;
	// vaddshs v13,v24,v1
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,336
	ctx.r9.s64 = ctx.r1.s64 + 336;
	// vaddshs v13,v23,v31
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v31.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,192
	ctx.r9.s64 = ctx.r1.s64 + 192;
	// vaddshs v13,v22,v30
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v30.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,256
	ctx.r9.s64 = ctx.r1.s64 + 256;
	// vaddshs v13,v21,v29
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)v29.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,496
	ctx.r9.s64 = ctx.r1.s64 + 496;
	// vaddshs v13,v20,v28
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v28.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,560
	ctx.r9.s64 = ctx.r1.s64 + 560;
	// vaddshs v13,v18,v27
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)v27.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,320
	ctx.r9.s64 = ctx.r1.s64 + 320;
	// vaddshs v13,v26,v19
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v19.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,480
	ctx.r9.s64 = ctx.r1.s64 + 480;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,208
	ctx.r9.s64 = ctx.r1.s64 + 208;
	// vaddshs v13,v25,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,544
	ctx.r9.s64 = ctx.r1.s64 + 544;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,368
	ctx.r9.s64 = ctx.r1.s64 + 368;
	// vaddshs v13,v24,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,336
	ctx.r9.s64 = ctx.r1.s64 + 336;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,240
	ctx.r9.s64 = ctx.r1.s64 + 240;
	// vaddshs v13,v23,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,192
	ctx.r9.s64 = ctx.r1.s64 + 192;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,400
	ctx.r9.s64 = ctx.r1.s64 + 400;
	// vaddshs v13,v22,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,256
	ctx.r9.s64 = ctx.r1.s64 + 256;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,432
	ctx.r9.s64 = ctx.r1.s64 + 432;
	// vaddshs v13,v21,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,496
	ctx.r9.s64 = ctx.r1.s64 + 496;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,464
	ctx.r9.s64 = ctx.r1.s64 + 464;
	// vaddshs v13,v20,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,560
	ctx.r9.s64 = ctx.r1.s64 + 560;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,352
	ctx.r9.s64 = ctx.r1.s64 + 352;
	// vaddshs v13,v18,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,320
	ctx.r9.s64 = ctx.r1.s64 + 320;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,208
	ctx.r9.s64 = ctx.r1.s64 + 208;
	// vaddshs v26,v26,v13
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,368
	ctx.r9.s64 = ctx.r1.s64 + 368;
	// vaddshs v25,v25,v13
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v11,v26,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,240
	ctx.r9.s64 = ctx.r1.s64 + 240;
	// vaddshs v24,v24,v13
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v10,v25,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vsrah v11,v11,v0
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,400
	ctx.r9.s64 = ctx.r1.s64 + 400;
	// vaddshs v23,v23,v13
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v9,v24,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vsrah v10,v10,v0
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v23,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v22,v22,v13
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vsrah v9,v9,v0
	// vpkshus v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v7,v7,v0
	// addi r9,r1,432
	ctx.r9.s64 = ctx.r1.s64 + 432;
	// vaddshs v6,v22,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vpkshus v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vsrah v6,v6,v0
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v21,v21,v13
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// addi r9,r1,464
	ctx.r9.s64 = ctx.r1.s64 + 464;
	// vaddshs v5,v21,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsrah v5,v5,v0
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v20,v20,v13
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vpkshus v9,v6,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// addi r9,r1,352
	ctx.r9.s64 = ctx.r1.s64 + 352;
	// vaddshs v4,v20,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsrah v4,v4,v0
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// subf r9,r20,r23
	ctx.r9.s64 = r23.s64 - r20.s64;
	// vaddshs v18,v18,v13
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// add r29,r9,r7
	r29.u64 = ctx.r9.u64 + ctx.r7.u64;
	// subf r9,r19,r23
	ctx.r9.s64 = r23.s64 - r19.s64;
	// add r28,r9,r8
	r28.u64 = ctx.r9.u64 + ctx.r8.u64;
	// vaddshs v8,v18,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// subf r9,r24,r23
	ctx.r9.s64 = r23.s64 - r24.s64;
	// stvlx v11,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// add r27,r9,r11
	r27.u64 = ctx.r9.u64 + r11.u64;
	// subf r9,r31,r23
	ctx.r9.s64 = r23.s64 - r31.s64;
	// vsrah v8,v8,v0
	// add r26,r9,r10
	r26.u64 = ctx.r9.u64 + ctx.r10.u64;
	// li r9,16
	ctx.r9.s64 = 16;
	// vpkshus v8,v4,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// stvrx v11,r29,r9
	ea = r29.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// addi r9,r1,320
	ctx.r9.s64 = ctx.r1.s64 + 320;
	// stvlx v10,0,r28
	ea = r28.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// stvrx v10,r28,r6
	ea = r28.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// stvlx v9,0,r27
	ea = r27.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// stvrx v9,r27,r22
	ea = r27.u32 + r22.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// stvlx v8,0,r26
	ea = r26.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// stvrx v8,r26,r18
	ea = r26.u32 + r18.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,208
	ctx.r9.s64 = ctx.r1.s64 + 208;
	// vaddshs v11,v11,v17
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v17.s16)));
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v10,v10,v16
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)v16.s16)));
	// addi r9,r1,368
	ctx.r9.s64 = ctx.r1.s64 + 368;
	// vsrah v10,v10,v0
	// vsrah v11,v11,v0
	// addi r18,r1,384
	r18.s64 = ctx.r1.s64 + 384;
	// addi r22,r1,512
	r22.s64 = ctx.r1.s64 + 512;
	// addi r29,r8,-16
	r29.s64 = ctx.r8.s64 + -16;
	// li r26,16
	r26.s64 = 16;
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,240
	ctx.r9.s64 = ctx.r1.s64 + 240;
	// vaddshs v9,v9,v15
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v15.s16)));
	// vpkshus v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,688
	ctx.r9.s64 = ctx.r1.s64 + 688;
	// vaddshs v8,v8,v14
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v14.s16)));
	// vsrah v9,v9,v0
	// lvx128 v7,r0,r9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,400
	ctx.r9.s64 = ctx.r1.s64 + 400;
	// vsrah v8,v8,v0
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,272
	ctx.r9.s64 = ctx.r1.s64 + 272;
	// vaddshs v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vpkshus v10,v9,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,432
	ctx.r9.s64 = ctx.r1.s64 + 432;
	// vsrah v7,v7,v0
	// vor v25,v10,v10
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v20,v10,v10
	_mm_store_si128((__m128i*)v20.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// vaddshs v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,464
	ctx.r9.s64 = ctx.r1.s64 + 464;
	// vsrah v6,v6,v0
	// lvx128 v4,r0,r9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,448
	ctx.r9.s64 = ctx.r1.s64 + 448;
	// vaddshs v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vpkshus v9,v7,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// lvx128 v6,r0,r18
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,480
	r18.s64 = ctx.r1.s64 + 480;
	// lvx128 v7,r0,r22
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r22,16
	r22.s64 = 16;
	// vaddshs v7,v19,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// lvx128 v4,r0,r9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,352
	ctx.r9.s64 = ctx.r1.s64 + 352;
	// vsrah v5,v5,v0
	// vor v19,v9,v9
	_mm_store_si128((__m128i*)v19.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v18,v9,v9
	_mm_store_si128((__m128i*)v18.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// lvx128 v26,r0,r9
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// subf r9,r24,r19
	ctx.r9.s64 = r19.s64 - r24.s64;
	// vaddshs v4,v26,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// add r28,r9,r11
	r28.u64 = ctx.r9.u64 + r11.u64;
	// subf r9,r31,r19
	ctx.r9.s64 = r19.s64 - r31.s64;
	// vsrah v4,v4,v0
	// add r27,r9,r10
	r27.u64 = ctx.r9.u64 + ctx.r10.u64;
	// li r9,16
	ctx.r9.s64 = 16;
	// vpkshus v8,v5,v4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// lvx128 v5,r0,r18
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,416
	r18.s64 = ctx.r1.s64 + 416;
	// vaddshs v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vor v17,v8,v8
	_mm_store_si128((__m128i*)v17.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v16,v8,v8
	_mm_store_si128((__m128i*)v16.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// lvx128 v5,r0,r18
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,544
	r18.s64 = ctx.r1.s64 + 544;
	// lvx128 v4,r0,r18
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,304
	r18.s64 = ctx.r1.s64 + 304;
	// vaddshs v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// lvx128 v4,r0,r18
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,336
	r18.s64 = ctx.r1.s64 + 336;
	// lvx128 v26,r0,r18
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,672
	r18.s64 = ctx.r1.s64 + 672;
	// vaddshs v4,v26,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// lvx128 v26,r0,r18
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,192
	r18.s64 = ctx.r1.s64 + 192;
	// lvx128 v24,r0,r18
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,224
	r18.s64 = ctx.r1.s64 + 224;
	// vaddshs v26,v24,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v26.s16)));
	// lvx128 v10,r0,r18
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,256
	r18.s64 = ctx.r1.s64 + 256;
	// lvx128 v9,r0,r18
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,288
	r18.s64 = ctx.r1.s64 + 288;
	// vaddshs v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// lvx128 v9,r0,r18
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,496
	r18.s64 = ctx.r1.s64 + 496;
	// lvx128 v8,r0,r18
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,528
	r18.s64 = ctx.r1.s64 + 528;
	// vaddshs v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// lvx128 v8,r0,r18
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,560
	r18.s64 = ctx.r1.s64 + 560;
	// lvx128 v15,r0,r18
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,608
	r18.s64 = ctx.r1.s64 + 608;
	// stvlx v11,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// vaddshs v8,v15,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvrx v11,r29,r9
	ea = r29.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// vsrah v11,v7,v0
	// stvlx v25,0,r8
	ea = ctx.r8.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v25.u8[15 - i]);
	// vsrah v25,v9,v0
	// stvrx v20,r8,r6
	ea = ctx.r8.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v20.u8[i]);
	// vsrah v7,v6,v0
	// stvlx v19,0,r28
	ea = r28.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v19.u8[15 - i]);
	// vsrah v6,v5,v0
	// stvrx v18,r28,r26
	ea = r28.u32 + r26.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v18.u8[i]);
	// vsrah v5,v4,v0
	// stvlx v17,0,r27
	ea = r27.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v17.u8[15 - i]);
	// vsrah v4,v26,v0
	// stvrx v16,r27,r22
	ea = r27.u32 + r22.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v16.u8[i]);
	// vpkshus v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// lvx128 v9,r0,r18
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,624
	r18.s64 = ctx.r1.s64 + 624;
	// vaddshs v7,v3,v9
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// subf r9,r19,r20
	ctx.r9.s64 = r20.s64 - r19.s64;
	// vsrah v26,v10,v0
	// vpkshus v10,v6,v5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// add r29,r9,r8
	r29.u64 = ctx.r9.u64 + ctx.r8.u64;
	// vsrah v8,v8,v0
	// subf r9,r24,r20
	ctx.r9.s64 = r20.s64 - r24.s64;
	// lvx128 v9,r0,r18
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,656
	r18.s64 = ctx.r1.s64 + 656;
	// vaddshs v6,v2,v9
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// add r28,r9,r11
	r28.u64 = ctx.r9.u64 + r11.u64;
	// subf r9,r31,r20
	ctx.r9.s64 = r20.s64 - r31.s64;
	// vpkshus v8,v25,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v25.s16)));
	// add r27,r9,r10
	r27.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lvx128 v9,r0,r18
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,592
	r18.s64 = ctx.r1.s64 + 592;
	// vaddshs v5,v1,v9
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// li r9,16
	ctx.r9.s64 = 16;
	// lvx128 v9,r0,r18
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r18,r1,640
	r18.s64 = ctx.r1.s64 + 640;
	// vaddshs v3,v31,v9
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// lvx128 v9,r0,r18
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v30,v9
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vpkshus v9,v4,v26
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// stvlx v11,0,r7
	ea = ctx.r7.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// stvrx v11,r7,r9
	ea = ctx.r7.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// lwz r9,60(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// stvlx v10,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// stvrx v10,r29,r6
	ea = r29.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// add r29,r7,r9
	r29.u64 = ctx.r7.u64 + ctx.r9.u64;
	// lwz r9,48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 48);
	// vsrah v4,v2,v0
	// stvlx v9,0,r28
	ea = r28.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// stvrx v9,r28,r26
	ea = r28.u32 + r26.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// addi r26,r1,96
	r26.s64 = ctx.r1.s64 + 96;
	// stvlx v8,0,r27
	ea = r27.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// add r28,r8,r9
	r28.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stvrx v8,r27,r22
	ea = r27.u32 + r22.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// lwz r9,76(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 76);
	// vsrah v8,v7,v0
	// vsrah v7,v6,v0
	// add r27,r11,r9
	r27.u64 = r11.u64 + ctx.r9.u64;
	// lvx128 v11,r0,r26
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r26,r1,128
	r26.s64 = ctx.r1.s64 + 128;
	// vaddshs v11,v29,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// li r9,16
	ctx.r9.s64 = 16;
	// vsrah v6,v5,v0
	// vsrah v5,v3,v0
	// lvx128 v10,r0,r26
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r26,r1,160
	r26.s64 = ctx.r1.s64 + 160;
	// vaddshs v10,v28,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vsrah v3,v11,v0
	// lvx128 v9,r0,r26
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v9,v27,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vpkshus v11,v8,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsrah v2,v10,v0
	// vsrah v1,v9,v0
	// vpkshus v10,v6,v5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vpkshus v9,v4,v3
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// li r12,32
	r12.s64 = 32;
	// lvx128 v13,r1,r12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// addi r7,r7,64
	ctx.r7.s64 = ctx.r7.s64 + 64;
	// vpkshus v8,v2,v1
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// addi r8,r8,64
	ctx.r8.s64 = ctx.r8.s64 + 64;
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stvlx v11,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stvrx v11,r29,r9
	ea = r29.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// lwz r9,52(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// vor v11,v8,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// stvlx v10,0,r28
	ea = r28.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// add r29,r10,r9
	r29.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stvrx v10,r28,r6
	ea = r28.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// li r9,16
	ctx.r9.s64 = 16;
	// stvlx v9,0,r27
	ea = r27.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// stvrx v9,r27,r9
	ea = r27.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// stvlx v11,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// stvrx v11,r29,r6
	ea = r29.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// bne cr6,0x82676aac
	if (!cr6.eq) goto loc_82676AAC;
	// lwz r28,92(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r6,908(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	// lwz r7,24(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_826772C8:
	// cmpw cr6,r3,r6
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r6.s32, xer);
	// bge cr6,0x826775e8
	if (!cr6.lt) goto loc_826775E8;
	// subf r11,r3,r6
	r11.s64 = ctx.r6.s64 - ctx.r3.s64;
	// add r10,r9,r20
	ctx.r10.u64 = ctx.r9.u64 + r20.u64;
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// add r11,r3,r23
	r11.u64 = ctx.r3.u64 + r23.u64;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r28,r11,3
	r28.s64 = r11.s64 + 3;
	// addi r26,r9,1
	r26.s64 = ctx.r9.s64 + 1;
	// add r9,r3,r24
	ctx.r9.u64 = ctx.r3.u64 + r24.u64;
	// add r8,r3,r31
	ctx.r8.u64 = ctx.r3.u64 + r31.u64;
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// subf r9,r24,r31
	ctx.r9.s64 = r31.s64 - r24.s64;
	// add r27,r3,r20
	r27.u64 = ctx.r3.u64 + r20.u64;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// add r25,r7,r30
	r25.u64 = ctx.r7.u64 + r30.u64;
	// stw r9,584(r1)
	PPC_STORE_U32(ctx.r1.u32 + 584, ctx.r9.u32);
	// subf r9,r24,r23
	ctx.r9.s64 = r23.s64 - r24.s64;
	// stw r9,576(r1)
	PPC_STORE_U32(ctx.r1.u32 + 576, ctx.r9.u32);
	// subf r9,r24,r19
	ctx.r9.s64 = r19.s64 - r24.s64;
	// stw r9,580(r1)
	PPC_STORE_U32(ctx.r1.u32 + 580, ctx.r9.u32);
	// subf r9,r31,r23
	ctx.r9.s64 = r23.s64 - r31.s64;
	// stw r9,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, ctx.r9.u32);
	// subf r9,r31,r19
	ctx.r9.s64 = r19.s64 - r31.s64;
	// stw r9,48(r1)
	PPC_STORE_U32(ctx.r1.u32 + 48, ctx.r9.u32);
	// subf r9,r23,r19
	ctx.r9.s64 = r19.s64 - r23.s64;
	// stw r9,588(r1)
	PPC_STORE_U32(ctx.r1.u32 + 588, ctx.r9.u32);
	// subf r9,r24,r10
	ctx.r9.s64 = ctx.r10.s64 - r24.s64;
	// stw r9,76(r1)
	PPC_STORE_U32(ctx.r1.u32 + 76, ctx.r9.u32);
	// subf r9,r31,r10
	ctx.r9.s64 = ctx.r10.s64 - r31.s64;
	// stw r9,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r9.u32);
	// subf r9,r23,r10
	ctx.r9.s64 = ctx.r10.s64 - r23.s64;
	// subf r10,r20,r10
	ctx.r10.s64 = ctx.r10.s64 - r20.s64;
	// stw r9,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r9.u32);
	// stw r10,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r10.u32);
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// stw r10,24(r1)
	PPC_STORE_U32(ctx.r1.u32 + 24, ctx.r10.u32);
loc_82677360:
	// lbzx r30,r21,r30
	r30.u64 = PPC_LOAD_U8(r21.u32 + r30.u32);
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// lbz r7,-1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// lbz r31,0(r25)
	r31.u64 = PPC_LOAD_U8(r25.u32 + 0);
	// lbz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r29,r7,2
	r29.u64 = __builtin_rotateleft32(ctx.r7.u32, 2);
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r4,1(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r3,2(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// rotlwi r6,r5,3
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r5.u32, 3);
	// stb r30,-2(r8)
	PPC_STORE_U8(ctx.r8.u32 + -2, r30.u8);
	// add r30,r10,r9
	r30.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rotlwi r9,r31,1
	ctx.r9.u64 = __builtin_rotateleft32(r31.u32, 1);
	// lwz r15,584(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	// rotlwi r24,r4,3
	r24.u64 = __builtin_rotateleft32(ctx.r4.u32, 3);
	// add r9,r31,r9
	ctx.r9.u64 = r31.u64 + ctx.r9.u64;
	// add r23,r31,r10
	r23.u64 = r31.u64 + ctx.r10.u64;
	// add r22,r9,r10
	r22.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rotlwi r9,r7,3
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r7.u32, 3);
	// subf r18,r5,r6
	r18.s64 = ctx.r6.s64 - ctx.r5.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// rotlwi r17,r3,3
	r17.u64 = __builtin_rotateleft32(ctx.r3.u32, 3);
	// subf r6,r4,r24
	ctx.r6.s64 = r24.s64 - ctx.r4.s64;
	// add r29,r7,r29
	r29.u64 = ctx.r7.u64 + r29.u64;
	// add r16,r9,r10
	r16.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rotlwi r24,r5,2
	r24.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// subf r9,r3,r17
	ctx.r9.s64 = r17.s64 - ctx.r3.s64;
	// srawi r31,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	r31.s64 = r31.s32 >> 2;
	// add r17,r29,r30
	r17.u64 = r29.u64 + r30.u64;
	// add r14,r5,r24
	r14.u64 = ctx.r5.u64 + r24.u64;
	// srawi r30,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r30.s64 = r23.s32 >> 1;
	// srawi r29,r22,2
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3) != 0);
	r29.s64 = r22.s32 >> 2;
	// clrlwi r24,r31,16
	r24.u64 = r31.u32 & 0xFFFF;
	// clrlwi r23,r30,16
	r23.u64 = r30.u32 & 0xFFFF;
	// clrlwi r22,r29,16
	r22.u64 = r29.u32 & 0xFFFF;
	// mr r31,r24
	r31.u64 = r24.u64;
	// mr r30,r23
	r30.u64 = r23.u64;
	// stbx r24,r11,r15
	PPC_STORE_U8(r11.u32 + r15.u32, r24.u8);
	// mr r29,r22
	r29.u64 = r22.u64;
	// add r24,r18,r31
	r24.u64 = r18.u64 + r31.u64;
	// stb r23,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r23.u8);
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// stb r22,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, r22.u8);
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// lwz r22,576(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	// srawi r16,r16,3
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x7) != 0);
	r16.s64 = r16.s32 >> 3;
	// srawi r24,r24,3
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7) != 0);
	r24.s64 = r24.s32 >> 3;
	// srawi r6,r6,3
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 3;
	// srawi r18,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	r18.s64 = ctx.r9.s32 >> 3;
	// rlwinm r9,r31,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r23,r17,3
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x7) != 0);
	r23.s64 = r17.s32 >> 3;
	// stb r16,-3(r28)
	PPC_STORE_U8(r28.u32 + -3, r16.u8);
	// stbx r24,r11,r22
	PPC_STORE_U8(r11.u32 + r22.u32, r24.u8);
	// add r22,r31,r9
	r22.u64 = r31.u64 + ctx.r9.u64;
	// lwz r24,68(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// subf r9,r20,r19
	ctx.r9.s64 = r19.s64 - r20.s64;
	// stb r18,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r18.u8);
	// rotlwi r18,r3,2
	r18.u64 = __builtin_rotateleft32(ctx.r3.u32, 2);
	// rlwinm r19,r29,1,0,30
	r19.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// add r17,r3,r18
	r17.u64 = ctx.r3.u64 + r18.u64;
	// rlwinm r18,r30,2,0,29
	r18.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// stbx r6,r8,r24
	PPC_STORE_U8(ctx.r8.u32 + r24.u32, ctx.r6.u8);
	// rlwinm r6,r30,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// stbx r23,r27,r9
	PPC_STORE_U8(r27.u32 + ctx.r9.u32, r23.u8);
	// rlwinm r24,r10,2,0,29
	r24.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r9,r4,2
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// rotlwi r23,r7,1
	r23.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// add r14,r22,r14
	r14.u64 = r22.u64 + r14.u64;
	// stw r18,64(r1)
	PPC_STORE_U32(ctx.r1.u32 + 64, r18.u32);
	// add r16,r30,r6
	r16.u64 = r30.u64 + ctx.r6.u64;
	// add r15,r4,r9
	r15.u64 = ctx.r4.u64 + ctx.r9.u64;
	// add r22,r10,r24
	r22.u64 = ctx.r10.u64 + r24.u64;
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// rlwinm r9,r31,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r6,r5,1
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r5.u32, 1);
	// add r19,r29,r19
	r19.u64 = r29.u64 + r19.u64;
	// rotlwi r18,r4,1
	r18.u64 = __builtin_rotateleft32(ctx.r4.u32, 1);
	// add r23,r7,r23
	r23.u64 = ctx.r7.u64 + r23.u64;
	// rlwinm r24,r29,2,0,29
	r24.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r24,72(r1)
	PPC_STORE_U32(ctx.r1.u32 + 72, r24.u32);
	// add r18,r4,r18
	r18.u64 = ctx.r4.u64 + r18.u64;
	// add r17,r19,r17
	r17.u64 = r19.u64 + r17.u64;
	// lwz r19,64(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 64);
	// add r23,r23,r22
	r23.u64 = r23.u64 + r22.u64;
	// rotlwi r24,r3,1
	r24.u64 = __builtin_rotateleft32(ctx.r3.u32, 1);
	// add r16,r16,r15
	r16.u64 = r16.u64 + r15.u64;
	// stw r18,64(r1)
	PPC_STORE_U32(ctx.r1.u32 + 64, r18.u32);
	// rlwinm r15,r10,3,0,28
	r15.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r14,r14,3
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x7) != 0);
	r14.s64 = r14.s32 >> 3;
	// add r19,r30,r19
	r19.u64 = r30.u64 + r19.u64;
	// srawi r16,r16,3
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x7) != 0);
	r16.s64 = r16.s32 >> 3;
	// rlwinm r18,r31,3,0,28
	r18.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r31,r9
	ctx.r9.u64 = r31.u64 + ctx.r9.u64;
	// add r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 + ctx.r6.u64;
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// srawi r6,r17,3
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x7) != 0);
	ctx.r6.s64 = r17.s32 >> 3;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// lwz r22,72(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 72);
	// stw r23,72(r1)
	PPC_STORE_U32(ctx.r1.u32 + 72, r23.u32);
	// add r23,r29,r22
	r23.u64 = r29.u64 + r22.u64;
	// add r22,r3,r24
	r22.u64 = ctx.r3.u64 + r24.u64;
	// subf r24,r10,r15
	r24.s64 = r15.s64 - ctx.r10.s64;
	// lwz r10,64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 64);
	// add r23,r23,r22
	r23.u64 = r23.u64 + r22.u64;
	// add r7,r24,r7
	ctx.r7.u64 = r24.u64 + ctx.r7.u64;
	// lwz r24,580(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	// add r19,r19,r10
	r19.u64 = r19.u64 + ctx.r10.u64;
	// subf r10,r31,r18
	ctx.r10.s64 = r18.s64 - r31.s64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// stbx r14,r11,r24
	PPC_STORE_U8(r11.u32 + r24.u32, r14.u8);
	// lwz r24,48(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 48);
	// stbx r16,r8,r24
	PPC_STORE_U8(ctx.r8.u32 + r24.u32, r16.u8);
	// lwz r24,588(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	// stbx r6,r28,r24
	PPC_STORE_U8(r28.u32 + r24.u32, ctx.r6.u8);
	// lwz r24,112(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r18,72(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 72);
	// srawi r31,r18,3
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x7) != 0);
	r31.s64 = r18.s32 >> 3;
	// srawi r9,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 3;
	// srawi r19,r19,3
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x7) != 0);
	r19.s64 = r19.s32 >> 3;
	// srawi r5,r23,3
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7) != 0);
	ctx.r5.s64 = r23.s32 >> 3;
	// lwz r23,84(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// stb r31,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r31.u8);
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// subf r10,r24,r20
	ctx.r10.s64 = r20.s64 - r24.s64;
	// stbx r9,r11,r10
	PPC_STORE_U8(r11.u32 + ctx.r10.u32, ctx.r9.u8);
	// subf r10,r31,r20
	ctx.r10.s64 = r20.s64 - r31.s64;
	// rlwinm r9,r30,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// stbx r19,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, r19.u8);
	// subf r10,r23,r20
	ctx.r10.s64 = r20.s64 - r23.s64;
	// lwz r19,56(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 56);
	// stbx r5,r28,r10
	PPC_STORE_U8(r28.u32 + ctx.r10.u32, ctx.r5.u8);
	// lwz r10,60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// stbx r7,r27,r10
	PPC_STORE_U8(r27.u32 + ctx.r10.u32, ctx.r7.u8);
	// subf r10,r30,r9
	ctx.r10.s64 = ctx.r9.s64 - r30.s64;
	// lwz r9,76(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 76);
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// add r7,r10,r4
	ctx.r7.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stbx r6,r11,r9
	PPC_STORE_U8(r11.u32 + ctx.r9.u32, ctx.r6.u8);
	// rlwinm r9,r29,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// subf r10,r29,r9
	ctx.r10.s64 = ctx.r9.s64 - r29.s64;
	// srawi r9,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 3;
	// lwz r7,52(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// stbx r9,r8,r7
	PPC_STORE_U8(ctx.r8.u32 + ctx.r7.u32, ctx.r9.u8);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// stbx r10,r28,r9
	PPC_STORE_U8(r28.u32 + ctx.r9.u32, ctx.r10.u8);
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// bne cr6,0x82677360
	if (!cr6.eq) goto loc_82677360;
	// lwz r6,908(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	// lwz r28,92(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r3,24(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
loc_826775E8:
	// lbzx r29,r21,r30
	r29.u64 = PPC_LOAD_U8(r21.u32 + r30.u32);
	// add r8,r3,r23
	ctx.r8.u64 = ctx.r3.u64 + r23.u64;
	// lbzx r11,r3,r24
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + r24.u32);
	// add r7,r3,r19
	ctx.r7.u64 = ctx.r3.u64 + r19.u64;
	// mr r27,r29
	r27.u64 = r29.u64;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mr r29,r31
	r29.u64 = r31.u64;
	// rotlwi r25,r11,3
	r25.u64 = __builtin_rotateleft32(r11.u32, 3);
	// add r5,r3,r20
	ctx.r5.u64 = ctx.r3.u64 + r20.u64;
	// stbx r27,r3,r31
	PPC_STORE_U8(ctx.r3.u32 + r31.u32, r27.u8);
	// mr r31,r24
	r31.u64 = r24.u64;
	// add r4,r3,r9
	ctx.r4.u64 = ctx.r3.u64 + ctx.r9.u64;
	// mr r24,r29
	r24.u64 = r29.u64;
	// rlwinm r3,r10,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r29,r11,1
	r29.u64 = __builtin_rotateleft32(r11.u32, 1);
	// subf r25,r11,r25
	r25.s64 = r25.s64 - r11.s64;
	// stw r31,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r31.u32);
	// rlwinm r27,r10,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rotlwi r26,r11,2
	r26.u64 = __builtin_rotateleft32(r11.u32, 2);
	// stw r24,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r24.u32);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rlwinm r22,r10,3,0,28
	r22.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r25,r25,r10
	r25.u64 = r25.u64 + ctx.r10.u64;
	// add r27,r10,r27
	r27.u64 = ctx.r10.u64 + r27.u64;
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// add r3,r3,r29
	ctx.r3.u64 = ctx.r3.u64 + r29.u64;
	// subf r10,r10,r22
	ctx.r10.s64 = r22.s64 - ctx.r10.s64;
	// srawi r29,r25,3
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x7) != 0);
	r29.s64 = r25.s32 >> 3;
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
	// srawi r27,r27,3
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7) != 0);
	r27.s64 = r27.s32 >> 3;
	// srawi r3,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 3;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// srawi r29,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	r29.s64 = ctx.r10.s32 >> 3;
	// clrlwi r10,r27,24
	ctx.r10.u64 = r27.u32 & 0xFF;
	// clrlwi r3,r3,24
	ctx.r3.u64 = ctx.r3.u32 & 0xFF;
	// add r4,r4,r20
	ctx.r4.u64 = ctx.r4.u64 + r20.u64;
	// stb r11,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r11.u8);
	// clrlwi r29,r29,24
	r29.u64 = r29.u32 & 0xFF;
	// stb r11,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, r11.u8);
	// add r23,r28,r23
	r23.u64 = r28.u64 + r23.u64;
	// stb r11,2(r8)
	PPC_STORE_U8(ctx.r8.u32 + 2, r11.u8);
	// add r19,r28,r19
	r19.u64 = r28.u64 + r19.u64;
	// stb r11,3(r8)
	PPC_STORE_U8(ctx.r8.u32 + 3, r11.u8);
	// add r20,r28,r20
	r20.u64 = r28.u64 + r20.u64;
	// stb r10,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, ctx.r10.u8);
	// stb r10,1(r7)
	PPC_STORE_U8(ctx.r7.u32 + 1, ctx.r10.u8);
	// stb r10,2(r7)
	PPC_STORE_U8(ctx.r7.u32 + 2, ctx.r10.u8);
	// stb r10,3(r7)
	PPC_STORE_U8(ctx.r7.u32 + 3, ctx.r10.u8);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r10,932(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 932);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r3,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r3.u8);
	// add r30,r30,r10
	r30.u64 = r30.u64 + ctx.r10.u64;
	// stb r3,1(r5)
	PPC_STORE_U8(ctx.r5.u32 + 1, ctx.r3.u8);
	// stb r3,2(r5)
	PPC_STORE_U8(ctx.r5.u32 + 2, ctx.r3.u8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r3,3(r5)
	PPC_STORE_U8(ctx.r5.u32 + 3, ctx.r3.u8);
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// stb r29,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r29.u8);
	// stb r29,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, r29.u8);
	// stb r29,2(r4)
	PPC_STORE_U8(ctx.r4.u32 + 2, r29.u8);
	// stb r29,3(r4)
	PPC_STORE_U8(ctx.r4.u32 + 3, r29.u8);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// stw r19,56(r1)
	PPC_STORE_U32(ctx.r1.u32 + 56, r19.u32);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// bne cr6,0x82676a3c
	if (!cr6.eq) goto loc_82676A3C;
loc_826776FC:
	// rlwinm r11,r6,0,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFC0;
	// li r27,0
	r27.s64 = 0;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82677b24
	if (!cr6.gt) goto loc_82677B24;
	// subf r6,r31,r23
	ctx.r6.s64 = r23.s64 - r31.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// rlwinm r11,r11,26,6,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// addi r8,r30,1
	ctx.r8.s64 = r30.s64 + 1;
	// stw r6,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, ctx.r6.u32);
	// subf r6,r31,r19
	ctx.r6.s64 = r19.s64 - r31.s64;
	// addi r5,r11,1
	ctx.r5.s64 = r11.s64 + 1;
	// addi r9,r19,16
	ctx.r9.s64 = r19.s64 + 16;
	// rlwinm r27,r5,4,0,27
	r27.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r31,48
	ctx.r10.s64 = r31.s64 + 48;
	// stw r6,48(r1)
	PPC_STORE_U32(ctx.r1.u32 + 48, ctx.r6.u32);
	// addi r6,r27,1
	ctx.r6.s64 = r27.s64 + 1;
	// addi r11,r24,32
	r11.s64 = r24.s64 + 32;
	// subf r26,r24,r31
	r26.s64 = r31.s64 - r24.s64;
	// subf r25,r24,r23
	r25.s64 = r23.s64 - r24.s64;
	// subf r22,r24,r19
	r22.s64 = r19.s64 - r24.s64;
	// rlwinm r28,r5,6,0,25
	r28.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 6) & 0xFFFFFFC0;
	// stw r6,24(r1)
	PPC_STORE_U32(ctx.r1.u32 + 24, ctx.r6.u32);
loc_82677760:
	// li r21,16
	r21.s64 = 16;
	// lvlx v10,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r18,16
	r18.s64 = 16;
	// addi r4,r8,-1
	ctx.r4.s64 = ctx.r8.s64 + -1;
	// li r6,16
	ctx.r6.s64 = 16;
	// addi r3,r11,-32
	ctx.r3.s64 = r11.s64 + -32;
	// lvrx v11,r8,r21
	temp.u32 = ctx.r8.u32 + r21.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r31,r11,-16
	r31.s64 = r11.s64 + -16;
	// vor v9,v10,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v11,r11,r18
	temp.u32 = r11.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r20,16
	r20.s64 = 16;
	// vor v29,v10,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvlx v10,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v11,r4,r6
	temp.u32 = ctx.r4.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r29,r11,16
	r29.s64 = r11.s64 + 16;
	// li r17,16
	r17.s64 = 16;
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r3,r21
	temp.u32 = ctx.r3.u32 + r21.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r4,r10,-48
	ctx.r4.s64 = ctx.r10.s64 + -48;
	// lvlx v8,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r10,-32
	ctx.r3.s64 = ctx.r10.s64 + -32;
	// vor v2,v8,v10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v7,r31,r20
	temp.u32 = r31.u32 + r20.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r31,r26,r11
	r31.u64 = r26.u64 + r11.u64;
	// lvrx v8,r29,r17
	temp.u32 = r29.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v31,v10,v7
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v6,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v10,v13,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v27,v6,v8
	_mm_store_si128((__m128i*)v27.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrglb v9,v13,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r29,16
	r29.s64 = 16;
	// vmrglb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v6,v10,v10
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v5,v9,v9
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v4,v8,v8
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v3,v7,v7
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v28,v6,v10
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v26,v5,v9
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v1,v4,v8
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v30,v3,v7
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v6,v4,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v5,v3,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v10,v1,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v7,v7,v26
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v9,v30,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v8,v8,v28
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vsrah v6,v6,v12
	// vsrah v5,v5,v12
	// vsrah v4,v10,v12
	// vsrah v3,v8,v12
	// vsrah v7,v7,v12
	// vsrah v9,v9,v12
	// vpkshus v10,v6,v5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vpkshus v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vpkshus v9,v4,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vmrglb v6,v11,v10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrghb v8,v11,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrghb v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrglb v7,v9,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v11,v8,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrglb v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v9,v6,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrglb v8,v6,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrglb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v5,v13,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvlx v11,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// vmrglb v4,v13,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvrx v11,r4,r6
	ea = ctx.r4.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// vmrghb v3,v13,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvlx v10,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// vmrglb v1,v13,v9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvrx v10,r3,r29
	ea = ctx.r3.u32 + r29.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// vor v11,v7,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// stvlx v9,0,r31
	ea = r31.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// vor v10,v6,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// stvrx v9,r31,r21
	ea = r31.u32 + r21.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// stvlx v8,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// vaddshs v26,v11,v11
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// vaddshs v25,v10,v10
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// stvrx v8,r10,r20
	ea = ctx.r10.u32 + r20.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// vor v9,v5,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vmrghb v30,v13,v29
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v4,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vmrghb v28,v13,v27
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v14,v26,v11
	_mm_store_si128((__m128i*)v14.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vmrglb v29,v13,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v26,v25,v10
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vmrglb v27,v13,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v24,v9,v9
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vmrghb v4,v13,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v19,v7,v7
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vmrglb v8,v13,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v6,v3,v3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vmrghb v3,v13,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v26,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// vaddshs v26,v24,v9
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vmrglb v2,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v5,v1,v1
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vmrghb v1,v13,v31
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v19,v19,v7
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vmrglb v31,v13,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v22,v30,v30
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// li r31,16
	r31.s64 = 16;
	// stvx v26,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// vaddshs v26,v3,v3
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v25,v2,v2
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v24,v1,v1
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v23,v31,v31
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// stvx v19,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v21,v29,v29
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v20,v28,v28
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// subf r6,r19,r23
	ctx.r6.s64 = r23.s64 - r19.s64;
	// vaddshs v19,v27,v27
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v3,v26,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// add r4,r6,r9
	ctx.r4.u64 = ctx.r6.u64 + ctx.r9.u64;
	// vaddshs v2,v25,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// li r6,16
	ctx.r6.s64 = 16;
	// vaddshs v1,v24,v1
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v31,v23,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v30,v22,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v29,v21,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v28,v20,v28
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v27,v19,v27
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v3,v26,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v2,v25,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v1,v24,v1
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v31,v23,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v30,v22,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v29,v21,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v28,v20,v28
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v27,v19,v27
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v18,v6,v6
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v17,v5,v5
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v16,v4,v4
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v15,v8,v8
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v26,v26,v3
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v25,v25,v2
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v24,v24,v1
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v23,v23,v31
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v22,v22,v30
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v21,v21,v29
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v20,v20,v28
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v19,v19,v27
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v18,v18,v6
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v17,v17,v5
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v16,v16,v4
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v15,v15,v8
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v11,v26,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v10,v25,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v9,v24,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v7,v23,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v6,v22,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v5,v21,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v4,v20,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v8,v19,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsrah v10,v10,v0
	// addi r20,r1,128
	r20.s64 = ctx.r1.s64 + 128;
	// vsrah v11,v11,v0
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// vsrah v9,v9,v0
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// vsrah v7,v7,v0
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// vsrah v6,v6,v0
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// vpkshus v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v5,v5,v0
	// vsrah v8,v8,v0
	// vsrah v4,v4,v0
	// vpkshus v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vpkshus v9,v6,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v5,v30,v18
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vpkshus v8,v4,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v4,v29,v17
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v17.s16)));
	// stvlx v11,0,r7
	ea = ctx.r7.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// stvrx v11,r7,r6
	ea = ctx.r7.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// lwz r6,68(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// vor v11,v9,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// stvlx v10,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// add r3,r6,r10
	ctx.r3.u64 = ctx.r6.u64 + ctx.r10.u64;
	// vor v9,v8,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// lvx128 v7,r0,r20
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r20.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r4,r25,r11
	ctx.r4.u64 = r25.u64 + r11.u64;
	// vaddshs v7,v1,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// li r20,16
	r20.s64 = 16;
	// addi r7,r7,64
	ctx.r7.s64 = ctx.r7.s64 + 64;
	// lvx128 v8,r0,r6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r6,16
	ctx.r6.s64 = 16;
	// stvrx v10,r29,r31
	ea = r29.u32 + r31.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// addi r31,r1,96
	r31.s64 = ctx.r1.s64 + 96;
	// vaddshs v10,v3,v14
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v14.s16)));
	// mr r29,r9
	r29.u64 = ctx.r9.u64;
	// vaddshs v8,v2,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v3,v28,v16
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v2,v27,v15
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v15.s16)));
	// lvx128 v6,r0,r31
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvlx v11,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// vaddshs v6,v31,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// stvrx v11,r4,r6
	ea = ctx.r4.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// vsrah v11,v10,v0
	// stvlx v9,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// vsrah v10,v8,v0
	// stvrx v9,r3,r21
	ea = ctx.r3.u32 + r21.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// vsrah v9,v7,v0
	// vsrah v8,v6,v0
	// lwz r6,48(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 48);
	// vsrah v7,v5,v0
	// addi r4,r9,-16
	ctx.r4.s64 = ctx.r9.s64 + -16;
	// vsrah v6,v4,v0
	// vpkshus v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v5,v3,v0
	// add r31,r6,r10
	r31.u64 = ctx.r6.u64 + ctx.r10.u64;
	// vsrah v4,v2,v0
	// vpkshus v10,v9,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// li r6,16
	ctx.r6.s64 = 16;
	// vpkshus v9,v7,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// mr r21,r9
	r21.u64 = ctx.r9.u64;
	// add r3,r22,r11
	ctx.r3.u64 = r22.u64 + r11.u64;
	// vpkshus v8,v5,v4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// stvlx v11,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stvrx v11,r4,r6
	ea = ctx.r4.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// stvlx v10,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// stvrx v10,r21,r20
	ea = r21.u32 + r20.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// stvlx v9,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// stvrx v9,r3,r18
	ea = ctx.r3.u32 + r18.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// stvlx v8,0,r31
	ea = r31.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// stvrx v8,r31,r17
	ea = r31.u32 + r17.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// bne cr6,0x82677760
	if (!cr6.eq) goto loc_82677760;
	// lwz r6,908(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	// lwz r7,24(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
loc_82677B24:
	// cmpw cr6,r28,r6
	cr6.compare<int32_t>(r28.s32, ctx.r6.s32, xer);
	// bge cr6,0x82677ca0
	if (!cr6.lt) goto loc_82677CA0;
	// subf r10,r28,r6
	ctx.r10.s64 = ctx.r6.s64 - r28.s64;
	// add r11,r28,r19
	r11.u64 = r28.u64 + r19.u64;
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// add r10,r28,r23
	ctx.r10.u64 = r28.u64 + r23.u64;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// add r8,r28,r24
	ctx.r8.u64 = r28.u64 + r24.u64;
	// addi r31,r9,1
	r31.s64 = ctx.r9.s64 + 1;
	// addi r3,r11,3
	ctx.r3.s64 = r11.s64 + 3;
	// rlwinm r9,r31,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// add r29,r7,r30
	r29.u64 = ctx.r7.u64 + r30.u64;
	// addi r11,r8,1
	r11.s64 = ctx.r8.s64 + 1;
	// subf r18,r24,r23
	r18.s64 = r23.s64 - r24.s64;
	// subf r17,r24,r19
	r17.s64 = r19.s64 - r24.s64;
	// stw r9,24(r1)
	PPC_STORE_U32(ctx.r1.u32 + 24, ctx.r9.u32);
loc_82677B6C:
	// lbz r8,0(r29)
	ctx.r8.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// lbzx r9,r27,r30
	ctx.r9.u64 = PPC_LOAD_U8(r27.u32 + r30.u32);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// rotlwi r26,r8,1
	r26.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lbz r7,-1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// rotlwi r28,r9,1
	r28.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r26,r8,r26
	r26.u64 = ctx.r8.u64 + r26.u64;
	// lbz r5,1(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// add r28,r9,r28
	r28.u64 = ctx.r9.u64 + r28.u64;
	// lbz r4,2(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// add r15,r26,r9
	r15.u64 = r26.u64 + ctx.r9.u64;
	// rotlwi r26,r7,3
	r26.u64 = __builtin_rotateleft32(ctx.r7.u32, 3);
	// add r16,r8,r9
	r16.u64 = ctx.r8.u64 + ctx.r9.u64;
	// subf r26,r7,r26
	r26.s64 = r26.s64 - ctx.r7.s64;
	// rotlwi r25,r6,3
	r25.u64 = __builtin_rotateleft32(ctx.r6.u32, 3);
	// add r9,r26,r9
	ctx.r9.u64 = r26.u64 + ctx.r9.u64;
	// rotlwi r21,r5,3
	r21.u64 = __builtin_rotateleft32(ctx.r5.u32, 3);
	// subf r22,r6,r25
	r22.s64 = r25.s64 - ctx.r6.s64;
	// rotlwi r20,r7,2
	r20.u64 = __builtin_rotateleft32(ctx.r7.u32, 2);
	// subf r25,r5,r21
	r25.s64 = r21.s64 - ctx.r5.s64;
	// stw r9,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r9.u32);
	// rotlwi r21,r6,2
	r21.u64 = __builtin_rotateleft32(ctx.r6.u32, 2);
	// add r9,r7,r20
	ctx.r9.u64 = ctx.r7.u64 + r20.u64;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + r28.u64;
	// add r6,r6,r21
	ctx.r6.u64 = ctx.r6.u64 + r21.u64;
	// add r21,r9,r28
	r21.u64 = ctx.r9.u64 + r28.u64;
	// srawi r9,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 2;
	// srawi r8,r16,1
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x1) != 0);
	ctx.r8.s64 = r16.s32 >> 1;
	// srawi r7,r15,2
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x3) != 0);
	ctx.r7.s64 = r15.s32 >> 2;
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// rotlwi r14,r4,3
	r14.u64 = __builtin_rotateleft32(ctx.r4.u32, 3);
	// clrlwi r8,r8,16
	ctx.r8.u64 = ctx.r8.u32 & 0xFFFF;
	// clrlwi r7,r7,16
	ctx.r7.u64 = ctx.r7.u32 & 0xFFFF;
	// subf r26,r4,r14
	r26.s64 = r14.s64 - ctx.r4.s64;
	// add r22,r22,r9
	r22.u64 = r22.u64 + ctx.r9.u64;
	// add r25,r25,r8
	r25.u64 = r25.u64 + ctx.r8.u64;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lwz r16,20(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// srawi r28,r16,3
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x7) != 0);
	r28.s64 = r16.s32 >> 3;
	// stb r28,-2(r10)
	PPC_STORE_U8(ctx.r10.u32 + -2, r28.u8);
	// rlwinm r28,r9,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// add r28,r26,r7
	r28.u64 = r26.u64 + ctx.r7.u64;
	// srawi r26,r22,3
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7) != 0);
	r26.s64 = r22.s32 >> 3;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// srawi r25,r25,3
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x7) != 0);
	r25.s64 = r25.s32 >> 3;
	// srawi r6,r28,3
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7) != 0);
	ctx.r6.s64 = r28.s32 >> 3;
	// srawi r28,r21,3
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x7) != 0);
	r28.s64 = r21.s32 >> 3;
	// srawi r22,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	r22.s64 = ctx.r9.s32 >> 3;
	// stbx r26,r11,r18
	PPC_STORE_U8(r11.u32 + r18.u32, r26.u8);
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// stb r25,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r25.u8);
	// stb r6,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, ctx.r6.u8);
	// add r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stb r28,-3(r3)
	PPC_STORE_U8(ctx.r3.u32 + -3, r28.u8);
	// rotlwi r28,r5,2
	r28.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// rlwinm r9,r7,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// stbx r22,r11,r17
	PPC_STORE_U8(r11.u32 + r17.u32, r22.u8);
	// rotlwi r8,r4,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// add r5,r5,r28
	ctx.r5.u64 = ctx.r5.u64 + r28.u64;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// add r7,r6,r5
	ctx.r7.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r8,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 3;
	// srawi r7,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 3;
	// subf r9,r23,r19
	ctx.r9.s64 = r19.s64 - r23.s64;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stb r7,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r7.u8);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// stbx r8,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u8);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x82677b6c
	if (!cr6.eq) goto loc_82677B6C;
	// lwz r28,24(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
loc_82677CA0:
	// lbzx r11,r28,r24
	r11.u64 = PPC_LOAD_U8(r28.u32 + r24.u32);
	// add r10,r28,r23
	ctx.r10.u64 = r28.u64 + r23.u64;
	// lbzx r8,r27,r30
	ctx.r8.u64 = PPC_LOAD_U8(r27.u32 + r30.u32);
	// add r9,r28,r19
	ctx.r9.u64 = r28.u64 + r19.u64;
	// rotlwi r6,r11,3
	ctx.r6.u64 = __builtin_rotateleft32(r11.u32, 3);
	// rotlwi r7,r8,1
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// rotlwi r5,r11,2
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 2);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - r11.s64;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// srawi r8,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 3;
	// srawi r7,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r7.s64 = r11.s32 >> 3;
	// clrlwi r11,r8,24
	r11.u64 = ctx.r8.u32 & 0xFF;
	// clrlwi r8,r7,24
	ctx.r8.u64 = ctx.r7.u32 & 0xFF;
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// stb r11,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, r11.u8);
	// stb r11,2(r10)
	PPC_STORE_U8(ctx.r10.u32 + 2, r11.u8);
	// stb r11,3(r10)
	PPC_STORE_U8(ctx.r10.u32 + 3, r11.u8);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// stb r8,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, ctx.r8.u8);
	// stb r8,2(r9)
	PPC_STORE_U8(ctx.r9.u32 + 2, ctx.r8.u8);
	// stb r8,3(r9)
	PPC_STORE_U8(ctx.r9.u32 + 3, ctx.r8.u8);
	// addi r1,r1,864
	ctx.r1.s64 = ctx.r1.s64 + 864;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82677D08"))) PPC_WEAK_FUNC(sub_82677D08);
PPC_FUNC_IMPL(__imp__sub_82677D08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bccc
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r21,r10
	r21.u64 = ctx.r10.u64;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// vspltish v4,3
	// mr r23,r9
	r23.u64 = ctx.r9.u64;
	// addi r10,r10,4240
	ctx.r10.s64 = ctx.r10.s64 + 4240;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// rlwinm r24,r27,0,0,27
	r24.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFF0;
	// lvx128 v7,r0,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// stvx v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,16
	ctx.r9.s64 = 16;
	// mr r20,r8
	r20.u64 = ctx.r8.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// add r31,r5,r26
	r31.u64 = ctx.r5.u64 + r26.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// lvx128 v13,r9,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,32
	ctx.r9.s64 = 32;
	// lvx128 v12,r9,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,48
	ctx.r9.s64 = 48;
	// lvx128 v11,r9,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,64
	ctx.r9.s64 = 64;
	// lvx128 v10,r9,r10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x82677e64
	if (!cr6.gt) goto loc_82677E64;
	// addi r11,r24,-1
	r11.s64 = r24.s64 + -1;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// rlwinm r11,r11,28,4,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// rlwinm r7,r8,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_82677DC0:
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx v8,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// li r30,16
	r30.s64 = 16;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lvrx v9,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v6,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r6,r30
	temp.u32 = ctx.r6.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// vor v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// vperm v9,v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// vperm v8,v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v6,v0,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v3,v6,v13
	// vslh v2,v9,v13
	// vslh v1,v5,v11
	// vslh v31,v8,v11
	// vslh v9,v9,v12
	// vslh v6,v6,v12
	// vslh v8,v8,v10
	// vslh v5,v5,v10
	// vaddshs v9,v2,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v6,v3,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vsubshs v8,v31,v8
	// vsubshs v5,v1,v5
	// vaddshs v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v6,v6,v5
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsrah v9,v9,v4
	// vsrah v8,v6,v4
	// vpkshus v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvlx v9,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// stvrx v9,r4,r30
	ea = ctx.r4.u32 + r30.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// bne cr6,0x82677dc0
	if (!cr6.eq) goto loc_82677DC0;
loc_82677E64:
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// bge cr6,0x82677eb4
	if (!cr6.lt) goto loc_82677EB4;
	// addi r7,r3,1
	ctx.r7.s64 = ctx.r3.s64 + 1;
loc_82677E7C:
	// srawi r8,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 7;
	// clrlwi r9,r10,25
	ctx.r9.u64 = ctx.r10.u32 & 0x7F;
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// lbzx r6,r7,r8
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r8.u32);
	// lbzx r4,r8,r3
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// subfic r8,r9,128
	xer.ca = ctx.r9.u32 <= 128;
	ctx.r8.s64 = 128 - ctx.r9.s64;
	// mullw r9,r6,r9
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r9.s32);
	// mullw r8,r4,r8
	ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// blt cr6,0x82677e7c
	if (cr6.lt) goto loc_82677E7C;
loc_82677EB4:
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// bge cr6,0x82677edc
	if (!cr6.lt) goto loc_82677EDC;
loc_82677EBC:
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// blt cr6,0x82677ebc
	if (cr6.lt) goto loc_82677EBC;
loc_82677EDC:
	// add r30,r3,r23
	r30.u64 = ctx.r3.u64 + r23.u64;
	// li r22,0
	r22.s64 = 0;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x82678578
	if (!cr6.gt) goto loc_82678578;
	// addi r28,r30,1
	r28.s64 = r30.s64 + 1;
loc_82677EF0:
	// clrlwi r11,r22,30
	r11.u64 = r22.u32 & 0x3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bgt cr6,0x82678568
	if (cr6.gt) goto loc_82678568;
	// lis r12,-32153
	r12.s64 = -2107179008;
	// addi r12,r12,32532
	r12.s64 = r12.s64 + 32532;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_82677F24;
	case 1:
		goto loc_82677F60;
	case 2:
		goto loc_82678164;
	case 3:
		goto loc_82678354;
	default:
		__builtin_unreachable();
	}
	// lwz r19,32548(r7)
	r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + 32548);
	// lwz r19,32608(r7)
	r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + 32608);
	// lwz r19,-32412(r7)
	r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + -32412);
	// lwz r19,-31916(r7)
	r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + -31916);
loc_82677F24:
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x82678568
	goto loc_82678568;
loc_82677F60:
	// li r3,0
	ctx.r3.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x826780a4
	if (!cr6.gt) goto loc_826780A4;
	// addi r11,r24,-1
	r11.s64 = r24.s64 + -1;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// rlwinm r11,r11,28,4,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// subf r5,r31,r29
	ctx.r5.s64 = r29.s64 - r31.s64;
	// rlwinm r3,r8,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r4,r31,r25
	ctx.r4.s64 = r25.s64 - r31.s64;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82677F94:
	// li r19,16
	r19.s64 = 16;
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 1;
	// li r18,16
	r18.s64 = 16;
	// add r6,r5,r10
	ctx.r6.u64 = ctx.r5.u64 + ctx.r10.u64;
	// li r17,16
	r17.s64 = 16;
	// lvrx v0,r9,r19
	temp.u32 = ctx.r9.u32 + r19.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// vor v0,v9,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v8,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r7,r18
	temp.u32 = ctx.r7.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r7,16
	ctx.r7.s64 = 16;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v6,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r6,r17
	temp.u32 = ctx.r6.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r6,16
	ctx.r6.s64 = 16;
	// vor v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vperm v6,v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// addi r9,r9,12
	ctx.r9.s64 = ctx.r9.s64 + 12;
	// vperm v9,v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// vmrghb v5,v0,v6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v4,v0,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v3,v5,v13
	// vslh v1,v4,v11
	// vslh v2,v6,v13
	// vslh v31,v9,v11
	// vslh v6,v6,v12
	// vslh v9,v9,v10
	// vslh v5,v5,v12
	// vslh v4,v4,v10
	// vaddshs v6,v2,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vsubshs v9,v31,v9
	// vaddshs v5,v3,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vmrghb v3,v0,v8
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v4,v1,v4
	// vmrglb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vspltish v0,1
	// vaddshs v6,v6,v9
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vspltish v9,3
	// vaddshs v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vslh v4,v3,v0
	// vslh v3,v8,v0
	// vsrah v6,v6,v9
	// vsrah v8,v5,v9
	// vspltish v5,2
	// vslh v1,v8,v0
	// vslh v2,v8,v5
	// vpkshus v8,v8,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v0,v6,v0
	// vslh v5,v6,v5
	// vaddshs v6,v2,v1
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v0,v5,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvlx v8,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// stvrx v8,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// vaddshs v8,v6,v4
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// add r7,r4,r10
	ctx.r7.u64 = ctx.r4.u64 + ctx.r10.u64;
	// vaddshs v0,v0,v3
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// vsrah v8,v8,v9
	// vsrah v0,v0,v9
	// vpkshus v0,v8,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvlx v0,0,r7
	ea = ctx.r7.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,r7,r6
	ea = ctx.r7.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// bne cr6,0x82677f94
	if (!cr6.eq) goto loc_82677F94;
loc_826780A4:
	// rlwinm r10,r3,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// add r10,r3,r10
	ctx.r10.u64 = ctx.r3.u64 + ctx.r10.u64;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// bge cr6,0x826780f4
	if (!cr6.lt) goto loc_826780F4;
loc_826780BC:
	// srawi r8,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 7;
	// clrlwi r9,r10,25
	ctx.r9.u64 = ctx.r10.u32 & 0x7F;
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// lbzx r6,r8,r28
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + r28.u32);
	// lbzx r5,r8,r30
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + r30.u32);
	// subfic r8,r9,128
	xer.ca = ctx.r9.u32 <= 128;
	ctx.r8.s64 = 128 - ctx.r9.s64;
	// mullw r9,r6,r9
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r9.s32);
	// mullw r8,r5,r8
	ctx.r8.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r8.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// stbx r9,r11,r31
	PPC_STORE_U8(r11.u32 + r31.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// blt cr6,0x826780bc
	if (cr6.lt) goto loc_826780BC;
loc_826780F4:
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// bge cr6,0x8267811c
	if (!cr6.lt) goto loc_8267811C;
loc_826780FC:
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// lbzx r9,r9,r30
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r30.u32);
	// stbx r9,r11,r31
	PPC_STORE_U8(r11.u32 + r31.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// blt cr6,0x826780fc
	if (cr6.lt) goto loc_826780FC;
loc_8267811C:
	// cmpw cr6,r7,r26
	cr6.compare<int32_t>(ctx.r7.s32, r26.s32, xer);
	// bge cr6,0x82678554
	if (!cr6.lt) goto loc_82678554;
	// add r11,r7,r29
	r11.u64 = ctx.r7.u64 + r29.u64;
	// subf r6,r29,r31
	ctx.r6.s64 = r31.s64 - r29.s64;
	// subf r5,r29,r25
	ctx.r5.s64 = r25.s64 - r29.s64;
	// subf r10,r7,r26
	ctx.r10.s64 = r26.s64 - ctx.r7.s64;
loc_82678134:
	// lbzx r9,r6,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r7,r9,1
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	// stbx r9,r5,r11
	PPC_STORE_U8(ctx.r5.u32 + r11.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x82678134
	if (!cr6.eq) goto loc_82678134;
	// b 0x82678554
	goto loc_82678554;
loc_82678164:
	// li r3,0
	ctx.r3.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x8267829c
	if (!cr6.gt) goto loc_8267829C;
	// addi r11,r24,-1
	r11.s64 = r24.s64 + -1;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// rlwinm r11,r11,28,4,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// subf r5,r31,r29
	ctx.r5.s64 = r29.s64 - r31.s64;
	// rlwinm r3,r8,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r4,r31,r25
	ctx.r4.s64 = r25.s64 - r31.s64;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82678198:
	// li r19,16
	r19.s64 = 16;
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 1;
	// li r18,16
	r18.s64 = 16;
	// add r6,r10,r5
	ctx.r6.u64 = ctx.r10.u64 + ctx.r5.u64;
	// li r17,16
	r17.s64 = 16;
	// lvrx v0,r9,r19
	temp.u32 = ctx.r9.u32 + r19.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// vor v0,v9,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v8,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r7,r18
	temp.u32 = ctx.r7.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r7,16
	ctx.r7.s64 = 16;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v6,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r6,r17
	temp.u32 = ctx.r6.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r6,16
	ctx.r6.s64 = 16;
	// vor v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vperm v6,v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// addi r9,r9,12
	ctx.r9.s64 = ctx.r9.s64 + 12;
	// vperm v9,v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// vmrghb v5,v0,v6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v4,v0,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v3,v5,v13
	// vslh v1,v4,v11
	// vslh v31,v9,v11
	// vslh v2,v6,v13
	// vslh v5,v5,v12
	// vslh v4,v4,v10
	// vslh v9,v9,v10
	// vslh v6,v6,v12
	// vaddshs v5,v3,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubshs v4,v1,v4
	// vsubshs v3,v31,v9
	// vspltish v9,3
	// vaddshs v6,v2,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vmrghb v2,v0,v8
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vspltish v0,2
	// vaddshs v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vor v31,v9,v9
	_mm_store_si128((__m128i*)v31.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vaddshs v6,v6,v3
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vor v1,v9,v9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vslh v3,v8,v0
	// vsrah v8,v5,v9
	// vsrah v9,v6,v9
	// vslh v4,v2,v0
	// vslh v6,v8,v0
	// vslh v5,v9,v0
	// vpkshus v0,v8,v9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v9,v4,v6
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v8,v3,v5
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvlx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// vsrah v9,v9,v1
	// stvrx v0,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// vsrah v0,v8,v31
	// add r7,r10,r4
	ctx.r7.u64 = ctx.r10.u64 + ctx.r4.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// vpkshus v0,v9,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvlx v0,0,r7
	ea = ctx.r7.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,r7,r6
	ea = ctx.r7.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// bne cr6,0x82678198
	if (!cr6.eq) goto loc_82678198;
loc_8267829C:
	// rlwinm r10,r3,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// add r10,r3,r10
	ctx.r10.u64 = ctx.r3.u64 + ctx.r10.u64;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// bge cr6,0x826782ec
	if (!cr6.lt) goto loc_826782EC;
loc_826782B4:
	// srawi r8,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 7;
	// clrlwi r9,r10,25
	ctx.r9.u64 = ctx.r10.u32 & 0x7F;
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// lbzx r7,r8,r28
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r8.u32 + r28.u32);
	// lbzx r6,r8,r30
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + r30.u32);
	// subfic r8,r9,128
	xer.ca = ctx.r9.u32 <= 128;
	ctx.r8.s64 = 128 - ctx.r9.s64;
	// mullw r9,r7,r9
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// mullw r8,r6,r8
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r8.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// stbx r9,r11,r31
	PPC_STORE_U8(r11.u32 + r31.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// blt cr6,0x826782b4
	if (cr6.lt) goto loc_826782B4;
loc_826782EC:
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// bge cr6,0x82678314
	if (!cr6.lt) goto loc_82678314;
loc_826782F4:
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// lbzx r9,r9,r30
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r30.u32);
	// stbx r9,r11,r31
	PPC_STORE_U8(r11.u32 + r31.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// blt cr6,0x826782f4
	if (cr6.lt) goto loc_826782F4;
loc_82678314:
	// cmpw cr6,r5,r26
	cr6.compare<int32_t>(ctx.r5.s32, r26.s32, xer);
	// bge cr6,0x82678554
	if (!cr6.lt) goto loc_82678554;
	// add r11,r5,r29
	r11.u64 = ctx.r5.u64 + r29.u64;
	// subf r7,r29,r31
	ctx.r7.s64 = r31.s64 - r29.s64;
	// subf r6,r29,r25
	ctx.r6.s64 = r25.s64 - r29.s64;
	// subf r10,r5,r26
	ctx.r10.s64 = r26.s64 - ctx.r5.s64;
loc_8267832C:
	// lbzx r9,r11,r7
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// stbx r9,r11,r6
	PPC_STORE_U8(r11.u32 + ctx.r6.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8267832c
	if (!cr6.eq) goto loc_8267832C;
	// b 0x82678554
	goto loc_82678554;
loc_82678354:
	// li r3,0
	ctx.r3.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82678498
	if (!cr6.gt) goto loc_82678498;
	// addi r11,r24,-1
	r11.s64 = r24.s64 + -1;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// rlwinm r11,r11,28,4,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// subf r5,r31,r29
	ctx.r5.s64 = r29.s64 - r31.s64;
	// rlwinm r3,r8,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r4,r31,r25
	ctx.r4.s64 = r25.s64 - r31.s64;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82678388:
	// li r19,16
	r19.s64 = 16;
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 1;
	// li r18,16
	r18.s64 = 16;
	// add r6,r10,r5
	ctx.r6.u64 = ctx.r10.u64 + ctx.r5.u64;
	// li r17,16
	r17.s64 = 16;
	// lvrx v0,r9,r19
	temp.u32 = ctx.r9.u32 + r19.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// vor v0,v9,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v8,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r7,r18
	temp.u32 = ctx.r7.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r7,16
	ctx.r7.s64 = 16;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v6,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r6,r17
	temp.u32 = ctx.r6.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r6,16
	ctx.r6.s64 = 16;
	// vor v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vperm v6,v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// addi r9,r9,12
	ctx.r9.s64 = ctx.r9.s64 + 12;
	// vperm v9,v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// vmrghb v5,v0,v6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v4,v0,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v3,v5,v13
	// vslh v1,v4,v11
	// vslh v2,v6,v13
	// vslh v31,v9,v11
	// vslh v5,v5,v12
	// vslh v4,v4,v10
	// vslh v6,v6,v12
	// vslh v9,v9,v10
	// vaddshs v5,v3,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubshs v4,v1,v4
	// vaddshs v6,v2,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vsubshs v3,v31,v9
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vspltish v0,1
	// vaddshs v4,v6,v3
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vspltish v6,2
	// vslh v2,v9,v0
	// vslh v31,v8,v0
	// vslh v3,v9,v6
	// vspltish v9,3
	// vslh v1,v8,v6
	// vsrah v6,v4,v9
	// vsrah v8,v5,v9
	// vaddshs v4,v3,v2
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vpkshus v5,v8,v6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v8,v8,v0
	// vslh v0,v6,v0
	// vaddshs v6,v1,v31
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v8,v4,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v0,v6,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvlx v5,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v5.u8[15 - i]);
	// vsrah v8,v8,v9
	// stvrx v5,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v5.u8[i]);
	// add r7,r10,r4
	ctx.r7.u64 = ctx.r10.u64 + ctx.r4.u64;
	// vsrah v0,v0,v9
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// vpkshus v0,v8,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvlx v0,0,r7
	ea = ctx.r7.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,r7,r6
	ea = ctx.r7.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// bne cr6,0x82678388
	if (!cr6.eq) goto loc_82678388;
loc_82678498:
	// rlwinm r10,r3,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// add r10,r3,r10
	ctx.r10.u64 = ctx.r3.u64 + ctx.r10.u64;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// bge cr6,0x826784e8
	if (!cr6.lt) goto loc_826784E8;
loc_826784B0:
	// srawi r8,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 7;
	// clrlwi r9,r10,25
	ctx.r9.u64 = ctx.r10.u32 & 0x7F;
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// lbzx r5,r8,r28
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + r28.u32);
	// lbzx r6,r8,r30
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + r30.u32);
	// subfic r8,r9,128
	xer.ca = ctx.r9.u32 <= 128;
	ctx.r8.s64 = 128 - ctx.r9.s64;
	// mullw r9,r5,r9
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r9.s32);
	// mullw r8,r6,r8
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r8.s32);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// stbx r9,r11,r31
	PPC_STORE_U8(r11.u32 + r31.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// blt cr6,0x826784b0
	if (cr6.lt) goto loc_826784B0;
loc_826784E8:
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// bge cr6,0x82678510
	if (!cr6.lt) goto loc_82678510;
loc_826784F0:
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// lbzx r9,r9,r30
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r30.u32);
	// stbx r9,r11,r31
	PPC_STORE_U8(r11.u32 + r31.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// blt cr6,0x826784f0
	if (cr6.lt) goto loc_826784F0;
loc_82678510:
	// cmpw cr6,r7,r26
	cr6.compare<int32_t>(ctx.r7.s32, r26.s32, xer);
	// bge cr6,0x82678554
	if (!cr6.lt) goto loc_82678554;
	// add r11,r7,r31
	r11.u64 = ctx.r7.u64 + r31.u64;
	// subf r6,r31,r29
	ctx.r6.s64 = r29.s64 - r31.s64;
	// subf r5,r31,r25
	ctx.r5.s64 = r25.s64 - r31.s64;
	// subf r10,r7,r26
	ctx.r10.s64 = r26.s64 - ctx.r7.s64;
loc_82678528:
	// lbzx r9,r6,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r7,r9,1
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	// stbx r9,r5,r11
	PPC_STORE_U8(ctx.r5.u32 + r11.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x82678528
	if (!cr6.eq) goto loc_82678528;
loc_82678554:
	// mr r11,r31
	r11.u64 = r31.u64;
	// mr r31,r29
	r31.u64 = r29.u64;
	// mr r29,r11
	r29.u64 = r11.u64;
	// add r30,r30,r23
	r30.u64 = r30.u64 + r23.u64;
	// add r28,r28,r23
	r28.u64 = r28.u64 + r23.u64;
loc_82678568:
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// add r25,r25,r20
	r25.u64 = r25.u64 + r20.u64;
	// cmpw cr6,r22,r21
	cr6.compare<int32_t>(r22.s32, r21.s32, xer);
	// blt cr6,0x82677ef0
	if (cr6.lt) goto loc_82677EF0;
loc_82678578:
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_82678580"))) PPC_WEAK_FUNC(sub_82678580);
PPC_FUNC_IMPL(__imp__sub_82678580) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bccc
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r22,r4
	r22.u64 = ctx.r4.u64;
	// li r31,-64
	r31.s64 = -64;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// vspltish v12,1
	// rlwinm r21,r6,0,0,27
	r21.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFF0;
	// vspltish v11,2
	// mullw r4,r11,r9
	ctx.r4.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// vspltish v13,3
	// lis r11,-32243
	r11.s64 = -2113077248;
	// add r29,r4,r3
	r29.u64 = ctx.r4.u64 + ctx.r3.u64;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// addi r11,r11,4384
	r11.s64 = r11.s64 + 4384;
	// add r4,r5,r7
	ctx.r4.u64 = ctx.r5.u64 + ctx.r7.u64;
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// lvx128 v7,r31,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r31,-48
	r31.s64 = -48;
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r31,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r31,-32
	r31.s64 = -32;
	// lvx128 v9,r31,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r31,-16
	r31.s64 = -16;
	// lvx128 v8,r31,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,0
	r11.s64 = 0;
	// ble cr6,0x826786b4
	if (!cr6.gt) goto loc_826786B4;
	// addi r11,r21,-1
	r11.s64 = r21.s64 + -1;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// rlwinm r11,r11,28,4,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// rlwinm r28,r30,4,0,27
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_82678610:
	// li r26,16
	r26.s64 = 16;
	// lvlx v4,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r27,r3,1
	r27.s64 = ctx.r3.s64 + 1;
	// li r25,16
	r25.s64 = 16;
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// lvrx v5,r3,r26
	temp.u32 = ctx.r3.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// mr r26,r31
	r26.u64 = r31.u64;
	// vor v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvlx v3,0,r27
	temp.u32 = r27.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v4,r27,r25
	temp.u32 = r27.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// mr r27,r31
	r27.u64 = r31.u64;
	// vor v4,v3,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// addi r3,r3,12
	ctx.r3.s64 = ctx.r3.s64 + 12;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// vperm v5,v5,v5,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// vperm v4,v4,v4,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v3,v0,v5
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v2,v0,v4
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v4,v0,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v1,v3,v10
	// vslh v31,v5,v10
	// vslh v30,v2,v8
	// vslh v29,v4,v8
	// vslh v5,v5,v9
	// vslh v3,v3,v9
	// vslh v4,v4,v6
	// vslh v2,v2,v6
	// vaddshs v5,v31,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v3,v1,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsubshs v4,v29,v4
	// vsubshs v2,v30,v2
	// vaddshs v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsrah v5,v5,v13
	// vsrah v4,v3,v13
	// vpkshus v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// stvlx v5,0,r27
	ea = r27.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v5.u8[15 - i]);
	// stvrx v5,r26,r25
	ea = r26.u32 + r25.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v5.u8[i]);
	// bne cr6,0x82678610
	if (!cr6.eq) goto loc_82678610;
loc_826786B4:
	// rlwinm r3,r28,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// add r3,r28,r3
	ctx.r3.u64 = r28.u64 + ctx.r3.u64;
	// rlwinm r3,r3,5,0,26
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 5) & 0xFFFFFFE0;
	// bge cr6,0x82678704
	if (!cr6.lt) goto loc_82678704;
	// addi r28,r29,1
	r28.s64 = r29.s64 + 1;
loc_826786CC:
	// srawi r30,r3,7
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7F) != 0);
	r30.s64 = ctx.r3.s32 >> 7;
	// clrlwi r31,r3,25
	r31.u64 = ctx.r3.u32 & 0x7F;
	// addi r3,r3,96
	ctx.r3.s64 = ctx.r3.s64 + 96;
	// lbzx r27,r28,r30
	r27.u64 = PPC_LOAD_U8(r28.u32 + r30.u32);
	// lbzx r26,r30,r29
	r26.u64 = PPC_LOAD_U8(r30.u32 + r29.u32);
	// subfic r30,r31,128
	xer.ca = r31.u32 <= 128;
	r30.s64 = 128 - r31.s64;
	// mullw r31,r27,r31
	r31.s64 = int64_t(r27.s32) * int64_t(r31.s32);
	// mullw r30,r26,r30
	r30.s64 = int64_t(r26.s32) * int64_t(r30.s32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// srawi r31,r31,7
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7F) != 0);
	r31.s64 = r31.s32 >> 7;
	// stbx r31,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, r31.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// blt cr6,0x826786cc
	if (cr6.lt) goto loc_826786CC;
loc_82678704:
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x8267872c
	if (!cr6.lt) goto loc_8267872C;
loc_8267870C:
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r3,96
	ctx.r3.s64 = ctx.r3.s64 + 96;
	// srawi r31,r31,7
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7F) != 0);
	r31.s64 = r31.s32 >> 7;
	// lbzx r31,r31,r29
	r31.u64 = PPC_LOAD_U8(r31.u32 + r29.u32);
	// stbx r31,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, r31.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x8267870c
	if (cr6.lt) goto loc_8267870C;
loc_8267872C:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r25,r29,r9
	r25.u64 = r29.u64 + ctx.r9.u64;
	// mr r20,r11
	r20.u64 = r11.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82678e84
	if (!cr6.lt) goto loc_82678E84;
	// addi r23,r25,1
	r23.s64 = r25.s64 + 1;
loc_82678744:
	// clrlwi r11,r20,30
	r11.u64 = r20.u32 & 0x3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bgt cr6,0x82678e74
	if (cr6.gt) goto loc_82678E74;
	// lis r12,-32152
	r12.s64 = -2107113472;
	// addi r12,r12,-30872
	r12.s64 = r12.s64 + -30872;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_82678D70;
	case 1:
		goto loc_82678778;
	case 2:
		goto loc_82678978;
	case 3:
		goto loc_82678B78;
	default:
		__builtin_unreachable();
	}
	// lwz r19,-29328(r7)
	r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + -29328);
	// lwz r19,-30856(r7)
	r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + -30856);
	// lwz r19,-30344(r7)
	r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + -30344);
	// lwz r19,-29832(r7)
	r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + -29832);
loc_82678778:
	// li r26,0
	r26.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x826788b0
	if (!cr6.gt) goto loc_826788B0;
	// addi r11,r21,-1
	r11.s64 = r21.s64 + -1;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// rlwinm r11,r11,28,4,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r31,r11,1
	r31.s64 = r11.s64 + 1;
	// subf r28,r4,r24
	r28.s64 = r24.s64 - ctx.r4.s64;
	// rlwinm r26,r31,4,0,27
	r26.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r27,r4,r22
	r27.s64 = r22.s64 - ctx.r4.s64;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_826787AC:
	// li r19,16
	r19.s64 = 16;
	// lvlx v4,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r3,1
	r30.s64 = ctx.r3.s64 + 1;
	// li r18,16
	r18.s64 = 16;
	// add r29,r28,r5
	r29.u64 = r28.u64 + ctx.r5.u64;
	// li r17,16
	r17.s64 = 16;
	// lvrx v5,r3,r19
	temp.u32 = ctx.r3.u32 + r19.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// vor v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvlx v3,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v4,r30,r18
	temp.u32 = r30.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r30,16
	r30.s64 = 16;
	// vor v4,v3,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvlx v2,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v3,r29,r17
	temp.u32 = r29.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r29,16
	r29.s64 = 16;
	// vperm v5,v5,v5,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vor v3,v2,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// addi r3,r3,12
	ctx.r3.s64 = ctx.r3.s64 + 12;
	// vperm v4,v4,v4,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// vmrghb v2,v0,v5
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v1,v0,v4
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v4,v0,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v31,v2,v10
	// vslh v30,v5,v10
	// vslh v29,v1,v8
	// vslh v28,v4,v8
	// vslh v5,v5,v9
	// vslh v4,v4,v6
	// vslh v2,v2,v9
	// vslh v1,v1,v6
	// vaddshs v30,v30,v5
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vmrglb v5,v0,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v4,v28,v4
	// vaddshs v31,v31,v2
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vmrghb v2,v0,v3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v1,v29,v1
	// vslh v29,v5,v12
	// vaddshs v4,v30,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vor v30,v5,v5
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vaddshs v3,v31,v1
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vslh v1,v2,v12
	// vsrah v4,v4,v13
	// vsrah v5,v3,v13
	// vaddshs v2,v1,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vslh v31,v4,v11
	// vpkshus v3,v5,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vslh v1,v5,v11
	// vaddshs v4,v31,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v5,v1,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvlx v3,0,r5
	ea = ctx.r5.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v3.u8[15 - i]);
	// stvrx v3,r5,r30
	ea = ctx.r5.u32 + r30.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v3.u8[i]);
	// vaddshs v3,v29,v30
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v5,v2,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// add r30,r27,r5
	r30.u64 = r27.u64 + ctx.r5.u64;
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// vaddshs v4,v3,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsrah v5,v5,v13
	// vsrah v4,v4,v13
	// vpkshus v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvlx v5,0,r30
	ea = r30.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v5.u8[15 - i]);
	// stvrx v5,r30,r29
	ea = r30.u32 + r29.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v5.u8[i]);
	// bne cr6,0x826787ac
	if (!cr6.eq) goto loc_826787AC;
loc_826788B0:
	// rlwinm r5,r26,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r30,r11
	r30.u64 = r11.u64;
	// add r5,r26,r5
	ctx.r5.u64 = r26.u64 + ctx.r5.u64;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// rlwinm r5,r5,5,0,26
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 5) & 0xFFFFFFE0;
	// bge cr6,0x82678900
	if (!cr6.lt) goto loc_82678900;
loc_826788C8:
	// srawi r31,r5,7
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7F) != 0);
	r31.s64 = ctx.r5.s32 >> 7;
	// clrlwi r3,r5,25
	ctx.r3.u64 = ctx.r5.u32 & 0x7F;
	// addi r5,r5,96
	ctx.r5.s64 = ctx.r5.s64 + 96;
	// lbzx r28,r23,r31
	r28.u64 = PPC_LOAD_U8(r23.u32 + r31.u32);
	// lbzx r29,r31,r25
	r29.u64 = PPC_LOAD_U8(r31.u32 + r25.u32);
	// subfic r31,r3,128
	xer.ca = ctx.r3.u32 <= 128;
	r31.s64 = 128 - ctx.r3.s64;
	// mullw r3,r28,r3
	ctx.r3.s64 = int64_t(r28.s32) * int64_t(ctx.r3.s32);
	// mullw r31,r29,r31
	r31.s64 = int64_t(r29.s32) * int64_t(r31.s32);
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// srawi r3,r3,7
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7F) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 7;
	// stbx r3,r11,r4
	PPC_STORE_U8(r11.u32 + ctx.r4.u32, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// blt cr6,0x826788c8
	if (cr6.lt) goto loc_826788C8;
loc_82678900:
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x82678928
	if (!cr6.lt) goto loc_82678928;
loc_82678908:
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// addi r5,r5,96
	ctx.r5.s64 = ctx.r5.s64 + 96;
	// srawi r3,r3,7
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7F) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 7;
	// lbzx r3,r3,r25
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r25.u32);
	// stbx r3,r11,r4
	PPC_STORE_U8(r11.u32 + ctx.r4.u32, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x82678908
	if (cr6.lt) goto loc_82678908;
loc_82678928:
	// cmpw cr6,r30,r7
	cr6.compare<int32_t>(r30.s32, ctx.r7.s32, xer);
	// bge cr6,0x82678e60
	if (!cr6.lt) goto loc_82678E60;
	// add r11,r30,r24
	r11.u64 = r30.u64 + r24.u64;
	// subf r28,r24,r4
	r28.s64 = ctx.r4.s64 - r24.s64;
	// subf r27,r24,r22
	r27.s64 = r22.s64 - r24.s64;
	// subf r5,r30,r7
	ctx.r5.s64 = ctx.r7.s64 - r30.s64;
loc_82678940:
	// lbzx r3,r28,r11
	ctx.r3.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r29,r3,2
	r29.u64 = __builtin_rotateleft32(ctx.r3.u32, 2);
	// rotlwi r30,r31,1
	r30.u64 = __builtin_rotateleft32(r31.u32, 1);
	// add r3,r3,r29
	ctx.r3.u64 = ctx.r3.u64 + r29.u64;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// srawi r3,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 3;
	// stbx r3,r27,r11
	PPC_STORE_U8(r27.u32 + r11.u32, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x82678940
	if (!cr6.eq) goto loc_82678940;
	// b 0x82678e60
	goto loc_82678E60;
loc_82678978:
	// li r26,0
	r26.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x82678ab0
	if (!cr6.gt) goto loc_82678AB0;
	// addi r11,r21,-1
	r11.s64 = r21.s64 + -1;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// rlwinm r11,r11,28,4,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r31,r11,1
	r31.s64 = r11.s64 + 1;
	// subf r28,r4,r24
	r28.s64 = r24.s64 - ctx.r4.s64;
	// rlwinm r26,r31,4,0,27
	r26.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r27,r4,r22
	r27.s64 = r22.s64 - ctx.r4.s64;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_826789AC:
	// li r19,16
	r19.s64 = 16;
	// lvlx v4,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r3,1
	r30.s64 = ctx.r3.s64 + 1;
	// li r18,16
	r18.s64 = 16;
	// add r29,r5,r28
	r29.u64 = ctx.r5.u64 + r28.u64;
	// li r17,16
	r17.s64 = 16;
	// lvrx v5,r3,r19
	temp.u32 = ctx.r3.u32 + r19.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// vor v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvlx v3,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v4,r30,r18
	temp.u32 = r30.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r30,16
	r30.s64 = 16;
	// vor v4,v3,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvlx v2,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v3,r29,r17
	temp.u32 = r29.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r29,16
	r29.s64 = 16;
	// vperm v5,v5,v5,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vor v3,v2,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// addi r3,r3,12
	ctx.r3.s64 = ctx.r3.s64 + 12;
	// vperm v4,v4,v4,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// vmrghb v2,v0,v5
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v1,v0,v4
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v4,v0,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v31,v2,v10
	// vslh v30,v5,v10
	// vslh v29,v1,v8
	// vslh v28,v4,v8
	// vslh v5,v5,v9
	// vslh v4,v4,v6
	// vslh v2,v2,v9
	// vslh v1,v1,v6
	// vaddshs v30,v30,v5
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vmrglb v5,v0,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v4,v28,v4
	// vaddshs v31,v31,v2
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vmrghb v2,v0,v3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v1,v29,v1
	// vslh v29,v5,v11
	// vaddshs v4,v30,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vor v30,v5,v5
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vaddshs v3,v31,v1
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vslh v1,v2,v11
	// vsrah v4,v4,v13
	// vsrah v5,v3,v13
	// vaddshs v2,v1,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vslh v31,v4,v12
	// vpkshus v3,v5,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vslh v1,v5,v12
	// vaddshs v4,v31,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v5,v1,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvlx v3,0,r5
	ea = ctx.r5.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v3.u8[15 - i]);
	// stvrx v3,r5,r30
	ea = ctx.r5.u32 + r30.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v3.u8[i]);
	// vaddshs v3,v29,v30
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v5,v2,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// add r30,r5,r27
	r30.u64 = ctx.r5.u64 + r27.u64;
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// vaddshs v4,v3,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsrah v5,v5,v13
	// vsrah v4,v4,v13
	// vpkshus v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvlx v5,0,r30
	ea = r30.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v5.u8[15 - i]);
	// stvrx v5,r30,r29
	ea = r30.u32 + r29.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v5.u8[i]);
	// bne cr6,0x826789ac
	if (!cr6.eq) goto loc_826789AC;
loc_82678AB0:
	// rlwinm r5,r26,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r30,r11
	r30.u64 = r11.u64;
	// add r5,r26,r5
	ctx.r5.u64 = r26.u64 + ctx.r5.u64;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// rlwinm r5,r5,5,0,26
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 5) & 0xFFFFFFE0;
	// bge cr6,0x82678b00
	if (!cr6.lt) goto loc_82678B00;
loc_82678AC8:
	// srawi r31,r5,7
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7F) != 0);
	r31.s64 = ctx.r5.s32 >> 7;
	// clrlwi r3,r5,25
	ctx.r3.u64 = ctx.r5.u32 & 0x7F;
	// addi r5,r5,96
	ctx.r5.s64 = ctx.r5.s64 + 96;
	// lbzx r28,r23,r31
	r28.u64 = PPC_LOAD_U8(r23.u32 + r31.u32);
	// lbzx r29,r31,r25
	r29.u64 = PPC_LOAD_U8(r31.u32 + r25.u32);
	// subfic r31,r3,128
	xer.ca = ctx.r3.u32 <= 128;
	r31.s64 = 128 - ctx.r3.s64;
	// mullw r3,r28,r3
	ctx.r3.s64 = int64_t(r28.s32) * int64_t(ctx.r3.s32);
	// mullw r31,r29,r31
	r31.s64 = int64_t(r29.s32) * int64_t(r31.s32);
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// srawi r3,r3,7
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7F) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 7;
	// stbx r3,r11,r4
	PPC_STORE_U8(r11.u32 + ctx.r4.u32, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// blt cr6,0x82678ac8
	if (cr6.lt) goto loc_82678AC8;
loc_82678B00:
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x82678b28
	if (!cr6.lt) goto loc_82678B28;
loc_82678B08:
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// addi r5,r5,96
	ctx.r5.s64 = ctx.r5.s64 + 96;
	// srawi r3,r3,7
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7F) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 7;
	// lbzx r3,r3,r25
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r25.u32);
	// stbx r3,r11,r4
	PPC_STORE_U8(r11.u32 + ctx.r4.u32, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x82678b08
	if (cr6.lt) goto loc_82678B08;
loc_82678B28:
	// cmpw cr6,r30,r7
	cr6.compare<int32_t>(r30.s32, ctx.r7.s32, xer);
	// bge cr6,0x82678e60
	if (!cr6.lt) goto loc_82678E60;
	// add r11,r30,r24
	r11.u64 = r30.u64 + r24.u64;
	// subf r28,r24,r4
	r28.s64 = ctx.r4.s64 - r24.s64;
	// subf r27,r24,r22
	r27.s64 = r22.s64 - r24.s64;
	// subf r5,r30,r7
	ctx.r5.s64 = ctx.r7.s64 - r30.s64;
loc_82678B40:
	// lbzx r3,r28,r11
	ctx.r3.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r29,r3,1
	r29.u64 = __builtin_rotateleft32(ctx.r3.u32, 1);
	// rotlwi r30,r31,2
	r30.u64 = __builtin_rotateleft32(r31.u32, 2);
	// add r3,r3,r29
	ctx.r3.u64 = ctx.r3.u64 + r29.u64;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// srawi r3,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 3;
	// stbx r3,r27,r11
	PPC_STORE_U8(r27.u32 + r11.u32, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x82678b40
	if (!cr6.eq) goto loc_82678B40;
	// b 0x82678e60
	goto loc_82678E60;
loc_82678B78:
	// li r26,0
	r26.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x82678cb0
	if (!cr6.gt) goto loc_82678CB0;
	// addi r11,r21,-1
	r11.s64 = r21.s64 + -1;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// rlwinm r11,r11,28,4,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r31,r11,1
	r31.s64 = r11.s64 + 1;
	// subf r28,r4,r24
	r28.s64 = r24.s64 - ctx.r4.s64;
	// rlwinm r26,r31,4,0,27
	r26.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r27,r4,r22
	r27.s64 = r22.s64 - ctx.r4.s64;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_82678BAC:
	// li r19,16
	r19.s64 = 16;
	// lvlx v4,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r3,1
	r30.s64 = ctx.r3.s64 + 1;
	// li r18,16
	r18.s64 = 16;
	// add r29,r28,r5
	r29.u64 = r28.u64 + ctx.r5.u64;
	// li r17,16
	r17.s64 = 16;
	// lvrx v5,r3,r19
	temp.u32 = ctx.r3.u32 + r19.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// vor v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvlx v3,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v4,r30,r18
	temp.u32 = r30.u32 + r18.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r30,16
	r30.s64 = 16;
	// vor v4,v3,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvlx v2,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v3,r29,r17
	temp.u32 = r29.u32 + r17.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r29,16
	r29.s64 = 16;
	// vperm v5,v5,v5,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vor v3,v2,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// addi r3,r3,12
	ctx.r3.s64 = ctx.r3.s64 + 12;
	// vperm v4,v4,v4,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// vmrghb v2,v0,v5
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v1,v0,v4
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v4,v0,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v31,v2,v10
	// vslh v30,v5,v10
	// vslh v29,v1,v8
	// vslh v2,v2,v9
	// vslh v28,v4,v8
	// vslh v5,v5,v9
	// vslh v1,v1,v6
	// vslh v4,v4,v6
	// vaddshs v2,v31,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v31,v30,v5
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vmrghb v5,v0,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v1,v29,v1
	// vsubshs v30,v28,v4
	// vmrglb v4,v0,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v3,v2,v1
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vslh v30,v4,v11
	// vslh v29,v4,v12
	// vor v28,v4,v4
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vsrah v4,v3,v13
	// vsrah v3,v2,v13
	// vslh v31,v5,v12
	// vslh v1,v5,v11
	// vpkshus v2,v4,v3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v1,v1,v31
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v31,v30,v29
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v5,v1,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvlx v2,0,r5
	ea = ctx.r5.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v2.u8[15 - i]);
	// stvrx v2,r5,r30
	ea = ctx.r5.u32 + r30.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v2.u8[i]);
	// vaddshs v2,v31,v28
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v28.s16)));
	// add r30,r27,r5
	r30.u64 = r27.u64 + ctx.r5.u64;
	// vaddshs v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// vaddshs v4,v2,v3
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsrah v5,v5,v13
	// vsrah v4,v4,v13
	// vpkshus v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvlx v5,0,r30
	ea = r30.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v5.u8[15 - i]);
	// stvrx v5,r30,r29
	ea = r30.u32 + r29.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v5.u8[i]);
	// bne cr6,0x82678bac
	if (!cr6.eq) goto loc_82678BAC;
loc_82678CB0:
	// rlwinm r5,r26,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r28,r11
	r28.u64 = r11.u64;
	// add r5,r26,r5
	ctx.r5.u64 = r26.u64 + ctx.r5.u64;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// rlwinm r5,r5,5,0,26
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 5) & 0xFFFFFFE0;
	// bge cr6,0x82678d00
	if (!cr6.lt) goto loc_82678D00;
loc_82678CC8:
	// srawi r31,r5,7
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7F) != 0);
	r31.s64 = ctx.r5.s32 >> 7;
	// clrlwi r3,r5,25
	ctx.r3.u64 = ctx.r5.u32 & 0x7F;
	// addi r5,r5,96
	ctx.r5.s64 = ctx.r5.s64 + 96;
	// lbzx r29,r23,r31
	r29.u64 = PPC_LOAD_U8(r23.u32 + r31.u32);
	// lbzx r30,r31,r25
	r30.u64 = PPC_LOAD_U8(r31.u32 + r25.u32);
	// subfic r31,r3,128
	xer.ca = ctx.r3.u32 <= 128;
	r31.s64 = 128 - ctx.r3.s64;
	// mullw r3,r29,r3
	ctx.r3.s64 = int64_t(r29.s32) * int64_t(ctx.r3.s32);
	// mullw r31,r30,r31
	r31.s64 = int64_t(r30.s32) * int64_t(r31.s32);
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// srawi r3,r3,7
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7F) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 7;
	// stbx r3,r11,r4
	PPC_STORE_U8(r11.u32 + ctx.r4.u32, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// blt cr6,0x82678cc8
	if (cr6.lt) goto loc_82678CC8;
loc_82678D00:
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x82678d28
	if (!cr6.lt) goto loc_82678D28;
loc_82678D08:
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// addi r5,r5,96
	ctx.r5.s64 = ctx.r5.s64 + 96;
	// srawi r3,r3,7
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7F) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 7;
	// lbzx r3,r3,r25
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r25.u32);
	// stbx r3,r11,r4
	PPC_STORE_U8(r11.u32 + ctx.r4.u32, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x82678d08
	if (cr6.lt) goto loc_82678D08;
loc_82678D28:
	// cmpw cr6,r28,r7
	cr6.compare<int32_t>(r28.s32, ctx.r7.s32, xer);
	// bge cr6,0x82678e74
	if (!cr6.lt) goto loc_82678E74;
	// add r11,r28,r4
	r11.u64 = r28.u64 + ctx.r4.u64;
	// subf r30,r4,r24
	r30.s64 = r24.s64 - ctx.r4.s64;
	// subf r29,r4,r22
	r29.s64 = r22.s64 - ctx.r4.s64;
	// subf r5,r28,r7
	ctx.r5.s64 = ctx.r7.s64 - r28.s64;
loc_82678D40:
	// lbzx r3,r30,r11
	ctx.r3.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r28,r3,3
	r28.u64 = __builtin_rotateleft32(ctx.r3.u32, 3);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// subf r3,r3,r28
	ctx.r3.s64 = r28.s64 - ctx.r3.s64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// srawi r3,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 3;
	// stbx r3,r29,r11
	PPC_STORE_U8(r29.u32 + r11.u32, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x82678d40
	if (!cr6.eq) goto loc_82678D40;
	// b 0x82678e74
	goto loc_82678E74;
loc_82678D70:
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x82678e1c
	if (!cr6.gt) goto loc_82678E1C;
	// addi r5,r21,-1
	ctx.r5.s64 = r21.s64 + -1;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// rlwinm r5,r5,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 28) & 0xFFFFFFF;
	// subf r31,r4,r24
	r31.s64 = r24.s64 - ctx.r4.s64;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// subf r30,r4,r22
	r30.s64 = r22.s64 - ctx.r4.s64;
	// rlwinm r28,r5,4,0,27
	r28.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
loc_82678D98:
	// li r29,16
	r29.s64 = 16;
	// lvlx v4,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// li r27,16
	r27.s64 = 16;
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lvrx v5,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// vor v4,v4,v5
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvlx v3,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v5,r3,r27
	temp.u32 = ctx.r3.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r3,r30,r11
	ctx.r3.u64 = r30.u64 + r11.u64;
	// vor v3,v3,v5
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// vmrghb v5,v0,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v4,v0,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v1,v0,v3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v31,v0,v3
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v3,v5,v11
	// vslh v2,v4,v11
	// vslh v30,v5,v12
	// vslh v29,v4,v12
	// vaddshs v5,v3,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v4,v2,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v5,v5,v30
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v4,v4,v29
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v5,v1,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v4,v31,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsrah v5,v5,v13
	// vsrah v4,v4,v13
	// vpkshus v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvlx v5,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v5.u8[15 - i]);
	// stvrx v5,r3,r29
	ea = ctx.r3.u32 + r29.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v5.u8[i]);
	// bne cr6,0x82678d98
	if (!cr6.eq) goto loc_82678D98;
loc_82678E1C:
	// cmpw cr6,r28,r7
	cr6.compare<int32_t>(r28.s32, ctx.r7.s32, xer);
	// bge cr6,0x82678e60
	if (!cr6.lt) goto loc_82678E60;
	// add r11,r28,r24
	r11.u64 = r28.u64 + r24.u64;
	// subf r30,r24,r4
	r30.s64 = ctx.r4.s64 - r24.s64;
	// subf r29,r24,r22
	r29.s64 = r22.s64 - r24.s64;
	// subf r5,r28,r7
	ctx.r5.s64 = ctx.r7.s64 - r28.s64;
loc_82678E34:
	// lbzx r3,r30,r11
	ctx.r3.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r28,r3,3
	r28.u64 = __builtin_rotateleft32(ctx.r3.u32, 3);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// subf r3,r3,r28
	ctx.r3.s64 = r28.s64 - ctx.r3.s64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// srawi r3,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 3;
	// stbx r3,r29,r11
	PPC_STORE_U8(r29.u32 + r11.u32, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x82678e34
	if (!cr6.eq) goto loc_82678E34;
loc_82678E60:
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r24,r11
	r24.u64 = r11.u64;
	// add r25,r25,r9
	r25.u64 = r25.u64 + ctx.r9.u64;
	// add r23,r23,r9
	r23.u64 = r23.u64 + ctx.r9.u64;
loc_82678E74:
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// add r22,r22,r8
	r22.u64 = r22.u64 + ctx.r8.u64;
	// cmpw cr6,r20,r10
	cr6.compare<int32_t>(r20.s32, ctx.r10.s32, xer);
	// blt cr6,0x82678744
	if (cr6.lt) goto loc_82678744;
loc_82678E84:
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_82678E88"))) PPC_WEAK_FUNC(sub_82678E88);
PPC_FUNC_IMPL(__imp__sub_82678E88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r23,r9
	r23.u64 = ctx.r9.u64;
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// add r19,r5,r7
	r19.u64 = ctx.r5.u64 + ctx.r7.u64;
	// lwz r8,100(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r14,r5
	r14.u64 = ctx.r5.u64;
	// stw r24,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r24.u32);
	// li r26,0
	r26.s64 = 0;
	// stw r23,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, r23.u32);
	// li r27,0
	r27.s64 = 0;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// mr r17,r19
	r17.u64 = r19.u64;
	// bge cr6,0x826790f4
	if (!cr6.lt) goto loc_826790F4;
	// add r9,r24,r23
	ctx.r9.u64 = r24.u64 + r23.u64;
	// lwz r18,132(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, ctx.r9.u32);
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r11,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r11.u32);
loc_82678EE4:
	// srawi r11,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	r11.s64 = ctx.r8.s32 >> 8;
	// lwz r10,-156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
	// clrlwi r16,r8,24
	r16.u64 = ctx.r8.u32 & 0xFF;
	// mullw r11,r11,r23
	r11.s64 = int64_t(r11.s32) * int64_t(r23.s32);
	// add r20,r11,r24
	r20.u64 = r11.u64 + r24.u64;
	// subfic r15,r16,256
	xer.ca = r16.u32 <= 256;
	r15.s64 = 256 - r16.s64;
	// add r28,r10,r11
	r28.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r20,r27
	cr6.compare<uint32_t>(r20.u32, r27.u32, xer);
	// bne cr6,0x82678fa4
	if (!cr6.eq) goto loc_82678FA4;
	// mr r25,r14
	r25.u64 = r14.u64;
	// mr r14,r17
	r14.u64 = r17.u64;
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// mr r17,r25
	r17.u64 = r25.u64;
	// ble cr6,0x82678f74
	if (!cr6.gt) goto loc_82678F74;
	// addi r27,r28,1
	r27.s64 = r28.s64 + 1;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// subf r26,r25,r9
	r26.s64 = ctx.r9.s64 - r25.s64;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
loc_82678F38:
	// lwz r31,0(r4)
	r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbzx r30,r26,r11
	r30.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lbzx r21,r27,r31
	r21.u64 = PPC_LOAD_U8(r27.u32 + r31.u32);
	// lbzx r22,r31,r28
	r22.u64 = PPC_LOAD_U8(r31.u32 + r28.u32);
	// subfic r31,r30,128
	xer.ca = r30.u32 <= 128;
	r31.s64 = 128 - r30.s64;
	// mullw r30,r21,r30
	r30.s64 = int64_t(r21.s32) * int64_t(r30.s32);
	// mullw r31,r22,r31
	r31.s64 = int64_t(r22.s32) * int64_t(r31.s32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// srawi r31,r31,7
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7F) != 0);
	r31.s64 = r31.s32 >> 7;
	// stb r31,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r31.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x82678f38
	if (!cr6.eq) goto loc_82678F38;
loc_82678F74:
	// cmpw cr6,r29,r7
	cr6.compare<int32_t>(r29.s32, ctx.r7.s32, xer);
	// bge cr6,0x82679084
	if (!cr6.lt) goto loc_82679084;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r18
	r11.u64 = r11.u64 + r18.u64;
loc_82678F84:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lbzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r28.u32);
	// stbx r10,r29,r25
	PPC_STORE_U8(r29.u32 + r25.u32, ctx.r10.u8);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmpw cr6,r29,r7
	cr6.compare<int32_t>(r29.s32, ctx.r7.s32, xer);
	// blt cr6,0x82678f84
	if (cr6.lt) goto loc_82678F84;
	// b 0x82679084
	goto loc_82679084;
loc_82678FA4:
	// cmplw cr6,r20,r26
	cr6.compare<uint32_t>(r20.u32, r26.u32, xer);
	// beq cr6,0x82679084
	if (cr6.eq) goto loc_82679084;
	// mr r14,r5
	r14.u64 = ctx.r5.u64;
	// mr r17,r19
	r17.u64 = r19.u64;
	// li r21,0
	r21.s64 = 0;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x82679040
	if (!cr6.gt) goto loc_82679040;
	// addi r25,r20,1
	r25.s64 = r20.s64 + 1;
	// addi r24,r28,1
	r24.s64 = r28.s64 + 1;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r30,r18
	r30.u64 = r18.u64;
	// subf r23,r5,r9
	r23.s64 = ctx.r9.s64 - ctx.r5.s64;
	// subf r22,r5,r19
	r22.s64 = r19.s64 - ctx.r5.s64;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
loc_82678FE0:
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// lbzx r4,r23,r11
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + r11.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// subfic r29,r4,128
	xer.ca = ctx.r4.u32 <= 128;
	r29.s64 = 128 - ctx.r4.s64;
	// lbzx r27,r25,r10
	r27.u64 = PPC_LOAD_U8(r25.u32 + ctx.r10.u32);
	// lbzx r26,r10,r20
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + r20.u32);
	// mullw r27,r27,r4
	r27.s64 = int64_t(r27.s32) * int64_t(ctx.r4.s32);
	// mullw r26,r26,r29
	r26.s64 = int64_t(r26.s32) * int64_t(r29.s32);
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// srawi r27,r27,7
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7F) != 0);
	r27.s64 = r27.s32 >> 7;
	// stb r27,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r27.u8);
	// lbzx r27,r24,r10
	r27.u64 = PPC_LOAD_U8(r24.u32 + ctx.r10.u32);
	// lbzx r26,r10,r28
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + r28.u32);
	// mullw r10,r27,r4
	ctx.r10.s64 = int64_t(r27.s32) * int64_t(ctx.r4.s32);
	// mullw r4,r26,r29
	ctx.r4.s64 = int64_t(r26.s32) * int64_t(r29.s32);
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// srawi r10,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 7;
	// stbx r10,r22,r11
	PPC_STORE_U8(r22.u32 + r11.u32, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x82678fe0
	if (!cr6.eq) goto loc_82678FE0;
	// lwz r23,68(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// lwz r24,20(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
loc_82679040:
	// cmpw cr6,r21,r7
	cr6.compare<int32_t>(r21.s32, ctx.r7.s32, xer);
	// bge cr6,0x82679084
	if (!cr6.lt) goto loc_82679084;
	// rlwinm r10,r21,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r21,r19
	r11.u64 = r21.u64 + r19.u64;
	// add r4,r10,r18
	ctx.r4.u64 = ctx.r10.u64 + r18.u64;
	// subf r30,r19,r5
	r30.s64 = ctx.r5.s64 - r19.s64;
	// subf r10,r21,r7
	ctx.r10.s64 = ctx.r7.s64 - r21.s64;
loc_8267905C:
	// lwz r31,0(r4)
	r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lbzx r29,r31,r20
	r29.u64 = PPC_LOAD_U8(r31.u32 + r20.u32);
	// stbx r29,r30,r11
	PPC_STORE_U8(r30.u32 + r11.u32, r29.u8);
	// lbzx r31,r31,r28
	r31.u64 = PPC_LOAD_U8(r31.u32 + r28.u32);
	// stb r31,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r31.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8267905c
	if (!cr6.eq) goto loc_8267905C;
loc_82679084:
	// mr r26,r20
	r26.u64 = r20.u64;
	// mr r27,r28
	r27.u64 = r28.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x826790d0
	if (!cr6.gt) goto loc_826790D0;
	// mr r11,r17
	r11.u64 = r17.u64;
	// subf r30,r17,r14
	r30.s64 = r14.s64 - r17.s64;
	// subf r29,r17,r3
	r29.s64 = ctx.r3.s64 - r17.s64;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
loc_826790A4:
	// lbzx r4,r30,r11
	ctx.r4.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mullw r4,r4,r15
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r15.s32);
	// mullw r31,r31,r16
	r31.s64 = int64_t(r31.s32) * int64_t(r16.s32);
	// add r4,r4,r31
	ctx.r4.u64 = ctx.r4.u64 + r31.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// srawi r4,r4,8
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 8;
	// stbx r4,r29,r11
	PPC_STORE_U8(r29.u32 + r11.u32, ctx.r4.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x826790a4
	if (!cr6.eq) goto loc_826790A4;
loc_826790D0:
	// lwz r11,-160(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// lwz r10,60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r3,r3,r10
	ctx.r3.u64 = ctx.r3.u64 + ctx.r10.u64;
	// lwz r10,108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r11,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r11.u32);
	// bne cr6,0x82678ee4
	if (!cr6.eq) goto loc_82678EE4;
loc_826790F4:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826790F8"))) PPC_WEAK_FUNC(sub_826790F8);
PPC_FUNC_IMPL(__imp__sub_826790F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-1104(r1)
	ea = -1104 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vspltisb v6,-1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// clrlwi r30,r8,30
	r30.u64 = ctx.r8.u32 & 0x3;
	// clrlwi r11,r9,30
	r11.u64 = ctx.r9.u32 & 0x3;
	// vspltish v11,1
	// vspltisb v5,15
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_set1_epi8(char(0xF)));
	// stw r4,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r4.u32);
	// stw r5,1140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1140, ctx.r5.u32);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// vslb v9,v6,v6
	ctx.v9.u8[0] = ctx.v6.u8[0] << (ctx.v6.u8[0] & 0x7);
	ctx.v9.u8[1] = ctx.v6.u8[1] << (ctx.v6.u8[1] & 0x7);
	ctx.v9.u8[2] = ctx.v6.u8[2] << (ctx.v6.u8[2] & 0x7);
	ctx.v9.u8[3] = ctx.v6.u8[3] << (ctx.v6.u8[3] & 0x7);
	ctx.v9.u8[4] = ctx.v6.u8[4] << (ctx.v6.u8[4] & 0x7);
	ctx.v9.u8[5] = ctx.v6.u8[5] << (ctx.v6.u8[5] & 0x7);
	ctx.v9.u8[6] = ctx.v6.u8[6] << (ctx.v6.u8[6] & 0x7);
	ctx.v9.u8[7] = ctx.v6.u8[7] << (ctx.v6.u8[7] & 0x7);
	ctx.v9.u8[8] = ctx.v6.u8[8] << (ctx.v6.u8[8] & 0x7);
	ctx.v9.u8[9] = ctx.v6.u8[9] << (ctx.v6.u8[9] & 0x7);
	ctx.v9.u8[10] = ctx.v6.u8[10] << (ctx.v6.u8[10] & 0x7);
	ctx.v9.u8[11] = ctx.v6.u8[11] << (ctx.v6.u8[11] & 0x7);
	ctx.v9.u8[12] = ctx.v6.u8[12] << (ctx.v6.u8[12] & 0x7);
	ctx.v9.u8[13] = ctx.v6.u8[13] << (ctx.v6.u8[13] & 0x7);
	ctx.v9.u8[14] = ctx.v6.u8[14] << (ctx.v6.u8[14] & 0x7);
	ctx.v9.u8[15] = ctx.v6.u8[15] << (ctx.v6.u8[15] & 0x7);
	// stw r6,1148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1148, ctx.r6.u32);
	// stw r7,1156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1156, ctx.r7.u32);
	// vspltish v0,2
	// stw r30,1164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1164, r30.u32);
	// vspltish v8,3
	// stw r11,1172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1172, r11.u32);
	// vspltish v13,4
	// vavgsh v16,v6,v9
	_mm_store_si128((__m128i*)v16.u8, _mm_avg_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vspltisb v12,0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_set1_epi8(char(0x0)));
	// vspltish v10,5
	// vslb v17,v5,v5
	v17.u8[0] = ctx.v5.u8[0] << (ctx.v5.u8[0] & 0x7);
	v17.u8[1] = ctx.v5.u8[1] << (ctx.v5.u8[1] & 0x7);
	v17.u8[2] = ctx.v5.u8[2] << (ctx.v5.u8[2] & 0x7);
	v17.u8[3] = ctx.v5.u8[3] << (ctx.v5.u8[3] & 0x7);
	v17.u8[4] = ctx.v5.u8[4] << (ctx.v5.u8[4] & 0x7);
	v17.u8[5] = ctx.v5.u8[5] << (ctx.v5.u8[5] & 0x7);
	v17.u8[6] = ctx.v5.u8[6] << (ctx.v5.u8[6] & 0x7);
	v17.u8[7] = ctx.v5.u8[7] << (ctx.v5.u8[7] & 0x7);
	v17.u8[8] = ctx.v5.u8[8] << (ctx.v5.u8[8] & 0x7);
	v17.u8[9] = ctx.v5.u8[9] << (ctx.v5.u8[9] & 0x7);
	v17.u8[10] = ctx.v5.u8[10] << (ctx.v5.u8[10] & 0x7);
	v17.u8[11] = ctx.v5.u8[11] << (ctx.v5.u8[11] & 0x7);
	v17.u8[12] = ctx.v5.u8[12] << (ctx.v5.u8[12] & 0x7);
	v17.u8[13] = ctx.v5.u8[13] << (ctx.v5.u8[13] & 0x7);
	v17.u8[14] = ctx.v5.u8[14] << (ctx.v5.u8[14] & 0x7);
	v17.u8[15] = ctx.v5.u8[15] << (ctx.v5.u8[15] & 0x7);
	// vspltish v7,6
	// vsubuhm v15,v16,v11
	_mm_store_si128((__m128i*)v15.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// bne cr6,0x826797a4
	if (!cr6.eq) goto loc_826797A4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826791e0
	if (!cr6.eq) goto loc_826791E0;
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8267a7bc
	if (!cr6.gt) goto loc_8267A7BC;
	// b 0x82679188
	goto loc_82679188;
loc_8267917C:
	// lwz r6,1148(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// lwz r5,1140(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	// lwz r4,1132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
loc_82679188:
	// lwz r9,1156(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// mullw r10,r11,r5
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// lvlx v13,r4,r10
	temp.u32 = ctx.r4.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// lvrx v0,r4,r9
	temp.u32 = ctx.r4.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvlx v0,r6,r11
	ea = ctx.r6.u32 + r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r10,1156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,1148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stvrx v0,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// blt cr6,0x8267917c
	if (cr6.lt) goto loc_8267917C;
	// addi r1,r1,1104
	ctx.r1.s64 = ctx.r1.s64 + 1104;
	// b 0x8239bd24
	return;
loc_826791E0:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x826795a4
	if (cr6.eq) goto loc_826795A4;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826793f8
	if (cr6.eq) goto loc_826793F8;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x8267a7bc
	if (!cr6.eq) goto loc_8267A7BC;
	// lwz r11,1188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82679210
	if (cr6.eq) goto loc_82679210;
	// vspltish v9,8
	// vslh v25,v9,v0
	// b 0x82679218
	goto loc_82679218;
loc_82679210:
	// vspltish v9,-5
	// vsrh v25,v9,v9
loc_82679218:
	// subf r11,r5,r4
	r11.s64 = ctx.r4.s64 - ctx.r5.s64;
	// lwz r7,1196(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vmrghb v8,v12,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v9,v12,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,352
	ctx.r10.s64 = ctx.r1.s64 + 352;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,64
	ctx.r10.s64 = ctx.r1.s64 + 64;
	// lvrx v9,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v8,v12,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// vmrglb v9,v12,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,368
	ctx.r10.s64 = ctx.r1.s64 + 368;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r5
	ctx.r10.u64 = r11.u64 + ctx.r5.u64;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,0
	r11.s64 = 0;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// vmrghb v8,v12,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// vmrglb v9,v12,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v8,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,384
	ctx.r9.s64 = ctx.r1.s64 + 384;
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x8267a7bc
	if (!cr6.gt) goto loc_8267A7BC;
	// b 0x826792d0
	goto loc_826792D0;
loc_826792C8:
	// lwz r5,1140(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	// lwz r10,1132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
loc_826792D0:
	// li r7,16
	ctx.r7.s64 = 16;
	// lvlx v9,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,384
	ctx.r8.s64 = ctx.r1.s64 + 384;
	// addi r4,r1,64
	ctx.r4.s64 = ctx.r1.s64 + 64;
	// lvrx v8,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// vor v6,v9,v8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// addi r3,r1,368
	ctx.r3.s64 = ctx.r1.s64 + 368;
	// lvx128 v9,r9,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,352
	ctx.r9.s64 = ctx.r1.s64 + 352;
	// lvx128 v8,r8,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v2,v9,v0
	// vslh v1,v8,v0
	// lvx128 v5,r4,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// lvx128 v4,r3,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v31,v9,v13
	// vslh v27,v9,v10
	// vmrghb v3,v12,v6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vadduhm v2,v2,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrglb v6,v12,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v30,v8,v13
	// addi r31,r1,96
	r31.s64 = ctx.r1.s64 + 96;
	// vslh v26,v8,v10
	// lvx128 v9,r11,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v1,v1,v8
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// lvx128 v8,r11,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v29,v5,v13
	// addi r7,r1,400
	ctx.r7.s64 = ctx.r1.s64 + 400;
	// vslh v28,v4,v13
	// vslh v5,v5,v11
	// stvx128 v3,r31,r11
	_mm_store_si128((__m128i*)(base + ((r31.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v4,v4,v11
	// vadduhm v29,v29,v9
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v28,v28,v8
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// stvx128 v6,r7,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v31,v31,v25
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v25.u16)));
	// li r11,16
	r11.s64 = 16;
	// vadduhm v2,v27,v2
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v30,v30,v25
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v1,v26,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v9,v9,v3
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v8,v8,v6
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v3,v31,v2
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v6,v30,v1
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v5,v5,v29
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v4,v4,v28
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v9,v9,v0
	// vslh v8,v8,v0
	// vadduhm v5,v3,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v6,v6,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vsubuhm v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vsubuhm v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vsrah v9,v9,v7
	// vsrah v8,v8,v7
	// vpkshus v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvlx v9,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// lwz r10,1148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// stvrx v9,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r10,1156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// lwz r9,1148(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// stw r6,1148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1148, ctx.r6.u32);
	// blt cr6,0x826792c8
	if (cr6.lt) goto loc_826792C8;
	// addi r1,r1,1104
	ctx.r1.s64 = ctx.r1.s64 + 1104;
	// b 0x8239bd24
	return;
loc_826793F8:
	// lwz r11,1188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	// vspltish v3,8
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8267940c
	if (!cr6.eq) goto loc_8267940C;
	// vspltish v3,7
loc_8267940C:
	// subf r11,r5,r4
	r11.s64 = ctx.r4.s64 - ctx.r5.s64;
	// lwz r7,1196(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// lvlx v0,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v11,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// vor v0,v0,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vmrghb v11,v12,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// stvx v11,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,352
	ctx.r10.s64 = ctx.r1.s64 + 352;
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,64
	ctx.r10.s64 = ctx.r1.s64 + 64;
	// lvrx v0,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vor v0,v11,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v12,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// vmrglb v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v11,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,368
	ctx.r10.s64 = ctx.r1.s64 + 368;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r5
	ctx.r10.u64 = r11.u64 + ctx.r5.u64;
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v0,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,0
	r11.s64 = 0;
	// vor v0,v11,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// vmrghb v11,v12,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// vmrglb v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,384
	ctx.r9.s64 = ctx.r1.s64 + 384;
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x8267a7bc
	if (!cr6.gt) goto loc_8267A7BC;
	// b 0x826794c4
	goto loc_826794C4;
loc_826794BC:
	// lwz r5,1140(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	// lwz r10,1132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
loc_826794C4:
	// li r9,16
	ctx.r9.s64 = 16;
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,64
	ctx.r7.s64 = ctx.r1.s64 + 64;
	// addi r4,r1,384
	ctx.r4.s64 = ctx.r1.s64 + 384;
	// lvrx v11,r10,r9
	temp.u32 = ctx.r10.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r9,r1,368
	ctx.r9.s64 = ctx.r1.s64 + 368;
	// vor v0,v0,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// addi r3,r1,48
	ctx.r3.s64 = ctx.r1.s64 + 48;
	// lvx128 v11,r8,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r1,352
	r31.s64 = ctx.r1.s64 + 352;
	// lvx128 v10,r7,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r1,400
	r29.s64 = ctx.r1.s64 + 400;
	// vadduhm v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvx128 v10,r4,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r9,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r1,96
	r30.s64 = ctx.r1.s64 + 96;
	// vadduhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghb v9,v12,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvx128 v5,r11,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r11,r31
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v7,v11,v8
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// vslh v6,v10,v8
	// vadduhm v4,v4,v0
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v5,v5,v9
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx128 v9,r30,r11
	_mm_store_si128((__m128i*)(base + ((r30.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r29,r11
	_mm_store_si128((__m128i*)(base + ((r29.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v0,v7,v11
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v11,v6,v10
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// vsubuhm v10,v3,v5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// li r11,16
	r11.s64 = 16;
	// vsubuhm v9,v3,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vadduhm v0,v0,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsrah v0,v0,v13
	// vsrah v11,v11,v13
	// vpkshus v0,v0,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvlx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r10,1148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// stvrx v0,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r10,1156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// lwz r9,1148(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// stw r6,1148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1148, ctx.r6.u32);
	// blt cr6,0x826794bc
	if (cr6.lt) goto loc_826794BC;
	// addi r1,r1,1104
	ctx.r1.s64 = ctx.r1.s64 + 1104;
	// b 0x8239bd24
	return;
loc_826795A4:
	// lwz r11,1188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826795bc
	if (cr6.eq) goto loc_826795BC;
	// vspltish v9,8
	// vslh v25,v9,v0
	// b 0x826795c4
	goto loc_826795C4;
loc_826795BC:
	// vspltish v9,-5
	// vsrh v25,v9,v9
loc_826795C4:
	// subf r11,r5,r4
	r11.s64 = ctx.r4.s64 - ctx.r5.s64;
	// lwz r7,1196(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vmrghb v8,v12,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v9,v12,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,352
	ctx.r10.s64 = ctx.r1.s64 + 352;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,64
	ctx.r10.s64 = ctx.r1.s64 + 64;
	// lvrx v9,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v8,v12,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// vmrglb v9,v12,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,368
	ctx.r10.s64 = ctx.r1.s64 + 368;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r5
	ctx.r10.u64 = r11.u64 + ctx.r5.u64;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,0
	r11.s64 = 0;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// vmrghb v8,v12,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// vmrglb v9,v12,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v8,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,384
	ctx.r9.s64 = ctx.r1.s64 + 384;
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x8267a7bc
	if (!cr6.gt) goto loc_8267A7BC;
	// b 0x8267967c
	goto loc_8267967C;
loc_82679674:
	// lwz r5,1140(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	// lwz r10,1132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
loc_8267967C:
	// li r7,16
	ctx.r7.s64 = 16;
	// lvlx v9,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r1,64
	ctx.r9.s64 = ctx.r1.s64 + 64;
	// addi r8,r1,368
	ctx.r8.s64 = ctx.r1.s64 + 368;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lvrx v8,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// vor v5,v9,v8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// lvx128 v9,r9,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,352
	ctx.r9.s64 = ctx.r1.s64 + 352;
	// lvx128 v8,r8,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v2,v9,v0
	// vslh v1,v8,v0
	// lvx128 v4,r4,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v3,r3,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v31,v9,v13
	// vslh v30,v8,v13
	// vmrghb v6,v12,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v27,v9,v10
	// vmrglb v5,v12,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v26,v8,v10
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// vadduhm v9,v2,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// vadduhm v8,v1,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// addi r31,r1,96
	r31.s64 = ctx.r1.s64 + 96;
	// vslh v29,v4,v13
	// addi r7,r1,400
	ctx.r7.s64 = ctx.r1.s64 + 400;
	// vslh v28,v3,v13
	// lvx128 v23,r11,r9
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v2,v31,v25
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v31,v30,v25
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v25.u16)));
	// lvx128 v24,r11,r10
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v9,v27,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx128 v6,r31,r11
	_mm_store_si128((__m128i*)(base + ((r31.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v8,v26,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// stvx128 v5,r7,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v1,v29,v6
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// li r11,16
	r11.s64 = 16;
	// vadduhm v30,v28,v5
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v4,v4,v11
	// vslh v3,v3,v11
	// vadduhm v9,v2,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v8,v31,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v6,v24,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v5,v23,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v4,v4,v1
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v3,v3,v30
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v6,v6,v0
	// vslh v5,v5,v0
	// vadduhm v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v8,v8,v3
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vsubuhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsubuhm v8,v8,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v9,v9,v7
	// vsrah v8,v8,v7
	// vpkshus v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvlx v9,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// lwz r10,1148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// stvrx v9,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r10,1156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// lwz r9,1148(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// stw r6,1148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1148, ctx.r6.u32);
	// blt cr6,0x82679674
	if (cr6.lt) goto loc_82679674;
	// addi r1,r1,1104
	ctx.r1.s64 = ctx.r1.s64 + 1104;
	// b 0x8239bd24
	return;
loc_826797A4:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82679c50
	if (!cr6.eq) goto loc_82679C50;
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// beq cr6,0x82679ab0
	if (cr6.eq) goto loc_82679AB0;
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// beq cr6,0x82679964
	if (cr6.eq) goto loc_82679964;
	// cmpwi cr6,r30,3
	cr6.compare<int32_t>(r30.s32, 3, xer);
	// bne cr6,0x8267a7bc
	if (!cr6.eq) goto loc_8267A7BC;
	// lwz r11,1188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826797dc
	if (cr6.eq) goto loc_826797DC;
	// vspltish v9,-5
	// vsrh v31,v9,v9
	// b 0x826797e4
	goto loc_826797E4;
loc_826797DC:
	// vspltish v9,8
	// vslh v31,v9,v0
loc_826797E4:
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r4,-1
	r11.s64 = ctx.r4.s64 + -1;
	// stw r10,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r10.u32);
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8267a7bc
	if (!cr6.gt) goto loc_8267A7BC;
loc_82679800:
	// li r9,16
	ctx.r9.s64 = 16;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// lvrx v9,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,48
	r11.s64 = ctx.r1.s64 + 48;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v6,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v6,v12,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v5,v12,v9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vsldoi v4,v9,v8,1
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), 15));
	// vsldoi v3,v9,v8,2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), 14));
	// vsldoi v2,v9,v8,3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), 13));
	// vmrghb v1,v12,v4
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,352
	r11.s64 = ctx.r1.s64 + 352;
	// vmrghb v9,v12,v3
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v8,v12,v3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v4,v12,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v26,v1,v13
	// vmrghb v3,v12,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,64
	r11.s64 = ctx.r1.s64 + 64;
	// vslh v30,v9,v0
	// vmrglb v2,v12,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v29,v8,v0
	// vslh v28,v9,v13
	// vslh v25,v9,v10
	// stvx v1,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vslh v27,v8,v13
	// vslh v24,v8,v10
	// vslh v1,v1,v11
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,384
	r11.s64 = ctx.r1.s64 + 384;
	// vadduhm v9,v30,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v30,v4,v13
	// vadduhm v27,v27,v31
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v31.u16)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,368
	r11.s64 = ctx.r1.s64 + 368;
	// vadduhm v8,v29,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v29,v28,v31
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v28,v26,v6
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v6,v6,v3
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// stvx v4,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// vadduhm v9,v25,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v8,v24,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v4,v4,v11
	// vslh v6,v6,v0
	// stvx v3,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,400
	r11.s64 = ctx.r1.s64 + 400;
	// vadduhm v3,v5,v2
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v5,v30,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v9,v29,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v8,v27,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// stvx v2,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v2,v1,v28
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// li r11,16
	r11.s64 = 16;
	// vslh v4,v3,v0
	// vadduhm v9,v9,v2
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v8,v8,v5
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vsubuhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsubuhm v8,v8,v4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vsrah v9,v9,v7
	// vsrah v8,v8,v7
	// vpkshus v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvlx v9,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// lwz r10,1148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// stvrx v9,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r9,1132(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,1140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,1156(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// stw r10,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r10.u32);
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// lwz r8,1148(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// add r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r9,1196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stw r6,1148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1148, ctx.r6.u32);
	// blt cr6,0x82679800
	if (cr6.lt) goto loc_82679800;
	// addi r1,r1,1104
	ctx.r1.s64 = ctx.r1.s64 + 1104;
	// b 0x8239bd24
	return;
loc_82679964:
	// lwz r11,1188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	// vspltish v7,7
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82679978
	if (!cr6.eq) goto loc_82679978;
	// vspltish v7,8
loc_82679978:
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r4,-1
	r11.s64 = ctx.r4.s64 + -1;
	// stw r10,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r10.u32);
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8267a7bc
	if (!cr6.gt) goto loc_8267A7BC;
loc_82679994:
	// li r9,16
	ctx.r9.s64 = 16;
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// lvrx v0,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,48
	r11.s64 = ctx.r1.s64 + 48;
	// vor v0,v11,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v10,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v11,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrghb v6,v12,v0
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v5,v12,v0
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vsldoi v10,v0,v11,1
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v11.u8), 15));
	// vsldoi v9,v0,v11,2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v11.u8), 14));
	// vsldoi v0,v0,v11,3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v11.u8), 13));
	// vmrghb v11,v12,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,352
	r11.s64 = ctx.r1.s64 + 352;
	// vmrghb v4,v12,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v10,v12,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v9,v12,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v3,v12,v0
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,64
	r11.s64 = ctx.r1.s64 + 64;
	// vmrglb v2,v12,v0
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vadduhm v0,v11,v4
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vadduhm v11,v10,v9
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx v4,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,368
	r11.s64 = ctx.r1.s64 + 368;
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,384
	r11.s64 = ctx.r1.s64 + 384;
	// vadduhm v10,v6,v3
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v6,v0,v8
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v9,v5,v2
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v5,v11,v8
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// vadduhm v0,v6,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsubuhm v10,v7,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsubuhm v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vadduhm v11,v5,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v3,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,400
	r11.s64 = ctx.r1.s64 + 400;
	// vadduhm v0,v0,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsrah v0,v0,v13
	// stvx v2,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,16
	r11.s64 = 16;
	// vsrah v11,v11,v13
	// vpkshus v0,v0,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvlx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r10,1148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// stvrx v0,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r9,1132(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,1140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	// lwz r8,1148(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,1156(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// add r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r9,1196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// stw r10,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r10.u32);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// stw r6,1148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1148, ctx.r6.u32);
	// blt cr6,0x82679994
	if (cr6.lt) goto loc_82679994;
	// addi r1,r1,1104
	ctx.r1.s64 = ctx.r1.s64 + 1104;
	// b 0x8239bd24
	return;
loc_82679AB0:
	// lwz r11,1188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82679ac8
	if (cr6.eq) goto loc_82679AC8;
	// vspltish v9,-5
	// vsrh v2,v9,v9
	// b 0x82679ad0
	goto loc_82679AD0;
loc_82679AC8:
	// vspltish v9,8
	// vslh v2,v9,v0
loc_82679AD0:
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r4,-1
	r11.s64 = ctx.r4.s64 + -1;
	// stw r10,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r10.u32);
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8267a7bc
	if (!cr6.gt) goto loc_8267A7BC;
loc_82679AEC:
	// li r9,16
	ctx.r9.s64 = 16;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// lvrx v9,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,48
	r11.s64 = ctx.r1.s64 + 48;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v6,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v1,v12,v9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v31,v12,v9
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vsldoi v6,v9,v8,1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), 15));
	// vsldoi v5,v9,v8,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), 14));
	// vsldoi v4,v9,v8,3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), 13));
	// vmrghb v9,v12,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v1,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,352
	r11.s64 = ctx.r1.s64 + 352;
	// vmrghb v3,v12,v5
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v8,v12,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v6,v12,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v30,v9,v0
	// vmrghb v5,v12,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v28,v9,v13
	// stvx v31,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,64
	r11.s64 = ctx.r1.s64 + 64;
	// vslh v29,v8,v0
	// vmrglb v4,v12,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v26,v3,v13
	// vslh v25,v9,v10
	// vslh v27,v8,v13
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vadduhm v9,v30,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v24,v8,v10
	// vslh v30,v6,v13
	// vadduhm v27,v27,v2
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// stvx v3,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,368
	r11.s64 = ctx.r1.s64 + 368;
	// vadduhm v9,v25,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v3,v3,v11
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,384
	r11.s64 = ctx.r1.s64 + 384;
	// vadduhm v8,v29,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v29,v28,v2
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v28,v26,v5
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// vadduhm v8,v24,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v6,v6,v11
	// vadduhm v9,v29,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v3,v3,v28
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v28.u16)));
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,400
	r11.s64 = ctx.r1.s64 + 400;
	// vadduhm v5,v1,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v1,v31,v4
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v8,v27,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v9,v9,v3
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// stvx v4,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v4,v30,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vslh v5,v5,v0
	// li r11,16
	r11.s64 = 16;
	// vadduhm v6,v6,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vslh v4,v1,v0
	// vsubuhm v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vadduhm v8,v8,v6
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vsrah v9,v9,v7
	// vsubuhm v8,v8,v4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vsrah v8,v8,v7
	// vpkshus v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvlx v9,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// lwz r10,1148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// stvrx v9,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r9,1132(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,1140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,1156(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// stw r10,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r10.u32);
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// lwz r8,1148(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// add r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r9,1196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stw r6,1148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1148, ctx.r6.u32);
	// blt cr6,0x82679aec
	if (cr6.lt) goto loc_82679AEC;
	// addi r1,r1,1104
	ctx.r1.s64 = ctx.r1.s64 + 1104;
	// b 0x8239bd24
	return;
loc_82679C50:
	// lwz r29,1188(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	// clrlwi r10,r30,31
	ctx.r10.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x82679ca4
	if (cr6.eq) goto loc_82679CA4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82679c88
	if (cr6.eq) goto loc_82679C88;
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82679c80
	if (cr6.eq) goto loc_82679C80;
	// vspltish v9,8
	// vadduhm v9,v9,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// b 0x82679cdc
	goto loc_82679CDC;
loc_82679C80:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82679c9c
	if (!cr6.eq) goto loc_82679C9C;
loc_82679C88:
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82679c9c
	if (!cr6.eq) goto loc_82679C9C;
	// vor v9,v11,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// b 0x82679cdc
	goto loc_82679CDC;
loc_82679C9C:
	// vor v9,v13,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// b 0x82679cdc
	goto loc_82679CDC;
loc_82679CA4:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82679cc8
	if (cr6.eq) goto loc_82679CC8;
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82679cc0
	if (cr6.eq) goto loc_82679CC0;
	// vspltish v9,15
	// b 0x82679cdc
	goto loc_82679CDC;
loc_82679CC0:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82679cd8
	if (!cr6.eq) goto loc_82679CD8;
loc_82679CC8:
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// vspltish v9,0
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82679cdc
	if (cr6.eq) goto loc_82679CDC;
loc_82679CD8:
	// vor v9,v8,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
loc_82679CDC:
	// lwz r9,1196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x8267a1e8
	if (cr6.eq) goto loc_8267A1E8;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x82679fc4
	if (cr6.eq) goto loc_82679FC4;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x8267a474
	if (!cr6.eq) goto loc_8267A474;
	// subf r11,r5,r4
	r11.s64 = ctx.r4.s64 - ctx.r5.s64;
	// li r8,16
	ctx.r8.s64 = 16;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// li r3,16
	ctx.r3.s64 = 16;
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// lvrx v7,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v6,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v5,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,48
	ctx.r7.s64 = ctx.r1.s64 + 48;
	// vor v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// li r31,16
	r31.s64 = 16;
	// vmrghb v5,v12,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// vmrglb v7,v12,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vmrghb v6,v12,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,688
	ctx.r7.s64 = ctx.r1.s64 + 688;
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,352
	ctx.r7.s64 = ctx.r1.s64 + 352;
	// stvx v6,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,64
	ctx.r7.s64 = ctx.r1.s64 + 64;
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v5,r10,r3
	temp.u32 = ctx.r10.u32 + ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v6,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// vor v6,v6,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v5,v12,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// vmrglb v7,v12,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v6,v12,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,704
	ctx.r7.s64 = ctx.r1.s64 + 704;
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,368
	ctx.r7.s64 = ctx.r1.s64 + 368;
	// stvx v6,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvrx v7,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r8,0
	ctx.r8.s64 = 0;
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v5,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// vor v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// stw r8,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r8.u32);
	// vmrghb v5,v12,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v7,v12,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r5
	ctx.r10.u64 = r11.u64 + ctx.r5.u64;
	// addi r11,r1,720
	r11.s64 = ctx.r1.s64 + 720;
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v7,v12,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// addi r11,r1,384
	r11.s64 = ctx.r1.s64 + 384;
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x8267a474
	if (!cr6.gt) goto loc_8267A474;
loc_82679E00:
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx v7,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// li r3,16
	ctx.r3.s64 = 16;
	// rlwinm r11,r8,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r31,r1,80
	r31.s64 = ctx.r1.s64 + 80;
	// lvrx v6,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r27,r1,64
	r27.s64 = ctx.r1.s64 + 64;
	// vor v4,v7,v6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvlx v6,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r7,r3
	temp.u32 = ctx.r7.u32 + ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r4,r1,384
	ctx.r4.s64 = ctx.r1.s64 + 384;
	// vor v31,v6,v7
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// addi r7,r1,48
	ctx.r7.s64 = ctx.r1.s64 + 48;
	// lvx128 v7,r31,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,720
	r28.s64 = ctx.r1.s64 + 720;
	// vmrghb v1,v12,v4
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v29,v7,v0
	// vmrglb v4,v12,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// vmrghb v31,v12,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// addi r26,r1,704
	r26.s64 = ctx.r1.s64 + 704;
	// addi r25,r1,96
	r25.s64 = ctx.r1.s64 + 96;
	// lvx128 v3,r27,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r27.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r24,r1,736
	r24.s64 = ctx.r1.s64 + 736;
	// lvx128 v5,r4,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r23,r1,400
	r23.s64 = ctx.r1.s64 + 400;
	// vslh v23,v3,v13
	// addi r31,r1,368
	r31.s64 = ctx.r1.s64 + 368;
	// lvx128 v6,r28,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r1,688
	ctx.r4.s64 = ctx.r1.s64 + 688;
	// vslh v26,v7,v13
	// addi r3,r1,352
	ctx.r3.s64 = ctx.r1.s64 + 352;
	// vadduhm v29,v29,v7
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v21,v7,v10
	// lvx128 v2,r26,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r4,r4,r11
	ctx.r4.u64 = ctx.r4.u64 + r11.u64;
	// lvx128 v30,r31,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v28,v6,v0
	// stvx128 v1,r25,r11
	_mm_store_si128((__m128i*)(base + ((r25.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v23,v23,v7
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// stvx128 v4,r24,r11
	_mm_store_si128((__m128i*)(base + ((r24.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v7,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// stvx128 v31,r23,r11
	_mm_store_si128((__m128i*)(base + ((r23.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// addi r3,r1,672
	ctx.r3.s64 = ctx.r1.s64 + 672;
	// vslh v22,v2,v13
	// vslh v25,v6,v13
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// vslh v20,v6,v10
	// vadduhm v28,v28,v6
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvx128 v6,r0,r4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v27,v5,v0
	// stvx v7,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,656
	ctx.r3.s64 = ctx.r1.s64 + 656;
	// vadduhm v22,v22,v6
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// vadduhm v6,v6,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vslh v18,v30,v13
	// vslh v24,v5,v13
	// vslh v19,v5,v10
	// vadduhm v27,v27,v5
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v6,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,32
	ctx.r3.s64 = ctx.r1.s64 + 32;
	// vadduhm v18,v18,v5
	_mm_store_si128((__m128i*)v18.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v18.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v5,v5,v31
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vslh v3,v3,v11
	// vslh v2,v2,v11
	// vslh v30,v30,v11
	// vadduhm v26,v26,v9
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx v5,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v25,v25,v9
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v24,v24,v9
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v4,v21,v29
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v1,v20,v28
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v20.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v31,v19,v27
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v19.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v3,v3,v23
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v2,v2,v22
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vadduhm v30,v30,v18
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v18.u16)));
	// vadduhm v4,v26,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// addi r3,r1,672
	ctx.r3.s64 = ctx.r1.s64 + 672;
	// vslh v29,v7,v0
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vslh v7,v7,v0
	// vadduhm v1,v25,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vadduhm v4,v4,v3
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v31,v24,v31
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v31.u16)));
	// stvx v29,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v28,v6,v0
	// addi r3,r1,656
	ctx.r3.s64 = ctx.r1.s64 + 656;
	// vslh v27,v5,v0
	// stw r8,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r8.u32);
	// vsubuhm v7,v4,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vadduhm v3,v1,v2
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v6,v6,v0
	// vslh v5,v5,v0
	// stvx v28,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v2,v31,v30
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// addi r3,r1,32
	ctx.r3.s64 = ctx.r1.s64 + 32;
	// vsrah v7,v7,v8
	// vsubuhm v6,v3,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsubuhm v5,v2,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v7,v6,v8
	// stvx v27,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v6,v5,v8
	// stvx v7,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82679e00
	if (cr6.lt) goto loc_82679E00;
	// b 0x8267a474
	goto loc_8267A474;
loc_82679FC4:
	// subf r11,r5,r4
	r11.s64 = ctx.r4.s64 - ctx.r5.s64;
	// li r8,16
	ctx.r8.s64 = 16;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// li r3,16
	ctx.r3.s64 = 16;
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// lvrx v7,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v6,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v5,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,48
	ctx.r7.s64 = ctx.r1.s64 + 48;
	// vor v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// li r31,16
	r31.s64 = 16;
	// vmrghb v5,v12,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// vmrglb v7,v12,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vmrghb v6,v12,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,688
	ctx.r7.s64 = ctx.r1.s64 + 688;
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,352
	ctx.r7.s64 = ctx.r1.s64 + 352;
	// stvx v6,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,64
	ctx.r7.s64 = ctx.r1.s64 + 64;
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v5,r10,r3
	temp.u32 = ctx.r10.u32 + ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v6,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// vor v6,v6,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v5,v12,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// vmrglb v7,v12,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v6,v12,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,704
	ctx.r7.s64 = ctx.r1.s64 + 704;
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,368
	ctx.r7.s64 = ctx.r1.s64 + 368;
	// stvx v6,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvrx v7,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r8,0
	ctx.r8.s64 = 0;
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v5,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// vor v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// stw r8,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r8.u32);
	// vmrghb v5,v12,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v7,v12,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r5
	ctx.r10.u64 = r11.u64 + ctx.r5.u64;
	// addi r11,r1,720
	r11.s64 = ctx.r1.s64 + 720;
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v7,v12,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// addi r11,r1,384
	r11.s64 = ctx.r1.s64 + 384;
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x8267a474
	if (!cr6.gt) goto loc_8267A474;
loc_8267A0CC:
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx v7,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// li r28,16
	r28.s64 = 16;
	// rlwinm r11,r8,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r27,r1,80
	r27.s64 = ctx.r1.s64 + 80;
	// lvrx v6,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r26,r1,64
	r26.s64 = ctx.r1.s64 + 64;
	// vor v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvlx v5,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r7,r28
	temp.u32 = ctx.r7.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r25,r1,720
	r25.s64 = ctx.r1.s64 + 720;
	// addi r24,r1,704
	r24.s64 = ctx.r1.s64 + 704;
	// vor v2,v5,v6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v6,r27,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r27.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r23,r1,384
	r23.s64 = ctx.r1.s64 + 384;
	// lvx128 v5,r26,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r22,r1,368
	r22.s64 = ctx.r1.s64 + 368;
	// vadduhm v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// addi r4,r1,48
	ctx.r4.s64 = ctx.r1.s64 + 48;
	// lvx128 v4,r25,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r1,688
	r31.s64 = ctx.r1.s64 + 688;
	// lvx128 v5,r24,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,352
	ctx.r3.s64 = ctx.r1.s64 + 352;
	// vadduhm v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghb v4,v12,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v7,v12,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// add r7,r31,r11
	ctx.r7.u64 = r31.u64 + r11.u64;
	// vmrghb v2,v12,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// lvx128 v1,r22,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r21,r1,96
	r21.s64 = ctx.r1.s64 + 96;
	// lvx128 v3,r23,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r23.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r20,r1,736
	r20.s64 = ctx.r1.s64 + 736;
	// vadduhm v3,v1,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// addi r19,r1,400
	r19.s64 = ctx.r1.s64 + 400;
	// lvx128 v1,r0,r4
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r7
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v1,v1,v4
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// lvx128 v29,r0,r3
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v31,v31,v7
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// stvx128 v4,r21,r11
	_mm_store_si128((__m128i*)(base + ((r21.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v4,v5,v8
	// stvx128 v7,r20,r11
	_mm_store_si128((__m128i*)(base + ((r20.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v29,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// stvx128 v2,r19,r11
	_mm_store_si128((__m128i*)(base + ((r19.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v30,v6,v8
	// vslh v2,v3,v8
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vadduhm v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// vsubuhm v7,v9,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vadduhm v6,v30,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vsubuhm v1,v9,v1
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vsubuhm v31,v9,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v31.u8)));
	// stw r8,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r8.u32);
	// vadduhm v4,v2,v3
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// vadduhm v6,v6,v1
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v5,v5,v31
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v7,v4,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsrah v6,v6,v11
	// vsrah v5,v5,v11
	// vsrah v7,v7,v11
	// stvx v6,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x8267a0cc
	if (cr6.lt) goto loc_8267A0CC;
	// b 0x8267a474
	goto loc_8267A474;
loc_8267A1E8:
	// subf r11,r5,r4
	r11.s64 = ctx.r4.s64 - ctx.r5.s64;
	// li r8,16
	ctx.r8.s64 = 16;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// li r3,16
	ctx.r3.s64 = 16;
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// lvrx v7,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v6,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v5,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,48
	ctx.r7.s64 = ctx.r1.s64 + 48;
	// vor v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// li r31,16
	r31.s64 = 16;
	// vmrghb v5,v12,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// vmrglb v7,v12,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vmrghb v6,v12,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,688
	ctx.r7.s64 = ctx.r1.s64 + 688;
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,352
	ctx.r7.s64 = ctx.r1.s64 + 352;
	// stvx v6,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,64
	ctx.r7.s64 = ctx.r1.s64 + 64;
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v5,r10,r3
	temp.u32 = ctx.r10.u32 + ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v6,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// vor v6,v6,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v5,v12,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, r11.u32);
	// vmrglb v7,v12,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v6,v12,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,704
	ctx.r7.s64 = ctx.r1.s64 + 704;
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,368
	ctx.r7.s64 = ctx.r1.s64 + 368;
	// stvx v6,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvrx v7,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r8,0
	ctx.r8.s64 = 0;
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvlx v5,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// vor v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// stw r8,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r8.u32);
	// vmrghb v5,v12,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrglb v7,v12,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r5
	ctx.r10.u64 = r11.u64 + ctx.r5.u64;
	// addi r11,r1,720
	r11.s64 = ctx.r1.s64 + 720;
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v7,v12,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// addi r11,r1,384
	r11.s64 = ctx.r1.s64 + 384;
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x8267a474
	if (!cr6.gt) goto loc_8267A474;
loc_8267A2F0:
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx v7,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// li r3,16
	ctx.r3.s64 = 16;
	// rlwinm r11,r8,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r28,r1,704
	r28.s64 = ctx.r1.s64 + 704;
	// lvrx v6,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r4,r1,368
	ctx.r4.s64 = ctx.r1.s64 + 368;
	// addi r31,r1,64
	r31.s64 = ctx.r1.s64 + 64;
	// vor v3,v7,v6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvrx v7,r7,r3
	temp.u32 = ctx.r7.u32 + ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r27,r1,80
	r27.s64 = ctx.r1.s64 + 80;
	// lvlx v6,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r26,r1,720
	r26.s64 = ctx.r1.s64 + 720;
	// vor v2,v6,v7
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v6,r28,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r4,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r1,384
	ctx.r4.s64 = ctx.r1.s64 + 384;
	// lvx128 v7,r31,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v28,v6,v0
	// vslh v25,v6,v13
	// vmrghb v4,v12,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v29,v7,v0
	// lvx128 v1,r27,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r27.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v27,v5,v0
	// lvx128 v31,r26,r11
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v20,v6,v10
	// lvx128 v30,r4,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v26,v7,v13
	// vmrglb v3,v12,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vadduhm v6,v28,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrghb v2,v12,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v21,v7,v10
	// addi r25,r1,96
	r25.s64 = ctx.r1.s64 + 96;
	// vslh v24,v5,v13
	// addi r24,r1,736
	r24.s64 = ctx.r1.s64 + 736;
	// vadduhm v7,v29,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// addi r23,r1,400
	r23.s64 = ctx.r1.s64 + 400;
	// vslh v19,v5,v10
	// addi r7,r1,48
	ctx.r7.s64 = ctx.r1.s64 + 48;
	// vadduhm v28,v25,v9
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// addi r4,r1,688
	ctx.r4.s64 = ctx.r1.s64 + 688;
	// vadduhm v5,v27,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// vslh v23,v1,v13
	// addi r3,r1,352
	ctx.r3.s64 = ctx.r1.s64 + 352;
	// vslh v22,v31,v13
	// stvx128 v4,r25,r11
	_mm_store_si128((__m128i*)(base + ((r25.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v25,v30,v13
	// stvx128 v3,r24,r11
	_mm_store_si128((__m128i*)(base + ((r24.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v29,v26,v9
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx128 v2,r23,r11
	_mm_store_si128((__m128i*)(base + ((r23.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v27,v24,v9
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// add r4,r4,r11
	ctx.r4.u64 = ctx.r4.u64 + r11.u64;
	// vadduhm v7,v21,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// vadduhm v26,v23,v4
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// vadduhm v24,v22,v3
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v6,v20,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v20.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v5,v19,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v19.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v25,v25,v2
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v1,v1,v11
	// stw r10,1132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1132, ctx.r10.u32);
	// vslh v31,v31,v11
	// vslh v30,v30,v11
	// vadduhm v7,v29,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// lvx128 v29,r0,r7
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v6,v28,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v1,v1,v26
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vadduhm v31,v31,v24
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v30,v30,v25
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v5,v27,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v4,v29,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// lvx128 v29,r0,r4
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v3,v29,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v2,v28,v2
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v7,v7,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v6,v6,v31
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v5,v5,v30
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v4,v4,v0
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vslh v3,v3,v0
	// vslh v2,v2,v0
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vsubuhm v7,v7,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vsubuhm v6,v6,v3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// stw r8,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r8.u32);
	// vsubuhm v5,v5,v2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vsrah v7,v7,v8
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v7,v6,v8
	// vsrah v6,v5,v8
	// stvx v7,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x8267a2f0
	if (cr6.lt) goto loc_8267A2F0;
loc_8267A474:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// vor v9,v15,v15
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v15.u8));
	// bne cr6,0x8267a484
	if (!cr6.eq) goto loc_8267A484;
	// vor v9,v16,v16
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v16.u8));
loc_8267A484:
	// vspltish v7,7
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// beq cr6,0x8267a698
	if (cr6.eq) goto loc_8267A698;
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// beq cr6,0x8267a5d4
	if (cr6.eq) goto loc_8267A5D4;
	// cmpwi cr6,r30,3
	cr6.compare<int32_t>(r30.s32, 3, xer);
	// bne cr6,0x8267a7bc
	if (!cr6.eq) goto loc_8267A7BC;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// ble cr6,0x8267a7bc
	if (!cr6.gt) goto loc_8267A7BC;
loc_8267A4B0:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r1,688
	ctx.r9.s64 = ctx.r1.s64 + 688;
	// addi r8,r1,352
	ctx.r8.s64 = ctx.r1.s64 + 352;
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// lvx128 v12,r9,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r11,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v5,v12,v0
	// lvx128 v8,r11,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v3,v12,v6,4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8), 12));
	// vsldoi v4,v8,v12,4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 12));
	// li r11,16
	r11.s64 = 16;
	// vsldoi v31,v12,v6,2
	_mm_store_si128((__m128i*)v31.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8), 14));
	// vsldoi v30,v12,v6,6
	_mm_store_si128((__m128i*)v30.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8), 10));
	// vsrah v6,v8,v0
	// vsldoi v2,v8,v12,2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 14));
	// vor v23,v5,v5
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vsldoi v1,v8,v12,6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 10));
	// vsrah v8,v3,v0
	// vsrah v12,v4,v0
	// vsrah v3,v31,v0
	// vsrah v4,v2,v0
	// vslh v29,v8,v0
	// vslh v31,v12,v0
	// vsrah v2,v1,v0
	// vslh v28,v8,v13
	// vslh v24,v8,v10
	// vsrah v1,v30,v0
	// vslh v30,v12,v13
	// vslh v25,v12,v10
	// vadduhm v8,v29,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v12,v31,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vslh v27,v4,v13
	// vslh v26,v3,v13
	// vor v29,v6,v6
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v31,v30,v9
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v6,v6,v2
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v5,v5,v1
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v12,v25,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v8,v24,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v4,v4,v11
	// vslh v3,v3,v11
	// vadduhm v2,v28,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v1,v27,v29
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v30,v26,v23
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v12,v31,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v8,v2,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v4,v4,v1
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v3,v3,v30
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v6,v6,v0
	// vslh v5,v5,v0
	// vadduhm v12,v12,v4
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v8,v8,v3
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vsubuhm v12,v12,v6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsubuhm v8,v8,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v12,v12,v7
	// vsrah v8,v8,v7
	// vpkshss v12,v12,v8
	// vaddubm v12,v12,v17
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v17.u8)));
	// stvlx v12,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// lwz r10,1148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// stvrx v12,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r10,1156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// lwz r9,1148(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// stw r6,1148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1148, ctx.r6.u32);
	// blt cr6,0x8267a4b0
	if (cr6.lt) goto loc_8267A4B0;
	// addi r1,r1,1104
	ctx.r1.s64 = ctx.r1.s64 + 1104;
	// b 0x8239bd24
	return;
loc_8267A5D4:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// ble cr6,0x8267a7bc
	if (!cr6.gt) goto loc_8267A7BC;
loc_8267A5E4:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// addi r9,r1,688
	ctx.r9.s64 = ctx.r1.s64 + 688;
	// addi r8,r1,352
	ctx.r8.s64 = ctx.r1.s64 + 352;
	// lvx128 v13,r11,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r9,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r11,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v11,v13,v0,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v10,v13,v0,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// li r11,16
	r11.s64 = 16;
	// vsldoi v6,v0,v12,2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 14));
	// vsldoi v5,v0,v12,4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 12));
	// vsldoi v3,v0,v12,6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 10));
	// vadduhm v12,v11,v10
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsldoi v4,v13,v0,6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vadduhm v11,v6,v5
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v0,v0,v3
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v13,v13,v4
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vslh v10,v12,v8
	// vslh v6,v11,v8
	// vsubuhm v0,v9,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubuhm v13,v9,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v11,v6,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vadduhm v0,v11,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsrah v13,v13,v7
	// vsrah v0,v0,v7
	// vpkshss v0,v13,v0
	// vaddubm v0,v0,v17
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v17.u8)));
	// stvlx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r10,1148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// stvrx v0,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r10,1156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// lwz r9,1148(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// stw r6,1148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1148, ctx.r6.u32);
	// blt cr6,0x8267a5e4
	if (cr6.lt) goto loc_8267A5E4;
	// addi r1,r1,1104
	ctx.r1.s64 = ctx.r1.s64 + 1104;
	// b 0x8239bd24
	return;
loc_8267A698:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// ble cr6,0x8267a7bc
	if (!cr6.gt) goto loc_8267A7BC;
loc_8267A6A8:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// addi r9,r1,688
	ctx.r9.s64 = ctx.r1.s64 + 688;
	// addi r8,r1,352
	ctx.r8.s64 = ctx.r1.s64 + 352;
	// lvx128 v8,r11,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r9,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v30,v8,v0
	// lvx128 v6,r11,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v5,v8,v12,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 14));
	// vsldoi v4,v12,v6,2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8), 14));
	// vsrah v29,v12,v0
	// vsldoi v3,v8,v12,4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 12));
	// li r11,16
	r11.s64 = 16;
	// vsldoi v2,v8,v12,6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 10));
	// vsldoi v1,v12,v6,4
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8), 12));
	// vsldoi v31,v12,v6,6
	_mm_store_si128((__m128i*)v31.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8), 10));
	// vsrah v12,v5,v0
	// vsrah v8,v4,v0
	// vsrah v6,v3,v0
	// vsrah v5,v2,v0
	// vslh v2,v12,v0
	// vsrah v3,v31,v0
	// vslh v31,v8,v0
	// vsrah v4,v1,v0
	// vslh v1,v12,v13
	// vslh v25,v12,v10
	// vadduhm v12,v2,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vslh v28,v8,v13
	// vslh v24,v8,v10
	// vslh v27,v6,v13
	// vadduhm v8,v31,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v26,v4,v13
	// vadduhm v2,v1,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v1,v30,v5
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v12,v25,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vslh v6,v6,v11
	// vadduhm v31,v29,v3
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v5,v27,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v8,v24,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v4,v4,v11
	// vadduhm v30,v28,v9
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v3,v26,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v12,v2,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v6,v6,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v8,v30,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v5,v4,v3
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v4,v1,v0
	// vadduhm v12,v12,v6
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v31,v0
	// vadduhm v8,v8,v5
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vsubuhm v12,v12,v4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vsubuhm v8,v8,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsrah v12,v12,v7
	// vsrah v8,v8,v7
	// vpkshss v12,v12,v8
	// vaddubm v12,v12,v17
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v17.u8)));
	// stvlx v12,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// lwz r10,1148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// stvrx v12,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// lwz r10,1156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	// lwz r9,1148(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r10,1196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, r11.u32);
	// stw r6,1148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1148, ctx.r6.u32);
	// blt cr6,0x8267a6a8
	if (cr6.lt) goto loc_8267A6A8;
loc_8267A7BC:
	// addi r1,r1,1104
	ctx.r1.s64 = ctx.r1.s64 + 1104;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_8267A7C4"))) PPC_WEAK_FUNC(sub_8267A7C4);
PPC_FUNC_IMPL(__imp__sub_8267A7C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267A7C8"))) PPC_WEAK_FUNC(sub_8267A7C8);
PPC_FUNC_IMPL(__imp__sub_8267A7C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lhz r11,0(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// lhz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267A7D8"))) PPC_WEAK_FUNC(sub_8267A7D8);
PPC_FUNC_IMPL(__imp__sub_8267A7D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-496(r1)
	ea = -496 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x823bb4e8
	sub_823BB4E8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x8267a824
	if (!cr0.eq) goto loc_8267A824;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r3,r11,-13920
	ctx.r3.s64 = r11.s64 + -13920;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// lwz r5,5860(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 5860);
	// lis r11,-32137
	r11.s64 = -2106130432;
	// lwz r4,5864(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 5864);
	// bl 0x8267b148
	sub_8267B148(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// bge 0x8267a824
	if (!cr0.lt) goto loc_8267A824;
	// li r3,1627
	ctx.r3.s64 = 1627;
loc_8267A824:
	// addi r1,r1,496
	ctx.r1.s64 = ctx.r1.s64 + 496;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267A834"))) PPC_WEAK_FUNC(sub_8267A834);
PPC_FUNC_IMPL(__imp__sub_8267A834) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267A838"))) PPC_WEAK_FUNC(sub_8267A838);
PPC_FUNC_IMPL(__imp__sub_8267A838) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x823bb4f8
	sub_823BB4F8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267A85C"))) PPC_WEAK_FUNC(sub_8267A85C);
PPC_FUNC_IMPL(__imp__sub_8267A85C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267A860"))) PPC_WEAK_FUNC(sub_8267A860);
PPC_FUNC_IMPL(__imp__sub_8267A860) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// addi r11,r11,-13920
	r11.s64 = r11.s64 + -13920;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r4,44(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne 0x8267a898
	if (!cr0.eq) goto loc_8267A898;
	// lis r3,-32747
	ctx.r3.s64 = -2146107392;
	// ori r3,r3,5
	ctx.r3.u64 = ctx.r3.u64 | 5;
	// b 0x8267a8d4
	goto loc_8267A8D4;
loc_8267A898:
	// lhz r5,40(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 40);
	// lis r11,-32152
	r11.s64 = -2107113472;
	// li r6,4
	ctx.r6.s64 = 4;
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// addi r7,r11,-22584
	ctx.r7.s64 = r11.s64 + -22584;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826a8200
	sub_826A8200(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x8267a8c8
	if (!cr0.eq) goto loc_8267A8C8;
	// lis r3,-32747
	ctx.r3.s64 = -2146107392;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x8267a8d4
	goto loc_8267A8D4;
loc_8267A8C8:
	// lhz r11,2(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_8267A8D4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267A8E8"))) PPC_WEAK_FUNC(sub_8267A8E8);
PPC_FUNC_IMPL(__imp__sub_8267A8E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// addi r3,r10,-13920
	ctx.r3.s64 = ctx.r10.s64 + -13920;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// bl 0x8267b0f0
	sub_8267B0F0(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x8267a97c
	if (cr0.lt) goto loc_8267A97C;
	// rlwinm. r11,r30,0,28,28
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8267a938
	if (!cr0.eq) goto loc_8267A938;
	// lwz r29,92(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_8267A938:
	// mulli r11,r29,120
	r11.s64 = r29.s64 * 120;
	// li r10,100
	ctx.r10.s64 = 100;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// divwu r11,r11,r10
	r11.u32 = r11.u32 / ctx.r10.u32;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// beq cr6,0x8267a958
	if (cr6.eq) goto loc_8267A958;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_8267A958:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x8267a968
	if (cr6.eq) goto loc_8267A968;
	// li r10,4096
	ctx.r10.s64 = 4096;
	// stw r10,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r10.u32);
loc_8267A968:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8267a97c
	if (cr6.eq) goto loc_8267A97C;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// addi r11,r11,11884
	r11.s64 = r11.s64 + 11884;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
loc_8267A97C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8267A984"))) PPC_WEAK_FUNC(sub_8267A984);
PPC_FUNC_IMPL(__imp__sub_8267A984) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267A988"))) PPC_WEAK_FUNC(sub_8267A988);
PPC_FUNC_IMPL(__imp__sub_8267A988) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lis r4,5
	ctx.r4.s64 = 327680;
	// addi r30,r6,7712
	r30.s64 = ctx.r6.s64 + 7712;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// ori r4,r4,32772
	ctx.r4.u64 = ctx.r4.u64 | 32772;
	// li r3,252
	ctx.r3.s64 = 252;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// addi r22,r30,4172
	r22.s64 = r30.s64 + 4172;
	// bl 0x826e50ac
	__imp__XMsgInProcessCall(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// li r10,0
	ctx.r10.s64 = 0;
	// blt 0x8267a9e0
	if (cr0.lt) goto loc_8267A9E0;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
loc_8267A9E0:
	// lwz r11,324(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// li r8,4096
	ctx.r8.s64 = 4096;
	// mr r29,r31
	r29.u64 = r31.u64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// stw r23,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r23.u32);
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// stw r24,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r24.u32);
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// addi r11,r31,76
	r11.s64 = r31.s64 + 76;
	// subf r31,r22,r25
	r31.s64 = r25.s64 - r22.s64;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// add r30,r30,r11
	r30.u64 = r30.u64 + r11.u64;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// add r11,r31,r30
	r11.u64 = r31.u64 + r30.u64;
	// addi r5,r10,-13920
	ctx.r5.s64 = ctx.r10.s64 + -13920;
	// lwz r10,348(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// stw r31,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r31.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// bl 0x8267b218
	sub_8267B218(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x8267aa64
	if (cr0.lt) goto loc_8267AA64;
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// lwz r11,340(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// stw r31,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r31.u32);
	// lwz r11,356(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// stw r29,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r29.u32);
loc_8267AA64:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_8267AA6C"))) PPC_WEAK_FUNC(sub_8267AA6C);
PPC_FUNC_IMPL(__imp__sub_8267AA6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267AA70"))) PPC_WEAK_FUNC(sub_8267AA70);
PPC_FUNC_IMPL(__imp__sub_8267AA70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// mr r23,r6
	r23.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r27,r8
	r27.u64 = ctx.r8.u64;
	// mr r26,r9
	r26.u64 = ctx.r9.u64;
	// mr r22,r10
	r22.u64 = ctx.r10.u64;
	// cmplwi cr6,r29,4
	cr6.compare<uint32_t>(r29.u32, 4, xer);
	// bge cr6,0x8267abec
	if (!cr6.lt) goto loc_8267ABEC;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x8267abec
	if (cr6.eq) goto loc_8267ABEC;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8267abec
	if (cr6.eq) goto loc_8267ABEC;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x8267abec
	if (cr6.eq) goto loc_8267ABEC;
	// li r12,-5952
	r12.s64 = -5952;
	// li r11,-1
	r11.s64 = -1;
	// stw r11,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r11.u32);
	// and. r10,r30,r12
	ctx.r10.u64 = r30.u64 & r12.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x8267abec
	if (!cr0.eq) goto loc_8267ABEC;
	// rlwinm. r10,r30,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8267aae0
	if (cr0.eq) goto loc_8267AAE0;
	// rlwinm. r11,r30,0,26,26
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267abec
	if (cr0.eq) goto loc_8267ABEC;
loc_8267AAE0:
	// rlwinm. r11,r30,0,27,27
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267aaf0
	if (cr0.eq) goto loc_8267AAF0;
	// rlwinm. r11,r30,0,29,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267abec
	if (cr0.eq) goto loc_8267ABEC;
loc_8267AAF0:
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267ab04
	if (cr0.eq) goto loc_8267AB04;
	// andi. r11,r30,44
	r11.u64 = r30.u64 & 44;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8267abec
	if (cr0.eq) goto loc_8267ABEC;
loc_8267AB04:
	// rlwinm. r11,r30,0,21,23
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x700;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267ab34
	if (cr0.eq) goto loc_8267AB34;
	// andi. r9,r30,10
	ctx.r9.u64 = r30.u64 & 10;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x8267abec
	if (cr0.eq) goto loc_8267ABEC;
	// rlwinm. r9,r30,0,30,30
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x8267ab34
	if (!cr0.eq) goto loc_8267AB34;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8267ab34
	if (cr6.eq) goto loc_8267AB34;
	// rlwinm r10,r30,0,21,21
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x400;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x8267abec
	if (!cr6.eq) goto loc_8267ABEC;
loc_8267AB34:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x826e50ec
	__imp__XamSessionCreateHandle(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8267abf0
	if (!cr0.eq) goto loc_8267ABF0;
	// li r25,0
	r25.s64 = 0;
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// bl 0x826e50dc
	__imp__XamSessionRefObjByHandle(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8267abf0
	if (!cr0.eq) goto loc_8267ABF0;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r4,11
	ctx.r4.s64 = 720896;
	// li r7,28
	ctx.r7.s64 = 28;
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// stw r29,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r29.u32);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// stw r23,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r23.u32);
	// ori r4,r4,16
	ctx.r4.u64 = ctx.r4.u64 | 16;
	// stw r24,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r24.u32);
	// li r3,251
	ctx.r3.s64 = 251;
	// stw r27,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r27.u32);
	// stw r28,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r28.u32);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// bl 0x826e4a1c
	__imp__XMsgStartIORequest(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x8267abac
	if (!cr0.lt) goto loc_8267ABAC;
	// li r31,1627
	r31.s64 = 1627;
	// b 0x8267abdc
	goto loc_8267ABDC;
loc_8267ABAC:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// bne cr6,0x8267abc8
	if (!cr6.eq) goto loc_8267ABC8;
	// bl 0x823ba710
	sub_823BA710(ctx, base);
	// subfic r11,r3,0
	xer.ca = ctx.r3.u32 <= 0;
	r11.s64 = 0 - ctx.r3.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// andi. r31,r11,1627
	r31.u64 = r11.u64 & 1627;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// b 0x8267abcc
	goto loc_8267ABCC;
loc_8267ABC8:
	// li r31,997
	r31.s64 = 997;
loc_8267ABCC:
	// cmplwi cr6,r31,997
	cr6.compare<uint32_t>(r31.u32, 997, xer);
	// beq cr6,0x8267abf0
	if (cr6.eq) goto loc_8267ABF0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8267abf0
	if (cr6.eq) goto loc_8267ABF0;
loc_8267ABDC:
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// bl 0x823b54e8
	sub_823B54E8(ctx, base);
	// stw r25,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r25.u32);
	// b 0x8267abf0
	goto loc_8267ABF0;
loc_8267ABEC:
	// li r31,87
	r31.s64 = 87;
loc_8267ABF0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_8267ABFC"))) PPC_WEAK_FUNC(sub_8267ABFC);
PPC_FUNC_IMPL(__imp__sub_8267ABFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267AC00"))) PPC_WEAK_FUNC(sub_8267AC00);
PPC_FUNC_IMPL(__imp__sub_8267AC00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x826e50dc
	__imp__XamSessionRefObjByHandle(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8267ac90
	if (!cr0.eq) goto loc_8267AC90;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r4,11
	ctx.r4.s64 = 720896;
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// ori r4,r4,17
	ctx.r4.u64 = ctx.r4.u64 | 17;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// li r3,251
	ctx.r3.s64 = 251;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// bl 0x826e4a1c
	__imp__XMsgStartIORequest(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x8267ac68
	if (!cr0.lt) goto loc_8267AC68;
	// li r31,1627
	r31.s64 = 1627;
	// b 0x8267ac88
	goto loc_8267AC88;
loc_8267AC68:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8267ac84
	if (!cr6.eq) goto loc_8267AC84;
	// bl 0x823ba710
	sub_823BA710(ctx, base);
	// subfic r11,r3,0
	xer.ca = ctx.r3.u32 <= 0;
	r11.s64 = 0 - ctx.r3.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// andi. r31,r11,1627
	r31.u64 = r11.u64 & 1627;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// b 0x8267ac88
	goto loc_8267AC88;
loc_8267AC84:
	// li r31,997
	r31.s64 = 997;
loc_8267AC88:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x826e4e4c
	__imp__ObDereferenceObject(ctx, base);
loc_8267AC90:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267ACAC"))) PPC_WEAK_FUNC(sub_8267ACAC);
PPC_FUNC_IMPL(__imp__sub_8267ACAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267ACB0"))) PPC_WEAK_FUNC(sub_8267ACB0);
PPC_FUNC_IMPL(__imp__sub_8267ACB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// bl 0x826e50dc
	__imp__XamSessionRefObjByHandle(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8267ad4c
	if (!cr0.eq) goto loc_8267AD4C;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r4,11
	ctx.r4.s64 = 720896;
	// li r7,20
	ctx.r7.s64 = 20;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// stw r28,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r28.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// stw r27,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r27.u32);
	// ori r4,r4,18
	ctx.r4.u64 = ctx.r4.u64 | 18;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// li r3,251
	ctx.r3.s64 = 251;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// bl 0x826e4a1c
	__imp__XMsgStartIORequest(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x8267ad24
	if (!cr0.lt) goto loc_8267AD24;
	// li r31,1627
	r31.s64 = 1627;
	// b 0x8267ad44
	goto loc_8267AD44;
loc_8267AD24:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8267ad40
	if (!cr6.eq) goto loc_8267AD40;
	// bl 0x823ba710
	sub_823BA710(ctx, base);
	// subfic r11,r3,0
	xer.ca = ctx.r3.u32 <= 0;
	r11.s64 = 0 - ctx.r3.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// andi. r31,r11,1627
	r31.u64 = r11.u64 & 1627;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// b 0x8267ad44
	goto loc_8267AD44;
loc_8267AD40:
	// li r31,997
	r31.s64 = 997;
loc_8267AD44:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x826e4e4c
	__imp__ObDereferenceObject(ctx, base);
loc_8267AD4C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8267AD58"))) PPC_WEAK_FUNC(sub_8267AD58);
PPC_FUNC_IMPL(__imp__sub_8267AD58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// bl 0x826e50dc
	__imp__XamSessionRefObjByHandle(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8267ade4
	if (!cr0.eq) goto loc_8267ADE4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r4,11
	ctx.r4.s64 = 720896;
	// li r7,16
	ctx.r7.s64 = 16;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// ori r4,r4,20
	ctx.r4.u64 = ctx.r4.u64 | 20;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// li r3,251
	ctx.r3.s64 = 251;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// bl 0x826e4a1c
	__imp__XMsgStartIORequest(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x8267adbc
	if (!cr0.lt) goto loc_8267ADBC;
	// li r31,1627
	r31.s64 = 1627;
	// b 0x8267addc
	goto loc_8267ADDC;
loc_8267ADBC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8267add8
	if (!cr6.eq) goto loc_8267ADD8;
	// bl 0x823ba710
	sub_823BA710(ctx, base);
	// subfic r11,r3,0
	xer.ca = ctx.r3.u32 <= 0;
	r11.s64 = 0 - ctx.r3.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// andi. r31,r11,1627
	r31.u64 = r11.u64 & 1627;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// b 0x8267addc
	goto loc_8267ADDC;
loc_8267ADD8:
	// li r31,997
	r31.s64 = 997;
loc_8267ADDC:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x826e4e4c
	__imp__ObDereferenceObject(ctx, base);
loc_8267ADE4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8267ADF0"))) PPC_WEAK_FUNC(sub_8267ADF0);
PPC_FUNC_IMPL(__imp__sub_8267ADF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x826e50dc
	__imp__XamSessionRefObjByHandle(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8267ae80
	if (!cr0.eq) goto loc_8267AE80;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r4,11
	ctx.r4.s64 = 720896;
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// ori r4,r4,21
	ctx.r4.u64 = ctx.r4.u64 | 21;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// li r3,251
	ctx.r3.s64 = 251;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// bl 0x826e4a1c
	__imp__XMsgStartIORequest(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x8267ae58
	if (!cr0.lt) goto loc_8267AE58;
	// li r31,1627
	r31.s64 = 1627;
	// b 0x8267ae78
	goto loc_8267AE78;
loc_8267AE58:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8267ae74
	if (!cr6.eq) goto loc_8267AE74;
	// bl 0x823ba710
	sub_823BA710(ctx, base);
	// subfic r11,r3,0
	xer.ca = ctx.r3.u32 <= 0;
	r11.s64 = 0 - ctx.r3.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// andi. r31,r11,1627
	r31.u64 = r11.u64 & 1627;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// b 0x8267ae78
	goto loc_8267AE78;
loc_8267AE74:
	// li r31,997
	r31.s64 = 997;
loc_8267AE78:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x826e4e4c
	__imp__ObDereferenceObject(ctx, base);
loc_8267AE80:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267AE9C"))) PPC_WEAK_FUNC(sub_8267AE9C);
PPC_FUNC_IMPL(__imp__sub_8267AE9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267AEA0"))) PPC_WEAK_FUNC(sub_8267AEA0);
PPC_FUNC_IMPL(__imp__sub_8267AEA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// rldicl r11,r31,16,48
	r11.u64 = __builtin_rotateleft64(r31.u64, 16) & 0xFFFF;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// clrlwi r10,r11,28
	ctx.r10.u64 = r11.u32 & 0xF;
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r10,9
	cr6.compare<uint32_t>(ctx.r10.u32, 9, xer);
	// bne cr6,0x8267aedc
	if (!cr6.eq) goto loc_8267AEDC;
	// rlwinm. r11,r11,0,24,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xC0;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8267af54
	if (!cr0.eq) goto loc_8267AF54;
loc_8267AEDC:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x826e50dc
	__imp__XamSessionRefObjByHandle(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// bne 0x8267af54
	if (!cr0.eq) goto loc_8267AF54;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r4,11
	ctx.r4.s64 = 720896;
	// li r7,24
	ctx.r7.s64 = 24;
	// std r31,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r31.u64);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// stw r28,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r28.u32);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// stw r27,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r27.u32);
	// ori r4,r4,37
	ctx.r4.u64 = ctx.r4.u64 | 37;
	// li r3,251
	ctx.r3.s64 = 251;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// bl 0x826e4a1c
	__imp__XMsgStartIORequest(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x8267af2c
	if (!cr0.lt) goto loc_8267AF2C;
	// li r30,1627
	r30.s64 = 1627;
	// b 0x8267af4c
	goto loc_8267AF4C;
loc_8267AF2C:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x8267af48
	if (!cr6.eq) goto loc_8267AF48;
	// bl 0x823ba710
	sub_823BA710(ctx, base);
	// subfic r11,r3,0
	xer.ca = ctx.r3.u32 <= 0;
	r11.s64 = 0 - ctx.r3.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// andi. r30,r11,1627
	r30.u64 = r11.u64 & 1627;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// b 0x8267af4c
	goto loc_8267AF4C;
loc_8267AF48:
	// li r30,997
	r30.s64 = 997;
loc_8267AF4C:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x826e4e4c
	__imp__ObDereferenceObject(ctx, base);
loc_8267AF54:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8267AF60"))) PPC_WEAK_FUNC(sub_8267AF60);
PPC_FUNC_IMPL(__imp__sub_8267AF60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r9,4
	ctx.r9.s64 = 4;
	// extsw r8,r4
	ctx.r8.s64 = ctx.r4.s32;
	// lwz r10,512(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 512);
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// std r8,8(r10)
	PPC_STORE_U64(ctx.r10.u32 + 8, ctx.r8.u64);
	// lwz r3,512(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 512);
	// addi r10,r3,1
	ctx.r10.s64 = ctx.r3.s64 + 1;
	// stw r10,512(r11)
	PPC_STORE_U32(r11.u32 + 512, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267AF90"))) PPC_WEAK_FUNC(sub_8267AF90);
PPC_FUNC_IMPL(__imp__sub_8267AF90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-720(r1)
	ea = -720 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r30,0
	r30.s64 = 0;
	// stw r3,740(r1)
	PPC_STORE_U32(ctx.r1.u32 + 740, ctx.r3.u32);
	// stw r4,748(r1)
	PPC_STORE_U32(ctx.r1.u32 + 748, ctx.r4.u32);
	// li r3,518
	ctx.r3.s64 = 518;
	// addi r4,r1,136
	ctx.r4.s64 = ctx.r1.s64 + 136;
	// stw r5,756(r1)
	PPC_STORE_U32(ctx.r1.u32 + 756, ctx.r5.u32);
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// stw r30,672(r1)
	PPC_STORE_U32(ctx.r1.u32 + 672, r30.u32);
	// stw r30,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r30.u32);
	// stw r30,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r30.u32);
	// bl 0x8267a860
	sub_8267A860(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x8267b0bc
	if (cr0.lt) goto loc_8267B0BC;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r3,136(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,132
	ctx.r7.s64 = ctx.r1.s64 + 132;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8267a8e8
	sub_8267A8E8(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x8267b0bc
	if (cr0.lt) goto loc_8267B0BC;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x826e50cc
	__imp__XamAlloc(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x8267b0bc
	if (cr0.lt) goto loc_8267B0BC;
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// lwz r8,132(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r7,r1,148
	ctx.r7.s64 = ctx.r1.s64 + 148;
	// lwz r4,136(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// addi r11,r1,140
	r11.s64 = ctx.r1.s64 + 140;
	// stw r30,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r30.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// li r3,518
	ctx.r3.s64 = 518;
	// lwz r7,128(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// bl 0x8267a988
	sub_8267A988(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x8267b0bc
	if (cr0.lt) goto loc_8267B0BC;
	// addi r4,r1,740
	ctx.r4.s64 = ctx.r1.s64 + 740;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x8267af60
	sub_8267AF60(ctx, base);
	// addi r4,r1,748
	ctx.r4.s64 = ctx.r1.s64 + 748;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x8267af60
	sub_8267AF60(ctx, base);
	// addi r4,r1,756
	ctx.r4.s64 = ctx.r1.s64 + 756;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x8267af60
	sub_8267AF60(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x8267af60
	sub_8267AF60(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x8267af60
	sub_8267AF60(ctx, base);
	// lis r4,5
	ctx.r4.s64 = 327680;
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// lwz r5,140(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// ori r4,r4,32800
	ctx.r4.u64 = ctx.r4.u64 | 32800;
	// li r3,252
	ctx.r3.s64 = 252;
	// bl 0x826e50ac
	__imp__XMsgInProcessCall(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bge 0x8267b0dc
	if (!cr0.lt) goto loc_8267B0DC;
loc_8267B0BC:
	// lwz r3,128(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8267b0cc
	if (cr6.eq) goto loc_8267B0CC;
	// bl 0x826e50bc
	__imp__XamFree(ctx, base);
loc_8267B0CC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823b9af0
	sub_823B9AF0(ctx, base);
	// li r30,1627
	r30.s64 = 1627;
	// b 0x8267b0e4
	goto loc_8267B0E4;
loc_8267B0DC:
	// lwz r3,128(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// bl 0x826e50bc
	__imp__XamFree(ctx, base);
loc_8267B0E4:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,720
	ctx.r1.s64 = ctx.r1.s64 + 720;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8267B0F0"))) PPC_WEAK_FUNC(sub_8267B0F0);
PPC_FUNC_IMPL(__imp__sub_8267B0F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// lhz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 40);
	// cmplw cr6,r4,r10
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r10.u32, xer);
	// blt cr6,0x8267b11c
	if (cr6.lt) goto loc_8267B11C;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// b 0x8267b138
	goto loc_8267B138;
loc_8267B11C:
	// lhz r9,42(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 42);
	// li r5,24
	ctx.r5.s64 = 24;
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// mullw r11,r9,r4
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_8267B138:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267B148"))) PPC_WEAK_FUNC(sub_8267B148);
PPC_FUNC_IMPL(__imp__sub_8267B148) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r5,44
	ctx.r5.s64 = 44;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r29,0
	r29.s64 = 0;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r9,r30,44
	ctx.r9.s64 = r30.s64 + 44;
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267b184
	if (cr0.eq) goto loc_8267B184;
	// lis r29,-32768
	r29.s64 = -2147483648;
	// ori r29,r29,16389
	r29.u64 = r29.u64 | 16389;
	// b 0x8267b20c
	goto loc_8267B20C;
loc_8267B184:
	// lhz r6,38(r31)
	ctx.r6.u64 = PPC_LOAD_U16(r31.u32 + 38);
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq 0x8267b19c
	if (cr0.eq) goto loc_8267B19C;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// stw r11,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r11.u32);
loc_8267B19C:
	// lhz r10,26(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 26);
	// lhz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U16(r31.u32 + 24);
	// lhz r11,40(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 40);
	// mullw r7,r10,r8
	ctx.r7.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// rotlwi r10,r11,2
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 2);
	// stw r9,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r9.u32);
	// lhz r5,42(r31)
	ctx.r5.u64 = PPC_LOAD_U16(r31.u32 + 42);
	// lhz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U16(r31.u32 + 32);
	// mullw r11,r5,r11
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r10,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r10.s64;
	// rotlwi r8,r3,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r3.u32, 1);
	// subf r10,r6,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r6.s64;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r9.u32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r10,-44
	ctx.r10.s64 = ctx.r10.s64 + -44;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r10,56(r31)
	PPC_STORE_U32(r31.u32 + 56, ctx.r10.u32);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// stw r9,52(r31)
	PPC_STORE_U32(r31.u32 + 52, ctx.r9.u32);
	// stw r10,64(r31)
	PPC_STORE_U32(r31.u32 + 64, ctx.r10.u32);
	// add r10,r8,r11
	ctx.r10.u64 = ctx.r8.u64 + r11.u64;
	// stw r11,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r11.u32);
	// stw r10,72(r31)
	PPC_STORE_U32(r31.u32 + 72, ctx.r10.u32);
loc_8267B20C:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8267B218"))) PPC_WEAK_FUNC(sub_8267B218);
PPC_FUNC_IMPL(__imp__sub_8267B218) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// mr r24,r10
	r24.u64 = ctx.r10.u64;
	// rlwinm. r11,r30,0,25,25
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267b25c
	if (cr0.eq) goto loc_8267B25C;
	// bl 0x826e4ddc
	__imp__KeGetCurrentProcessType(ctx, base);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// bne cr6,0x8267b25c
	if (!cr6.eq) goto loc_8267B25C;
	// ori r30,r30,16
	r30.u64 = r30.u64 | 16;
loc_8267B25C:
	// lwz r11,252(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r3,284(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r9,260(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r8,244(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r5,276(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// stw r3,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r3.u32);
	// addi r3,r31,52
	ctx.r3.s64 = r31.s64 + 52;
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// stw r28,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r28.u32);
	// stw r27,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r27.u32);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// stw r26,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r26.u32);
	// stw r25,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r25.u32);
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// stw r9,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r9.u32);
	// stw r24,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r24.u32);
	// stw r8,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r8.u32);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x8267b2f8
	sub_8267B2F8(ctx, base);
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r11,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r11.u32);
	// beq cr6,0x8267b2ec
	if (cr6.eq) goto loc_8267B2EC;
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x8267b2e0
	if (cr0.eq) goto loc_8267B2E0;
	// bl 0x826a8d58
	sub_826A8D58(ctx, base);
loc_8267B2E0:
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// li r10,997
	ctx.r10.s64 = 997;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_8267B2EC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_8267B2F8"))) PPC_WEAK_FUNC(sub_8267B2F8);
PPC_FUNC_IMPL(__imp__sub_8267B2F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// stw r4,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r4.u32);
	// stw r5,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r5.u32);
	// stw r6,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r6.u32);
	// stw r7,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r7.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267B314"))) PPC_WEAK_FUNC(sub_8267B314);
PPC_FUNC_IMPL(__imp__sub_8267B314) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267B318"))) PPC_WEAK_FUNC(sub_8267B318);
PPC_FUNC_IMPL(__imp__sub_8267B318) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r3,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r3.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r4,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r4.u32);
	// stw r5,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r5.u32);
	// bne cr6,0x8267b358
	if (!cr6.eq) goto loc_8267B358;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lis r11,8
	r11.s64 = 524288;
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x8267b35c
	goto loc_8267B35C;
loc_8267B358:
	// li r8,0
	ctx.r8.s64 = 0;
loc_8267B35C:
	// lis r4,7
	ctx.r4.s64 = 458752;
	// li r7,12
	ctx.r7.s64 = 12;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// li r5,0
	ctx.r5.s64 = 0;
	// ori r4,r4,26
	ctx.r4.u64 = ctx.r4.u64 | 26;
	// li r3,250
	ctx.r3.s64 = 250;
	// bl 0x826e50fc
	__imp__XMsgStartIORequestEx(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x8267b398
	if (cr0.lt) goto loc_8267B398;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823b6720
	sub_823B6720(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x8267b398
	if (cr0.lt) goto loc_8267B398;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8267b3b0
	goto loc_8267B3B0;
loc_8267B398:
	// rlwinm r11,r3,0,3,15
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x1FFF0000;
	// lis r10,7
	ctx.r10.s64 = 458752;
	// clrlwi r3,r3,16
	ctx.r3.u64 = ctx.r3.u32 & 0xFFFF;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// beq cr6,0x8267b3b0
	if (cr6.eq) goto loc_8267B3B0;
	// li r3,1627
	ctx.r3.s64 = 1627;
loc_8267B3B0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267B3C0"))) PPC_WEAK_FUNC(sub_8267B3C0);
PPC_FUNC_IMPL(__imp__sub_8267B3C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// lis r4,7
	ctx.r4.s64 = 458752;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// ori r4,r4,27
	ctx.r4.u64 = ctx.r4.u64 | 27;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// li r3,250
	ctx.r3.s64 = 250;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// bl 0x826e50ac
	__imp__XMsgInProcessCall(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x8267b40c
	if (cr0.lt) goto loc_8267B40C;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8267b424
	goto loc_8267B424;
loc_8267B40C:
	// rlwinm r11,r3,0,3,15
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x1FFF0000;
	// lis r10,7
	ctx.r10.s64 = 458752;
	// clrlwi r3,r3,16
	ctx.r3.u64 = ctx.r3.u32 & 0xFFFF;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// beq cr6,0x8267b424
	if (cr6.eq) goto loc_8267B424;
	// li r3,1627
	ctx.r3.s64 = 1627;
loc_8267B424:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267B434"))) PPC_WEAK_FUNC(sub_8267B434);
PPC_FUNC_IMPL(__imp__sub_8267B434) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267B438"))) PPC_WEAK_FUNC(sub_8267B438);
PPC_FUNC_IMPL(__imp__sub_8267B438) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r11,4400
	r11.s64 = r11.s64 + 4400;
	// clrlwi. r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// beq 0x8267b464
	if (cr0.eq) goto loc_8267B464;
	// bl 0x821e7b68
	sub_821E7B68(ctx, base);
loc_8267B464:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267B47C"))) PPC_WEAK_FUNC(sub_8267B47C);
PPC_FUNC_IMPL(__imp__sub_8267B47C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267B480"))) PPC_WEAK_FUNC(sub_8267B480);
PPC_FUNC_IMPL(__imp__sub_8267B480) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x826e4a4c
	__imp__XamGetSystemVersion(ctx, base);
	// lis r10,8
	ctx.r10.s64 = 524288;
	// rlwinm r11,r3,0,8,23
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFF00;
	// ori r10,r10,41216
	ctx.r10.u64 = ctx.r10.u64 | 41216;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x8267b538
	if (!cr6.lt) goto loc_8267B538;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// addi r28,r11,5868
	r28.s64 = r11.s64 + 5868;
	// addi r3,r28,4
	ctx.r3.s64 = r28.s64 + 4;
	// bl 0x826e4b6c
	__imp__RtlEnterCriticalSection(ctx, base);
	// lis r29,-32126
	r29.s64 = -2105409536;
	// lwz r11,-13840(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + -13840);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8267b4cc
	if (!cr6.eq) goto loc_8267B4CC;
	// li r30,1627
	r30.s64 = 1627;
	// b 0x8267b52c
	goto loc_8267B52C;
loc_8267B4CC:
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x8267b3c0
	sub_8267B3C0(ctx, base);
	// lis r31,-32126
	r31.s64 = -2105409536;
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// bne 0x8267b520
	if (!cr0.eq) goto loc_8267B520;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r11,r11,0
	r11.s64 = r11.s64 + 0;
	// li r3,2
	ctx.r3.s64 = 2;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,-13836(r31)
	PPC_STORE_U32(r31.u32 + -13836, r11.u32);
	// bl 0x8267b318
	sub_8267B318(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// bne 0x8267b520
	if (!cr0.eq) goto loc_8267B520;
	// li r11,1
	r11.s64 = 1;
	// stw r11,-13840(r29)
	PPC_STORE_U32(r29.u32 + -13840, r11.u32);
	// b 0x8267b52c
	goto loc_8267B52C;
loc_8267B520:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-13840(r29)
	PPC_STORE_U32(r29.u32 + -13840, r11.u32);
	// stw r11,-13836(r31)
	PPC_STORE_U32(r31.u32 + -13836, r11.u32);
loc_8267B52C:
	// addi r3,r28,4
	ctx.r3.s64 = r28.s64 + 4;
	// bl 0x826e4b7c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// b 0x8267b54c
	goto loc_8267B54C;
loc_8267B538:
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x8267b318
	sub_8267B318(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_8267B54C:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8267B558"))) PPC_WEAK_FUNC(sub_8267B558);
PPC_FUNC_IMPL(__imp__sub_8267B558) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x826e4a4c
	__imp__XamGetSystemVersion(ctx, base);
	// lis r10,8
	ctx.r10.s64 = 524288;
	// rlwinm r11,r3,0,8,23
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFF00;
	// ori r10,r10,41216
	ctx.r10.u64 = ctx.r10.u64 | 41216;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x8267b5e4
	if (!cr6.lt) goto loc_8267B5E4;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// addi r28,r11,5868
	r28.s64 = r11.s64 + 5868;
	// addi r3,r28,4
	ctx.r3.s64 = r28.s64 + 4;
	// bl 0x826e4b6c
	__imp__RtlEnterCriticalSection(ctx, base);
	// lis r31,-32126
	r31.s64 = -2105409536;
	// lwz r11,-13840(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + -13840);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8267b5a4
	if (!cr6.eq) goto loc_8267B5A4;
	// li r29,1627
	r29.s64 = 1627;
	// b 0x8267b5d8
	goto loc_8267B5D8;
loc_8267B5A4:
	// lis r30,-32126
	r30.s64 = -2105409536;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r3,2
	ctx.r3.s64 = 2;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,-13836(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + -13836);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x8267b5c4
	if (cr6.eq) goto loc_8267B5C4;
	// li r4,4
	ctx.r4.s64 = 4;
loc_8267B5C4:
	// bl 0x8267b318
	sub_8267B318(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r11,-13836(r30)
	PPC_STORE_U32(r30.u32 + -13836, r11.u32);
	// stw r11,-13840(r31)
	PPC_STORE_U32(r31.u32 + -13840, r11.u32);
loc_8267B5D8:
	// addi r3,r28,4
	ctx.r3.s64 = r28.s64 + 4;
	// bl 0x826e4b7c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// b 0x8267b5f8
	goto loc_8267B5F8;
loc_8267B5E4:
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,4
	ctx.r4.s64 = 4;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x8267b318
	sub_8267B318(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
loc_8267B5F8:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8267B604"))) PPC_WEAK_FUNC(sub_8267B604);
PPC_FUNC_IMPL(__imp__sub_8267B604) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267B608"))) PPC_WEAK_FUNC(sub_8267B608);
PPC_FUNC_IMPL(__imp__sub_8267B608) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x8267b668
	if (cr6.eq) goto loc_8267B668;
	// lwz r11,28(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8267b668
	if (cr0.eq) goto loc_8267B668;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r9,r11,1320
	ctx.r9.s64 = r11.s64 + 1320;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// stw r10,20(r8)
	PPC_STORE_U32(ctx.r8.u32 + 20, ctx.r10.u32);
	// stw r10,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r10.u32);
	// stw r10,24(r8)
	PPC_STORE_U32(ctx.r8.u32 + 24, ctx.r10.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// stw r10,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r10.u32);
	// stw r9,100(r11)
	PPC_STORE_U32(r11.u32 + 100, ctx.r9.u32);
	// stw r9,72(r11)
	PPC_STORE_U32(r11.u32 + 72, ctx.r9.u32);
	// stw r9,68(r11)
	PPC_STORE_U32(r11.u32 + 68, ctx.r9.u32);
	// blr 
	return;
loc_8267B668:
	// li r3,-2
	ctx.r3.s64 = -2;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267B670"))) PPC_WEAK_FUNC(sub_8267B670);
PPC_FUNC_IMPL(__imp__sub_8267B670) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x8267b780
	if (cr6.eq) goto loc_8267B780;
	// lbz r11,0(r5)
	r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// cmplwi cr6,r11,49
	cr6.compare<uint32_t>(r11.u32, 49, xer);
	// bne cr6,0x8267b780
	if (!cr6.eq) goto loc_8267B780;
	// cmpwi cr6,r6,56
	cr6.compare<int32_t>(ctx.r6.s32, 56, xer);
	// bne cr6,0x8267b780
	if (!cr6.eq) goto loc_8267B780;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8267b6b0
	if (!cr6.eq) goto loc_8267B6B0;
	// li r3,-2
	ctx.r3.s64 = -2;
	// b 0x8267b784
	goto loc_8267B784;
loc_8267B6B0:
	// li r29,0
	r29.s64 = 0;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r29,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r29.u32);
	// bne cr6,0x8267b6d4
	if (!cr6.eq) goto loc_8267B6D4;
	// lis r11,-32152
	r11.s64 = -2107113472;
	// stw r29,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r29.u32);
	// addi r11,r11,-5504
	r11.s64 = r11.s64 + -5504;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
loc_8267B6D4:
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267b6ec
	if (!cr6.eq) goto loc_8267B6EC;
	// lis r11,-32152
	r11.s64 = -2107113472;
	// addi r11,r11,-5496
	r11.s64 = r11.s64 + -5496;
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
loc_8267B6EC:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// li r5,7080
	ctx.r5.s64 = 7080;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// li r4,1
	ctx.r4.s64 = 1;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr. r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne 0x8267b714
	if (!cr0.eq) goto loc_8267B714;
	// li r3,-4
	ctx.r3.s64 = -4;
	// b 0x8267b784
	goto loc_8267B784;
loc_8267B714:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r4,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r4.u32);
	// bge cr6,0x8267b72c
	if (!cr6.lt) goto loc_8267B72C;
	// neg r30,r30
	r30.s64 = -r30.s64;
	// stw r29,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, r29.u32);
	// b 0x8267b744
	goto loc_8267B744;
loc_8267B72C:
	// srawi r11,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r11.s64 = r30.s32 >> 4;
	// cmpwi cr6,r30,48
	cr6.compare<int32_t>(r30.s32, 48, xer);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, r11.u32);
	// bge cr6,0x8267b744
	if (!cr6.lt) goto loc_8267B744;
	// clrlwi r30,r30,28
	r30.u64 = r30.u32 & 0xF;
loc_8267B744:
	// addi r11,r30,-8
	r11.s64 = r30.s64 + -8;
	// cmplwi cr6,r11,7
	cr6.compare<uint32_t>(r11.u32, 7, xer);
	// bgt cr6,0x8267b764
	if (cr6.gt) goto loc_8267B764;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r30,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, r30.u32);
	// stw r29,44(r4)
	PPC_STORE_U32(ctx.r4.u32 + 44, r29.u32);
	// bl 0x8267b608
	sub_8267B608(ctx, base);
	// b 0x8267b784
	goto loc_8267B784;
loc_8267B764:
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,-2
	ctx.r3.s64 = -2;
	// stw r29,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r29.u32);
	// b 0x8267b784
	goto loc_8267B784;
loc_8267B780:
	// li r3,-6
	ctx.r3.s64 = -6;
loc_8267B784:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8267B78C"))) PPC_WEAK_FUNC(sub_8267B78C);
PPC_FUNC_IMPL(__imp__sub_8267B78C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267B790"))) PPC_WEAK_FUNC(sub_8267B790);
PPC_FUNC_IMPL(__imp__sub_8267B790) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r30,1
	r30.s64 = 1;
	// lwz r31,28(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x8267b7e8
	if (!cr0.eq) goto loc_8267B7E8;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r3,40(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// lwz r10,32(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 32);
	// slw r4,r30,r11
	ctx.r4.u64 = r11.u8 & 0x20 ? 0 : (r30.u32 << (r11.u8 & 0x3F));
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r3.u32);
	// bne 0x8267b7e8
	if (!cr0.eq) goto loc_8267B7E8;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8267b8bc
	goto loc_8267B8BC;
loc_8267B7E8:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267b80c
	if (!cr6.eq) goto loc_8267B80C;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// stw r27,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r27.u32);
	// stw r27,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r27.u32);
	// slw r11,r30,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r30.u32 << (r11.u8 & 0x3F));
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
loc_8267B80C:
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// lwz r5,32(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// subf r30,r11,r29
	r30.s64 = r29.s64 - r11.s64;
	// cmplw cr6,r30,r5
	cr6.compare<uint32_t>(r30.u32, ctx.r5.u32, xer);
	// blt cr6,0x8267b838
	if (cr6.lt) goto loc_8267B838;
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// subf r4,r5,r11
	ctx.r4.s64 = r11.s64 - ctx.r5.s64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// stw r27,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r27.u32);
	// b 0x8267b8b4
	goto loc_8267B8B4;
loc_8267B838:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// subf r29,r11,r5
	r29.s64 = ctx.r5.s64 - r11.s64;
	// cmplw cr6,r29,r30
	cr6.compare<uint32_t>(r29.u32, r30.u32, xer);
	// ble cr6,0x8267b84c
	if (!cr6.gt) goto loc_8267B84C;
	// mr r29,r30
	r29.u64 = r30.u64;
loc_8267B84C:
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// subf r4,r30,r11
	ctx.r4.s64 = r11.s64 - r30.s64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// subf. r30,r29,r30
	r30.s64 = r30.s64 - r29.s64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x8267b888
	if (cr0.eq) goto loc_8267B888;
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// subf r4,r30,r11
	ctx.r4.s64 = r11.s64 - r30.s64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// b 0x8267b8b4
	goto loc_8267B8B4;
loc_8267B888:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// bne cr6,0x8267b8a4
	if (!cr6.eq) goto loc_8267B8A4;
	// stw r27,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r27.u32);
loc_8267B8A4:
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x8267b8b8
	if (!cr6.lt) goto loc_8267B8B8;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
loc_8267B8B4:
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
loc_8267B8B8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_8267B8BC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8267B8C4"))) PPC_WEAK_FUNC(sub_8267B8C4);
PPC_FUNC_IMPL(__imp__sub_8267B8C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267B8C8"))) PPC_WEAK_FUNC(sub_8267B8C8);
PPC_FUNC_IMPL(__imp__sub_8267B8C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// stw r4,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r4.u32);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x8267cbc4
	if (cr6.eq) goto loc_8267CBC4;
	// lwz r31,28(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 28);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x8267cbc4
	if (cr0.eq) goto loc_8267CBC4;
	// lwz r11,12(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267cbc4
	if (cr6.eq) goto loc_8267CBC4;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267b914
	if (!cr6.eq) goto loc_8267B914;
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267cbc4
	if (!cr6.eq) goto loc_8267CBC4;
loc_8267B914:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpwi cr6,r11,11
	cr6.compare<int32_t>(r11.s32, 11, xer);
	// bne cr6,0x8267b928
	if (!cr6.eq) goto loc_8267B928;
	// li r11,12
	r11.s64 = 12;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_8267B928:
	// lwz r21,16(r24)
	r21.u64 = PPC_LOAD_U32(r24.u32 + 16);
	// li r11,0
	r11.s64 = 0;
	// lwz r25,4(r24)
	r25.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// mr r27,r21
	r27.u64 = r21.u64;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r20,12(r24)
	r20.u64 = PPC_LOAD_U32(r24.u32 + 12);
	// lwz r26,0(r24)
	r26.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmplwi cr6,r10,28
	cr6.compare<uint32_t>(ctx.r10.u32, 28, xer);
	// lwz r29,48(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r30,52(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// stw r25,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r25.u32);
	// bgt cr6,0x8267cbc4
	if (cr6.gt) goto loc_8267CBC4;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// li r23,1
	r23.s64 = 1;
	// addi r19,r11,4408
	r19.s64 = r11.s64 + 4408;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// li r22,27
	r22.s64 = 27;
	// addi r16,r11,6844
	r16.s64 = r11.s64 + 6844;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r15,r11,-5252
	r15.s64 = r11.s64 + -5252;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r14,r11,6812
	r14.s64 = r11.s64 + 6812;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r11,r11,-968
	r11.s64 = r11.s64 + -968;
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r11.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r11,r11,-996
	r11.s64 = r11.s64 + -996;
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,6788
	r11.s64 = r11.s64 + 6788;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,6760
	r11.s64 = r11.s64 + 6760;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r18,r11,-1140
	r18.s64 = r11.s64 + -1140;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,6732
	r11.s64 = r11.s64 + 6732;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r11,r11,-1112
	r11.s64 = r11.s64 + -1112;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r11,r11,-1076
	r11.s64 = r11.s64 + -1076;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r11,r11,-1044
	r11.s64 = r11.s64 + -1044;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,6712
	r11.s64 = r11.s64 + 6712;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,6684
	r11.s64 = r11.s64 + 6684;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r11,r11,-5296
	r11.s64 = r11.s64 + -5296;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r17,r11,-5324
	r17.s64 = r11.s64 + -5324;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r11,r11,-5276
	r11.s64 = r11.s64 + -5276;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
loc_8267BA28:
	// lis r12,-32243
	r12.s64 = -2113077248;
	// addi r12,r12,6624
	r12.s64 = r12.s64 + 6624;
	// rlwinm r0,r10,1,0,30
	r0.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r0,r12,r0
	r0.u64 = PPC_LOAD_U16(r12.u32 + r0.u32);
	// lis r12,-32152
	r12.s64 = -2107113472;
	// addi r12,r12,-17840
	r12.s64 = r12.s64 + -17840;
	// add r12,r12,r0
	r12.u64 = r12.u64 + r0.u64;
	// mtctr r12
	ctr.u64 = r12.u64;
	// nop 
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi r10,0
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x8267ba88
	if (!cr0.eq) goto loc_8267BA88;
	// li r11,12
	r11.s64 = 12;
loc_8267BA60:
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// b 0x8267cbb8
	goto loc_8267CBB8;
loc_8267BA68:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_8267BA88:
	// cmplwi cr6,r30,16
	cr6.compare<uint32_t>(r30.u32, 16, xer);
	// blt cr6,0x8267ba68
	if (cr6.lt) goto loc_8267BA68;
	// rlwinm. r11,r10,0,30,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267bae8
	if (cr0.eq) goto loc_8267BAE8;
	// cmplwi cr6,r29,35615
	cr6.compare<uint32_t>(r29.u32, 35615, xer);
	// bne cr6,0x8267bae8
	if (!cr6.eq) goto loc_8267BAE8;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// li r11,31
	r11.s64 = 31;
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
	// li r5,2
	ctx.r5.s64 = 2;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// stb r11,84(r1)
	PPC_STORE_U8(ctx.r1.u32 + 84, r11.u8);
	// li r11,139
	r11.s64 = 139;
	// stb r11,85(r1)
	PPC_STORE_U8(ctx.r1.u32 + 85, r11.u8);
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// li r29,0
	r29.s64 = 0;
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
	// li r30,0
	r30.s64 = 0;
	// stw r23,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r23.u32);
	// b 0x8267cbb8
	goto loc_8267CBB8;
loc_8267BAE8:
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// beq 0x8267bb84
	if (cr0.eq) goto loc_8267BB84;
	// rlwinm r10,r29,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r11,r29,8,16,23
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 8) & 0xFF00;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// li r10,31
	ctx.r10.s64 = 31;
	// divwu r10,r11,r10
	ctx.r10.u32 = r11.u32 / ctx.r10.u32;
	// mulli r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 * 31;
	// subf. r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8267bb84
	if (!cr0.eq) goto loc_8267BB84;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// beq cr6,0x8267bb2c
	if (cr6.eq) goto loc_8267BB2C;
loc_8267BB24:
	// stw r17,24(r24)
	PPC_STORE_U32(r24.u32 + 24, r17.u32);
	// b 0x8267cbb4
	goto loc_8267CBB4;
loc_8267BB2C:
	// rlwinm r29,r29,28,4,31
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 28) & 0xFFFFFFF;
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// addi r30,r30,-4
	r30.s64 = r30.s64 + -4;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8267bb54
	if (!cr6.gt) goto loc_8267BB54;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
loc_8267BB4C:
	// stw r11,24(r24)
	PPC_STORE_U32(r24.u32 + 24, r11.u32);
	// b 0x8267cbb4
	goto loc_8267CBB4;
loc_8267BB54:
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8267f368
	sub_8267F368(ctx, base);
	// not r10,r29
	ctx.r10.u64 = ~r29.u64;
	// li r11,9
	r11.s64 = 9;
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
	// li r29,0
	r29.s64 = 0;
	// stw r3,48(r24)
	PPC_STORE_U32(r24.u32 + 48, ctx.r3.u32);
	// rlwimi r11,r10,24,30,30
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 24) & 0x2) | (r11.u64 & 0xFFFFFFFFFFFFFFFD);
	// li r30,0
	r30.s64 = 0;
	// b 0x8267ba60
	goto loc_8267BA60;
loc_8267BB84:
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// b 0x8267bb4c
	goto loc_8267BB4C;
loc_8267BB8C:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// cmplwi cr6,r30,16
	cr6.compare<uint32_t>(r30.u32, 16, xer);
	// blt cr6,0x8267bb8c
	if (cr6.lt) goto loc_8267BB8C;
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// stw r29,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r29.u32);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bne cr6,0x8267bb24
	if (!cr6.eq) goto loc_8267BB24;
	// rlwinm. r11,r29,0,16,18
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xE000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267bbd4
	if (cr0.eq) goto loc_8267BBD4;
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// b 0x8267bb4c
	goto loc_8267BB4C;
loc_8267BBD4:
	// rlwinm. r11,r29,0,22,22
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x200;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267bbfc
	if (cr0.eq) goto loc_8267BBFC;
	// rlwinm r11,r29,24,8,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 24) & 0xFFFFFF;
	// stb r29,84(r1)
	PPC_STORE_U8(ctx.r1.u32 + 84, r29.u8);
	// li r5,2
	ctx.r5.s64 = 2;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// stb r11,85(r1)
	PPC_STORE_U8(ctx.r1.u32 + 85, r11.u8);
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
loc_8267BBFC:
	// li r11,2
	r11.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// li r30,0
	r30.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// b 0x8267bc30
	goto loc_8267BC30;
loc_8267BC10:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_8267BC30:
	// cmplwi cr6,r30,32
	cr6.compare<uint32_t>(r30.u32, 32, xer);
	// blt cr6,0x8267bc10
	if (cr6.lt) goto loc_8267BC10;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwinm. r11,r11,0,22,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267bc74
	if (cr0.eq) goto loc_8267BC74;
	// rlwinm r11,r29,24,8,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 24) & 0xFFFFFF;
	// stb r29,84(r1)
	PPC_STORE_U8(ctx.r1.u32 + 84, r29.u8);
	// rlwinm r10,r29,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0xFFFF;
	// li r5,4
	ctx.r5.s64 = 4;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// stb r11,85(r1)
	PPC_STORE_U8(ctx.r1.u32 + 85, r11.u8);
	// rlwinm r11,r29,8,24,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 8) & 0xFF;
	// stb r10,86(r1)
	PPC_STORE_U8(ctx.r1.u32 + 86, ctx.r10.u8);
	// stb r11,87(r1)
	PPC_STORE_U8(ctx.r1.u32 + 87, r11.u8);
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
loc_8267BC74:
	// li r11,3
	r11.s64 = 3;
	// li r29,0
	r29.s64 = 0;
	// li r30,0
	r30.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// b 0x8267bca8
	goto loc_8267BCA8;
loc_8267BC88:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_8267BCA8:
	// cmplwi cr6,r30,16
	cr6.compare<uint32_t>(r30.u32, 16, xer);
	// blt cr6,0x8267bc88
	if (cr6.lt) goto loc_8267BC88;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwinm. r11,r11,0,22,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267bcdc
	if (cr0.eq) goto loc_8267BCDC;
	// rlwinm r11,r29,24,8,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 24) & 0xFFFFFF;
	// stb r29,84(r1)
	PPC_STORE_U8(ctx.r1.u32 + 84, r29.u8);
	// li r5,2
	ctx.r5.s64 = 2;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// stb r11,85(r1)
	PPC_STORE_U8(ctx.r1.u32 + 85, r11.u8);
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
loc_8267BCDC:
	// li r11,4
	r11.s64 = 4;
	// li r29,0
	r29.s64 = 0;
	// li r30,0
	r30.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwinm. r11,r11,0,21,21
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x400;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267bd5c
	if (cr0.eq) goto loc_8267BD5C;
	// b 0x8267bd1c
	goto loc_8267BD1C;
loc_8267BCFC:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_8267BD1C:
	// cmplwi cr6,r30,16
	cr6.compare<uint32_t>(r30.u32, 16, xer);
	// blt cr6,0x8267bcfc
	if (cr6.lt) goto loc_8267BCFC;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// stw r29,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r29.u32);
	// rlwinm. r11,r11,0,22,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267bd54
	if (cr0.eq) goto loc_8267BD54;
	// rlwinm r11,r29,24,8,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 24) & 0xFFFFFF;
	// stb r29,84(r1)
	PPC_STORE_U8(ctx.r1.u32 + 84, r29.u8);
	// li r5,2
	ctx.r5.s64 = 2;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// stb r11,85(r1)
	PPC_STORE_U8(ctx.r1.u32 + 85, r11.u8);
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
loc_8267BD54:
	// li r29,0
	r29.s64 = 0;
	// li r30,0
	r30.s64 = 0;
loc_8267BD5C:
	// li r11,5
	r11.s64 = 5;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwinm. r10,r11,0,21,21
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x400;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8267bdc4
	if (cr0.eq) goto loc_8267BDC4;
	// lwz r28,56(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r28,r25
	cr6.compare<uint32_t>(r28.u32, r25.u32, xer);
	// ble cr6,0x8267bd80
	if (!cr6.gt) goto loc_8267BD80;
	// mr r28,r25
	r28.u64 = r25.u64;
loc_8267BD80:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8267bdb8
	if (cr6.eq) goto loc_8267BDB8;
	// rlwinm. r11,r11,0,22,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267bda4
	if (cr0.eq) goto loc_8267BDA4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
loc_8267BDA4:
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// subf r25,r28,r25
	r25.s64 = r25.s64 - r28.s64;
	// add r26,r28,r26
	r26.u64 = r28.u64 + r26.u64;
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
loc_8267BDB8:
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267cc18
	if (!cr6.eq) goto loc_8267CC18;
loc_8267BDC4:
	// li r11,6
	r11.s64 = 6;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwinm. r10,r11,0,20,20
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x800;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8267be2c
	if (cr0.eq) goto loc_8267BE2C;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// li r28,0
	r28.s64 = 0;
loc_8267BDE4:
	// lbzx r27,r28,r26
	r27.u64 = PPC_LOAD_U8(r28.u32 + r26.u32);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// cmplwi r27,0
	cr0.compare<uint32_t>(r27.u32, 0, xer);
	// beq 0x8267bdfc
	if (cr0.eq) goto loc_8267BDFC;
	// cmplw cr6,r28,r25
	cr6.compare<uint32_t>(r28.u32, r25.u32, xer);
	// blt cr6,0x8267bde4
	if (cr6.lt) goto loc_8267BDE4;
loc_8267BDFC:
	// rlwinm. r11,r11,0,18,18
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267be18
	if (cr0.eq) goto loc_8267BE18;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
loc_8267BE18:
	// subf r25,r28,r25
	r25.s64 = r25.s64 - r28.s64;
	// add r26,r28,r26
	r26.u64 = r28.u64 + r26.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x8267cc14
	if (!cr6.eq) goto loc_8267CC14;
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_8267BE2C:
	// li r11,7
	r11.s64 = 7;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwinm. r10,r11,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8267be94
	if (cr0.eq) goto loc_8267BE94;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// li r28,0
	r28.s64 = 0;
loc_8267BE4C:
	// lbzx r27,r28,r26
	r27.u64 = PPC_LOAD_U8(r28.u32 + r26.u32);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// cmplwi r27,0
	cr0.compare<uint32_t>(r27.u32, 0, xer);
	// beq 0x8267be64
	if (cr0.eq) goto loc_8267BE64;
	// cmplw cr6,r28,r25
	cr6.compare<uint32_t>(r28.u32, r25.u32, xer);
	// blt cr6,0x8267be4c
	if (cr6.lt) goto loc_8267BE4C;
loc_8267BE64:
	// rlwinm. r11,r11,0,18,18
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267be80
	if (cr0.eq) goto loc_8267BE80;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
loc_8267BE80:
	// subf r25,r28,r25
	r25.s64 = r25.s64 - r28.s64;
	// add r26,r28,r26
	r26.u64 = r28.u64 + r26.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x8267cc14
	if (!cr6.eq) goto loc_8267CC14;
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_8267BE94:
	// li r11,8
	r11.s64 = 8;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwinm. r11,r11,0,22,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267bef0
	if (cr0.eq) goto loc_8267BEF0;
	// b 0x8267becc
	goto loc_8267BECC;
loc_8267BEAC:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_8267BECC:
	// cmplwi cr6,r30,16
	cr6.compare<uint32_t>(r30.u32, 16, xer);
	// blt cr6,0x8267beac
	if (cr6.lt) goto loc_8267BEAC;
	// lhz r11,22(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 22);
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// beq cr6,0x8267bee8
	if (cr6.eq) goto loc_8267BEE8;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// b 0x8267bb4c
	goto loc_8267BB4C;
loc_8267BEE8:
	// li r29,0
	r29.s64 = 0;
	// li r30,0
	r30.s64 = 0;
loc_8267BEF0:
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
	// stw r3,48(r24)
	PPC_STORE_U32(r24.u32 + 48, ctx.r3.u32);
loc_8267BF08:
	// li r11,11
	r11.s64 = 11;
	// b 0x8267ba60
	goto loc_8267BA60;
loc_8267BF10:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// cmplwi cr6,r30,32
	cr6.compare<uint32_t>(r30.u32, 32, xer);
	// blt cr6,0x8267bf10
	if (cr6.lt) goto loc_8267BF10;
	// rlwinm r10,r29,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r11,r29,0,16,23
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFF00;
	// rlwinm r9,r29,24,16,23
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 24) & 0xFF00;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r10,r29,8,24,31
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 8) & 0xFF;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// li r8,10
	ctx.r8.s64 = 10;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// li r29,0
	r29.s64 = 0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// li r30,0
	r30.s64 = 0;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r11,48(r24)
	PPC_STORE_U32(r24.u32 + 48, r11.u32);
	// stw r8,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r8.u32);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8267cbd0
	if (cr6.eq) goto loc_8267CBD0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8267f368
	sub_8267F368(ctx, base);
	// li r11,11
	r11.s64 = 11;
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
	// stw r3,48(r24)
	PPC_STORE_U32(r24.u32 + 48, ctx.r3.u32);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8267bfec
	if (cr6.eq) goto loc_8267BFEC;
	// clrlwi r11,r30,29
	r11.u64 = r30.u32 & 0x7;
	// li r10,24
	ctx.r10.s64 = 24;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// srw r29,r29,r11
	r29.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// b 0x8267cbb8
	goto loc_8267CBB8;
loc_8267BFCC:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_8267BFEC:
	// cmplwi cr6,r30,3
	cr6.compare<uint32_t>(r30.u32, 3, xer);
	// blt cr6,0x8267bfcc
	if (cr6.lt) goto loc_8267BFCC;
	// clrlwi r11,r29,31
	r11.u64 = r29.u32 & 0x1;
	// rlwinm r10,r29,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r9,r30,-1
	ctx.r9.s64 = r30.s64 + -1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// clrlwi r11,r10,30
	r11.u64 = ctx.r10.u32 & 0x3;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x8267c05c
	if (cr6.lt) goto loc_8267C05C;
	// beq cr6,0x8267c038
	if (cr6.eq) goto loc_8267C038;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// blt cr6,0x8267c030
	if (cr6.lt) goto loc_8267C030;
	// bne cr6,0x8267c064
	if (!cr6.eq) goto loc_8267C064;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// stw r11,24(r24)
	PPC_STORE_U32(r24.u32 + 24, r11.u32);
	// stw r22,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r22.u32);
	// b 0x8267c064
	goto loc_8267C064;
loc_8267C030:
	// li r11,15
	r11.s64 = 15;
	// b 0x8267c060
	goto loc_8267C060;
loc_8267C038:
	// addi r11,r19,2048
	r11.s64 = r19.s64 + 2048;
	// stw r19,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r19.u32);
	// li r8,9
	ctx.r8.s64 = 9;
	// stw r11,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r11.u32);
	// li r11,5
	r11.s64 = 5;
	// stw r8,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r8.u32);
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
	// li r11,18
	r11.s64 = 18;
	// b 0x8267c060
	goto loc_8267C060;
loc_8267C05C:
	// li r11,13
	r11.s64 = 13;
loc_8267C060:
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_8267C064:
	// rlwinm r29,r10,30,2,31
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r30,r9,-2
	r30.s64 = ctx.r9.s64 + -2;
	// b 0x8267cbb8
	goto loc_8267CBB8;
	// clrlwi r11,r30,29
	r11.u64 = r30.u32 & 0x7;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// srw r29,r29,r11
	r29.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// b 0x8267c0a0
	goto loc_8267C0A0;
loc_8267C080:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_8267C0A0:
	// cmplwi cr6,r30,32
	cr6.compare<uint32_t>(r30.u32, 32, xer);
	// blt cr6,0x8267c080
	if (cr6.lt) goto loc_8267C080;
	// not r10,r29
	ctx.r10.u64 = ~r29.u64;
	// clrlwi r11,r29,16
	r11.u64 = r29.u32 & 0xFFFF;
	// rlwinm r10,r10,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x8267c0c4
	if (cr6.eq) goto loc_8267C0C4;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// b 0x8267bb4c
	goto loc_8267BB4C;
loc_8267C0C4:
	// li r10,14
	ctx.r10.s64 = 14;
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
	// li r29,0
	r29.s64 = 0;
	// li r30,0
	r30.s64 = 0;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// lwz r28,56(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplwi r28,0
	cr0.compare<uint32_t>(r28.u32, 0, xer);
	// beq 0x8267bf08
	if (cr0.eq) goto loc_8267BF08;
	// cmplw cr6,r28,r25
	cr6.compare<uint32_t>(r28.u32, r25.u32, xer);
	// ble cr6,0x8267c0f0
	if (!cr6.gt) goto loc_8267C0F0;
	// mr r28,r25
	r28.u64 = r25.u64;
loc_8267C0F0:
	// cmplw cr6,r28,r21
	cr6.compare<uint32_t>(r28.u32, r21.u32, xer);
	// ble cr6,0x8267c0fc
	if (!cr6.gt) goto loc_8267C0FC;
	// mr r28,r21
	r28.u64 = r21.u64;
loc_8267C0FC:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// subf r25,r28,r25
	r25.s64 = r25.s64 - r28.s64;
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	// add r26,r28,r26
	r26.u64 = r28.u64 + r26.u64;
	// subf r21,r28,r21
	r21.s64 = r21.s64 - r28.s64;
	// add r20,r28,r20
	r20.u64 = r28.u64 + r20.u64;
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
	// b 0x8267cbb8
	goto loc_8267CBB8;
loc_8267C134:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// cmplwi cr6,r30,14
	cr6.compare<uint32_t>(r30.u32, 14, xer);
	// blt cr6,0x8267c134
	if (cr6.lt) goto loc_8267C134;
	// clrlwi r10,r29,27
	ctx.r10.u64 = r29.u32 & 0x1F;
	// rlwinm r11,r29,27,5,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 27) & 0x7FFFFFF;
	// addi r9,r10,257
	ctx.r9.s64 = ctx.r10.s64 + 257;
	// clrlwi r10,r11,27
	ctx.r10.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r29,r11,28,4,31
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// stw r9,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r9.u32);
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// addi r30,r30,-14
	r30.s64 = r30.s64 + -14;
	// cmplwi cr6,r9,286
	cr6.compare<uint32_t>(ctx.r9.u32, 286, xer);
	// stw r10,92(r31)
	PPC_STORE_U32(r31.u32 + 92, ctx.r10.u32);
	// clrlwi r10,r11,28
	ctx.r10.u64 = r11.u32 & 0xF;
	// addi r11,r10,4
	r11.s64 = ctx.r10.s64 + 4;
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// bgt cr6,0x8267c1bc
	if (cr6.gt) goto loc_8267C1BC;
	// lwz r11,92(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// bgt cr6,0x8267c1bc
	if (cr6.gt) goto loc_8267C1BC;
	// li r11,16
	r11.s64 = 16;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r10,96(r31)
	PPC_STORE_U32(r31.u32 + 96, ctx.r10.u32);
	// b 0x8267c220
	goto loc_8267C220;
loc_8267C1BC:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// b 0x8267bb4c
	goto loc_8267BB4C;
loc_8267C1C4:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_8267C1E4:
	// cmplwi cr6,r30,3
	cr6.compare<uint32_t>(r30.u32, 3, xer);
	// blt cr6,0x8267c1c4
	if (cr6.lt) goto loc_8267C1C4;
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// addi r10,r19,2176
	ctx.r10.s64 = r19.s64 + 2176;
	// clrlwi r9,r29,29
	ctx.r9.u64 = r29.u32 & 0x7;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r29,r29,29,3,31
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 29) & 0x1FFFFFFF;
	// addi r30,r30,-3
	r30.s64 = r30.s64 + -3;
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// addi r11,r11,52
	r11.s64 = r11.s64 + 52;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r9,r11,r31
	PPC_STORE_U16(r11.u32 + r31.u32, ctx.r9.u16);
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
loc_8267C220:
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8267c1e4
	if (cr6.lt) goto loc_8267C1E4;
	// b 0x8267c260
	goto loc_8267C260;
loc_8267C234:
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// addi r10,r19,2176
	ctx.r10.s64 = r19.s64 + 2176;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,52
	r11.s64 = r11.s64 + 52;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r10,r11,r31
	PPC_STORE_U16(r11.u32 + r31.u32, ctx.r10.u16);
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
loc_8267C260:
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// cmplwi cr6,r11,19
	cr6.compare<uint32_t>(r11.u32, 19, xer);
	// blt cr6,0x8267c234
	if (cr6.lt) goto loc_8267C234;
	// addi r11,r31,1320
	r11.s64 = r31.s64 + 1320;
	// addi r6,r31,100
	ctx.r6.s64 = r31.s64 + 100;
	// addi r7,r31,76
	ctx.r7.s64 = r31.s64 + 76;
	// li r10,7
	ctx.r10.s64 = 7;
	// addi r8,r31,744
	ctx.r8.s64 = r31.s64 + 744;
	// li r5,19
	ctx.r5.s64 = 19;
	// stw r11,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r11.u32);
	// addi r4,r31,104
	ctx.r4.s64 = r31.s64 + 104;
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r10.u32);
	// bl 0x8267eef0
	sub_8267EEF0(ctx, base);
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x8267c2b0
	if (cr0.eq) goto loc_8267C2B0;
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// b 0x8267bb4c
	goto loc_8267BB4C;
loc_8267C2B0:
	// li r11,17
	r11.s64 = 17;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r10,96(r31)
	PPC_STORE_U32(r31.u32 + 96, ctx.r10.u32);
	// b 0x8267c4f8
	goto loc_8267C4F8;
loc_8267C2C4:
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,68(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// slw r11,r23,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r11,r11,r29
	r11.u64 = r11.u64 & r29.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// b 0x8267c320
	goto loc_8267C320;
loc_8267C2E4:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// lwz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r9,68(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// slw r11,r23,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r10.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r11,r11,r29
	r11.u64 = r11.u64 & r29.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
loc_8267C320:
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lbz r11,81(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// cmplw cr6,r10,r30
	cr6.compare<uint32_t>(ctx.r10.u32, r30.u32, xer);
	// bgt cr6,0x8267c2e4
	if (cr6.gt) goto loc_8267C2E4;
	// lhz r9,82(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// bge cr6,0x8267c39c
	if (!cr6.lt) goto loc_8267C39C;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// b 0x8267c36c
	goto loc_8267C36C;
loc_8267C34C:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r10,r10,r30
	ctx.r10.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
loc_8267C36C:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x8267c34c
	if (cr6.lt) goto loc_8267C34C;
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// srw r29,r29,r11
	r29.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// addi r10,r10,52
	ctx.r10.s64 = ctx.r10.s64 + 52;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r9,r10,r31
	PPC_STORE_U16(ctx.r10.u32 + r31.u32, ctx.r9.u16);
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,96(r31)
	PPC_STORE_U32(r31.u32 + 96, ctx.r10.u32);
	// b 0x8267c4f8
	goto loc_8267C4F8;
loc_8267C39C:
	// bne cr6,0x8267c408
	if (!cr6.eq) goto loc_8267C408;
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// addi r11,r9,2
	r11.s64 = ctx.r9.s64 + 2;
	// b 0x8267c3cc
	goto loc_8267C3CC;
loc_8267C3AC:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r10,r10,r30
	ctx.r10.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
loc_8267C3CC:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x8267c3ac
	if (cr6.lt) goto loc_8267C3AC;
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// subf r30,r9,r30
	r30.s64 = r30.s64 - ctx.r9.s64;
	// srw r29,r29,r9
	r29.u64 = ctx.r9.u8 & 0x20 ? 0 : (r29.u32 >> (ctx.r9.u8 & 0x3F));
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8267c514
	if (cr0.eq) goto loc_8267C514;
	// addi r10,r11,51
	ctx.r10.s64 = r11.s64 + 51;
	// clrlwi r11,r29,30
	r11.u64 = r29.u32 & 0x3;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// rlwinm r29,r29,30,2,31
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r30,r30,-2
	r30.s64 = r30.s64 + -2;
	// lhzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r31.u32);
	// b 0x8267c4ac
	goto loc_8267C4AC;
loc_8267C408:
	// cmplwi cr6,r10,17
	cr6.compare<uint32_t>(ctx.r10.u32, 17, xer);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// bne cr6,0x8267c460
	if (!cr6.eq) goto loc_8267C460;
	// addi r10,r11,3
	ctx.r10.s64 = r11.s64 + 3;
	// b 0x8267c43c
	goto loc_8267C43C;
loc_8267C41C:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r9,r9,r30
	ctx.r9.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r9,r29
	r29.u64 = ctx.r9.u64 + r29.u64;
loc_8267C43C:
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// blt cr6,0x8267c41c
	if (cr6.lt) goto loc_8267C41C;
	// subf r9,r11,r30
	ctx.r9.s64 = r30.s64 - r11.s64;
	// srw r11,r29,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// addi r30,r9,-3
	r30.s64 = ctx.r9.s64 + -3;
	// clrlwi r9,r11,29
	ctx.r9.u64 = r11.u32 & 0x7;
	// rlwinm r29,r11,29,3,31
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// addi r11,r9,3
	r11.s64 = ctx.r9.s64 + 3;
	// b 0x8267c4a8
	goto loc_8267C4A8;
loc_8267C460:
	// addi r10,r11,7
	ctx.r10.s64 = r11.s64 + 7;
	// b 0x8267c488
	goto loc_8267C488;
loc_8267C468:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r9,r9,r30
	ctx.r9.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r9,r29
	r29.u64 = ctx.r9.u64 + r29.u64;
loc_8267C488:
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// blt cr6,0x8267c468
	if (cr6.lt) goto loc_8267C468;
	// subf r9,r11,r30
	ctx.r9.s64 = r30.s64 - r11.s64;
	// srw r11,r29,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// addi r30,r9,-7
	r30.s64 = ctx.r9.s64 + -7;
	// clrlwi r9,r11,25
	ctx.r9.u64 = r11.u32 & 0x7F;
	// rlwinm r29,r11,25,7,31
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1FFFFFF;
	// addi r11,r9,11
	r11.s64 = ctx.r9.s64 + 11;
loc_8267C4A8:
	// li r10,0
	ctx.r10.s64 = 0;
loc_8267C4AC:
	// lwz r7,96(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// lwz r9,92(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// lwz r8,88(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmplw cr6,r7,r9
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r9.u32, xer);
	// bgt cr6,0x8267c514
	if (cr6.gt) goto loc_8267C514;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267c4f8
	if (cr6.eq) goto loc_8267C4F8;
	// clrlwi r9,r10,16
	ctx.r9.u64 = ctx.r10.u32 & 0xFFFF;
loc_8267C4D4:
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,52
	ctx.r10.s64 = ctx.r10.s64 + 52;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r9,r10,r31
	PPC_STORE_U16(ctx.r10.u32 + r31.u32, ctx.r9.u16);
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,96(r31)
	PPC_STORE_U32(r31.u32 + 96, ctx.r10.u32);
	// bne 0x8267c4d4
	if (!cr0.eq) goto loc_8267C4D4;
loc_8267C4F8:
	// lwz r10,88(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// lwz r11,92(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// lwz r9,96(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x8267c2c4
	if (cr6.lt) goto loc_8267C2C4;
	// b 0x8267c51c
	goto loc_8267C51C;
loc_8267C514:
	// stw r18,24(r24)
	PPC_STORE_U32(r24.u32 + 24, r18.u32);
	// stw r22,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r22.u32);
loc_8267C51C:
	// addi r11,r31,1320
	r11.s64 = r31.s64 + 1320;
	// lwz r5,88(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// addi r28,r31,100
	r28.s64 = r31.s64 + 100;
	// addi r7,r31,76
	ctx.r7.s64 = r31.s64 + 76;
	// li r10,9
	ctx.r10.s64 = 9;
	// addi r27,r31,744
	r27.s64 = r31.s64 + 744;
	// addi r4,r31,104
	ctx.r4.s64 = r31.s64 + 104;
	// stw r11,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r11.u32);
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// stw r10,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r10.u32);
	// bl 0x8267eef0
	sub_8267EEF0(ctx, base);
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x8267c570
	if (cr0.eq) goto loc_8267C570;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// stw r11,24(r24)
	PPC_STORE_U32(r24.u32 + 24, r11.u32);
	// b 0x8267cbb4
	goto loc_8267CBB4;
loc_8267C570:
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// addi r7,r31,80
	ctx.r7.s64 = r31.s64 + 80;
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// addi r11,r11,52
	r11.s64 = r11.s64 + 52;
	// lwz r5,92(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// li r3,2
	ctx.r3.s64 = 2;
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// stw r10,72(r31)
	PPC_STORE_U32(r31.u32 + 72, ctx.r10.u32);
	// li r11,6
	r11.s64 = 6;
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// bl 0x8267eef0
	sub_8267EEF0(ctx, base);
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// beq 0x8267c5c0
	if (cr0.eq) goto loc_8267C5C0;
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// b 0x8267bb4c
	goto loc_8267BB4C;
loc_8267C5C0:
	// li r11,18
	r11.s64 = 18;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// cmplwi cr6,r25,6
	cr6.compare<uint32_t>(r25.u32, 6, xer);
	// blt cr6,0x8267c618
	if (cr6.lt) goto loc_8267C618;
	// cmplwi cr6,r21,258
	cr6.compare<uint32_t>(r21.u32, 258, xer);
	// blt cr6,0x8267c618
	if (cr6.lt) goto loc_8267C618;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// stw r20,12(r24)
	PPC_STORE_U32(r24.u32 + 12, r20.u32);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r21,16(r24)
	PPC_STORE_U32(r24.u32 + 16, r21.u32);
	// stw r26,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r26.u32);
	// stw r25,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r25.u32);
	// stw r29,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r29.u32);
	// stw r30,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r30.u32);
	// bl 0x8267ea90
	sub_8267EA90(ctx, base);
	// lwz r20,12(r24)
	r20.u64 = PPC_LOAD_U32(r24.u32 + 12);
	// lwz r21,16(r24)
	r21.u64 = PPC_LOAD_U32(r24.u32 + 16);
	// lwz r26,0(r24)
	r26.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r25,4(r24)
	r25.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// lwz r29,48(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r30,52(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// b 0x8267cbb8
	goto loc_8267CBB8;
loc_8267C618:
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r7,68(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// slw r11,r23,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r11,r11,r29
	r11.u64 = r11.u64 & r29.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r7
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// b 0x8267c674
	goto loc_8267C674;
loc_8267C638:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// lwz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r9,68(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// slw r11,r23,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r10.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r11,r11,r29
	r11.u64 = r11.u64 & r29.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
loc_8267C674:
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lbz r10,81(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// bgt cr6,0x8267c638
	if (cr6.gt) goto loc_8267C638;
	// lbz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq 0x8267c730
	if (cr0.eq) goto loc_8267C730;
	// rlwinm. r9,r8,0,24,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xF0;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x8267c730
	if (!cr0.eq) goto loc_8267C730;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lbz r11,89(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 89);
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lhz r9,82(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// slw r8,r23,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r8.u8 & 0x3F));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// and r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 & r29.u64;
	// srw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
	// b 0x8267c704
	goto loc_8267C704;
loc_8267C6C4:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r8,0(r26)
	ctx.r8.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// lbz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 88);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r7,68(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// add r6,r10,r11
	ctx.r6.u64 = ctx.r10.u64 + r11.u64;
	// lhz r9,90(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// slw r10,r8,r30
	ctx.r10.u64 = r30.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// slw r10,r23,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r6.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 & r29.u64;
	// srw r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
loc_8267C704:
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r7
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r7.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lbz r10,81(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// bgt cr6,0x8267c6c4
	if (cr6.gt) goto loc_8267C6C4;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// srw r29,r29,r11
	r29.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
loc_8267C730:
	// clrlwi r11,r10,24
	r11.u64 = ctx.r10.u32 & 0xFF;
	// lhz r9,82(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// lbz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,56(r31)
	PPC_STORE_U32(r31.u32 + 56, ctx.r9.u32);
	// srw r29,r29,r11
	r29.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// bne 0x8267c758
	if (!cr0.eq) goto loc_8267C758;
	// li r11,23
	r11.s64 = 23;
	// b 0x8267ba60
	goto loc_8267BA60;
loc_8267C758:
	// rlwinm. r11,r10,0,26,26
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8267bf08
	if (!cr0.eq) goto loc_8267BF08;
	// rlwinm. r11,r10,0,25,25
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8267c770
	if (cr0.eq) goto loc_8267C770;
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// b 0x8267bb4c
	goto loc_8267BB4C;
loc_8267C770:
	// clrlwi r11,r10,28
	r11.u64 = ctx.r10.u32 & 0xF;
	// li r10,19
	ctx.r10.s64 = 19;
	// stw r11,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r11.u32);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// lwz r11,64(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8267c7e0
	if (cr0.eq) goto loc_8267C7E0;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bge cr6,0x8267c7c0
	if (!cr6.lt) goto loc_8267C7C0;
loc_8267C794:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// lwz r9,64(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r10,r10,r30
	ctx.r10.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// cmplw cr6,r30,r9
	cr6.compare<uint32_t>(r30.u32, ctx.r9.u32, xer);
	// blt cr6,0x8267c794
	if (cr6.lt) goto loc_8267C794;
loc_8267C7C0:
	// slw r10,r23,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 << (r11.u8 & 0x3F));
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 & r29.u64;
	// srw r29,r29,r11
	r29.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
loc_8267C7E0:
	// li r11,20
	r11.s64 = 20;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r7,72(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// slw r11,r23,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r11,r11,r29
	r11.u64 = r11.u64 & r29.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r7
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// b 0x8267c844
	goto loc_8267C844;
loc_8267C808:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// lwz r10,80(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r9,72(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// slw r11,r23,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r10.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r11,r11,r29
	r11.u64 = r11.u64 & r29.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
loc_8267C844:
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lbz r10,81(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// bgt cr6,0x8267c808
	if (cr6.gt) goto loc_8267C808;
	// lbz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// rlwinm. r9,r8,0,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFF0;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x8267c8f8
	if (!cr0.eq) goto loc_8267C8F8;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lbz r11,89(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 89);
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lhz r9,82(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// slw r8,r23,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r8.u8 & 0x3F));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// and r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 & r29.u64;
	// srw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
	// b 0x8267c8cc
	goto loc_8267C8CC;
loc_8267C88C:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r8,0(r26)
	ctx.r8.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// lbz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 88);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r7,72(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// add r6,r10,r11
	ctx.r6.u64 = ctx.r10.u64 + r11.u64;
	// lhz r9,90(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// slw r10,r8,r30
	ctx.r10.u64 = r30.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// slw r10,r23,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r6.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 & r29.u64;
	// srw r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
loc_8267C8CC:
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r7
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r7.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lbz r10,81(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// bgt cr6,0x8267c88c
	if (cr6.gt) goto loc_8267C88C;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// srw r29,r29,r11
	r29.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
loc_8267C8F8:
	// lbz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// clrlwi r11,r10,24
	r11.u64 = ctx.r10.u32 & 0xFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// rlwinm. r10,r9,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// srw r29,r29,r11
	r29.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// beq 0x8267c918
	if (cr0.eq) goto loc_8267C918;
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// b 0x8267bb4c
	goto loc_8267BB4C;
loc_8267C918:
	// clrlwi r10,r9,28
	ctx.r10.u64 = ctx.r9.u32 & 0xF;
	// lhz r11,82(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// li r9,21
	ctx.r9.s64 = 21;
	// stw r11,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r11.u32);
	// stw r10,64(r31)
	PPC_STORE_U32(r31.u32 + 64, ctx.r10.u32);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// lwz r11,64(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8267c990
	if (cr0.eq) goto loc_8267C990;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bge cr6,0x8267c970
	if (!cr6.lt) goto loc_8267C970;
loc_8267C944:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// lwz r9,64(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r10,r10,r30
	ctx.r10.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// cmplw cr6,r30,r9
	cr6.compare<uint32_t>(r30.u32, ctx.r9.u32, xer);
	// blt cr6,0x8267c944
	if (cr6.lt) goto loc_8267C944;
loc_8267C970:
	// slw r10,r23,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 << (r11.u8 & 0x3F));
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 & r29.u64;
	// srw r29,r29,r11
	r29.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r11.u32);
loc_8267C990:
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// subf r11,r21,r11
	r11.s64 = r11.s64 - r21.s64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x8267c9b0
	if (!cr6.gt) goto loc_8267C9B0;
	// stw r14,24(r24)
	PPC_STORE_U32(r24.u32 + 24, r14.u32);
	// b 0x8267cbb4
	goto loc_8267CBB4;
loc_8267C9B0:
	// li r11,22
	r11.s64 = 22;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// subf r9,r21,r27
	ctx.r9.s64 = r27.s64 - r21.s64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// ble cr6,0x8267ca14
	if (!cr6.gt) goto loc_8267CA14;
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8267c9f8
	if (!cr6.gt) goto loc_8267C9F8;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// b 0x8267ca04
	goto loc_8267CA04;
loc_8267C9F8:
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
loc_8267CA04:
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8267ca20
	if (!cr6.gt) goto loc_8267CA20;
	// b 0x8267ca1c
	goto loc_8267CA1C;
loc_8267CA14:
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// subf r9,r11,r20
	ctx.r9.s64 = r20.s64 - r11.s64;
loc_8267CA1C:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8267CA20:
	// cmplw cr6,r11,r21
	cr6.compare<uint32_t>(r11.u32, r21.u32, xer);
	// ble cr6,0x8267ca2c
	if (!cr6.gt) goto loc_8267CA2C;
	// mr r11,r21
	r11.u64 = r21.u64;
loc_8267CA2C:
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// subf r21,r11,r21
	r21.s64 = r21.s64 - r11.s64;
	// stw r10,56(r31)
	PPC_STORE_U32(r31.u32 + 56, ctx.r10.u32);
loc_8267CA38:
	// lbz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r10,0(r20)
	PPC_STORE_U8(r20.u32 + 0, ctx.r10.u8);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// bne 0x8267ca38
	if (!cr0.eq) goto loc_8267CA38;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267cbb8
	if (!cr6.eq) goto loc_8267CBB8;
loc_8267CA5C:
	// li r11,18
	r11.s64 = 18;
	// b 0x8267ba60
	goto loc_8267BA60;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// stb r11,0(r20)
	PPC_STORE_U8(r20.u32 + 0, r11.u8);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// b 0x8267ca5c
	goto loc_8267CA5C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8267cb58
	if (cr6.eq) goto loc_8267CB58;
	// b 0x8267cab0
	goto loc_8267CAB0;
loc_8267CA90:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_8267CAB0:
	// cmplwi cr6,r30,32
	cr6.compare<uint32_t>(r30.u32, 32, xer);
	// blt cr6,0x8267ca90
	if (cr6.lt) goto loc_8267CA90;
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// subf. r5,r21,r27
	ctx.r5.s64 = r27.s64 - r21.s64;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r11,20(r24)
	PPC_STORE_U32(r24.u32 + 20, r11.u32);
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// beq 0x8267cb00
	if (cr0.eq) goto loc_8267CB00;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r4,r5,r20
	ctx.r4.s64 = r20.s64 - ctx.r5.s64;
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8267caf4
	if (cr6.eq) goto loc_8267CAF4;
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// b 0x8267caf8
	goto loc_8267CAF8;
loc_8267CAF4:
	// bl 0x8267f368
	sub_8267F368(ctx, base);
loc_8267CAF8:
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
	// stw r3,48(r24)
	PPC_STORE_U32(r24.u32 + 48, ctx.r3.u32);
loc_8267CB00:
	// mr r27,r21
	r27.u64 = r21.u64;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// beq cr6,0x8267cb1c
	if (cr6.eq) goto loc_8267CB1C;
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x8267cb3c
	goto loc_8267CB3C;
loc_8267CB1C:
	// rlwinm r10,r29,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r11,r29,0,16,23
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFF00;
	// rlwinm r9,r29,24,16,23
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 24) & 0xFF00;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r10,r29,8,24,31
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 8) & 0xFF;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_8267CB3C:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x8267cb50
	if (cr6.eq) goto loc_8267CB50;
	// stw r15,24(r24)
	PPC_STORE_U32(r24.u32 + 24, r15.u32);
	// b 0x8267cbb4
	goto loc_8267CBB4;
loc_8267CB50:
	// li r29,0
	r29.s64 = 0;
	// li r30,0
	r30.s64 = 0;
loc_8267CB58:
	// li r11,25
	r11.s64 = 25;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8267cbf8
	if (cr6.eq) goto loc_8267CBF8;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8267cbf8
	if (cr6.eq) goto loc_8267CBF8;
	// b 0x8267cb9c
	goto loc_8267CB9C;
loc_8267CB7C:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8267cc18
	if (cr6.eq) goto loc_8267CC18;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_8267CB9C:
	// cmplwi cr6,r30,32
	cr6.compare<uint32_t>(r30.u32, 32, xer);
	// blt cr6,0x8267cb7c
	if (cr6.lt) goto loc_8267CB7C;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// beq cr6,0x8267cbf0
	if (cr6.eq) goto loc_8267CBF0;
	// stw r16,24(r24)
	PPC_STORE_U32(r24.u32 + 24, r16.u32);
loc_8267CBB4:
	// stw r22,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r22.u32);
loc_8267CBB8:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r10,28
	cr6.compare<uint32_t>(ctx.r10.u32, 28, xer);
	// ble cr6,0x8267ba28
	if (!cr6.gt) goto loc_8267BA28;
loc_8267CBC4:
	// li r3,-2
	ctx.r3.s64 = -2;
loc_8267CBC8:
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x8239bd10
	return;
loc_8267CBD0:
	// stw r20,12(r24)
	PPC_STORE_U32(r24.u32 + 12, r20.u32);
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r21,16(r24)
	PPC_STORE_U32(r24.u32 + 16, r21.u32);
	// stw r26,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r26.u32);
	// stw r25,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r25.u32);
	// stw r29,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r29.u32);
	// stw r30,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r30.u32);
	// b 0x8267cbc8
	goto loc_8267CBC8;
loc_8267CBF0:
	// li r29,0
	r29.s64 = 0;
	// li r30,0
	r30.s64 = 0;
loc_8267CBF8:
	// li r11,26
	r11.s64 = 26;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r23,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r23.u32);
	// b 0x8267cc18
	goto loc_8267CC18;
	// li r11,-3
	r11.s64 = -3;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// b 0x8267cc18
	goto loc_8267CC18;
loc_8267CC14:
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_8267CC18:
	// stw r20,12(r24)
	PPC_STORE_U32(r24.u32 + 12, r20.u32);
	// stw r21,16(r24)
	PPC_STORE_U32(r24.u32 + 16, r21.u32);
	// stw r26,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r26.u32);
	// stw r25,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r25.u32);
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// stw r29,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r29.u32);
	// stw r30,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r30.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267cc54
	if (!cr6.eq) goto loc_8267CC54;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpwi cr6,r11,24
	cr6.compare<int32_t>(r11.s32, 24, xer);
	// bge cr6,0x8267cc78
	if (!cr6.lt) goto loc_8267CC78;
	// lwz r11,16(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 16);
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// beq cr6,0x8267cc78
	if (cr6.eq) goto loc_8267CC78;
loc_8267CC54:
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x8267b790
	sub_8267B790(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x8267cc78
	if (cr0.eq) goto loc_8267CC78;
	// li r11,28
	r11.s64 = 28;
	// li r3,-4
	ctx.r3.s64 = -4;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// b 0x8267cbc8
	goto loc_8267CBC8;
loc_8267CC78:
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// lwz r10,16(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 16);
	// lwz r9,148(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// subf r30,r10,r27
	r30.s64 = r27.s64 - ctx.r10.s64;
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// subf r29,r11,r9
	r29.s64 = ctx.r9.s64 - r11.s64;
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// stw r11,20(r24)
	PPC_STORE_U32(r24.u32 + 20, r11.u32);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// beq cr6,0x8267ccf4
	if (cr6.eq) goto loc_8267CCF4;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8267ccf4
	if (cr6.eq) goto loc_8267CCF4;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,12(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 12);
	// subf r4,r30,r11
	ctx.r4.s64 = r11.s64 - r30.s64;
	// beq cr6,0x8267cce8
	if (cr6.eq) goto loc_8267CCE8;
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// b 0x8267ccec
	goto loc_8267CCEC;
loc_8267CCE8:
	// bl 0x8267f368
	sub_8267F368(ctx, base);
loc_8267CCEC:
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
	// stw r3,48(r24)
	PPC_STORE_U32(r24.u32 + 48, ctx.r3.u32);
loc_8267CCF4:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// subfic r9,r10,0
	xer.ca = ctx.r10.u32 <= 0;
	ctx.r9.s64 = 0 - ctx.r10.s64;
	// lwz r10,52(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// addi r11,r11,-11
	r11.s64 = r11.s64 + -11;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + xer.ca < xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + xer.ca;
	xer.ca = temp.u8;
	// cntlzw r8,r11
	ctx.r8.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r9,0,25,25
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x40;
	// rlwinm r9,r8,2,24,24
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0x80;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,44(r24)
	PPC_STORE_U32(r24.u32 + 44, r11.u32);
	// bne cr6,0x8267cd34
	if (!cr6.eq) goto loc_8267CD34;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8267cd40
	if (cr6.eq) goto loc_8267CD40;
loc_8267CD34:
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x8267cd54
	if (!cr6.eq) goto loc_8267CD54;
loc_8267CD40:
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8267cbc8
	if (!cr6.eq) goto loc_8267CBC8;
	// li r3,-5
	ctx.r3.s64 = -5;
	// b 0x8267cbc8
	goto loc_8267CBC8;
loc_8267CD54:
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// b 0x8267cbc8
	goto loc_8267CBC8;
	// li r3,-4
	ctx.r3.s64 = -4;
	// b 0x8267cbc8
	goto loc_8267CBC8;
}

__attribute__((alias("__imp__sub_8267CD64"))) PPC_WEAK_FUNC(sub_8267CD64);
PPC_FUNC_IMPL(__imp__sub_8267CD64) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267CD68"))) PPC_WEAK_FUNC(sub_8267CD68);
PPC_FUNC_IMPL(__imp__sub_8267CD68) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8267cdd8
	if (cr6.eq) goto loc_8267CDD8;
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x8267cdd8
	if (cr0.eq) goto loc_8267CDD8;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8267cdd8
	if (cr0.eq) goto loc_8267CDD8;
	// lwz r4,44(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8267cdb4
	if (cr0.eq) goto loc_8267CDB4;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8267CDB4:
	// lwz r4,28(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r11,0
	r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// b 0x8267cddc
	goto loc_8267CDDC;
loc_8267CDD8:
	// li r3,-2
	ctx.r3.s64 = -2;
loc_8267CDDC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267CDF0"))) PPC_WEAK_FUNC(sub_8267CDF0);
PPC_FUNC_IMPL(__imp__sub_8267CDF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r30,20(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// ble cr6,0x8267ce20
	if (!cr6.gt) goto loc_8267CE20;
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
loc_8267CE20:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8267ce94
	if (cr6.eq) goto loc_8267CE94;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,16(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// subf r9,r30,r9
	ctx.r9.s64 = ctx.r9.s64 - r30.s64;
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
	// stw r9,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r9.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// subf r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8267ce94
	if (!cr6.eq) goto loc_8267CE94;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
loc_8267CE94:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267CEAC"))) PPC_WEAK_FUNC(sub_8267CEAC);
PPC_FUNC_IMPL(__imp__sub_8267CEAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267CEB0"))) PPC_WEAK_FUNC(sub_8267CEB0);
PPC_FUNC_IMPL(__imp__sub_8267CEB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8267d4b8
	if (cr6.eq) goto loc_8267D4B8;
	// lwz r31,28(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x8267d4b8
	if (cr0.eq) goto loc_8267D4B8;
	// cmplwi cr6,r27,4
	cr6.compare<uint32_t>(r27.u32, 4, xer);
	// bgt cr6,0x8267d4b8
	if (cr6.gt) goto loc_8267D4B8;
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267d4a8
	if (cr6.eq) goto loc_8267D4A8;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267cf04
	if (!cr6.eq) goto loc_8267CF04;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267d4a8
	if (!cr6.eq) goto loc_8267D4A8;
loc_8267CF04:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi cr6,r11,666
	cr6.compare<int32_t>(r11.s32, 666, xer);
	// bne cr6,0x8267cf18
	if (!cr6.eq) goto loc_8267CF18;
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// bne cr6,0x8267d4a8
	if (!cr6.eq) goto loc_8267D4A8;
loc_8267CF18:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8267cf3c
	if (!cr6.eq) goto loc_8267CF3C;
loc_8267CF24:
	// lis r11,-32243
	r11.s64 = -2113077248;
	// li r3,-5
	ctx.r3.s64 = -5;
	// addi r11,r11,7056
	r11.s64 = r11.s64 + 7056;
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r11.u32);
	// b 0x8267d4bc
	goto loc_8267D4BC;
loc_8267CF3C:
	// lwz r28,32(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmpwi cr6,r11,42
	cr6.compare<int32_t>(r11.s32, 42, xer);
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// li r29,0
	r29.s64 = 0;
	// stw r27,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r27.u32);
	// bne cr6,0x8267d1cc
	if (!cr6.eq) goto loc_8267D1CC;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8267d088
	if (!cr6.eq) goto loc_8267D088;
	// lwz r8,20(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// li r11,31
	r11.s64 = 31;
	// lwz r7,8(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r10,139
	ctx.r10.s64 = 139;
	// li r9,8
	ctx.r9.s64 = 8;
	// stbx r11,r7,r8
	PPC_STORE_U8(ctx.r7.u32 + ctx.r8.u32, r11.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r10,r8,r11
	PPC_STORE_U8(ctx.r8.u32 + r11.u32, ctx.r10.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r29,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r29.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r29,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r29.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r29,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r29.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r29,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r29.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r29,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r29.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,124(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 124);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r10,9
	cr6.compare<int32_t>(ctx.r10.s32, 9, xer);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// bne cr6,0x8267d024
	if (!cr6.eq) goto loc_8267D024;
	// li r10,2
	ctx.r10.s64 = 2;
	// b 0x8267d040
	goto loc_8267D040;
loc_8267D024:
	// lwz r9,128(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// cmpwi cr6,r9,2
	cr6.compare<int32_t>(ctx.r9.s32, 2, xer);
	// bge cr6,0x8267d03c
	if (!cr6.lt) goto loc_8267D03C;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// bge cr6,0x8267d040
	if (!cr6.lt) goto loc_8267D040;
loc_8267D03C:
	// li r10,4
	ctx.r10.s64 = 4;
loc_8267D040:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r8,255
	ctx.r8.s64 = 255;
	// li r7,113
	ctx.r7.s64 = 113;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stbx r10,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r10.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r8,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r8.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// stw r7,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r7.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// b 0x8267d1c8
	goto loc_8267D1C8;
loc_8267D088:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r10,128(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// rlwinm r11,r11,12,0,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFFFF000;
	// addi r10,r11,2048
	ctx.r10.s64 = r11.s64 + 2048;
	// bge cr6,0x8267d0d8
	if (!cr6.lt) goto loc_8267D0D8;
	// lwz r11,124(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 124);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// blt cr6,0x8267d0d8
	if (cr6.lt) goto loc_8267D0D8;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bge cr6,0x8267d0c0
	if (!cr6.lt) goto loc_8267D0C0;
	// li r11,1
	r11.s64 = 1;
	// b 0x8267d0dc
	goto loc_8267D0DC;
loc_8267D0C0:
	// addi r11,r11,-6
	r11.s64 = r11.s64 + -6;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// b 0x8267d0dc
	goto loc_8267D0DC;
loc_8267D0D8:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_8267D0DC:
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8267d0f4
	if (cr6.eq) goto loc_8267D0F4;
	// ori r11,r11,32
	r11.u64 = r11.u64 | 32;
loc_8267D0F4:
	// li r10,31
	ctx.r10.s64 = 31;
	// li r9,113
	ctx.r9.s64 = 113;
	// divwu r10,r11,r10
	ctx.r10.u32 = r11.u32 / ctx.r10.u32;
	// mulli r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 * 31;
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// rlwinm r8,r11,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFFFFFF;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// stbx r8,r9,r10
	PPC_STORE_U8(ctx.r9.u32 + ctx.r10.u32, ctx.r8.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r7,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r7.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r9,100(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
	// beq cr6,0x8267d1b8
	if (cr6.eq) goto loc_8267D1B8;
	// lhz r11,48(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 48);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r8,r11,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFFFFFF;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// stbx r8,r9,r10
	PPC_STORE_U8(ctx.r9.u32 + ctx.r10.u32, ctx.r8.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r7,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r7.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lhz r10,50(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// rlwinm r8,r10,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// stbx r8,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r8.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r10,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r10.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
loc_8267D1B8:
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8267f368
	sub_8267F368(ctx, base);
loc_8267D1C8:
	// stw r3,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r3.u32);
loc_8267D1CC:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8267d1fc
	if (cr6.eq) goto loc_8267D1FC;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267d218
	if (!cr6.eq) goto loc_8267D218;
loc_8267D1EC:
	// li r11,-1
	r11.s64 = -1;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// b 0x8267d4bc
	goto loc_8267D4BC;
loc_8267D1FC:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267d218
	if (!cr6.eq) goto loc_8267D218;
	// cmpw cr6,r27,r28
	cr6.compare<int32_t>(r27.s32, r28.s32, xer);
	// bgt cr6,0x8267d218
	if (cr6.gt) goto loc_8267D218;
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// bne cr6,0x8267cf24
	if (!cr6.eq) goto loc_8267CF24;
loc_8267D218:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi cr6,r11,666
	cr6.compare<int32_t>(r11.s32, 666, xer);
	// bne cr6,0x8267d230
	if (!cr6.eq) goto loc_8267D230;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8267cf24
	if (!cr6.eq) goto loc_8267CF24;
loc_8267D230:
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8267d258
	if (!cr6.eq) goto loc_8267D258;
	// lwz r10,108(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8267d258
	if (!cr6.eq) goto loc_8267D258;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x8267d31c
	if (cr6.eq) goto loc_8267D31C;
	// cmpwi cr6,r11,666
	cr6.compare<int32_t>(r11.s32, 666, xer);
	// beq cr6,0x8267d31c
	if (cr6.eq) goto loc_8267D31C;
loc_8267D258:
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r10,124(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 124);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// addi r11,r11,6928
	r11.s64 = r11.s64 + 6928;
	// mulli r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 * 12;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// beq cr6,0x8267d290
	if (cr6.eq) goto loc_8267D290;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// bne cr6,0x8267d298
	if (!cr6.eq) goto loc_8267D298;
loc_8267D290:
	// li r11,666
	r11.s64 = 666;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
loc_8267D298:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8267d48c
	if (cr6.eq) goto loc_8267D48C;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// beq cr6,0x8267d48c
	if (cr6.eq) goto loc_8267D48C;
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// bne cr6,0x8267d31c
	if (!cr6.eq) goto loc_8267D31C;
	// cmpwi cr6,r27,1
	cr6.compare<int32_t>(r27.s32, 1, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bne cr6,0x8267d2c4
	if (!cr6.eq) goto loc_8267D2C4;
	// bl 0x82680e50
	sub_82680E50(ctx, base);
	// b 0x8267d308
	goto loc_8267D308;
loc_8267D2C4:
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82680dc0
	sub_82680DC0(ctx, base);
	// cmpwi cr6,r27,3
	cr6.compare<int32_t>(r27.s32, 3, xer);
	// bne cr6,0x8267d308
	if (!cr6.eq) goto loc_8267D308;
	// lwz r11,68(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sth r29,-2(r11)
	PPC_STORE_U16(r11.u32 + -2, r29.u16);
	// lwz r11,68(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// lwz r3,60(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_8267D308:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267d1ec
	if (cr6.eq) goto loc_8267D1EC;
loc_8267D31C:
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// bne cr6,0x8267d4a0
	if (!cr6.eq) goto loc_8267D4A0;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bgt 0x8267d338
	if (cr0.gt) goto loc_8267D338;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8267d4bc
	goto loc_8267D4BC;
loc_8267D338:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8267d3fc
	if (!cr6.eq) goto loc_8267D3FC;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,48(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// stbx r10,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r10.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lbz r9,50(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 50);
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lbz r9,49(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 49);
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lbz r9,48(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 48);
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lbz r9,10(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 10);
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lbz r9,9(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 9);
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lbz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 8);
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
	// b 0x8267d454
	goto loc_8267D454;
loc_8267D3FC:
	// lhz r11,48(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 48);
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// rlwinm r8,r11,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFFFFFF;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// stbx r8,r9,r10
	PPC_STORE_U8(ctx.r9.u32 + ctx.r10.u32, ctx.r8.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r7,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r7.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lhz r10,50(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// rlwinm r8,r10,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// stbx r8,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r8.u8);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stbx r10,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r10.u8);
loc_8267D454:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x8267d47c
	if (!cr0.gt) goto loc_8267D47C;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
loc_8267D47C:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x8267d4bc
	goto loc_8267D4BC;
loc_8267D48C:
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267d4a0
	if (!cr6.eq) goto loc_8267D4A0;
	// li r11,-1
	r11.s64 = -1;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
loc_8267D4A0:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8267d4bc
	goto loc_8267D4BC;
loc_8267D4A8:
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,7056
	r11.s64 = r11.s64 + 7056;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r11.u32);
loc_8267D4B8:
	// li r3,-2
	ctx.r3.s64 = -2;
loc_8267D4BC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8267D4C4"))) PPC_WEAK_FUNC(sub_8267D4C4);
PPC_FUNC_IMPL(__imp__sub_8267D4C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267D4C8"))) PPC_WEAK_FUNC(sub_8267D4C8);
PPC_FUNC_IMPL(__imp__sub_8267D4C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8267d5b8
	if (cr6.eq) goto loc_8267D5B8;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8267d5b8
	if (cr0.eq) goto loc_8267D5B8;
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r30,42
	cr6.compare<int32_t>(r30.s32, 42, xer);
	// beq cr6,0x8267d510
	if (cr6.eq) goto loc_8267D510;
	// cmpwi cr6,r30,113
	cr6.compare<int32_t>(r30.s32, 113, xer);
	// beq cr6,0x8267d510
	if (cr6.eq) goto loc_8267D510;
	// cmpwi cr6,r30,666
	cr6.compare<int32_t>(r30.s32, 666, xer);
	// bne cr6,0x8267d5b8
	if (!cr6.eq) goto loc_8267D5B8;
loc_8267D510:
	// lwz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8267d52c
	if (cr0.eq) goto loc_8267D52C;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8267D52C:
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r4,60(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8267d54c
	if (cr0.eq) goto loc_8267D54C;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8267D54C:
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r4,56(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8267d56c
	if (cr0.eq) goto loc_8267D56C;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8267D56C:
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r4,48(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8267d58c
	if (cr0.eq) goto loc_8267D58C;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8267D58C:
	// lwz r4,28(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r30,113
	cr6.compare<int32_t>(r30.s32, 113, xer);
	// stw r3,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r3.u32);
	// bne cr6,0x8267d5bc
	if (!cr6.eq) goto loc_8267D5BC;
	// li r3,-3
	ctx.r3.s64 = -3;
	// b 0x8267d5bc
	goto loc_8267D5BC;
loc_8267D5B8:
	// li r3,-2
	ctx.r3.s64 = -2;
loc_8267D5BC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267D5D4"))) PPC_WEAK_FUNC(sub_8267D5D4);
PPC_FUNC_IMPL(__imp__sub_8267D5D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267D5D8"))) PPC_WEAK_FUNC(sub_8267D5D8);
PPC_FUNC_IMPL(__imp__sub_8267D5D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce4
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r7,36(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// lwz r11,100(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 100);
	// lwz r28,48(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// addi r5,r7,-262
	ctx.r5.s64 = ctx.r7.s64 + -262;
	// lwz r9,112(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	// lwz r26,116(r10)
	r26.u64 = PPC_LOAD_U32(ctx.r10.u32 + 116);
	// add r8,r28,r11
	ctx.r8.u64 = r28.u64 + r11.u64;
	// lwz r27,136(r10)
	r27.u64 = PPC_LOAD_U32(ctx.r10.u32 + 136);
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// cmplw cr6,r11,r5
	cr6.compare<uint32_t>(r11.u32, ctx.r5.u32, xer);
	// ble cr6,0x8267d61c
	if (!cr6.gt) goto loc_8267D61C;
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// addi r25,r11,262
	r25.s64 = r11.s64 + 262;
	// b 0x8267d620
	goto loc_8267D620;
loc_8267D61C:
	// li r25,0
	r25.s64 = 0;
loc_8267D620:
	// add r11,r6,r8
	r11.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lwz r7,132(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 132);
	// lwz r30,56(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	// addi r5,r8,258
	ctx.r5.s64 = ctx.r8.s64 + 258;
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// lwz r29,44(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// lbz r31,-1(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// blt cr6,0x8267d648
	if (cr6.lt) goto loc_8267D648;
	// rlwinm r26,r26,30,2,31
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 30) & 0x3FFFFFFF;
loc_8267D648:
	// lwz r3,108(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 108);
	// cmplw cr6,r27,r3
	cr6.compare<uint32_t>(r27.u32, ctx.r3.u32, xer);
	// ble cr6,0x8267d658
	if (!cr6.gt) goto loc_8267D658;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
loc_8267D658:
	// add r11,r28,r4
	r11.u64 = r28.u64 + ctx.r4.u64;
	// clrlwi r24,r7,24
	r24.u64 = ctx.r7.u32 & 0xFF;
	// add r9,r11,r6
	ctx.r9.u64 = r11.u64 + ctx.r6.u64;
	// lbz r23,0(r9)
	r23.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplw cr6,r23,r24
	cr6.compare<uint32_t>(r23.u32, r24.u32, xer);
	// bne cr6,0x8267d7a4
	if (!cr6.eq) goto loc_8267D7A4;
	// lbz r9,-1(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + -1);
	// clrlwi r24,r31,24
	r24.u64 = r31.u32 & 0xFF;
	// cmplw cr6,r9,r24
	cr6.compare<uint32_t>(ctx.r9.u32, r24.u32, xer);
	// bne cr6,0x8267d7a4
	if (!cr6.eq) goto loc_8267D7A4;
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r24,0(r8)
	r24.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// cmplw cr6,r9,r24
	cr6.compare<uint32_t>(ctx.r9.u32, r24.u32, xer);
	// bne cr6,0x8267d7a4
	if (!cr6.eq) goto loc_8267D7A4;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// lbz r11,1(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplw cr6,r24,r11
	cr6.compare<uint32_t>(r24.u32, r11.u32, xer);
	// bne cr6,0x8267d7a4
	if (!cr6.eq) goto loc_8267D7A4;
	// addi r11,r8,2
	r11.s64 = ctx.r8.s64 + 2;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
loc_8267D6AC:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplw cr6,r8,r24
	cr6.compare<uint32_t>(ctx.r8.u32, r24.u32, xer);
	// bne cr6,0x8267d774
	if (!cr6.eq) goto loc_8267D774;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplw cr6,r8,r24
	cr6.compare<uint32_t>(ctx.r8.u32, r24.u32, xer);
	// bne cr6,0x8267d774
	if (!cr6.eq) goto loc_8267D774;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplw cr6,r8,r24
	cr6.compare<uint32_t>(ctx.r8.u32, r24.u32, xer);
	// bne cr6,0x8267d774
	if (!cr6.eq) goto loc_8267D774;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplw cr6,r8,r24
	cr6.compare<uint32_t>(ctx.r8.u32, r24.u32, xer);
	// bne cr6,0x8267d774
	if (!cr6.eq) goto loc_8267D774;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplw cr6,r8,r24
	cr6.compare<uint32_t>(ctx.r8.u32, r24.u32, xer);
	// bne cr6,0x8267d774
	if (!cr6.eq) goto loc_8267D774;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplw cr6,r8,r24
	cr6.compare<uint32_t>(ctx.r8.u32, r24.u32, xer);
	// bne cr6,0x8267d774
	if (!cr6.eq) goto loc_8267D774;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplw cr6,r8,r24
	cr6.compare<uint32_t>(ctx.r8.u32, r24.u32, xer);
	// bne cr6,0x8267d774
	if (!cr6.eq) goto loc_8267D774;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplw cr6,r8,r24
	cr6.compare<uint32_t>(ctx.r8.u32, r24.u32, xer);
	// bne cr6,0x8267d774
	if (!cr6.eq) goto loc_8267D774;
	// cmplw cr6,r11,r5
	cr6.compare<uint32_t>(r11.u32, ctx.r5.u32, xer);
	// blt cr6,0x8267d6ac
	if (cr6.lt) goto loc_8267D6AC;
loc_8267D774:
	// subf r11,r5,r11
	r11.s64 = r11.s64 - ctx.r5.s64;
	// addi r8,r5,-258
	ctx.r8.s64 = ctx.r5.s64 + -258;
	// addi r11,r11,258
	r11.s64 = r11.s64 + 258;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// ble cr6,0x8267d7a4
	if (!cr6.gt) goto loc_8267D7A4;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// stw r4,104(r10)
	PPC_STORE_U32(ctx.r10.u32 + 104, ctx.r4.u32);
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// bge cr6,0x8267d7c0
	if (!cr6.lt) goto loc_8267D7C0;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// lbz r31,-1(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
loc_8267D7A4:
	// and r11,r29,r4
	r11.u64 = r29.u64 & ctx.r4.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r4,r11,r30
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// cmplw cr6,r4,r25
	cr6.compare<uint32_t>(ctx.r4.u32, r25.u32, xer);
	// ble cr6,0x8267d7c0
	if (!cr6.gt) goto loc_8267D7C0;
	// addic. r26,r26,-1
	xer.ca = r26.u32 > 0;
	r26.s64 = r26.s64 + -1;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// bne 0x8267d658
	if (!cr0.eq) goto loc_8267D658;
loc_8267D7C0:
	// cmplw cr6,r6,r3
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r3.u32, xer);
	// bgt cr6,0x8267d7cc
	if (cr6.gt) goto loc_8267D7CC;
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
loc_8267D7CC:
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_8267D7D0"))) PPC_WEAK_FUNC(sub_8267D7D0);
PPC_FUNC_IMPL(__imp__sub_8267D7D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,48(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lwz r11,100(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 100);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// addi r9,r11,258
	ctx.r9.s64 = r11.s64 + 258;
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bne cr6,0x8267d900
	if (!cr6.eq) goto loc_8267D900;
	// lbz r8,1(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r7,1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bne cr6,0x8267d900
	if (!cr6.eq) goto loc_8267D900;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
loc_8267D80C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bne cr6,0x8267d8d4
	if (!cr6.eq) goto loc_8267D8D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bne cr6,0x8267d8d4
	if (!cr6.eq) goto loc_8267D8D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bne cr6,0x8267d8d4
	if (!cr6.eq) goto loc_8267D8D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bne cr6,0x8267d8d4
	if (!cr6.eq) goto loc_8267D8D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bne cr6,0x8267d8d4
	if (!cr6.eq) goto loc_8267D8D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bne cr6,0x8267d8d4
	if (!cr6.eq) goto loc_8267D8D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bne cr6,0x8267d8d4
	if (!cr6.eq) goto loc_8267D8D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bne cr6,0x8267d8d4
	if (!cr6.eq) goto loc_8267D8D4;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x8267d80c
	if (cr6.lt) goto loc_8267D80C;
loc_8267D8D4:
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r11,r11,258
	r11.s64 = r11.s64 + 258;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// blt cr6,0x8267d900
	if (cr6.lt) goto loc_8267D900;
	// lwz r10,108(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 108);
	// stw r4,104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 104, ctx.r4.u32);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8267d8f8
	if (!cr6.gt) goto loc_8267D8F8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8267D8F8:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
loc_8267D900:
	// li r3,2
	ctx.r3.s64 = 2;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267D908"))) PPC_WEAK_FUNC(sub_8267D908);
PPC_FUNC_IMPL(__imp__sub_8267D908) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r28,36(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 36);
loc_8267D91C:
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r9,52(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// lwz r8,108(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// add r10,r28,r10
	ctx.r10.u64 = r28.u64 + ctx.r10.u64;
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r10,r10,-262
	ctx.r10.s64 = ctx.r10.s64 + -262;
	// subf r26,r11,r9
	r26.s64 = ctx.r9.s64 - r11.s64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8267d9e4
	if (cr6.lt) goto loc_8267D9E4;
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// add r4,r3,r28
	ctx.r4.u64 = ctx.r3.u64 + r28.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r9,100(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r8,104(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// subf r9,r28,r9
	ctx.r9.s64 = ctx.r9.s64 - r28.s64;
	// lwz r7,84(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r11,68(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// subf r8,r28,r8
	ctx.r8.s64 = ctx.r8.s64 - r28.s64;
	// subf r7,r28,r7
	ctx.r7.s64 = ctx.r7.s64 - r28.s64;
	// lwz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// stw r9,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r9.u32);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r8,104(r31)
	PPC_STORE_U32(r31.u32 + 104, ctx.r8.u32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r7,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r7.u32);
loc_8267D988:
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
	// lhz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r28
	cr6.compare<uint32_t>(ctx.r9.u32, r28.u32, xer);
	// subf r9,r28,r9
	ctx.r9.s64 = ctx.r9.s64 - r28.s64;
	// bge cr6,0x8267d9a0
	if (!cr6.lt) goto loc_8267D9A0;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8267D9A0:
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// bne 0x8267d988
	if (!cr0.eq) goto loc_8267D988;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_8267D9BC:
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// subf r10,r28,r10
	ctx.r10.s64 = ctx.r10.s64 - r28.s64;
	// bge cr6,0x8267d9d4
	if (!cr6.lt) goto loc_8267D9D4;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8267D9D4:
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// bne 0x8267d9bc
	if (!cr0.eq) goto loc_8267D9BC;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
loc_8267D9E4:
	// lwz r30,0(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267dafc
	if (cr6.eq) goto loc_8267DAFC;
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r27,r11,r10
	r27.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// mr r29,r11
	r29.u64 = r11.u64;
	// cmplw cr6,r29,r26
	cr6.compare<uint32_t>(r29.u32, r26.u32, xer);
	// ble cr6,0x8267da1c
	if (!cr6.gt) goto loc_8267DA1C;
	// mr r29,r26
	r29.u64 = r26.u64;
loc_8267DA1C:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x8267da2c
	if (!cr6.eq) goto loc_8267DA2C;
	// li r11,0
	r11.s64 = 0;
	// b 0x8267daa0
	goto loc_8267DAA0;
loc_8267DA2C:
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// lwz r10,28(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// lwz r11,24(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8267da58
	if (!cr6.eq) goto loc_8267DA58;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r3,48(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// bl 0x8267f368
	sub_8267F368(ctx, base);
	// b 0x8267da70
	goto loc_8267DA70;
loc_8267DA58:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8267da74
	if (!cr6.eq) goto loc_8267DA74;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r3,48(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// bl 0x8267f868
	sub_8267F868(ctx, base);
loc_8267DA70:
	// stw r3,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r3.u32);
loc_8267DA74:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r11,r29
	r11.u64 = r29.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// add r10,r29,r10
	ctx.r10.u64 = r29.u64 + ctx.r10.u64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
loc_8267DAA0:
	// lwz r10,108(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplwi cr6,r9,3
	cr6.compare<uint32_t>(ctx.r9.u32, 3, xer);
	// stw r9,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r9.u32);
	// blt cr6,0x8267dae4
	if (cr6.lt) goto loc_8267DAE4;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r8,80(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r7,76(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// stw r10,64(r31)
	PPC_STORE_U32(r31.u32 + 64, ctx.r10.u32);
	// lbz r11,1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// slw r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r8.u8 & 0x3F));
	// xor r11,r8,r11
	r11.u64 = ctx.r8.u64 ^ r11.u64;
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// stw r11,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r11.u32);
loc_8267DAE4:
	// cmplwi cr6,r9,262
	cr6.compare<uint32_t>(ctx.r9.u32, 262, xer);
	// bge cr6,0x8267dafc
	if (!cr6.lt) goto loc_8267DAFC;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267d91c
	if (!cr6.eq) goto loc_8267D91C;
loc_8267DAFC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8267DB04"))) PPC_WEAK_FUNC(sub_8267DB04);
PPC_FUNC_IMPL(__imp__sub_8267DB04) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267DB08"))) PPC_WEAK_FUNC(sub_8267DB08);
PPC_FUNC_IMPL(__imp__sub_8267DB08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r30,0
	r30.s64 = 0;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// ori r30,r30,65535
	r30.u64 = r30.u64 | 65535;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r11,r11,-5
	r11.s64 = r11.s64 + -5;
	// cmplwi cr6,r11,65535
	cr6.compare<uint32_t>(r11.u32, 65535, xer);
	// bge cr6,0x8267db38
	if (!cr6.lt) goto loc_8267DB38;
	// mr r30,r11
	r30.u64 = r11.u64;
loc_8267DB38:
	// lwz r10,108(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bgt cr6,0x8267db58
	if (cr6.gt) goto loc_8267DB58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8267d908
	sub_8267D908(ctx, base);
	// lwz r10,108(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x8267dc48
	if (cr0.eq) goto loc_8267DC48;
loc_8267DB58:
	// lwz r9,100(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// li r11,0
	r11.s64 = 0;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r11,108(r31)
	PPC_STORE_U32(r31.u32 + 108, r11.u32);
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// stw r10,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x8267db88
	if (cr0.eq) goto loc_8267DB88;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x8267dbdc
	if (cr6.lt) goto loc_8267DBDC;
loc_8267DB88:
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// stw r9,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r9.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r10.u32);
	// blt cr6,0x8267dba8
	if (cr6.lt) goto loc_8267DBA8;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8267dbac
	goto loc_8267DBAC;
loc_8267DBA8:
	// li r4,0
	ctx.r4.s64 = 0;
loc_8267DBAC:
	// li r6,0
	ctx.r6.s64 = 0;
	// subf r5,r11,r9
	ctx.r5.s64 = ctx.r9.s64 - r11.s64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82681188
	sub_82681188(ctx, base);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267dc3c
	if (cr6.eq) goto loc_8267DC3C;
loc_8267DBDC:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r9,100(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// subf r5,r11,r9
	ctx.r5.s64 = ctx.r9.s64 - r11.s64;
	// addi r10,r10,-262
	ctx.r10.s64 = ctx.r10.s64 + -262;
	// cmplw cr6,r5,r10
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r10.u32, xer);
	// blt cr6,0x8267db38
	if (cr6.lt) goto loc_8267DB38;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x8267dc0c
	if (cr6.lt) goto loc_8267DC0C;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8267dc10
	goto loc_8267DC10;
loc_8267DC0C:
	// li r4,0
	ctx.r4.s64 = 0;
loc_8267DC10:
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82681188
	sub_82681188(ctx, base);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267db38
	if (!cr6.eq) goto loc_8267DB38;
loc_8267DC3C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_8267DC40:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_8267DC48:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8267dc3c
	if (cr6.eq) goto loc_8267DC3C;
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x8267dc68
	if (cr0.lt) goto loc_8267DC68;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8267dc6c
	goto loc_8267DC6C;
loc_8267DC68:
	// li r4,0
	ctx.r4.s64 = 0;
loc_8267DC6C:
	// addi r10,r29,-4
	ctx.r10.s64 = r29.s64 + -4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r6,r10,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// subf r5,r11,r10
	ctx.r5.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x82681188
	sub_82681188(ctx, base);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267dcb8
	if (!cr6.eq) goto loc_8267DCB8;
	// addi r11,r29,-4
	r11.s64 = r29.s64 + -4;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r11,28,30,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x2;
	// b 0x8267dc40
	goto loc_8267DC40;
loc_8267DCB8:
	// cmpwi cr6,r29,4
	cr6.compare<int32_t>(r29.s32, 4, xer);
	// li r3,3
	ctx.r3.s64 = 3;
	// beq cr6,0x8267dc40
	if (cr6.eq) goto loc_8267DC40;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8267dc40
	goto loc_8267DC40;
}

__attribute__((alias("__imp__sub_8267DCCC"))) PPC_WEAK_FUNC(sub_8267DCCC);
PPC_FUNC_IMPL(__imp__sub_8267DCCC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267DCD0"))) PPC_WEAK_FUNC(sub_8267DCD0);
PPC_FUNC_IMPL(__imp__sub_8267DCD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// li r29,0
	r29.s64 = 0;
	// addi r27,r11,17200
	r27.s64 = r11.s64 + 17200;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r30,r29
	r30.u64 = r29.u64;
	// addi r28,r11,17712
	r28.s64 = r11.s64 + 17712;
loc_8267DCFC:
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmplwi cr6,r11,262
	cr6.compare<uint32_t>(r11.u32, 262, xer);
	// bge cr6,0x8267dd2c
	if (!cr6.lt) goto loc_8267DD2C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8267d908
	sub_8267D908(ctx, base);
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmplwi cr6,r11,262
	cr6.compare<uint32_t>(r11.u32, 262, xer);
	// bge cr6,0x8267dd24
	if (!cr6.lt) goto loc_8267DD24;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x8267e0a0
	if (cr6.eq) goto loc_8267E0A0;
loc_8267DD24:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267e0ac
	if (cr6.eq) goto loc_8267E0AC;
loc_8267DD2C:
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// blt cr6,0x8267dda8
	if (cr6.lt) goto loc_8267DDA8;
	// lwz r7,64(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// lwz r9,80(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r5,44(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// lwz r8,76(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// lwz r6,60(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// lwz r4,56(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// slw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r11,2(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// xor r11,r9,r11
	r11.u64 = ctx.r9.u64 ^ r11.u64;
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r11.u32);
	// lhzx r11,r10,r6
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r6.u32);
	// sthx r11,r7,r4
	PPC_STORE_U16(ctx.r7.u32 + ctx.r4.u32, r11.u16);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lwz r9,64(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// and r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 & r11.u64;
	// lwz r8,56(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// lwz r7,60(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r30,r10,r8
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// sthx r11,r9,r7
	PPC_STORE_U16(ctx.r9.u32 + ctx.r7.u32, r11.u16);
loc_8267DDA8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8267de04
	if (cr6.eq) goto loc_8267DE04;
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// subf r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	// addi r11,r11,-262
	r11.s64 = r11.s64 + -262;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bgt cr6,0x8267de04
	if (cr6.gt) goto loc_8267DE04;
	// lwz r11,128(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bge cr6,0x8267dde4
	if (!cr6.lt) goto loc_8267DDE4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8267d5d8
	sub_8267D5D8(ctx, base);
	// b 0x8267de00
	goto loc_8267DE00;
loc_8267DDE4:
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x8267de04
	if (!cr6.eq) goto loc_8267DE04;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bne cr6,0x8267de04
	if (!cr6.eq) goto loc_8267DE04;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8267d7d0
	sub_8267D7D0(ctx, base);
loc_8267DE00:
	// stw r3,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r3.u32);
loc_8267DE04:
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// blt cr6,0x8267dfcc
	if (cr6.lt) goto loc_8267DFCC;
	// lwz r8,100(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// addi r11,r11,253
	r11.s64 = r11.s64 + 253;
	// lwz r10,104(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// lwz r7,5784(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r9,5788(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5788);
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// addis r10,r6,1
	ctx.r10.s64 = ctx.r6.s64 + 65536;
	// sthx r5,r8,r9
	PPC_STORE_U16(ctx.r8.u32 + ctx.r9.u32, ctx.r5.u16);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r9,5784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// lwz r8,5776(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 5776);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// cmplwi cr6,r10,256
	cr6.compare<uint32_t>(ctx.r10.u32, 256, xer);
	// stbx r11,r8,r9
	PPC_STORE_U8(ctx.r8.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,5784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,5784(r31)
	PPC_STORE_U32(r31.u32 + 5784, r11.u32);
	// lbzx r11,r7,r28
	r11.u64 = PPC_LOAD_U8(ctx.r7.u32 + r28.u32);
	// addi r11,r11,292
	r11.s64 = r11.s64 + 292;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r9,r11,r31
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + r31.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// sthx r9,r11,r31
	PPC_STORE_U16(r11.u32 + r31.u32, ctx.r9.u16);
	// bge cr6,0x8267de90
	if (!cr6.lt) goto loc_8267DE90;
	// lbzx r11,r10,r27
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// b 0x8267de9c
	goto loc_8267DE9C;
loc_8267DE90:
	// rlwinm r11,r10,25,7,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x1FFFFFF;
	// addi r10,r27,256
	ctx.r10.s64 = r27.s64 + 256;
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
loc_8267DE9C:
	// addi r11,r11,608
	r11.s64 = r11.s64 + 608;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r31.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// sthx r10,r11,r31
	PPC_STORE_U16(r11.u32 + r31.u32, ctx.r10.u16);
	// lwz r10,5780(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 5780);
	// lwz r9,5784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// lwz r8,108(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r7,120(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - r11.s64;
	// stw r10,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r10.u32);
	// bgt cr6,0x8267df84
	if (cr6.gt) goto loc_8267DF84;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// blt cr6,0x8267df84
	if (cr6.lt) goto loc_8267DF84;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
loc_8267DEF4:
	// lwz r8,64(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// lwz r7,80(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r4,44(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lwz r6,76(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lwz r5,60(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// stw r11,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r11.u32);
	// slw r8,r8,r7
	ctx.r8.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r7.u8 & 0x3F));
	// and r7,r4,r11
	ctx.r7.u64 = ctx.r4.u64 & r11.u64;
	// lbz r11,2(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r11.u32);
	// lhzx r11,r10,r5
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r5.u32);
	// sthx r11,r7,r3
	PPC_STORE_U16(ctx.r7.u32 + ctx.r3.u32, r11.u16);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lwz r8,64(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// and r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 & r11.u64;
	// lwz r7,56(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// lwz r6,60(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r30,r10,r7
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r7.u32);
	// sthx r11,r8,r6
	PPC_STORE_U16(ctx.r8.u32 + ctx.r6.u32, r11.u16);
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
	// bne 0x8267def4
	if (!cr0.eq) goto loc_8267DEF4;
	// b 0x8267e03c
	goto loc_8267E03C;
loc_8267DF84:
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// stw r29,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r29.u32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r8,80(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r7,76(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// stw r11,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// stw r10,64(r31)
	PPC_STORE_U32(r31.u32 + 64, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lbz r11,1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// slw r10,r10,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r8.u8 & 0x3F));
	// xor r11,r10,r11
	r11.u64 = ctx.r10.u64 ^ r11.u64;
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// stw r11,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r11.u32);
	// b 0x8267e048
	goto loc_8267E048;
loc_8267DFCC:
	// lwz r8,5784(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,5788(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 5788);
	// lbzx r11,r11,r9
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// sthx r29,r8,r10
	PPC_STORE_U16(ctx.r8.u32 + ctx.r10.u32, r29.u16);
	// lwz r9,5784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r8,5776(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 5776);
	// addi r10,r10,35
	ctx.r10.s64 = ctx.r10.s64 + 35;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stbx r11,r8,r9
	PPC_STORE_U8(ctx.r8.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,5784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,5784(r31)
	PPC_STORE_U32(r31.u32 + 5784, r11.u32);
	// lhzx r11,r10,r31
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + r31.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sthx r11,r10,r31
	PPC_STORE_U16(ctx.r10.u32 + r31.u32, r11.u16);
	// lwz r9,5780(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5780);
	// lwz r10,108(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// lwz r8,5784(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// stw r10,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r10.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
loc_8267E03C:
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r11.u32);
loc_8267E048:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8267dcfc
	if (cr6.eq) goto loc_8267DCFC;
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x8267e068
	if (cr0.lt) goto loc_8267E068;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// b 0x8267e06c
	goto loc_8267E06C;
loc_8267E068:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
loc_8267E06C:
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// subf r5,r11,r10
	ctx.r5.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x82681188
	sub_82681188(ctx, base);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267dcfc
	if (!cr6.eq) goto loc_8267DCFC;
loc_8267E0A0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_8267E0A4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_8267E0AC:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x8267e0c4
	if (cr0.lt) goto loc_8267E0C4;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8267e0c8
	goto loc_8267E0C8;
loc_8267E0C4:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
loc_8267E0C8:
	// addi r10,r26,-4
	ctx.r10.s64 = r26.s64 + -4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r6,r10,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// subf r5,r11,r10
	ctx.r5.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x82681188
	sub_82681188(ctx, base);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267e114
	if (!cr6.eq) goto loc_8267E114;
	// addi r11,r26,-4
	r11.s64 = r26.s64 + -4;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r11,28,30,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x2;
	// b 0x8267e0a4
	goto loc_8267E0A4;
loc_8267E114:
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// li r3,3
	ctx.r3.s64 = 3;
	// beq cr6,0x8267e0a4
	if (cr6.eq) goto loc_8267E0A4;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8267e0a4
	goto loc_8267E0A4;
}

__attribute__((alias("__imp__sub_8267E128"))) PPC_WEAK_FUNC(sub_8267E128);
PPC_FUNC_IMPL(__imp__sub_8267E128) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// li r29,0
	r29.s64 = 0;
	// addi r25,r11,17200
	r25.s64 = r11.s64 + 17200;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r27,r11,17712
	r27.s64 = r11.s64 + 17712;
	// lis r11,0
	r11.s64 = 0;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r28,r29
	r28.u64 = r29.u64;
	// li r26,2
	r26.s64 = 2;
	// ori r30,r11,65535
	r30.u64 = r11.u64 | 65535;
loc_8267E160:
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmplwi cr6,r11,262
	cr6.compare<uint32_t>(r11.u32, 262, xer);
	// bge cr6,0x8267e190
	if (!cr6.lt) goto loc_8267E190;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8267d908
	sub_8267D908(ctx, base);
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmplwi cr6,r11,262
	cr6.compare<uint32_t>(r11.u32, 262, xer);
	// bge cr6,0x8267e188
	if (!cr6.lt) goto loc_8267E188;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x8267e588
	if (cr6.eq) goto loc_8267E588;
loc_8267E188:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267e5b8
	if (cr6.eq) goto loc_8267E5B8;
loc_8267E190:
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// blt cr6,0x8267e20c
	if (cr6.lt) goto loc_8267E20C;
	// lwz r7,64(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// lwz r9,80(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r5,44(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// lwz r8,76(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// lwz r6,60(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// lwz r4,56(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// slw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r11,2(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// xor r11,r9,r11
	r11.u64 = ctx.r9.u64 ^ r11.u64;
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r11.u32);
	// lhzx r11,r10,r6
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r6.u32);
	// sthx r11,r7,r4
	PPC_STORE_U16(ctx.r7.u32 + ctx.r4.u32, r11.u16);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lwz r9,64(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// and r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 & r11.u64;
	// lwz r8,56(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// lwz r7,60(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r28,r10,r8
	r28.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// sthx r11,r9,r7
	PPC_STORE_U16(ctx.r9.u32 + ctx.r7.u32, r11.u16);
loc_8267E20C:
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// lwz r10,104(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// stw r26,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r26.u32);
	// stw r11,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r11.u32);
	// stw r10,92(r31)
	PPC_STORE_U32(r31.u32 + 92, ctx.r10.u32);
	// beq cr6,0x8267e2c4
	if (cr6.eq) goto loc_8267E2C4;
	// lwz r10,120(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x8267e2c4
	if (!cr6.lt) goto loc_8267E2C4;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// addi r10,r11,-262
	ctx.r10.s64 = r11.s64 + -262;
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x8267e2c4
	if (cr6.gt) goto loc_8267E2C4;
	// lwz r10,128(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bge cr6,0x8267e26c
	if (!cr6.lt) goto loc_8267E26C;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8267d5d8
	sub_8267D5D8(ctx, base);
	// b 0x8267e288
	goto loc_8267E288;
loc_8267E26C:
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// bne cr6,0x8267e28c
	if (!cr6.eq) goto loc_8267E28C;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x8267e28c
	if (!cr6.eq) goto loc_8267E28C;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8267d7d0
	sub_8267D7D0(ctx, base);
loc_8267E288:
	// stw r3,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r3.u32);
loc_8267E28C:
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bgt cr6,0x8267e2c4
	if (cr6.gt) goto loc_8267E2C4;
	// lwz r10,128(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// beq cr6,0x8267e2c0
	if (cr6.eq) goto loc_8267E2C0;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x8267e2c4
	if (!cr6.eq) goto loc_8267E2C4;
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,104(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmplwi cr6,r11,4096
	cr6.compare<uint32_t>(r11.u32, 4096, xer);
	// ble cr6,0x8267e2c4
	if (!cr6.gt) goto loc_8267E2C4;
loc_8267E2C0:
	// stw r26,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r26.u32);
loc_8267E2C4:
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// blt cr6,0x8267e4b0
	if (cr6.lt) goto loc_8267E4B0;
	// lwz r10,88(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bgt cr6,0x8267e4b0
	if (cr6.gt) goto loc_8267E4B0;
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// addi r11,r11,253
	r11.s64 = r11.s64 + 253;
	// lwz r8,92(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// lwz r9,108(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// lwz r6,5784(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r7,5788(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 5788);
	// add r9,r8,r30
	ctx.r9.u64 = ctx.r8.u64 + r30.u64;
	// addi r8,r10,-3
	ctx.r8.s64 = ctx.r10.s64 + -3;
	// clrlwi r10,r9,16
	ctx.r10.u64 = ctx.r9.u32 & 0xFFFF;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// sthx r4,r6,r7
	PPC_STORE_U16(ctx.r6.u32 + ctx.r7.u32, ctx.r4.u16);
	// lwz r9,5784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// cmplwi cr6,r10,256
	cr6.compare<uint32_t>(ctx.r10.u32, 256, xer);
	// lwz r7,5776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 5776);
	// stbx r11,r7,r9
	PPC_STORE_U8(ctx.r7.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,5784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,5784(r31)
	PPC_STORE_U32(r31.u32 + 5784, r11.u32);
	// lbzx r11,r5,r27
	r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + r27.u32);
	// addi r11,r11,292
	r11.s64 = r11.s64 + 292;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r9,r11,r31
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + r31.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// sthx r9,r11,r31
	PPC_STORE_U16(r11.u32 + r31.u32, ctx.r9.u16);
	// bge cr6,0x8267e364
	if (!cr6.lt) goto loc_8267E364;
	// lbzx r11,r10,r25
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + r25.u32);
	// b 0x8267e370
	goto loc_8267E370;
loc_8267E364:
	// rlwinm r11,r10,25,7,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x1FFFFFF;
	// addi r10,r25,256
	ctx.r10.s64 = r25.s64 + 256;
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
loc_8267E370:
	// addi r11,r11,608
	r11.s64 = r11.s64 + 608;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r31.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// sthx r10,r11,r31
	PPC_STORE_U16(r11.u32 + r31.u32, ctx.r10.u16);
	// lwz r10,5780(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 5780);
	// lwz r9,5784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// lwz r7,108(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r11,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r11.u32);
	// stw r10,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r10.u32);
loc_8267E3B8:
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// stw r11,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r11.u32);
	// bgt cr6,0x8267e43c
	if (cr6.gt) goto loc_8267E43C;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lwz r5,64(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// lwz r7,80(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// and r11,r11,r3
	r11.u64 = r11.u64 & ctx.r3.u64;
	// lwz r6,76(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,60(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// lwz r28,56(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// slw r11,r5,r7
	r11.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r7.u8 & 0x3F));
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r11.u32);
	// lhzx r11,r10,r4
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r4.u32);
	// sthx r11,r3,r28
	PPC_STORE_U16(ctx.r3.u32 + r28.u32, r11.u16);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lwz r7,64(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// and r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 & r11.u64;
	// lwz r6,56(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// lwz r5,60(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r28,r10,r6
	r28.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r6.u32);
	// sthx r11,r7,r5
	PPC_STORE_U16(ctx.r7.u32 + ctx.r5.u32, r11.u16);
loc_8267E43C:
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r11.u32);
	// bne 0x8267e3b8
	if (!cr0.eq) goto loc_8267E3B8;
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r29,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r29.u32);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// stw r26,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r26.u32);
	// stw r10,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r10.u32);
	// beq cr6,0x8267e160
	if (cr6.eq) goto loc_8267E160;
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x8267e480
	if (cr0.lt) goto loc_8267E480;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r4,r9,r11
	ctx.r4.u64 = ctx.r9.u64 + r11.u64;
	// b 0x8267e484
	goto loc_8267E484;
loc_8267E480:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
loc_8267E484:
	// li r6,0
	ctx.r6.s64 = 0;
	// subf r5,r11,r10
	ctx.r5.s64 = ctx.r10.s64 - r11.s64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82681188
	sub_82681188(ctx, base);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// b 0x8267e580
	goto loc_8267E580;
loc_8267E4B0:
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8267e594
	if (cr6.eq) goto loc_8267E594;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r8,5784(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r9,5788(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5788);
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r11,-1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// sthx r29,r10,r9
	PPC_STORE_U16(ctx.r10.u32 + ctx.r9.u32, r29.u16);
	// lwz r9,5784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r8,5776(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 5776);
	// addi r10,r10,35
	ctx.r10.s64 = ctx.r10.s64 + 35;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stbx r11,r8,r9
	PPC_STORE_U8(ctx.r8.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,5784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,5784(r31)
	PPC_STORE_U32(r31.u32 + 5784, r11.u32);
	// lhzx r11,r10,r31
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + r31.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sthx r11,r10,r31
	PPC_STORE_U16(ctx.r10.u32 + r31.u32, r11.u16);
	// lwz r11,5780(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 5780);
	// lwz r10,5784(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x8267e560
	if (!cr6.eq) goto loc_8267E560;
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x8267e538
	if (cr0.lt) goto loc_8267E538;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// b 0x8267e53c
	goto loc_8267E53C;
loc_8267E538:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
loc_8267E53C:
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// subf r5,r11,r10
	ctx.r5.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x82681188
	sub_82681188(ctx, base);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
loc_8267E560:
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r10,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r10.u32);
	// stw r11,108(r31)
	PPC_STORE_U32(r31.u32 + 108, r11.u32);
	// lwz r11,16(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
loc_8267E580:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267e160
	if (!cr6.eq) goto loc_8267E160;
loc_8267E588:
	// li r3,0
	ctx.r3.s64 = 0;
loc_8267E58C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8267E594:
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r10,96(r31)
	PPC_STORE_U32(r31.u32 + 96, ctx.r10.u32);
	// stw r11,108(r31)
	PPC_STORE_U32(r31.u32 + 108, r11.u32);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r11.u32);
	// b 0x8267e160
	goto loc_8267E160;
loc_8267E5B8:
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8267e618
	if (cr6.eq) goto loc_8267E618;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r9,5788(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5788);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,5784(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r11,-1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// sthx r29,r8,r9
	PPC_STORE_U16(ctx.r8.u32 + ctx.r9.u32, r29.u16);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r9,5784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// lwz r8,5776(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 5776);
	// addi r10,r10,35
	ctx.r10.s64 = ctx.r10.s64 + 35;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stbx r11,r8,r9
	PPC_STORE_U8(ctx.r8.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,5784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 5784);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,5784(r31)
	PPC_STORE_U32(r31.u32 + 5784, r11.u32);
	// lhzx r11,r10,r31
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + r31.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sthx r11,r10,r31
	PPC_STORE_U16(ctx.r10.u32 + r31.u32, r11.u16);
	// stw r29,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r29.u32);
loc_8267E618:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x8267e630
	if (cr0.lt) goto loc_8267E630;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8267e634
	goto loc_8267E634;
loc_8267E630:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
loc_8267E634:
	// addi r10,r24,-4
	ctx.r10.s64 = r24.s64 + -4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r6,r10,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// subf r5,r11,r10
	ctx.r5.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x82681188
	sub_82681188(ctx, base);
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// bl 0x8267cdf0
	sub_8267CDF0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267e680
	if (!cr6.eq) goto loc_8267E680;
	// addi r11,r24,-4
	r11.s64 = r24.s64 + -4;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r11,28,30,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x2;
	// b 0x8267e58c
	goto loc_8267E58C;
loc_8267E680:
	// cmpwi cr6,r24,4
	cr6.compare<int32_t>(r24.s32, 4, xer);
	// li r3,3
	ctx.r3.s64 = 3;
	// beq cr6,0x8267e58c
	if (cr6.eq) goto loc_8267E58C;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8267e58c
	goto loc_8267E58C;
}

__attribute__((alias("__imp__sub_8267E694"))) PPC_WEAK_FUNC(sub_8267E694);
PPC_FUNC_IMPL(__imp__sub_8267E694) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267E698"))) PPC_WEAK_FUNC(sub_8267E698);
PPC_FUNC_IMPL(__imp__sub_8267E698) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x8267e7f4
	if (cr6.eq) goto loc_8267E7F4;
	// lwz r31,28(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x8267e7f4
	if (cr0.eq) goto loc_8267E7F4;
	// lwz r11,32(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267e7f4
	if (cr6.eq) goto loc_8267E7F4;
	// lwz r11,36(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 36);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267e7f4
	if (cr6.eq) goto loc_8267E7F4;
	// li r30,0
	r30.s64 = 0;
	// li r28,2
	r28.s64 = 2;
	// stw r30,20(r29)
	PPC_STORE_U32(r29.u32 + 20, r30.u32);
	// stw r30,8(r29)
	PPC_STORE_U32(r29.u32 + 8, r30.u32);
	// stw r30,24(r29)
	PPC_STORE_U32(r29.u32 + 24, r30.u32);
	// stw r28,44(r29)
	PPC_STORE_U32(r29.u32 + 44, r28.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bge 0x8267e70c
	if (!cr0.lt) goto loc_8267E70C;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
loc_8267E70C:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// subfic r10,r11,0
	xer.ca = r11.u32 <= 0;
	ctx.r10.s64 = 0 - r11.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// subfe r11,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	r11.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r11,r11,0,31,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF9;
	// rlwinm r11,r11,0,26,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFBF;
	// addi r11,r11,113
	r11.s64 = r11.s64 + 113;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne cr6,0x8267e744
	if (!cr6.eq) goto loc_8267E744;
	// bl 0x8267f868
	sub_8267F868(ctx, base);
	// b 0x8267e748
	goto loc_8267E748;
loc_8267E744:
	// bl 0x8267f368
	sub_8267F368(ctx, base);
loc_8267E748:
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,48(r29)
	PPC_STORE_U32(r29.u32 + 48, r11.u32);
	// stw r30,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r30.u32);
	// bl 0x82680a40
	sub_82680A40(ctx, base);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r11.u32);
	// lwz r11,68(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sth r30,-2(r11)
	PPC_STORE_U16(r11.u32 + -2, r30.u16);
	// lwz r11,68(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// lwz r3,60(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r10,124(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 124);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r11,r11,6928
	r11.s64 = r11.s64 + 6928;
	// mulli r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 * 12;
	// addi r9,r11,2
	ctx.r9.s64 = r11.s64 + 2;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
	// addi r7,r11,6
	ctx.r7.s64 = r11.s64 + 6;
	// lhzx r9,r10,r9
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// stw r9,120(r31)
	PPC_STORE_U32(r31.u32 + 120, ctx.r9.u32);
	// lhzx r11,r10,r11
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// stw r11,132(r31)
	PPC_STORE_U32(r31.u32 + 132, r11.u32);
	// lhzx r11,r10,r8
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// stw r11,136(r31)
	PPC_STORE_U32(r31.u32 + 136, r11.u32);
	// lhzx r11,r10,r7
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r7.u32);
	// stw r30,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r30.u32);
	// stw r30,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r30.u32);
	// stw r30,108(r31)
	PPC_STORE_U32(r31.u32 + 108, r30.u32);
	// stw r28,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r28.u32);
	// stw r11,116(r31)
	PPC_STORE_U32(r31.u32 + 116, r11.u32);
	// stw r28,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r28.u32);
	// stw r30,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r30.u32);
	// stw r30,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r30.u32);
	// b 0x8267e7f8
	goto loc_8267E7F8;
loc_8267E7F4:
	// li r3,-2
	ctx.r3.s64 = -2;
loc_8267E7F8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8267E800"))) PPC_WEAK_FUNC(sub_8267E800);
PPC_FUNC_IMPL(__imp__sub_8267E800) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r27,1
	r27.s64 = 1;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// mr r26,r27
	r26.u64 = r27.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8267ea70
	if (cr6.eq) goto loc_8267EA70;
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplwi cr6,r11,49
	cr6.compare<uint32_t>(r11.u32, 49, xer);
	// bne cr6,0x8267ea70
	if (!cr6.eq) goto loc_8267EA70;
	// cmplwi cr6,r10,56
	cr6.compare<uint32_t>(ctx.r10.u32, 56, xer);
	// bne cr6,0x8267ea70
	if (!cr6.eq) goto loc_8267EA70;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8267e854
	if (!cr6.eq) goto loc_8267E854;
loc_8267E84C:
	// li r3,-2
	ctx.r3.s64 = -2;
	// b 0x8267ea74
	goto loc_8267EA74;
loc_8267E854:
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r10,24(r30)
	PPC_STORE_U32(r30.u32 + 24, ctx.r10.u32);
	// bne cr6,0x8267e878
	if (!cr6.eq) goto loc_8267E878;
	// lis r11,-32152
	r11.s64 = -2107113472;
	// stw r10,40(r30)
	PPC_STORE_U32(r30.u32 + 40, ctx.r10.u32);
	// addi r11,r11,-5504
	r11.s64 = r11.s64 + -5504;
	// stw r11,32(r30)
	PPC_STORE_U32(r30.u32 + 32, r11.u32);
loc_8267E878:
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8267e890
	if (!cr6.eq) goto loc_8267E890;
	// lis r11,-32152
	r11.s64 = -2107113472;
	// addi r11,r11,-5496
	r11.s64 = r11.s64 + -5496;
	// stw r11,36(r30)
	PPC_STORE_U32(r30.u32 + 36, r11.u32);
loc_8267E890:
	// cmpwi cr6,r25,-1
	cr6.compare<int32_t>(r25.s32, -1, xer);
	// bne cr6,0x8267e89c
	if (!cr6.eq) goto loc_8267E89C;
	// li r25,6
	r25.s64 = 6;
loc_8267E89C:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bge cr6,0x8267e8b0
	if (!cr6.lt) goto loc_8267E8B0;
	// mr r26,r10
	r26.u64 = ctx.r10.u64;
	// neg r29,r29
	r29.s64 = -r29.s64;
	// b 0x8267e8c0
	goto loc_8267E8C0;
loc_8267E8B0:
	// cmpwi cr6,r29,15
	cr6.compare<int32_t>(r29.s32, 15, xer);
	// ble cr6,0x8267e8c0
	if (!cr6.gt) goto loc_8267E8C0;
	// li r26,2
	r26.s64 = 2;
	// addi r29,r29,-16
	r29.s64 = r29.s64 + -16;
loc_8267E8C0:
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bgt cr6,0x8267e84c
	if (cr6.gt) goto loc_8267E84C;
	// cmpwi cr6,r5,8
	cr6.compare<int32_t>(ctx.r5.s32, 8, xer);
	// bne cr6,0x8267e84c
	if (!cr6.eq) goto loc_8267E84C;
	// addi r11,r29,-8
	r11.s64 = r29.s64 + -8;
	// cmplwi cr6,r11,7
	cr6.compare<uint32_t>(r11.u32, 7, xer);
	// bgt cr6,0x8267e84c
	if (cr6.gt) goto loc_8267E84C;
	// cmplwi cr6,r25,9
	cr6.compare<uint32_t>(r25.u32, 9, xer);
	// bgt cr6,0x8267e84c
	if (cr6.gt) goto loc_8267E84C;
	// cmplwi cr6,r24,3
	cr6.compare<uint32_t>(r24.u32, 3, xer);
	// bgt cr6,0x8267e84c
	if (cr6.gt) goto loc_8267E84C;
	// cmpwi cr6,r29,8
	cr6.compare<int32_t>(r29.s32, 8, xer);
	// bne cr6,0x8267e8fc
	if (!cr6.eq) goto loc_8267E8FC;
	// li r29,9
	r29.s64 = 9;
loc_8267E8FC:
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// li r5,5816
	ctx.r5.s64 = 5816;
	// lwz r3,40(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// li r4,1
	ctx.r4.s64 = 1;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8267e924
	if (!cr0.eq) goto loc_8267E924;
loc_8267E91C:
	// li r3,-4
	ctx.r3.s64 = -4;
	// b 0x8267ea74
	goto loc_8267EA74;
loc_8267E924:
	// addi r11,r28,7
	r11.s64 = r28.s64 + 7;
	// stw r31,28(r30)
	PPC_STORE_U32(r30.u32 + 28, r31.u32);
	// slw r4,r27,r29
	ctx.r4.u64 = r29.u8 & 0x20 ? 0 : (r27.u32 << (r29.u8 & 0x3F));
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// addi r8,r11,2
	ctx.r8.s64 = r11.s64 + 2;
	// stw r26,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r26.u32);
	// li r9,3
	ctx.r9.s64 = 3;
	// stw r29,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r29.u32);
	// li r5,2
	ctx.r5.s64 = 2;
	// stw r11,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r11.u32);
	// divwu r9,r8,r9
	ctx.r9.u32 = ctx.r8.u32 / ctx.r9.u32;
	// stw r4,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r4.u32);
	// stw r9,80(r31)
	PPC_STORE_U32(r31.u32 + 80, ctx.r9.u32);
	// slw r10,r27,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r27.u32 << (r11.u8 & 0x3F));
	// addi r11,r4,-1
	r11.s64 = ctx.r4.s64 + -1;
	// stw r10,68(r31)
	PPC_STORE_U32(r31.u32 + 68, ctx.r10.u32);
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
	// addi r11,r10,-1
	r11.s64 = ctx.r10.s64 + -1;
	// stw r11,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r11.u32);
	// lwz r3,40(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// lwz r4,36(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// li r5,2
	ctx.r5.s64 = 2;
	// lwz r3,40(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,56(r31)
	PPC_STORE_U32(r31.u32 + 56, ctx.r3.u32);
	// lwz r4,68(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// li r5,2
	ctx.r5.s64 = 2;
	// lwz r3,40(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r11,r28,6
	r11.s64 = r28.s64 + 6;
	// stw r3,60(r31)
	PPC_STORE_U32(r31.u32 + 60, ctx.r3.u32);
	// li r5,4
	ctx.r5.s64 = 4;
	// slw r4,r27,r11
	ctx.r4.u64 = r11.u8 & 0x20 ? 0 : (r27.u32 << (r11.u8 & 0x3F));
	// stw r4,5780(r31)
	PPC_STORE_U32(r31.u32 + 5780, ctx.r4.u32);
	// lwz r3,40(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,5780(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 5780);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r3,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r3.u32);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
	// beq cr6,0x8267ea4c
	if (cr6.eq) goto loc_8267EA4C;
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8267ea4c
	if (cr6.eq) goto loc_8267EA4C;
	// lwz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8267ea4c
	if (cr6.eq) goto loc_8267EA4C;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8267ea4c
	if (cr6.eq) goto loc_8267EA4C;
	// rlwinm r10,r11,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r25,124(r31)
	PPC_STORE_U32(r31.u32 + 124, r25.u32);
	// mulli r11,r11,3
	r11.s64 = r11.s64 * 3;
	// stw r24,128(r31)
	PPC_STORE_U32(r31.u32 + 128, r24.u32);
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r10,5788(r31)
	PPC_STORE_U32(r31.u32 + 5788, ctx.r10.u32);
	// stw r11,5776(r31)
	PPC_STORE_U32(r31.u32 + 5776, r11.u32);
	// stb r9,29(r31)
	PPC_STORE_U8(r31.u32 + 29, ctx.r9.u8);
	// bl 0x8267e698
	sub_8267E698(ctx, base);
	// b 0x8267ea74
	goto loc_8267EA74;
loc_8267EA4C:
	// li r11,666
	r11.s64 = 666;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,7056
	r11.s64 = r11.s64 + 7056;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r11.u32);
	// bl 0x8267d4c8
	sub_8267D4C8(ctx, base);
	// b 0x8267e91c
	goto loc_8267E91C;
loc_8267EA70:
	// li r3,-6
	ctx.r3.s64 = -6;
loc_8267EA74:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_8267EA7C"))) PPC_WEAK_FUNC(sub_8267EA7C);
PPC_FUNC_IMPL(__imp__sub_8267EA7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267EA80"))) PPC_WEAK_FUNC(sub_8267EA80);
PPC_FUNC_IMPL(__imp__sub_8267EA80) {
	PPC_FUNC_PROLOGUE();
	// mullw r3,r4,r5
	ctx.r3.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// b 0x8239d488
	sub_8239D488(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8267EA88"))) PPC_WEAK_FUNC(sub_8267EA88);
PPC_FUNC_IMPL(__imp__sub_8267EA88) {
	PPC_FUNC_PROLOGUE();
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// b 0x8239cdf0
	sub_8239CDF0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8267EA90"))) PPC_WEAK_FUNC(sub_8267EA90);
PPC_FUNC_IMPL(__imp__sub_8267EA90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bccc
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r29,1
	r29.s64 = 1;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r30,r11,-1
	r30.s64 = r11.s64 + -1;
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r7,16(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,76(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 76);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r6,80(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 80);
	// addi r21,r11,-5
	r21.s64 = r11.s64 + -5;
	// lwz r27,32(r9)
	r27.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// lwz r20,36(r9)
	r20.u64 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	// lwz r28,40(r9)
	r28.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// lwz r19,44(r9)
	r19.u64 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	// lwz r26,68(r9)
	r26.u64 = PPC_LOAD_U32(ctx.r9.u32 + 68);
	// lwz r25,72(r9)
	r25.u64 = PPC_LOAD_U32(ctx.r9.u32 + 72);
	// slw r11,r29,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r10.u8 & 0x3F));
	// slw r10,r29,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r6.u8 & 0x3F));
	// subf r6,r4,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r4.s64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r23,r11,-1
	r23.s64 = r11.s64 + -1;
	// lwz r11,52(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 52);
	// addi r22,r10,-1
	r22.s64 = ctx.r10.s64 + -1;
	// lwz r10,48(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	// add r24,r6,r8
	r24.u64 = ctx.r6.u64 + ctx.r8.u64;
	// addi r18,r7,-257
	r18.s64 = ctx.r7.s64 + -257;
loc_8267EB08:
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x8267eb38
	if (!cr6.lt) goto loc_8267EB38;
	// addi r7,r30,1
	ctx.r7.s64 = r30.s64 + 1;
	// addi r30,r7,1
	r30.s64 = ctx.r7.s64 + 1;
	// lbz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// slw r7,r6,r11
	ctx.r7.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// lbz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// slw r7,r7,r11
	ctx.r7.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
loc_8267EB38:
	// and r7,r23,r10
	ctx.r7.u64 = r23.u64 & ctx.r10.u64;
	// b 0x8267eb64
	goto loc_8267EB64;
loc_8267EB40:
	// rlwinm. r6,r7,0,27,27
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne 0x8267eb90
	if (!cr0.eq) goto loc_8267EB90;
	// rlwinm. r6,r7,0,25,25
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne 0x8267ee70
	if (!cr0.eq) goto loc_8267EE70;
	// slw r7,r29,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r7.u8 & 0x3F));
	// lhz r6,-142(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + -142);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// and r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 & ctx.r10.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
loc_8267EB64:
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r7,r26
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + r26.u32);
	// stw r7,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r7.u32);
	// lbz r7,-143(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + -143);
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// srw r10,r10,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r7.u8 & 0x3F));
	// lbz r7,-144(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + -144);
	// cmplwi r7,0
	cr0.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne 0x8267eb40
	if (!cr0.eq) goto loc_8267EB40;
	// lhz r7,-142(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + -142);
	// b 0x8267ee3c
	goto loc_8267EE3C;
loc_8267EB90:
	// lhz r5,-142(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + -142);
	// clrlwi. r7,r7,28
	ctx.r7.u64 = ctx.r7.u32 & 0xF;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x8267ebd0
	if (cr0.eq) goto loc_8267EBD0;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bge cr6,0x8267ebb8
	if (!cr6.lt) goto loc_8267EBB8;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lbz r6,0(r30)
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// slw r6,r6,r11
	ctx.r6.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
loc_8267EBB8:
	// slw r6,r29,r7
	ctx.r6.u64 = ctx.r7.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r7.u8 & 0x3F));
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// and r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 & ctx.r10.u64;
	// srw r10,r10,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r7.u8 & 0x3F));
	// add r5,r6,r5
	ctx.r5.u64 = ctx.r6.u64 + ctx.r5.u64;
loc_8267EBD0:
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x8267ec00
	if (!cr6.lt) goto loc_8267EC00;
	// addi r7,r30,1
	ctx.r7.s64 = r30.s64 + 1;
	// addi r30,r7,1
	r30.s64 = ctx.r7.s64 + 1;
	// lbz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// slw r7,r6,r11
	ctx.r7.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// lbz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// slw r7,r7,r11
	ctx.r7.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
loc_8267EC00:
	// and r7,r22,r10
	ctx.r7.u64 = r22.u64 & ctx.r10.u64;
	// b 0x8267ec24
	goto loc_8267EC24;
loc_8267EC08:
	// rlwinm. r6,r7,0,25,25
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne 0x8267ee64
	if (!cr0.eq) goto loc_8267EE64;
	// slw r7,r29,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r7.u8 & 0x3F));
	// lhz r6,-142(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + -142);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// and r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 & ctx.r10.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
loc_8267EC24:
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r7,r25
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + r25.u32);
	// stw r7,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r7.u32);
	// lbz r7,-143(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + -143);
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// srw r10,r10,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r7.u8 & 0x3F));
	// lbz r7,-144(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + -144);
	// rlwinm. r6,r7,0,27,27
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq 0x8267ec08
	if (cr0.eq) goto loc_8267EC08;
	// clrlwi r7,r7,28
	ctx.r7.u64 = ctx.r7.u32 & 0xF;
	// lhz r4,-142(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + -142);
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bge cr6,0x8267ec88
	if (!cr6.lt) goto loc_8267EC88;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lbz r6,0(r30)
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// slw r6,r6,r11
	ctx.r6.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bge cr6,0x8267ec88
	if (!cr6.lt) goto loc_8267EC88;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lbz r6,0(r30)
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// slw r6,r6,r11
	ctx.r6.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
loc_8267EC88:
	// slw r6,r29,r7
	ctx.r6.u64 = ctx.r7.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r7.u8 & 0x3F));
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// and r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 & ctx.r10.u64;
	// srw r10,r10,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r7.u8 & 0x3F));
	// add r31,r6,r4
	r31.u64 = ctx.r6.u64 + ctx.r4.u64;
	// subf r7,r24,r8
	ctx.r7.s64 = ctx.r8.s64 - r24.s64;
	// cmplw cr6,r31,r7
	cr6.compare<uint32_t>(r31.u32, ctx.r7.u32, xer);
	// ble cr6,0x8267eddc
	if (!cr6.gt) goto loc_8267EDDC;
	// subf r6,r7,r31
	ctx.r6.s64 = r31.s64 - ctx.r7.s64;
	// cmplw cr6,r6,r20
	cr6.compare<uint32_t>(ctx.r6.u32, r20.u32, xer);
	// bgt cr6,0x8267ee58
	if (cr6.gt) goto loc_8267EE58;
	// addi r4,r19,-1
	ctx.r4.s64 = r19.s64 + -1;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// subf r7,r6,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r6.s64;
	// bne cr6,0x8267ecf4
	if (!cr6.eq) goto loc_8267ECF4;
	// cmplw cr6,r6,r5
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r5.u32, xer);
	// add r7,r7,r27
	ctx.r7.u64 = ctx.r7.u64 + r27.u64;
	// bge cr6,0x8267ed84
	if (!cr6.lt) goto loc_8267ED84;
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
loc_8267ECD8:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lbz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// stb r4,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r4.u8);
	// bne 0x8267ecd8
	if (!cr0.eq) goto loc_8267ECD8;
	// b 0x8267ed80
	goto loc_8267ED80;
loc_8267ECF4:
	// cmplw cr6,r28,r6
	cr6.compare<uint32_t>(r28.u32, ctx.r6.u32, xer);
	// add r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 + r28.u64;
	// bge cr6,0x8267ed5c
	if (!cr6.lt) goto loc_8267ED5C;
	// subf r6,r28,r6
	ctx.r6.s64 = ctx.r6.s64 - r28.s64;
	// add r7,r7,r27
	ctx.r7.u64 = ctx.r7.u64 + r27.u64;
	// cmplw cr6,r6,r5
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r5.u32, xer);
	// bge cr6,0x8267ed84
	if (!cr6.lt) goto loc_8267ED84;
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
loc_8267ED14:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lbz r17,0(r7)
	r17.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// stb r17,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r17.u8);
	// bne 0x8267ed14
	if (!cr0.eq) goto loc_8267ED14;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// cmplw cr6,r28,r5
	cr6.compare<uint32_t>(r28.u32, ctx.r5.u32, xer);
	// bge cr6,0x8267ed84
	if (!cr6.lt) goto loc_8267ED84;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// subf r5,r28,r5
	ctx.r5.s64 = ctx.r5.s64 - r28.s64;
loc_8267ED40:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lbz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// stb r4,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r4.u8);
	// bne 0x8267ed40
	if (!cr0.eq) goto loc_8267ED40;
	// b 0x8267ed80
	goto loc_8267ED80;
loc_8267ED5C:
	// cmplw cr6,r6,r5
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r5.u32, xer);
	// bge cr6,0x8267ed84
	if (!cr6.lt) goto loc_8267ED84;
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
loc_8267ED68:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lbz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// stb r4,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r4.u8);
	// bne 0x8267ed68
	if (!cr0.eq) goto loc_8267ED68;
loc_8267ED80:
	// subf r7,r31,r8
	ctx.r7.s64 = ctx.r8.s64 - r31.s64;
loc_8267ED84:
	// cmplwi cr6,r5,2
	cr6.compare<uint32_t>(ctx.r5.u32, 2, xer);
	// ble cr6,0x8267ee1c
	if (!cr6.gt) goto loc_8267EE1C;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// li r4,3
	ctx.r4.s64 = 3;
	// divwu r6,r6,r4
	ctx.r6.u32 = ctx.r6.u32 / ctx.r4.u32;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
loc_8267ED9C:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// addi r5,r5,-3
	ctx.r5.s64 = ctx.r5.s64 + -3;
	// lbz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stb r4,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r4.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lbz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stb r4,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r4.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lbz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// stb r4,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r4.u8);
	// bne 0x8267ed9c
	if (!cr0.eq) goto loc_8267ED9C;
	// b 0x8267ee1c
	goto loc_8267EE1C;
loc_8267EDDC:
	// subf r7,r31,r8
	ctx.r7.s64 = ctx.r8.s64 - r31.s64;
loc_8267EDE0:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r5,r5,-3
	ctx.r5.s64 = ctx.r5.s64 + -3;
	// cmplwi cr6,r5,2
	cr6.compare<uint32_t>(ctx.r5.u32, 2, xer);
	// lbz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stb r6,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r6.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lbz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stb r6,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r6.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lbz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// stb r6,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r6.u8);
	// bgt cr6,0x8267ede0
	if (cr6.gt) goto loc_8267EDE0;
loc_8267EE1C:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x8267ee44
	if (cr6.eq) goto loc_8267EE44;
	// lbz r6,1(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// cmplwi cr6,r5,1
	cr6.compare<uint32_t>(ctx.r5.u32, 1, xer);
	// stb r6,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r6.u8);
	// ble cr6,0x8267ee44
	if (!cr6.gt) goto loc_8267EE44;
	// lbz r7,2(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
loc_8267EE3C:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
loc_8267EE44:
	// cmplw cr6,r30,r21
	cr6.compare<uint32_t>(r30.u32, r21.u32, xer);
	// bge cr6,0x8267ee98
	if (!cr6.lt) goto loc_8267EE98;
	// cmplw cr6,r8,r18
	cr6.compare<uint32_t>(ctx.r8.u32, r18.u32, xer);
	// blt cr6,0x8267eb08
	if (cr6.lt) goto loc_8267EB08;
	// b 0x8267ee98
	goto loc_8267EE98;
loc_8267EE58:
	// lis r7,-32243
	ctx.r7.s64 = -2113077248;
	// addi r7,r7,6812
	ctx.r7.s64 = ctx.r7.s64 + 6812;
	// b 0x8267ee8c
	goto loc_8267EE8C;
loc_8267EE64:
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// addi r7,r7,-968
	ctx.r7.s64 = ctx.r7.s64 + -968;
	// b 0x8267ee8c
	goto loc_8267EE8C;
loc_8267EE70:
	// rlwinm. r7,r7,0,26,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x8267ee84
	if (cr0.eq) goto loc_8267EE84;
	// li r7,11
	ctx.r7.s64 = 11;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// b 0x8267ee98
	goto loc_8267EE98;
loc_8267EE84:
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// addi r7,r7,-996
	ctx.r7.s64 = ctx.r7.s64 + -996;
loc_8267EE8C:
	// li r6,27
	ctx.r6.s64 = 27;
	// stw r7,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r7.u32);
	// stw r6,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
loc_8267EE98:
	// addi r6,r8,1
	ctx.r6.s64 = ctx.r8.s64 + 1;
	// rlwinm r7,r11,29,3,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// stw r6,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r6.u32);
	// rlwinm r6,r7,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r7,r7,r30
	ctx.r7.s64 = r30.s64 - ctx.r7.s64;
	// subf r6,r6,r11
	ctx.r6.s64 = r11.s64 - ctx.r6.s64;
	// addi r5,r7,1
	ctx.r5.s64 = ctx.r7.s64 + 1;
	// cmplw cr6,r7,r21
	cr6.compare<uint32_t>(ctx.r7.u32, r21.u32, xer);
	// cmplw cr6,r8,r18
	cr6.compare<uint32_t>(ctx.r8.u32, r18.u32, xer);
	// stw r5,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r5.u32);
	// slw r11,r29,r6
	r11.u64 = ctx.r6.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r6.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r10,r11,r10
	ctx.r10.u64 = r11.u64 & ctx.r10.u64;
	// subf r11,r7,r21
	r11.s64 = r21.s64 - ctx.r7.s64;
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// subf r11,r8,r18
	r11.s64 = r18.s64 - ctx.r8.s64;
	// addi r11,r11,257
	r11.s64 = r11.s64 + 257;
	// stw r11,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r11.u32);
	// stw r10,48(r9)
	PPC_STORE_U32(ctx.r9.u32 + 48, ctx.r10.u32);
	// stw r6,52(r9)
	PPC_STORE_U32(ctx.r9.u32 + 52, ctx.r6.u32);
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_8267EEF0"))) PPC_WEAK_FUNC(sub_8267EEF0);
PPC_FUNC_IMPL(__imp__sub_8267EEF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc8
	// li r16,0
	r16.s64 = 0;
	// addi r10,r1,-208
	ctx.r10.s64 = ctx.r1.s64 + -208;
	// mr r9,r16
	ctx.r9.u64 = r16.u64;
	// li r11,16
	r11.s64 = 16;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_8267EF0C:
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x8267ef0c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8267EF0C;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x8267ef4c
	if (cr6.eq) goto loc_8267EF4C;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_8267EF28:
	// lhz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r31,r1,-208
	r31.s64 = ctx.r1.s64 + -208;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r9,r9,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lhzx r30,r9,r31
	r30.u64 = PPC_LOAD_U16(ctx.r9.u32 + r31.u32);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// sthx r30,r9,r31
	PPC_STORE_U16(ctx.r9.u32 + r31.u32, r30.u16);
	// bne 0x8267ef28
	if (!cr0.eq) goto loc_8267EF28;
loc_8267EF4C:
	// lwz r18,0(r7)
	r18.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// li r17,15
	r17.s64 = 15;
	// addi r11,r1,-178
	r11.s64 = ctx.r1.s64 + -178;
loc_8267EF58:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x8267ef74
	if (!cr0.eq) goto loc_8267EF74;
	// addi r17,r17,-1
	r17.s64 = r17.s64 + -1;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// cmplwi cr6,r17,1
	cr6.compare<uint32_t>(r17.u32, 1, xer);
	// bge cr6,0x8267ef58
	if (!cr6.lt) goto loc_8267EF58;
loc_8267EF74:
	// cmplw cr6,r18,r17
	cr6.compare<uint32_t>(r18.u32, r17.u32, xer);
	// ble cr6,0x8267ef80
	if (!cr6.gt) goto loc_8267EF80;
	// mr r18,r17
	r18.u64 = r17.u64;
loc_8267EF80:
	// cmplwi cr6,r17,0
	cr6.compare<uint32_t>(r17.u32, 0, xer);
	// bne cr6,0x8267ef90
	if (!cr6.eq) goto loc_8267EF90;
loc_8267EF88:
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x8267f364
	goto loc_8267F364;
loc_8267EF90:
	// li r20,1
	r20.s64 = 1;
	// addi r11,r1,-206
	r11.s64 = ctx.r1.s64 + -206;
	// mr r28,r20
	r28.u64 = r20.u64;
loc_8267EF9C:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x8267efb8
	if (!cr0.eq) goto loc_8267EFB8;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi cr6,r28,15
	cr6.compare<uint32_t>(r28.u32, 15, xer);
	// ble cr6,0x8267ef9c
	if (!cr6.gt) goto loc_8267EF9C;
loc_8267EFB8:
	// cmplw cr6,r18,r28
	cr6.compare<uint32_t>(r18.u32, r28.u32, xer);
	// bge cr6,0x8267efc4
	if (!cr6.lt) goto loc_8267EFC4;
	// mr r18,r28
	r18.u64 = r28.u64;
loc_8267EFC4:
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// addi r11,r1,-206
	r11.s64 = ctx.r1.s64 + -206;
loc_8267EFD0:
	// lhz r31,0(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf. r10,r31,r10
	ctx.r10.s64 = ctx.r10.s64 - r31.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt 0x8267ef88
	if (cr0.lt) goto loc_8267EF88;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi cr6,r9,15
	cr6.compare<uint32_t>(ctx.r9.u32, 15, xer);
	// ble cr6,0x8267efd0
	if (!cr6.gt) goto loc_8267EFD0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8267f010
	if (!cr6.gt) goto loc_8267F010;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8267ef88
	if (cr6.eq) goto loc_8267EF88;
	// lhz r11,-208(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + -208);
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x8267ef88
	if (!cr6.eq) goto loc_8267EF88;
loc_8267F010:
	// sth r16,-174(r1)
	PPC_STORE_U16(ctx.r1.u32 + -174, r16.u16);
	// li r11,2
	r11.s64 = 2;
loc_8267F018:
	// addi r10,r1,-176
	ctx.r10.s64 = ctx.r1.s64 + -176;
	// addi r9,r1,-208
	ctx.r9.s64 = ctx.r1.s64 + -208;
	// addi r31,r1,-174
	r31.s64 = ctx.r1.s64 + -174;
	// lhzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// lhzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sthx r10,r11,r31
	PPC_STORE_U16(r11.u32 + r31.u32, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// blt cr6,0x8267f018
	if (cr6.lt) goto loc_8267F018;
	// mr r31,r16
	r31.u64 = r16.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x8267f09c
	if (cr6.eq) goto loc_8267F09C;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_8267F050:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x8267f08c
	if (cr0.eq) goto loc_8267F08C;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// addi r30,r1,-176
	r30.s64 = ctx.r1.s64 + -176;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// addi r9,r1,-176
	ctx.r9.s64 = ctx.r1.s64 + -176;
	// lhzx r10,r10,r30
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r30.u32);
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// sthx r31,r10,r8
	PPC_STORE_U16(ctx.r10.u32 + ctx.r8.u32, r31.u16);
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// lhzx r30,r10,r9
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// sthx r30,r10,r9
	PPC_STORE_U16(ctx.r10.u32 + ctx.r9.u32, r30.u16);
loc_8267F08C:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplw cr6,r31,r5
	cr6.compare<uint32_t>(r31.u32, ctx.r5.u32, xer);
	// blt cr6,0x8267f050
	if (cr6.lt) goto loc_8267F050;
loc_8267F09C:
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// blt cr6,0x8267f0d8
	if (cr6.lt) goto loc_8267F0D8;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,7336
	r11.s64 = r11.s64 + 7336;
	// beq cr6,0x8267f0c0
	if (cr6.eq) goto loc_8267F0C0;
	// addi r21,r11,-64
	r21.s64 = r11.s64 + -64;
	// mr r22,r11
	r22.u64 = r11.u64;
	// li r23,-1
	r23.s64 = -1;
	// b 0x8267f0e4
	goto loc_8267F0E4;
loc_8267F0C0:
	// addi r10,r11,-192
	ctx.r10.s64 = r11.s64 + -192;
	// addi r11,r11,-128
	r11.s64 = r11.s64 + -128;
	// addi r21,r10,-514
	r21.s64 = ctx.r10.s64 + -514;
	// addi r22,r11,-514
	r22.s64 = r11.s64 + -514;
	// li r23,256
	r23.s64 = 256;
	// b 0x8267f0e4
	goto loc_8267F0E4;
loc_8267F0D8:
	// mr r22,r8
	r22.u64 = ctx.r8.u64;
	// mr r21,r8
	r21.u64 = ctx.r8.u64;
	// li r23,19
	r23.s64 = 19;
loc_8267F0E4:
	// slw r24,r20,r18
	r24.u64 = r18.u8 & 0x20 ? 0 : (r20.u32 << (r18.u8 & 0x3F));
	// lwz r27,0(r6)
	r27.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// mr r30,r16
	r30.u64 = r16.u64;
	// mr r31,r16
	r31.u64 = r16.u64;
	// li r26,-1
	r26.s64 = -1;
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// addi r19,r24,-1
	r19.s64 = r24.s64 + -1;
	// bne cr6,0x8267f110
	if (!cr6.eq) goto loc_8267F110;
	// cmplwi cr6,r24,1286
	cr6.compare<uint32_t>(r24.u32, 1286, xer);
	// bge cr6,0x8267f360
	if (!cr6.lt) goto loc_8267F360;
loc_8267F110:
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
loc_8267F114:
	// lhz r11,0(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 0);
	// subf r10,r31,r28
	ctx.r10.s64 = r28.s64 - r31.s64;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// cmpw cr6,r9,r23
	cr6.compare<int32_t>(ctx.r9.s32, r23.s32, xer);
	// stb r10,-223(r1)
	PPC_STORE_U8(ctx.r1.u32 + -223, ctx.r10.u8);
	// bge cr6,0x8267f138
	if (!cr6.lt) goto loc_8267F138;
	// stb r16,-224(r1)
	PPC_STORE_U8(ctx.r1.u32 + -224, r16.u8);
	// sth r11,-222(r1)
	PPC_STORE_U16(ctx.r1.u32 + -222, r11.u16);
	// b 0x8267f164
	goto loc_8267F164;
loc_8267F138:
	// ble cr6,0x8267f158
	if (!cr6.gt) goto loc_8267F158;
	// lhz r11,0(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 0);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lhzx r9,r11,r22
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + r22.u32);
	// lhzx r11,r11,r21
	r11.u64 = PPC_LOAD_U16(r11.u32 + r21.u32);
	// stb r9,-224(r1)
	PPC_STORE_U8(ctx.r1.u32 + -224, ctx.r9.u8);
	// sth r11,-222(r1)
	PPC_STORE_U16(ctx.r1.u32 + -222, r11.u16);
	// b 0x8267f164
	goto loc_8267F164;
loc_8267F158:
	// li r11,96
	r11.s64 = 96;
	// sth r16,-222(r1)
	PPC_STORE_U16(ctx.r1.u32 + -222, r16.u16);
	// stb r11,-224(r1)
	PPC_STORE_U8(ctx.r1.u32 + -224, r11.u8);
loc_8267F164:
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// srw r9,r30,r31
	ctx.r9.u64 = r31.u8 & 0x20 ? 0 : (r30.u32 >> (r31.u8 & 0x3F));
	// slw r10,r20,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r20.u32 << (ctx.r10.u8 & 0x3F));
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + r27.u64;
loc_8267F180:
	// lwz r29,-224(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -224);
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf. r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r29,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r29.u32);
	// bne 0x8267f180
	if (!cr0.eq) goto loc_8267F180;
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// slw r11,r20,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r20.u32 << (r11.u8 & 0x3F));
	// b 0x8267f1a4
	goto loc_8267F1A4;
loc_8267F1A0:
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
loc_8267F1A4:
	// and. r10,r11,r30
	ctx.r10.u64 = r11.u64 & r30.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x8267f1a0
	if (!cr0.eq) goto loc_8267F1A0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267f1c4
	if (cr6.eq) goto loc_8267F1C4;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// and r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 & r30.u64;
	// add r30,r10,r11
	r30.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8267f1c8
	goto loc_8267F1C8;
loc_8267F1C4:
	// mr r30,r16
	r30.u64 = r16.u64;
loc_8267F1C8:
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r1,-208
	ctx.r9.s64 = ctx.r1.s64 + -208;
	// addi r25,r25,2
	r25.s64 = r25.s64 + 2;
	// lhzx r11,r10,r9
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// clrlwi. r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// sthx r11,r10,r9
	PPC_STORE_U16(ctx.r10.u32 + ctx.r9.u32, r11.u16);
	// bne 0x8267f200
	if (!cr0.eq) goto loc_8267F200;
	// cmplw cr6,r28,r17
	cr6.compare<uint32_t>(r28.u32, r17.u32, xer);
	// beq cr6,0x8267f2bc
	if (cr6.eq) goto loc_8267F2BC;
	// lhz r11,0(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 0);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lhzx r28,r11,r4
	r28.u64 = PPC_LOAD_U16(r11.u32 + ctx.r4.u32);
loc_8267F200:
	// cmplw cr6,r28,r18
	cr6.compare<uint32_t>(r28.u32, r18.u32, xer);
	// ble cr6,0x8267f114
	if (!cr6.gt) goto loc_8267F114;
	// and r29,r19,r30
	r29.u64 = r19.u64 & r30.u64;
	// cmplw cr6,r29,r26
	cr6.compare<uint32_t>(r29.u32, r26.u32, xer);
	// beq cr6,0x8267f114
	if (cr6.eq) goto loc_8267F114;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8267f220
	if (!cr6.eq) goto loc_8267F220;
	// mr r31,r18
	r31.u64 = r18.u64;
loc_8267F220:
	// subf r11,r31,r28
	r11.s64 = r28.s64 - r31.s64;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r27,r10,r27
	r27.u64 = ctx.r10.u64 + r27.u64;
	// add r10,r31,r11
	ctx.r10.u64 = r31.u64 + r11.u64;
	// cmplw cr6,r10,r17
	cr6.compare<uint32_t>(ctx.r10.u32, r17.u32, xer);
	// slw r8,r20,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (r20.u32 << (r11.u8 & 0x3F));
	// bge cr6,0x8267f26c
	if (!cr6.lt) goto loc_8267F26C;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r1,-208
	ctx.r5.s64 = ctx.r1.s64 + -208;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
loc_8267F248:
	// lhz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// subf. r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble 0x8267f26c
	if (!cr0.gt) goto loc_8267F26C;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r10,r17
	cr6.compare<uint32_t>(ctx.r10.u32, r17.u32, xer);
	// blt cr6,0x8267f248
	if (cr6.lt) goto loc_8267F248;
loc_8267F26C:
	// slw r5,r20,r11
	ctx.r5.u64 = r11.u8 & 0x20 ? 0 : (r20.u32 << (r11.u8 & 0x3F));
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// add r24,r5,r24
	r24.u64 = ctx.r5.u64 + r24.u64;
	// bne cr6,0x8267f284
	if (!cr6.eq) goto loc_8267F284;
	// cmplwi cr6,r24,1286
	cr6.compare<uint32_t>(r24.u32, 1286, xer);
	// bge cr6,0x8267f360
	if (!cr6.lt) goto loc_8267F360;
loc_8267F284:
	// mr r26,r29
	r26.u64 = r29.u64;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// stbx r9,r11,r10
	PPC_STORE_U8(r11.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// stb r18,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, r18.u8);
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// subf r9,r10,r27
	ctx.r9.s64 = r27.s64 - ctx.r10.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r10,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 2;
	// sth r10,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r10.u16);
	// b 0x8267f114
	goto loc_8267F114;
loc_8267F2BC:
	// subf r11,r31,r28
	r11.s64 = r28.s64 - r31.s64;
	// sth r16,-222(r1)
	PPC_STORE_U16(ctx.r1.u32 + -222, r16.u16);
	// li r10,64
	ctx.r10.s64 = 64;
	// stb r11,-223(r1)
	PPC_STORE_U8(ctx.r1.u32 + -223, r11.u8);
	// stb r10,-224(r1)
	PPC_STORE_U8(ctx.r1.u32 + -224, ctx.r10.u8);
	// b 0x8267f33c
	goto loc_8267F33C;
loc_8267F2D4:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8267f2f8
	if (cr6.eq) goto loc_8267F2F8;
	// and r11,r19,r30
	r11.u64 = r19.u64 & r30.u64;
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// beq cr6,0x8267f2f8
	if (cr6.eq) goto loc_8267F2F8;
	// lwz r27,0(r6)
	r27.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// mr r31,r16
	r31.u64 = r16.u64;
	// mr r28,r18
	r28.u64 = r18.u64;
	// stb r18,-223(r1)
	PPC_STORE_U8(ctx.r1.u32 + -223, r18.u8);
loc_8267F2F8:
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// lwz r9,-224(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -224);
	// srw r10,r30,r31
	ctx.r10.u64 = r31.u8 & 0x20 ? 0 : (r30.u32 >> (r31.u8 & 0x3F));
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r10,r27
	PPC_STORE_U32(ctx.r10.u32 + r27.u32, ctx.r9.u32);
	// slw r11,r20,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r20.u32 << (r11.u8 & 0x3F));
	// b 0x8267f318
	goto loc_8267F318;
loc_8267F314:
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
loc_8267F318:
	// and. r10,r11,r30
	ctx.r10.u64 = r11.u64 & r30.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x8267f314
	if (!cr0.eq) goto loc_8267F314;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8267f338
	if (cr6.eq) goto loc_8267F338;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// and r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 & r30.u64;
	// add r30,r10,r11
	r30.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8267f33c
	goto loc_8267F33C;
loc_8267F338:
	// mr r30,r16
	r30.u64 = r16.u64;
loc_8267F33C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8267f2d4
	if (!cr6.eq) goto loc_8267F2D4;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm r11,r24,2,0,29
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r11.u32);
	// stw r18,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r18.u32);
	// b 0x8267f364
	goto loc_8267F364;
loc_8267F360:
	// li r3,1
	ctx.r3.s64 = 1;
loc_8267F364:
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_8267F368"))) PPC_WEAK_FUNC(sub_8267F368);
PPC_FUNC_IMPL(__imp__sub_8267F368) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcd4
	// clrlwi r11,r3,16
	r11.u64 = ctx.r3.u32 & 0xFFFF;
	// rlwinm r9,r3,16,16,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 16) & 0xFFFF;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x8267f388
	if (!cr6.eq) goto loc_8267F388;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8267f4d4
	goto loc_8267F4D4;
loc_8267F388:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x8267f4cc
	if (cr6.eq) goto loc_8267F4CC;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r19,r10,65521
	r19.u64 = ctx.r10.u64 | 65521;
loc_8267F398:
	// cmplwi cr6,r5,5552
	cr6.compare<uint32_t>(ctx.r5.u32, 5552, xer);
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// blt cr6,0x8267f3a8
	if (cr6.lt) goto loc_8267F3A8;
	// li r20,5552
	r20.s64 = 5552;
loc_8267F3A8:
	// subf r5,r20,r5
	ctx.r5.s64 = ctx.r5.s64 - r20.s64;
	// cmpwi cr6,r20,16
	cr6.compare<int32_t>(r20.s32, 16, xer);
	// blt cr6,0x8267f48c
	if (cr6.lt) goto loc_8267F48C;
	// rlwinm r10,r20,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 28) & 0xFFFFFFF;
	// rlwinm r8,r10,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r20,r8,r20
	r20.s64 = r20.s64 - ctx.r8.s64;
loc_8267F3C0:
	// lbz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lbz r21,1(r4)
	r21.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// lbz r22,2(r4)
	r22.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// lbz r23,3(r4)
	r23.u64 = PPC_LOAD_U8(ctx.r4.u32 + 3);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lbz r24,4(r4)
	r24.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// add r11,r21,r11
	r11.u64 = r21.u64 + r11.u64;
	// lbz r25,5(r4)
	r25.u64 = PPC_LOAD_U8(ctx.r4.u32 + 5);
	// lbz r26,6(r4)
	r26.u64 = PPC_LOAD_U8(ctx.r4.u32 + 6);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lbz r27,7(r4)
	r27.u64 = PPC_LOAD_U8(ctx.r4.u32 + 7);
	// add r11,r22,r11
	r11.u64 = r22.u64 + r11.u64;
	// lbz r28,8(r4)
	r28.u64 = PPC_LOAD_U8(ctx.r4.u32 + 8);
	// lbz r29,9(r4)
	r29.u64 = PPC_LOAD_U8(ctx.r4.u32 + 9);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lbz r30,10(r4)
	r30.u64 = PPC_LOAD_U8(ctx.r4.u32 + 10);
	// add r11,r23,r11
	r11.u64 = r23.u64 + r11.u64;
	// lbz r31,11(r4)
	r31.u64 = PPC_LOAD_U8(ctx.r4.u32 + 11);
	// lbz r3,12(r4)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r4.u32 + 12);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lbz r6,13(r4)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r4.u32 + 13);
	// add r11,r24,r11
	r11.u64 = r24.u64 + r11.u64;
	// lbz r7,14(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 14);
	// lbz r8,15(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 15);
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r25,r11
	r11.u64 = r25.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r26,r11
	r11.u64 = r26.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r27,r11
	r11.u64 = r27.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r28,r11
	r11.u64 = r28.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// bne 0x8267f3c0
	if (!cr0.eq) goto loc_8267F3C0;
loc_8267F48C:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x8267f4ac
	if (cr6.eq) goto loc_8267F4AC;
loc_8267F494:
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addic. r20,r20,-1
	xer.ca = r20.u32 > 0;
	r20.s64 = r20.s64 + -1;
	cr0.compare<int32_t>(r20.s32, 0, xer);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// bne 0x8267f494
	if (!cr0.eq) goto loc_8267F494;
loc_8267F4AC:
	// divwu r10,r11,r19
	ctx.r10.u32 = r11.u32 / r19.u32;
	// divwu r8,r9,r19
	ctx.r8.u32 = ctx.r9.u32 / r19.u32;
	// mullw r10,r10,r19
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r19.s32);
	// mullw r8,r8,r19
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r19.s32);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x8267f398
	if (!cr6.eq) goto loc_8267F398;
loc_8267F4CC:
	// rlwinm r10,r9,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 16) & 0xFFFF0000;
	// or r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 | r11.u64;
loc_8267F4D4:
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_8267F4D8"))) PPC_WEAK_FUNC(sub_8267F4D8);
PPC_FUNC_IMPL(__imp__sub_8267F4D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcd8
	// rlwinm r10,r3,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r11,r3,0,16,23
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFF00;
	// rlwinm r9,r3,24,16,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 24) & 0xFF00;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r10,r3,8,24,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 8) & 0xFF;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// not r10,r11
	ctx.r10.u64 = ~r11.u64;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,7400
	r11.s64 = r11.s64 + 7400;
	// beq cr6,0x8267f548
	if (cr6.eq) goto loc_8267F548;
loc_8267F514:
	// clrlwi. r9,r4,30
	ctx.r9.u64 = ctx.r4.u32 & 0x3;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x8267f548
	if (cr0.eq) goto loc_8267F548;
	// lbz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rlwinm r8,r10,8,24,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// xor r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// addi r8,r11,4096
	ctx.r8.s64 = r11.s64 + 4096;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lwzx r9,r9,r8
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	// xor r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 ^ ctx.r10.u64;
	// bne 0x8267f514
	if (!cr0.eq) goto loc_8267F514;
loc_8267F548:
	// addi r9,r4,-4
	ctx.r9.s64 = ctx.r4.s64 + -4;
	// cmplwi cr6,r5,32
	cr6.compare<uint32_t>(ctx.r5.u32, 32, xer);
	// blt cr6,0x8267f7a4
	if (cr6.lt) goto loc_8267F7A4;
	// rlwinm r8,r5,27,5,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x7FFFFFF;
loc_8267F558:
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r7,r11,6144
	ctx.r7.s64 = r11.s64 + 6144;
	// addi r6,r11,5120
	ctx.r6.s64 = r11.s64 + 5120;
	// addi r4,r11,7168
	ctx.r4.s64 = r11.s64 + 7168;
	// addi r3,r11,4096
	ctx.r3.s64 = r11.s64 + 4096;
	// lwz r23,0(r9)
	r23.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r31,r11,6144
	r31.s64 = r11.s64 + 6144;
	// xor r10,r23,r10
	ctx.r10.u64 = r23.u64 ^ ctx.r10.u64;
	// addi r30,r11,5120
	r30.s64 = r11.s64 + 5120;
	// rlwinm r22,r10,18,22,29
	r22.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3FC;
	// rlwinm r21,r10,26,22,29
	r21.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FC;
	// lwz r23,0(r9)
	r23.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r20,r10,10,22,29
	r20.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x3FC;
	// rlwinm r10,r10,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FC;
	// addi r29,r11,7168
	r29.s64 = r11.s64 + 7168;
	// lwzx r7,r22,r7
	ctx.r7.u64 = PPC_LOAD_U32(r22.u32 + ctx.r7.u32);
	// addi r28,r11,4096
	r28.s64 = r11.s64 + 4096;
	// lwzx r6,r21,r6
	ctx.r6.u64 = PPC_LOAD_U32(r21.u32 + ctx.r6.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwzx r4,r20,r4
	ctx.r4.u64 = PPC_LOAD_U32(r20.u32 + ctx.r4.u32);
	// addi r27,r11,6144
	r27.s64 = r11.s64 + 6144;
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// addi r26,r11,5120
	r26.s64 = r11.s64 + 5120;
	// xor r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r4.u64;
	// addi r25,r11,7168
	r25.s64 = r11.s64 + 7168;
	// xor r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r24,r11,4096
	r24.s64 = r11.s64 + 4096;
	// xor r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 ^ r23.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// rlwinm r6,r10,18,22,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3FC;
	// rlwinm r4,r10,26,22,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FC;
	// rlwinm r3,r10,10,22,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x3FC;
	// rlwinm r10,r10,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FC;
	// lwzx r6,r6,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	// lwzx r4,r4,r30
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + r30.u32);
	// addi r30,r11,7168
	r30.s64 = r11.s64 + 7168;
	// lwzx r3,r3,r29
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + r29.u32);
	// addi r29,r11,7168
	r29.s64 = r11.s64 + 7168;
	// xor r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r4.u64;
	// lwzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r28.u32);
	// addi r28,r11,4096
	r28.s64 = r11.s64 + 4096;
	// xor r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r3.u64;
	// xor r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 ^ ctx.r10.u64;
	// xor r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r7.u64;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// rlwinm r6,r10,18,22,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3FC;
	// rlwinm r4,r10,26,22,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FC;
	// rlwinm r3,r10,10,22,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x3FC;
	// rlwinm r10,r10,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FC;
	// lwzx r6,r6,r27
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r27.u32);
	// lwzx r4,r4,r26
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + r26.u32);
	// lwzx r3,r3,r25
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + r25.u32);
	// xor r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r4.u64;
	// lwzx r10,r10,r24
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r24.u32);
	// addi r4,r11,6144
	ctx.r4.s64 = r11.s64 + 6144;
	// xor r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r3.u64;
	// addi r3,r11,5120
	ctx.r3.s64 = r11.s64 + 5120;
	// xor r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 ^ ctx.r10.u64;
	// xor r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r7.u64;
	// rlwinm r7,r10,18,22,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3FC;
	// rlwinm r6,r10,26,22,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FC;
	// rlwinm r31,r10,10,22,29
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x3FC;
	// rlwinm r10,r10,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FC;
	// lwzx r7,r7,r4
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r4.u32);
	// addi r4,r11,4096
	ctx.r4.s64 = r11.s64 + 4096;
	// lwzx r6,r6,r3
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r3.u32);
	// addi r3,r11,4096
	ctx.r3.s64 = r11.s64 + 4096;
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// lwzx r6,r31,r30
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + r30.u32);
	// addi r31,r11,6144
	r31.s64 = r11.s64 + 6144;
	// lwzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r4.u32);
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// addi r6,r11,5120
	ctx.r6.s64 = r11.s64 + 5120;
	// xor r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// addi r7,r11,6144
	ctx.r7.s64 = r11.s64 + 6144;
	// addi r4,r11,7168
	ctx.r4.s64 = r11.s64 + 7168;
	// addi r30,r11,5120
	r30.s64 = r11.s64 + 5120;
	// lwz r27,0(r9)
	r27.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// xor r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 ^ r27.u64;
	// addi r5,r5,-32
	ctx.r5.s64 = ctx.r5.s64 + -32;
	// rlwinm r26,r10,18,22,29
	r26.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3FC;
	// rlwinm r25,r10,26,22,29
	r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FC;
	// lwz r27,0(r9)
	r27.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r24,r10,10,22,29
	r24.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x3FC;
	// rlwinm r10,r10,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FC;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwzx r7,r26,r7
	ctx.r7.u64 = PPC_LOAD_U32(r26.u32 + ctx.r7.u32);
	// lwzx r6,r25,r6
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + ctx.r6.u32);
	// lwzx r4,r24,r4
	ctx.r4.u64 = PPC_LOAD_U32(r24.u32 + ctx.r4.u32);
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// xor r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r4.u64;
	// xor r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// xor r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 ^ r27.u64;
	// rlwinm r6,r10,18,22,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3FC;
	// rlwinm r4,r10,26,22,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FC;
	// rlwinm r3,r10,10,22,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x3FC;
	// rlwinm r10,r10,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FC;
	// lwzx r6,r6,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	// addi r31,r11,5120
	r31.s64 = r11.s64 + 5120;
	// lwzx r4,r4,r30
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + r30.u32);
	// lwzx r3,r3,r29
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + r29.u32);
	// addi r29,r11,7168
	r29.s64 = r11.s64 + 7168;
	// xor r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r4.u64;
	// lwzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r28.u32);
	// xor r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r3.u64;
	// addi r3,r11,6144
	ctx.r3.s64 = r11.s64 + 6144;
	// xor r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 ^ ctx.r10.u64;
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// xor r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r7.u64;
	// rlwinm r7,r10,18,22,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3FC;
	// rlwinm r4,r10,26,22,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FC;
	// rlwinm r30,r10,10,22,29
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x3FC;
	// rlwinm r10,r10,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FC;
	// lwzx r7,r7,r3
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r3.u32);
	// addi r3,r11,4096
	ctx.r3.s64 = r11.s64 + 4096;
	// lwzx r4,r4,r31
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + r31.u32);
	// xor r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r4.u64;
	// lwzx r4,r30,r29
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + r29.u32);
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// xor r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r4.u64;
	// addi r4,r11,7168
	ctx.r4.s64 = r11.s64 + 7168;
	// xor r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// addi r7,r11,6144
	ctx.r7.s64 = r11.s64 + 6144;
	// xor r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r6.u64;
	// addi r6,r11,5120
	ctx.r6.s64 = r11.s64 + 5120;
	// rlwinm r31,r10,18,22,29
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3FC;
	// rlwinm r30,r10,26,22,29
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FC;
	// rlwinm r29,r10,10,22,29
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x3FC;
	// rlwinm r10,r10,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FC;
	// addi r3,r11,4096
	ctx.r3.s64 = r11.s64 + 4096;
	// lwzx r7,r31,r7
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + ctx.r7.u32);
	// lwzx r6,r30,r6
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + ctx.r6.u32);
	// lwzx r4,r29,r4
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + ctx.r4.u32);
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// xor r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r4.u64;
	// xor r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// bne 0x8267f558
	if (!cr0.eq) goto loc_8267F558;
loc_8267F7A4:
	// cmplwi cr6,r5,4
	cr6.compare<uint32_t>(ctx.r5.u32, 4, xer);
	// blt cr6,0x8267f804
	if (cr6.lt) goto loc_8267F804;
	// rlwinm r8,r5,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
loc_8267F7B0:
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r7,r11,6144
	ctx.r7.s64 = r11.s64 + 6144;
	// addi r6,r11,5120
	ctx.r6.s64 = r11.s64 + 5120;
	// addi r4,r11,7168
	ctx.r4.s64 = r11.s64 + 7168;
	// addi r3,r11,4096
	ctx.r3.s64 = r11.s64 + 4096;
	// lwz r31,0(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r5,r5,-4
	ctx.r5.s64 = ctx.r5.s64 + -4;
	// xor r10,r31,r10
	ctx.r10.u64 = r31.u64 ^ ctx.r10.u64;
	// rlwinm r31,r10,18,22,29
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3FC;
	// rlwinm r30,r10,26,22,29
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FC;
	// rlwinm r29,r10,10,22,29
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x3FC;
	// rlwinm r10,r10,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FC;
	// lwzx r7,r31,r7
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + ctx.r7.u32);
	// lwzx r6,r30,r6
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + ctx.r6.u32);
	// lwzx r4,r29,r4
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + ctx.r4.u32);
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// xor r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r4.u64;
	// xor r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// bne 0x8267f7b0
	if (!cr0.eq) goto loc_8267F7B0;
loc_8267F804:
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x8267f83c
	if (cr6.eq) goto loc_8267F83C;
loc_8267F810:
	// lbz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// rlwinm r7,r10,8,24,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// xor r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 ^ ctx.r8.u64;
	// addi r7,r11,4096
	ctx.r7.s64 = r11.s64 + 4096;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwzx r8,r8,r7
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	// xor r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 ^ ctx.r10.u64;
	// bne 0x8267f810
	if (!cr0.eq) goto loc_8267F810;
loc_8267F83C:
	// not r11,r10
	r11.u64 = ~ctx.r10.u64;
	// rlwinm r10,r11,0,16,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFF00;
	// rlwinm r8,r11,16,0,15
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r9,r11,24,16,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF00;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r10,r11,8,24,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFF;
	// rlwinm r11,r8,8,0,23
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_8267F864"))) PPC_WEAK_FUNC(sub_8267F864);
PPC_FUNC_IMPL(__imp__sub_8267F864) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267F868"))) PPC_WEAK_FUNC(sub_8267F868);
PPC_FUNC_IMPL(__imp__sub_8267F868) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x8267f878
	if (!cr6.eq) goto loc_8267F878;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8267F878:
	// b 0x8267f4d8
	sub_8267F4D8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8267F87C"))) PPC_WEAK_FUNC(sub_8267F87C);
PPC_FUNC_IMPL(__imp__sub_8267F87C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267F880"))) PPC_WEAK_FUNC(sub_8267F880);
PPC_FUNC_IMPL(__imp__sub_8267F880) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// addi r10,r5,725
	ctx.r10.s64 = ctx.r5.s64 + 725;
	// rlwinm r11,r5,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r10,r3
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// lwz r10,5192(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5192);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x8267f958
	if (cr6.gt) goto loc_8267F958;
	// rlwinm r31,r6,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
loc_8267F8A8:
	// bge cr6,0x8267f8fc
	if (!cr6.lt) goto loc_8267F8FC;
	// addi r10,r11,726
	ctx.r10.s64 = r11.s64 + 726;
	// addi r9,r11,725
	ctx.r9.s64 = r11.s64 + 725;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r10,r3
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// lwzx r7,r9,r3
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r3.u32);
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r4.u32);
	// lhzx r9,r9,r4
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r4.u32);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x8267f8f8
	if (cr6.lt) goto loc_8267F8F8;
	// bne cr6,0x8267f8fc
	if (!cr6.eq) goto loc_8267F8FC;
	// add r10,r7,r3
	ctx.r10.u64 = ctx.r7.u64 + ctx.r3.u64;
	// add r9,r8,r3
	ctx.r9.u64 = ctx.r8.u64 + ctx.r3.u64;
	// lbz r10,5200(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5200);
	// lbz r9,5200(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 5200);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bgt cr6,0x8267f8fc
	if (cr6.gt) goto loc_8267F8FC;
loc_8267F8F8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_8267F8FC:
	// addi r10,r11,725
	ctx.r10.s64 = r11.s64 + 725;
	// lhzx r9,r31,r4
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + ctx.r4.u32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r8,r8,r4
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r4.u32);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// blt cr6,0x8267f958
	if (cr6.lt) goto loc_8267F958;
	// bne cr6,0x8267f938
	if (!cr6.eq) goto loc_8267F938;
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r8,r6,r3
	ctx.r8.u64 = ctx.r6.u64 + ctx.r3.u64;
	// lbz r9,5200(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 5200);
	// lbz r8,5200(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 5200);
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// ble cr6,0x8267f958
	if (!cr6.gt) goto loc_8267F958;
loc_8267F938:
	// addi r9,r5,725
	ctx.r9.s64 = ctx.r5.s64 + 725;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stwx r10,r9,r3
	PPC_STORE_U32(ctx.r9.u32 + ctx.r3.u32, ctx.r10.u32);
	// lwz r10,5192(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5192);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8267f8a8
	if (!cr6.gt) goto loc_8267F8A8;
loc_8267F958:
	// addi r11,r5,725
	r11.s64 = ctx.r5.s64 + 725;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r6,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, ctx.r6.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267F96C"))) PPC_WEAK_FUNC(sub_8267F96C);
PPC_FUNC_IMPL(__imp__sub_8267F96C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267F970"))) PPC_WEAK_FUNC(sub_8267F970);
PPC_FUNC_IMPL(__imp__sub_8267F970) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce4
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// addi r10,r3,2868
	ctx.r10.s64 = ctx.r3.s64 + 2868;
	// lwz r25,0(r4)
	r25.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// li r28,0
	r28.s64 = 0;
	// lwz r23,4(r4)
	r23.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// li r8,0
	ctx.r8.s64 = 0;
	// li r9,16
	ctx.r9.s64 = 16;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r26,4(r11)
	r26.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r30,8(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r27,16(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8267F9A8:
	// sth r8,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r8.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x8267f9a8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8267F9A8;
	// lwz r11,5196(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5196);
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,725
	r11.s64 = r11.s64 + 725;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r3
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r3.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// sth r10,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r10.u16);
	// lwz r11,5196(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5196);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,573
	cr6.compare<int32_t>(r11.s32, 573, xer);
	// bge cr6,0x8267fbb0
	if (!cr6.lt) goto loc_8267FBB0;
	// addi r10,r11,725
	ctx.r10.s64 = r11.s64 + 725;
	// subfic r29,r11,573
	xer.ca = r11.u32 <= 573;
	r29.s64 = 573 - r11.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r24,r29,r11
	r24.u64 = r29.u64 + r11.u64;
	// add r31,r10,r3
	r31.u64 = ctx.r10.u64 + ctx.r3.u64;
loc_8267F9F8:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r5,r25
	ctx.r7.u64 = ctx.r5.u64 + r25.u64;
	// lhz r11,2(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// rotlwi r11,r11,2
	r11.u64 = __builtin_rotateleft32(r11.u32, 2);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// lhz r11,2(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// ble cr6,0x8267fa28
	if (!cr6.gt) goto loc_8267FA28;
	// mr r11,r27
	r11.u64 = r27.u64;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
loc_8267FA28:
	// cmpw cr6,r10,r23
	cr6.compare<int32_t>(ctx.r10.s32, r23.s32, xer);
	// sth r11,2(r7)
	PPC_STORE_U16(ctx.r7.u32 + 2, r11.u16);
	// bgt cr6,0x8267faa0
	if (cr6.gt) goto loc_8267FAA0;
	// addi r8,r11,1434
	ctx.r8.s64 = r11.s64 + 1434;
	// li r9,0
	ctx.r9.s64 = 0;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r10,r30
	cr6.compare<int32_t>(ctx.r10.s32, r30.s32, xer);
	// lhzx r6,r8,r3
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r3.u32);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// sthx r6,r8,r3
	PPC_STORE_U16(ctx.r8.u32 + ctx.r3.u32, ctx.r6.u16);
	// blt cr6,0x8267fa60
	if (cr6.lt) goto loc_8267FA60;
	// subf r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r26
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r26.u32);
loc_8267FA60:
	// lhz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + r11.u64;
	// lwz r8,5792(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5792);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// mullw r10,r7,r11
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r10,5792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5792, ctx.r10.u32);
	// beq cr6,0x8267faa0
	if (cr6.eq) goto loc_8267FAA0;
	// add r10,r5,r4
	ctx.r10.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lhz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,5796(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5796);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,5796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5796, r11.u32);
loc_8267FAA0:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// bne 0x8267f9f8
	if (!cr0.eq) goto loc_8267F9F8;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8267fbb0
	if (cr6.eq) goto loc_8267FBB0;
	// addi r11,r27,1434
	r11.s64 = r27.s64 + 1434;
	// addi r6,r27,-1
	ctx.r6.s64 = r27.s64 + -1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r11,r3
	ctx.r7.u64 = r11.u64 + ctx.r3.u64;
	// lis r11,0
	r11.s64 = 0;
	// ori r9,r11,65535
	ctx.r9.u64 = r11.u64 | 65535;
loc_8267FACC:
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// addi r10,r11,1434
	ctx.r10.s64 = r11.s64 + 1434;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// b 0x8267fae8
	goto loc_8267FAE8;
loc_8267FAE0:
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
loc_8267FAE8:
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq 0x8267fae0
	if (cr0.eq) goto loc_8267FAE0;
	// addi r10,r11,1434
	ctx.r10.s64 = r11.s64 + 1434;
	// addi r11,r11,1435
	r11.s64 = r11.s64 + 1435;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addic. r28,r28,-2
	xer.ca = r28.u32 > 1;
	r28.s64 = r28.s64 + -2;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// lhzx r5,r10,r3
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r3.u32);
	// lhzx r8,r11,r3
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + ctx.r3.u32);
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sthx r5,r10,r3
	PPC_STORE_U16(ctx.r10.u32 + ctx.r3.u32, ctx.r5.u16);
	// sthx r8,r11,r3
	PPC_STORE_U16(r11.u32 + ctx.r3.u32, ctx.r8.u16);
	// lhz r11,0(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// sth r11,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, r11.u16);
	// bgt 0x8267facc
	if (cr0.gt) goto loc_8267FACC;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x8267fbb0
	if (cr6.eq) goto loc_8267FBB0;
loc_8267FB3C:
	// lhz r5,0(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// cmpwi r5,0
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq 0x8267fba4
	if (cr0.eq) goto loc_8267FBA4;
	// addi r11,r24,725
	r11.s64 = r24.s64 + 725;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r3
	ctx.r6.u64 = r11.u64 + ctx.r3.u64;
loc_8267FB54:
	// addi r6,r6,-4
	ctx.r6.s64 = ctx.r6.s64 + -4;
	// addi r24,r24,-1
	r24.s64 = r24.s64 + -1;
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmpw cr6,r11,r23
	cr6.compare<int32_t>(r11.s32, r23.s32, xer);
	// bgt cr6,0x8267fb9c
	if (cr6.gt) goto loc_8267FB9C;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x8267fb98
	if (cr6.eq) goto loc_8267FB98;
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// lwz r8,5792(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5792);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r10,5792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5792, ctx.r10.u32);
	// sth r9,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r9.u16);
loc_8267FB98:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
loc_8267FB9C:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x8267fb54
	if (!cr6.eq) goto loc_8267FB54;
loc_8267FBA4:
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r7,r7,-2
	ctx.r7.s64 = ctx.r7.s64 + -2;
	// bne 0x8267fb3c
	if (!cr0.eq) goto loc_8267FB3C;
loc_8267FBB0:
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_8267FBB4"))) PPC_WEAK_FUNC(sub_8267FBB4);
PPC_FUNC_IMPL(__imp__sub_8267FBB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267FBB8"))) PPC_WEAK_FUNC(sub_8267FBB8);
PPC_FUNC_IMPL(__imp__sub_8267FBB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lhz r7,2(r4)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r4.u32 + 2);
	// li r6,-1
	ctx.r6.s64 = -1;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r11,7
	r11.s64 = 7;
	// li r9,4
	ctx.r9.s64 = 4;
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x8267fbe0
	if (!cr0.eq) goto loc_8267FBE0;
	// li r11,138
	r11.s64 = 138;
	// li r9,3
	ctx.r9.s64 = 3;
loc_8267FBE0:
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// li r31,-1
	r31.s64 = -1;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// sth r31,6(r10)
	PPC_STORE_U16(ctx.r10.u32 + 6, r31.u16);
	// blt cr6,0x8267fcd4
	if (cr6.lt) goto loc_8267FCD4;
	// addi r4,r4,6
	ctx.r4.s64 = ctx.r4.s64 + 6;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
loc_8267FC00:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// lhz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x8267fc1c
	if (!cr6.lt) goto loc_8267FC1C;
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// beq cr6,0x8267fcc8
	if (cr6.eq) goto loc_8267FCC8;
loc_8267FC1C:
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// bge cr6,0x8267fc3c
	if (!cr6.lt) goto loc_8267FC3C;
	// addi r11,r10,669
	r11.s64 = ctx.r10.s64 + 669;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r9,r11,r3
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + ctx.r3.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sthx r9,r11,r3
	PPC_STORE_U16(r11.u32 + ctx.r3.u32, ctx.r9.u16);
	// b 0x8267fc94
	goto loc_8267FC94;
loc_8267FC3C:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8267fc70
	if (cr6.eq) goto loc_8267FC70;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// beq cr6,0x8267fc60
	if (cr6.eq) goto loc_8267FC60;
	// addi r11,r10,669
	r11.s64 = ctx.r10.s64 + 669;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r9,r11,r3
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + ctx.r3.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// sthx r9,r11,r3
	PPC_STORE_U16(r11.u32 + ctx.r3.u32, ctx.r9.u16);
loc_8267FC60:
	// lhz r11,2740(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2740);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,2740(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2740, r11.u16);
	// b 0x8267fc94
	goto loc_8267FC94;
loc_8267FC70:
	// cmpwi cr6,r8,10
	cr6.compare<int32_t>(ctx.r8.s32, 10, xer);
	// bgt cr6,0x8267fc88
	if (cr6.gt) goto loc_8267FC88;
	// lhz r11,2744(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2744);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,2744(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2744, r11.u16);
	// b 0x8267fc94
	goto loc_8267FC94;
loc_8267FC88:
	// lhz r11,2748(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2748);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,2748(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2748, r11.u16);
loc_8267FC94:
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne cr6,0x8267fcac
	if (!cr6.eq) goto loc_8267FCAC;
	// li r11,138
	r11.s64 = 138;
	// b 0x8267fcb8
	goto loc_8267FCB8;
loc_8267FCAC:
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// bne cr6,0x8267fcc0
	if (!cr6.eq) goto loc_8267FCC0;
	// li r11,6
	r11.s64 = 6;
loc_8267FCB8:
	// li r9,3
	ctx.r9.s64 = 3;
	// b 0x8267fcc8
	goto loc_8267FCC8;
loc_8267FCC0:
	// li r11,7
	r11.s64 = 7;
	// li r9,4
	ctx.r9.s64 = 4;
loc_8267FCC8:
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// bne 0x8267fc00
	if (!cr0.eq) goto loc_8267FC00;
loc_8267FCD4:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8267FCDC"))) PPC_WEAK_FUNC(sub_8267FCDC);
PPC_FUNC_IMPL(__imp__sub_8267FCDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8267FCE0"))) PPC_WEAK_FUNC(sub_8267FCE0);
PPC_FUNC_IMPL(__imp__sub_8267FCE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf0
	// lhz r31,2(r4)
	r31.u64 = PPC_LOAD_U16(ctx.r4.u32 + 2);
	// li r9,-1
	ctx.r9.s64 = -1;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r11,7
	r11.s64 = 7;
	// li r10,4
	ctx.r10.s64 = 4;
	// cmpwi r31,0
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8267fd0c
	if (!cr0.eq) goto loc_8267FD0C;
	// li r11,138
	r11.s64 = 138;
	// li r10,3
	ctx.r10.s64 = 3;
loc_8267FD0C:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// blt cr6,0x82680230
	if (cr6.lt) goto loc_82680230;
	// lis r8,0
	ctx.r8.s64 = 0;
	// addi r30,r4,6
	r30.s64 = ctx.r4.s64 + 6;
	// addi r29,r5,1
	r29.s64 = ctx.r5.s64 + 1;
	// ori r4,r8,65533
	ctx.r4.u64 = ctx.r8.u64 | 65533;
loc_8267FD24:
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lhz r31,0(r30)
	r31.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// bge cr6,0x8267fd40
	if (!cr6.lt) goto loc_8267FD40;
	// cmpw cr6,r5,r31
	cr6.compare<int32_t>(ctx.r5.s32, r31.s32, xer);
	// beq cr6,0x82680224
	if (cr6.eq) goto loc_82680224;
loc_8267FD40:
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// bge cr6,0x8267fdf8
	if (!cr6.lt) goto loc_8267FDF8;
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r5,669
	ctx.r10.s64 = ctx.r5.s64 + 669;
	// add r7,r11,r3
	ctx.r7.u64 = r11.u64 + ctx.r3.u64;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_8267FD58:
	// lhz r9,2678(r7)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2678);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// subfic r11,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	r11.s64 = 16 - ctx.r9.s64;
	// lhz r28,5808(r3)
	r28.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// lhzx r11,r8,r3
	r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r3.u32);
	// ble cr6,0x8267fdd8
	if (!cr6.gt) goto loc_8267FDD8;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r27,20(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mr r26,r11
	r26.u64 = r11.u64;
	// or r11,r10,r28
	r11.u64 = ctx.r10.u64 | r28.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r27
	PPC_STORE_U8(ctx.r10.u32 + r27.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r28,8(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r28,r11
	PPC_STORE_U8(r28.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r26,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r26.u32 >> (r11.u8 & 0x3F));
	// b 0x8267fde8
	goto loc_8267FDE8;
loc_8267FDD8:
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
loc_8267FDE8:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne 0x8267fd58
	if (!cr0.eq) goto loc_8267FD58;
	// b 0x826801f0
	goto loc_826801F0;
loc_8267FDF8:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x8267ffcc
	if (cr6.eq) goto loc_8267FFCC;
	// cmpw cr6,r5,r9
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r9.s32, xer);
	// beq cr6,0x8267feb4
	if (cr6.eq) goto loc_8267FEB4;
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lhz r9,2678(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2678);
	// subfic r11,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	r11.s64 = 16 - ctx.r9.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// addi r11,r5,669
	r11.s64 = ctx.r5.s64 + 669;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r11,r11,r3
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r3.u32);
	// ble cr6,0x8267fe98
	if (!cr6.gt) goto loc_8267FE98;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r8,5808(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// mr r27,r11
	r27.u64 = r11.u64;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// or r11,r10,r8
	r11.u64 = ctx.r10.u64 | ctx.r8.u64;
	// lwz r28,8(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r28,r7
	PPC_STORE_U8(r28.u32 + ctx.r7.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r8,r11
	PPC_STORE_U8(ctx.r8.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r27,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r27.u32 >> (r11.u8 & 0x3F));
	// b 0x8267feac
	goto loc_8267FEAC;
loc_8267FE98:
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
loc_8267FEAC:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
loc_8267FEB4:
	// lhz r9,2742(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2742);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// subfic r11,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	r11.s64 = 16 - ctx.r9.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// lhz r11,2740(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2740);
	// ble cr6,0x8267ff34
	if (!cr6.gt) goto loc_8267FF34;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r8,5808(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// mr r28,r11
	r28.u64 = r11.u64;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// or r11,r10,r8
	r11.u64 = ctx.r10.u64 | ctx.r8.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r7
	PPC_STORE_U8(ctx.r10.u32 + ctx.r7.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r8,r11
	PPC_STORE_U8(ctx.r8.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r28,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r28.u32 >> (r11.u8 & 0x3F));
	// b 0x8267ff48
	goto loc_8267FF48;
loc_8267FF34:
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
loc_8267FF48:
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// cmpwi cr6,r10,14
	cr6.compare<int32_t>(ctx.r10.s32, 14, xer);
	// ble cr6,0x8267ffc0
	if (!cr6.gt) goto loc_8267FFC0;
	// addi r11,r6,-3
	r11.s64 = ctx.r6.s64 + -3;
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r7,r11,16
	ctx.r7.u64 = r11.u32 & 0xFFFF;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r10,r9
	r11.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r8
	PPC_STORE_U8(ctx.r10.u32 + ctx.r8.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-14
	ctx.r10.s64 = r11.s64 + -14;
loc_8267FFAC:
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// srw r11,r7,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (r11.u8 & 0x3F));
	// b 0x826801ec
	goto loc_826801EC;
loc_8267FFC0:
	// addi r9,r10,2
	ctx.r9.s64 = ctx.r10.s64 + 2;
loc_8267FFC4:
	// add r8,r6,r4
	ctx.r8.u64 = ctx.r6.u64 + ctx.r4.u64;
	// b 0x826801dc
	goto loc_826801DC;
loc_8267FFCC:
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// cmpwi cr6,r6,10
	cr6.compare<int32_t>(ctx.r6.s32, 10, xer);
	// bgt cr6,0x826800d8
	if (cr6.gt) goto loc_826800D8;
	// lhz r9,2746(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2746);
	// subfic r11,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	r11.s64 = 16 - ctx.r9.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// lhz r11,2744(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2744);
	// ble cr6,0x82680054
	if (!cr6.gt) goto loc_82680054;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r8,5808(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// mr r28,r11
	r28.u64 = r11.u64;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// or r11,r10,r8
	r11.u64 = ctx.r10.u64 | ctx.r8.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r7
	PPC_STORE_U8(ctx.r10.u32 + ctx.r7.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r8,r11
	PPC_STORE_U8(ctx.r8.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r28,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r28.u32 >> (r11.u8 & 0x3F));
	// b 0x82680068
	goto loc_82680068;
loc_82680054:
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
loc_82680068:
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// cmpwi cr6,r10,13
	cr6.compare<int32_t>(ctx.r10.s32, 13, xer);
	// ble cr6,0x826800d0
	if (!cr6.gt) goto loc_826800D0;
	// addi r11,r6,-3
	r11.s64 = ctx.r6.s64 + -3;
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r7,r11,16
	ctx.r7.u64 = r11.u32 & 0xFFFF;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r10,r9
	r11.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r8
	PPC_STORE_U8(ctx.r10.u32 + ctx.r8.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-13
	ctx.r10.s64 = r11.s64 + -13;
	// b 0x8267ffac
	goto loc_8267FFAC;
loc_826800D0:
	// addi r9,r10,3
	ctx.r9.s64 = ctx.r10.s64 + 3;
	// b 0x8267ffc4
	goto loc_8267FFC4;
loc_826800D8:
	// lhz r9,2750(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2750);
	// subfic r11,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	r11.s64 = 16 - ctx.r9.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// lhz r11,2748(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2748);
	// ble cr6,0x82680154
	if (!cr6.gt) goto loc_82680154;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r8,5808(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// mr r28,r11
	r28.u64 = r11.u64;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// or r11,r10,r8
	r11.u64 = ctx.r10.u64 | ctx.r8.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r7
	PPC_STORE_U8(ctx.r10.u32 + ctx.r7.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r8,r11
	PPC_STORE_U8(ctx.r8.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r28,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r28.u32 >> (r11.u8 & 0x3F));
	// b 0x82680168
	goto loc_82680168;
loc_82680154:
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
loc_82680168:
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// cmpwi cr6,r10,9
	cr6.compare<int32_t>(ctx.r10.s32, 9, xer);
	// ble cr6,0x826801d0
	if (!cr6.gt) goto loc_826801D0;
	// addi r11,r6,-11
	r11.s64 = ctx.r6.s64 + -11;
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r7,r11,16
	ctx.r7.u64 = r11.u32 & 0xFFFF;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r10,r9
	r11.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r8
	PPC_STORE_U8(ctx.r10.u32 + ctx.r8.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-9
	ctx.r10.s64 = r11.s64 + -9;
	// b 0x8267ffac
	goto loc_8267FFAC;
loc_826801D0:
	// addis r8,r6,1
	ctx.r8.s64 = ctx.r6.s64 + 65536;
	// addi r9,r10,7
	ctx.r9.s64 = ctx.r10.s64 + 7;
	// addi r8,r8,-11
	ctx.r8.s64 = ctx.r8.s64 + -11;
loc_826801DC:
	// lhz r11,5808(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// slw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
loc_826801EC:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
loc_826801F0:
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x82680208
	if (!cr6.eq) goto loc_82680208;
	// li r11,138
	r11.s64 = 138;
	// b 0x82680214
	goto loc_82680214;
loc_82680208:
	// cmpw cr6,r5,r31
	cr6.compare<int32_t>(ctx.r5.s32, r31.s32, xer);
	// bne cr6,0x8268021c
	if (!cr6.eq) goto loc_8268021C;
	// li r11,6
	r11.s64 = 6;
loc_82680214:
	// li r10,3
	ctx.r10.s64 = 3;
	// b 0x82680224
	goto loc_82680224;
loc_8268021C:
	// li r11,7
	r11.s64 = 7;
	// li r10,4
	ctx.r10.s64 = 4;
loc_82680224:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// bne 0x8267fd24
	if (!cr0.eq) goto loc_8267FD24;
loc_82680230:
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_82680234"))) PPC_WEAK_FUNC(sub_82680234);
PPC_FUNC_IMPL(__imp__sub_82680234) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82680238"))) PPC_WEAK_FUNC(sub_82680238);
PPC_FUNC_IMPL(__imp__sub_82680238) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// cmpwi cr6,r10,11
	cr6.compare<int32_t>(ctx.r10.s32, 11, xer);
	// ble cr6,0x826802bc
	if (!cr6.gt) goto loc_826802BC;
	// addi r11,r4,-257
	r11.s64 = ctx.r4.s64 + -257;
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r7,r11,16
	ctx.r7.u64 = r11.u32 & 0xFFFF;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r10,r9
	r11.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r11,r9
	PPC_STORE_U8(r11.u32 + ctx.r9.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-11
	ctx.r10.s64 = r11.s64 + -11;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r7,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (r11.u8 & 0x3F));
	// b 0x826802d8
	goto loc_826802D8;
loc_826802BC:
	// addis r9,r4,1
	ctx.r9.s64 = ctx.r4.s64 + 65536;
	// addi r11,r10,5
	r11.s64 = ctx.r10.s64 + 5;
	// addi r9,r9,-257
	ctx.r9.s64 = ctx.r9.s64 + -257;
	// stw r11,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, r11.u32);
	// slw r11,r9,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
loc_826802D8:
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// cmpwi cr6,r10,11
	cr6.compare<int32_t>(ctx.r10.s32, 11, xer);
	// ble cr6,0x82680350
	if (!cr6.gt) goto loc_82680350;
	// addi r11,r31,-1
	r11.s64 = r31.s64 + -1;
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r7,r11,16
	ctx.r7.u64 = r11.u32 & 0xFFFF;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r10,r9
	r11.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r11,r9
	PPC_STORE_U8(r11.u32 + ctx.r9.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-11
	ctx.r10.s64 = r11.s64 + -11;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r7,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (r11.u8 & 0x3F));
	// b 0x8268036c
	goto loc_8268036C;
loc_82680350:
	// addis r8,r31,1
	ctx.r8.s64 = r31.s64 + 65536;
	// lhz r11,5808(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// addi r9,r10,5
	ctx.r9.s64 = ctx.r10.s64 + 5;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
	// slw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
loc_8268036C:
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// cmpwi cr6,r10,12
	cr6.compare<int32_t>(ctx.r10.s32, 12, xer);
	// ble cr6,0x826803e4
	if (!cr6.gt) goto loc_826803E4;
	// addi r11,r6,-4
	r11.s64 = ctx.r6.s64 + -4;
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r7,r11,16
	ctx.r7.u64 = r11.u32 & 0xFFFF;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r10,r9
	r11.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r11,r9
	PPC_STORE_U8(r11.u32 + ctx.r9.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-12
	ctx.r10.s64 = r11.s64 + -12;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r7,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (r11.u8 & 0x3F));
	// b 0x82680400
	goto loc_82680400;
loc_826803E4:
	// addis r8,r6,1
	ctx.r8.s64 = ctx.r6.s64 + 65536;
	// lhz r11,5808(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
	// slw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
loc_82680400:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x826804b8
	if (!cr6.gt) goto loc_826804B8;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r8,r11,15708
	ctx.r8.s64 = r11.s64 + 15708;
loc_82680418:
	// lbzx r11,r9,r8
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r8.u32);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// rotlwi r11,r11,2
	r11.u64 = __builtin_rotateleft32(r11.u32, 2);
	// lhz r7,5808(r3)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// cmpwi cr6,r10,13
	cr6.compare<int32_t>(ctx.r10.s32, 13, xer);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lhz r11,2678(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 2678);
	// ble cr6,0x82680498
	if (!cr6.gt) goto loc_82680498;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mr r29,r11
	r29.u64 = r11.u64;
	// lwz r30,8(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// or r11,r10,r7
	r11.u64 = ctx.r10.u64 | ctx.r7.u64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r5,r30
	PPC_STORE_U8(ctx.r5.u32 + r30.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r11,r7
	PPC_STORE_U8(r11.u32 + ctx.r7.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-13
	ctx.r10.s64 = r11.s64 + -13;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r29,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// b 0x826804a8
	goto loc_826804A8;
loc_82680498:
	// addi r5,r10,3
	ctx.r5.s64 = ctx.r10.s64 + 3;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// stw r5,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r5.u32);
loc_826804A8:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// cmpw cr6,r9,r6
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, xer);
	// blt cr6,0x82680418
	if (cr6.lt) goto loc_82680418;
loc_826804B8:
	// addi r5,r4,-1
	ctx.r5.s64 = ctx.r4.s64 + -1;
	// addi r4,r3,140
	ctx.r4.s64 = ctx.r3.s64 + 140;
	// bl 0x8267fce0
	sub_8267FCE0(ctx, base);
	// addi r5,r31,-1
	ctx.r5.s64 = r31.s64 + -1;
	// addi r4,r3,2432
	ctx.r4.s64 = ctx.r3.s64 + 2432;
	// bl 0x8267fce0
	sub_8267FCE0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_826804D8"))) PPC_WEAK_FUNC(sub_826804D8);
PPC_FUNC_IMPL(__imp__sub_826804D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce4
	// lwz r11,5784(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5784);
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82680884
	if (cr6.eq) goto loc_82680884;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// li r27,0
	r27.s64 = 0;
	// addi r28,r11,17200
	r28.s64 = r11.s64 + 17200;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r31,r11,15592
	r31.s64 = r11.s64 + 15592;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r30,r11,17712
	r30.s64 = r11.s64 + 17712;
loc_8268050C:
	// lwz r11,5788(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5788);
	// lwz r10,5776(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5776);
	// lhzx r6,r27,r11
	ctx.r6.u64 = PPC_LOAD_U16(r27.u32 + r11.u32);
	// addi r27,r27,2
	r27.s64 = r27.s64 + 2;
	// lbzx r7,r29,r10
	ctx.r7.u64 = PPC_LOAD_U8(r29.u32 + ctx.r10.u32);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne 0x826805c0
	if (!cr0.eq) goto loc_826805C0;
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// subfic r8,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	ctx.r8.s64 = 16 - ctx.r9.s64;
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// lhz r8,5808(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// ble cr6,0x826805ac
	if (!cr6.gt) goto loc_826805AC;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// or r11,r10,r8
	r11.u64 = ctx.r10.u64 | ctx.r8.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r7
	PPC_STORE_U8(ctx.r10.u32 + ctx.r7.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r11,r8
	PPC_STORE_U8(r11.u32 + ctx.r8.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// srw r11,r6,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 >> (r11.u8 & 0x3F));
	// b 0x82680854
	goto loc_82680854;
loc_826805AC:
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
	// b 0x8268085c
	goto loc_8268085C;
loc_826805C0:
	// lbzx r10,r7,r30
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r7.u32 + r30.u32);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// rotlwi r8,r10,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// addi r10,r10,257
	ctx.r10.s64 = ctx.r10.s64 + 257;
	// add r9,r8,r4
	ctx.r9.u64 = ctx.r8.u64 + ctx.r4.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r9,1030(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 1030);
	// lhzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r4.u32);
	// subfic r26,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	r26.s64 = 16 - ctx.r9.s64;
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// lhz r26,5808(r3)
	r26.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// ble cr6,0x82680654
	if (!cr6.gt) goto loc_82680654;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// lwz r25,20(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r24,8(r3)
	r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// or r11,r11,r26
	r11.u64 = r11.u64 | r26.u64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r24,r25
	PPC_STORE_U8(r24.u32 + r25.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r26,8(r3)
	r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r11,r26
	PPC_STORE_U8(r11.u32 + r26.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r23,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 >> (r11.u8 & 0x3F));
	// b 0x82680664
	goto loc_82680664;
loc_82680654:
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// or r11,r11,r26
	r11.u64 = r11.u64 | r26.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
loc_82680664:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// lwzx r9,r8,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x82680710
	if (cr0.eq) goto loc_82680710;
	// addi r10,r31,2376
	ctx.r10.s64 = r31.s64 + 2376;
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// subfic r26,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	r26.s64 = 16 - ctx.r9.s64;
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// lwzx r10,r8,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// lhz r8,5808(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// ble cr6,0x826806fc
	if (!cr6.gt) goto loc_826806FC;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// clrlwi r8,r10,16
	ctx.r8.u64 = ctx.r10.u32 & 0xFFFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r7,r10
	PPC_STORE_U8(ctx.r7.u32 + ctx.r10.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r11,r7
	PPC_STORE_U8(r11.u32 + ctx.r7.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r8,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (r11.u8 & 0x3F));
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// b 0x82680710
	goto loc_82680710;
loc_826806FC:
	// slw r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// or r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 | ctx.r8.u64;
	// stw r11,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, r11.u32);
	// sth r10,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r10.u16);
loc_82680710:
	// addi r7,r6,-1
	ctx.r7.s64 = ctx.r6.s64 + -1;
	// cmplwi cr6,r7,256
	cr6.compare<uint32_t>(ctx.r7.u32, 256, xer);
	// bge cr6,0x82680724
	if (!cr6.lt) goto loc_82680724;
	// lbzx r11,r7,r28
	r11.u64 = PPC_LOAD_U8(ctx.r7.u32 + r28.u32);
	// b 0x82680730
	goto loc_82680730;
loc_82680724:
	// rlwinm r11,r7,25,7,31
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// addi r10,r28,256
	ctx.r10.s64 = r28.s64 + 256;
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
loc_82680730:
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// add r11,r8,r5
	r11.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// subfic r6,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	ctx.r6.s64 = 16 - ctx.r9.s64;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// lhz r6,5808(r3)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// ble cr6,0x826807b8
	if (!cr6.gt) goto loc_826807B8;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r26,20(r3)
	r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mr r25,r11
	r25.u64 = r11.u64;
	// or r11,r10,r6
	r11.u64 = ctx.r10.u64 | ctx.r6.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r26
	PPC_STORE_U8(ctx.r10.u32 + r26.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r11,r6
	PPC_STORE_U8(r11.u32 + ctx.r6.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r25,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r25.u32 >> (r11.u8 & 0x3F));
	// b 0x826807c8
	goto loc_826807C8;
loc_826807B8:
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r11,r6
	r11.u64 = r11.u64 | ctx.r6.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
loc_826807C8:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// addi r11,r31,136
	r11.s64 = r31.s64 + 136;
	// lwzx r9,r8,r11
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x82680878
	if (cr0.eq) goto loc_82680878;
	// addi r10,r31,2496
	ctx.r10.s64 = r31.s64 + 2496;
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// subfic r6,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	ctx.r6.s64 = 16 - ctx.r9.s64;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// lwzx r10,r8,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// lhz r8,5808(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// ble cr6,0x82680864
	if (!cr6.gt) goto loc_82680864;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// clrlwi r8,r10,16
	ctx.r8.u64 = ctx.r10.u32 & 0xFFFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r7,r10
	PPC_STORE_U8(ctx.r7.u32 + ctx.r10.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r11,r7
	PPC_STORE_U8(r11.u32 + ctx.r7.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// srw r11,r8,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (r11.u8 & 0x3F));
loc_82680854:
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
loc_8268085C:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// b 0x82680878
	goto loc_82680878;
loc_82680864:
	// slw r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// or r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 | ctx.r8.u64;
	// stw r11,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, r11.u32);
	// sth r10,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r10.u16);
loc_82680878:
	// lwz r11,5784(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5784);
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// blt cr6,0x8268050c
	if (cr6.lt) goto loc_8268050C;
loc_82680884:
	// lhz r9,1026(r4)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r4.u32 + 1026);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// subfic r11,r9,16
	xer.ca = ctx.r9.u32 <= 16;
	r11.s64 = 16 - ctx.r9.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// lhz r11,1024(r4)
	r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 1024);
	// ble cr6,0x82680904
	if (!cr6.gt) goto loc_82680904;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r8,5808(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// or r11,r10,r8
	r11.u64 = ctx.r10.u64 | ctx.r8.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r7
	PPC_STORE_U8(ctx.r10.u32 + ctx.r7.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r11,r8
	PPC_STORE_U8(r11.u32 + ctx.r8.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r6,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 >> (r11.u8 & 0x3F));
	// b 0x82680918
	goto loc_82680918;
loc_82680904:
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
loc_82680918:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// lhz r11,1026(r4)
	r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 1026);
	// stw r11,5804(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5804, r11.u32);
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82680928"))) PPC_WEAK_FUNC(sub_82680928);
PPC_FUNC_IMPL(__imp__sub_82680928) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// ble cr6,0x82680964
	if (!cr6.gt) goto loc_82680964;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lhz r11,5808(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stbx r11,r9,r10
	PPC_STORE_U8(ctx.r9.u32 + ctx.r10.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
	// b 0x8268097c
	goto loc_8268097C;
loc_82680964:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82680988
	if (!cr6.gt) goto loc_82680988;
	// lhz r11,5808(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
loc_8268097C:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
loc_82680988:
	// li r11,0
	r11.s64 = 0;
	// li r10,8
	ctx.r10.s64 = 8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stw r11,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, r11.u32);
	// stw r10,5804(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5804, ctx.r10.u32);
	// beq cr6,0x82680a08
	if (cr6.eq) goto loc_82680A08;
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// not r11,r5
	r11.u64 = ~ctx.r5.u64;
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r9,r5,24,24,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFF;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// rlwinm r31,r11,24,24,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// stbx r5,r7,r8
	PPC_STORE_U8(ctx.r7.u32 + ctx.r8.u32, ctx.r5.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r9,r8,r11
	PPC_STORE_U8(ctx.r8.u32 + r11.u32, ctx.r9.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r6,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r6.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r31,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, r31.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
loc_82680A08:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82680a38
	if (cr6.eq) goto loc_82680A38;
loc_82680A10:
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stbx r11,r9,r10
	PPC_STORE_U8(ctx.r9.u32 + ctx.r10.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// bne 0x82680a10
	if (!cr0.eq) goto loc_82680A10;
loc_82680A38:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82680A40"))) PPC_WEAK_FUNC(sub_82680A40);
PPC_FUNC_IMPL(__imp__sub_82680A40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	// lis r11,-32137
	r11.s64 = -2106130432;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,5900
	r11.s64 = r11.s64 + 5900;
	// addi r9,r3,140
	ctx.r9.s64 = ctx.r3.s64 + 140;
	// addi r6,r11,20
	ctx.r6.s64 = r11.s64 + 20;
	// addi r5,r11,40
	ctx.r5.s64 = r11.s64 + 40;
	// addi r8,r3,2432
	ctx.r8.s64 = ctx.r3.s64 + 2432;
	// sth r10,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r10.u16);
	// addi r7,r3,2676
	ctx.r7.s64 = ctx.r3.s64 + 2676;
	// stw r11,2840(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2840, r11.u32);
	// li r4,8
	ctx.r4.s64 = 8;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// stw r6,2852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2852, ctx.r6.u32);
	// li r11,286
	r11.s64 = 286;
	// stw r5,2864(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2864, ctx.r5.u32);
	// stw r9,2832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2832, ctx.r9.u32);
	// stw r8,2844(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2844, ctx.r8.u32);
	// stw r4,5804(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5804, ctx.r4.u32);
	// stw r7,2856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2856, ctx.r7.u32);
loc_82680A8C:
	// sth r10,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r10.u16);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne 0x82680a8c
	if (!cr0.eq) goto loc_82680A8C;
	// li r11,30
	r11.s64 = 30;
loc_82680AA0:
	// sth r10,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r10.u16);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bne 0x82680aa0
	if (!cr0.eq) goto loc_82680AA0;
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// li r11,19
	r11.s64 = 19;
loc_82680AB8:
	// sth r10,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r10.u16);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne 0x82680ab8
	if (!cr0.eq) goto loc_82680AB8;
	// li r11,1
	r11.s64 = 1;
	// stw r10,5796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5796, ctx.r10.u32);
	// stw r10,5792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5792, ctx.r10.u32);
	// stw r10,5800(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5800, ctx.r10.u32);
	// stw r10,5784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5784, ctx.r10.u32);
	// sth r11,1164(r3)
	PPC_STORE_U16(ctx.r3.u32 + 1164, r11.u16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82680AE4"))) PPC_WEAK_FUNC(sub_82680AE4);
PPC_FUNC_IMPL(__imp__sub_82680AE4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82680AE8"))) PPC_WEAK_FUNC(sub_82680AE8);
PPC_FUNC_IMPL(__imp__sub_82680AE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// li r24,0
	r24.s64 = 0;
	// li r9,573
	ctx.r9.s64 = 573;
	// li r25,-1
	r25.s64 = -1;
	// mr r11,r24
	r11.u64 = r24.u64;
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// lwz r30,0(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r27,12(r10)
	r27.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r24,5192(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5192, r24.u32);
	// cmpwi r27,0
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// stw r9,5196(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5196, ctx.r9.u32);
	// ble 0x82680b74
	if (!cr0.gt) goto loc_82680B74;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
loc_82680B2C:
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x82680b60
	if (cr0.eq) goto loc_82680B60;
	// lwz r10,5192(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5192);
	// add r8,r11,r3
	ctx.r8.u64 = r11.u64 + ctx.r3.u64;
	// mr r25,r11
	r25.u64 = r11.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r6,r10,725
	ctx.r6.s64 = ctx.r10.s64 + 725;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,5192(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5192, ctx.r10.u32);
	// stwx r11,r6,r3
	PPC_STORE_U32(ctx.r6.u32 + ctx.r3.u32, r11.u32);
	// stb r24,5200(r8)
	PPC_STORE_U8(ctx.r8.u32 + 5200, r24.u8);
	// b 0x82680b64
	goto loc_82680B64;
loc_82680B60:
	// sth r24,2(r9)
	PPC_STORE_U16(ctx.r9.u32 + 2, r24.u16);
loc_82680B64:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// blt cr6,0x82680b2c
	if (cr6.lt) goto loc_82680B2C;
loc_82680B74:
	// lwz r11,5192(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5192);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bge cr6,0x82680bf8
	if (!cr6.lt) goto loc_82680BF8;
	// addi r8,r3,5200
	ctx.r8.s64 = ctx.r3.s64 + 5200;
loc_82680B84:
	// cmpwi cr6,r25,2
	cr6.compare<int32_t>(r25.s32, 2, xer);
	// bge cr6,0x82680b98
	if (!cr6.lt) goto loc_82680B98;
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// mr r11,r25
	r11.u64 = r25.u64;
	// b 0x82680b9c
	goto loc_82680B9C;
loc_82680B98:
	// mr r11,r24
	r11.u64 = r24.u64;
loc_82680B9C:
	// lwz r10,5192(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5192);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// addi r5,r10,725
	ctx.r5.s64 = ctx.r10.s64 + 725;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,5192(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5192, ctx.r10.u32);
	// stwx r11,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, r11.u32);
	// sthx r6,r9,r30
	PPC_STORE_U16(ctx.r9.u32 + r30.u32, ctx.r6.u16);
	// stbx r24,r8,r11
	PPC_STORE_U8(ctx.r8.u32 + r11.u32, r24.u8);
	// lwz r11,5792(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5792);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,5792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5792, r11.u32);
	// beq cr6,0x82680bec
	if (cr6.eq) goto loc_82680BEC;
	// add r11,r9,r7
	r11.u64 = ctx.r9.u64 + ctx.r7.u64;
	// lwz r10,5796(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5796);
	// lhz r11,2(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// stw r11,5796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5796, r11.u32);
loc_82680BEC:
	// lwz r11,5192(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5192);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// blt cr6,0x82680b84
	if (cr6.lt) goto loc_82680B84;
loc_82680BF8:
	// stw r25,4(r26)
	PPC_STORE_U32(r26.u32 + 4, r25.u32);
	// lwz r11,5192(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5192);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r31,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r31.s64 = temp.s64;
	// b 0x82680c1c
	goto loc_82680C1C;
loc_82680C0C:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8267f880
	sub_8267F880(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
loc_82680C1C:
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// bge cr6,0x82680c0c
	if (!cr6.lt) goto loc_82680C0C;
	// rlwinm r11,r27,2,0,29
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r29,r3,5200
	r29.s64 = ctx.r3.s64 + 5200;
	// add r28,r11,r30
	r28.u64 = r11.u64 + r30.u64;
loc_82680C30:
	// lwz r11,5192(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5192);
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r31,2904(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2904);
	// addi r10,r11,725
	ctx.r10.s64 = r11.s64 + 725;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// stw r11,5192(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5192, r11.u32);
	// stw r10,2904(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2904, ctx.r10.u32);
	// bl 0x8267f880
	sub_8267F880(ctx, base);
	// lwz r10,5196(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5196);
	// rlwinm r9,r31,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,2904(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2904);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,725
	ctx.r7.s64 = ctx.r10.s64 + 725;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,5196(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5196, ctx.r10.u32);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// stwx r31,r7,r3
	PPC_STORE_U32(ctx.r7.u32 + ctx.r3.u32, r31.u32);
	// lwz r10,5196(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5196);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r7,r10,725
	ctx.r7.s64 = ctx.r10.s64 + 725;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,5196(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5196, ctx.r10.u32);
	// stwx r11,r7,r3
	PPC_STORE_U32(ctx.r7.u32 + ctx.r3.u32, r11.u32);
	// lhz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// sth r10,0(r28)
	PPC_STORE_U16(r28.u32 + 0, ctx.r10.u16);
	// lbzx r11,r29,r11
	r11.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// lbzx r10,r29,r31
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + r31.u32);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82680cc4
	if (cr6.lt) goto loc_82680CC4;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82680CC4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r7,r27,16
	ctx.r7.u64 = r27.u32 & 0xFFFF;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stbx r11,r29,r27
	PPC_STORE_U8(r29.u32 + r27.u32, r11.u8);
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// sth r7,2(r8)
	PPC_STORE_U16(ctx.r8.u32 + 2, ctx.r7.u16);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// sth r7,2(r9)
	PPC_STORE_U16(ctx.r9.u32 + 2, ctx.r7.u16);
	// stw r10,2904(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2904, ctx.r10.u32);
	// bl 0x8267f880
	sub_8267F880(ctx, base);
	// lwz r11,5192(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5192);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bge cr6,0x82680c30
	if (!cr6.lt) goto loc_82680C30;
	// lwz r11,5196(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5196);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r10,2904(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2904);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r11,725
	ctx.r9.s64 = r11.s64 + 725;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,5196(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5196, r11.u32);
	// stwx r10,r9,r3
	PPC_STORE_U32(ctx.r9.u32 + ctx.r3.u32, ctx.r10.u32);
	// bl 0x8267f970
	sub_8267F970(ctx, base);
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// addi r8,r1,82
	ctx.r8.s64 = ctx.r1.s64 + 82;
	// addi r10,r3,2868
	ctx.r10.s64 = ctx.r3.s64 + 2868;
	// li r11,15
	r11.s64 = 15;
loc_82680D34:
	// lhz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// sth r9,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r9.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bne 0x82680d34
	if (!cr0.eq) goto loc_82680D34;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x82680db8
	if (cr6.lt) goto loc_82680DB8;
	// addi r6,r25,1
	ctx.r6.s64 = r25.s64 + 1;
loc_82680D68:
	// lhz r8,2(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x82680dac
	if (cr0.eq) goto loc_82680DAC;
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lhzx r11,r10,r9
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// addi r5,r11,1
	ctx.r5.s64 = r11.s64 + 1;
	// sthx r5,r10,r9
	PPC_STORE_U16(ctx.r10.u32 + ctx.r9.u32, ctx.r5.u16);
loc_82680D8C:
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// or r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 | ctx.r7.u64;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// bgt 0x82680d8c
	if (cr0.gt) goto loc_82680D8C;
	// rlwinm r11,r7,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
loc_82680DAC:
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// bne 0x82680d68
	if (!cr0.eq) goto loc_82680D68;
loc_82680DB8:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_82680DC0"))) PPC_WEAK_FUNC(sub_82680DC0);
PPC_FUNC_IMPL(__imp__sub_82680DC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// ble cr6,0x82680e30
	if (!cr6.gt) goto loc_82680E30;
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// slw r11,r6,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r9,r6,16
	ctx.r9.u64 = ctx.r6.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r8,r11
	PPC_STORE_U8(ctx.r8.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-13
	ctx.r10.s64 = r11.s64 + -13;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// b 0x82680e44
	goto loc_82680E44;
loc_82680E30:
	// slw r10,r6,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// stw r11,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, r11.u32);
	// lhz r11,5808(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
loc_82680E44:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x82680928
	sub_82680928(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82680E50"))) PPC_WEAK_FUNC(sub_82680E50);
PPC_FUNC_IMPL(__imp__sub_82680E50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// li r8,2
	ctx.r8.s64 = 2;
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// ble cr6,0x82680ec4
	if (!cr6.gt) goto loc_82680EC4;
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// slw r11,r8,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r9,r8,16
	ctx.r9.u64 = ctx.r8.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r7,r10
	PPC_STORE_U8(ctx.r7.u32 + ctx.r10.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r7,r11
	PPC_STORE_U8(ctx.r7.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-13
	ctx.r10.s64 = r11.s64 + -13;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// b 0x82680ed8
	goto loc_82680ED8;
loc_82680EC4:
	// addi r10,r11,3
	ctx.r10.s64 = r11.s64 + 3;
	// slw r11,r8,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
loc_82680ED8:
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// li r11,0
	r11.s64 = 0;
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// cmpwi cr6,r10,9
	cr6.compare<int32_t>(ctx.r10.s32, 9, xer);
	// ble cr6,0x82680f50
	if (!cr6.gt) goto loc_82680F50;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r7,r11,16
	ctx.r7.u64 = r11.u32 & 0xFFFF;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// sth r10,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r10.u16);
	// stbx r10,r6,r9
	PPC_STORE_U8(ctx.r6.u32 + ctx.r9.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// stbx r9,r6,r10
	PPC_STORE_U8(ctx.r6.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r9.u32);
	// addi r9,r10,-9
	ctx.r9.s64 = ctx.r10.s64 + -9;
	// subfic r10,r10,16
	xer.ca = ctx.r10.u32 <= 16;
	ctx.r10.s64 = 16 - ctx.r10.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
	// srw r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r10.u8 & 0x3F));
	// b 0x82680f60
	goto loc_82680F60;
loc_82680F50:
	// addi r7,r10,7
	ctx.r7.s64 = ctx.r10.s64 + 7;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r7,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r7.u32);
loc_82680F60:
	// sth r10,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r10.u16);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// cmpwi cr6,r10,16
	cr6.compare<int32_t>(ctx.r10.s32, 16, xer);
	// bne cr6,0x82680fb0
	if (!cr6.eq) goto loc_82680FB0;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stbx r10,r7,r9
	PPC_STORE_U8(ctx.r7.u32 + ctx.r9.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// stbx r9,r7,r10
	PPC_STORE_U8(ctx.r7.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r11,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, r11.u32);
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// b 0x82680fe8
	goto loc_82680FE8;
loc_82680FB0:
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// blt cr6,0x82680fe8
	if (cr6.lt) goto loc_82680FE8;
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stbx r10,r9,r7
	PPC_STORE_U8(ctx.r9.u32 + ctx.r7.u32, ctx.r10.u8);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// lbz r7,5808(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,-8
	ctx.r10.s64 = ctx.r10.s64 + -8;
	// stw r9,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r9.u32);
	// sth r7,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r7.u16);
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
loc_82680FE8:
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// lwz r9,5804(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5804);
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// addi r9,r9,11
	ctx.r9.s64 = ctx.r9.s64 + 11;
	// cmpwi cr6,r9,9
	cr6.compare<int32_t>(ctx.r9.s32, 9, xer);
	// bge cr6,0x82681178
	if (!cr6.lt) goto loc_82681178;
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// cmpwi cr6,r10,13
	cr6.compare<int32_t>(ctx.r10.s32, 13, xer);
	// ble cr6,0x8268106c
	if (!cr6.gt) goto loc_8268106C;
	// slw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
	// clrlwi r7,r8,16
	ctx.r7.u64 = ctx.r8.u32 & 0xFFFF;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// sth r10,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r10.u16);
	// stbx r10,r8,r9
	PPC_STORE_U8(ctx.r8.u32 + ctx.r9.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// stbx r9,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r9.u32);
	// addi r9,r10,-13
	ctx.r9.s64 = ctx.r10.s64 + -13;
	// subfic r10,r10,16
	xer.ca = ctx.r10.u32 <= 16;
	ctx.r10.s64 = 16 - ctx.r10.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
	// srw r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r10.u8 & 0x3F));
	// b 0x8268107c
	goto loc_8268107C;
loc_8268106C:
	// addi r7,r10,3
	ctx.r7.s64 = ctx.r10.s64 + 3;
	// slw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r7,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r7.u32);
loc_8268107C:
	// sth r10,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r10.u16);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// cmpwi cr6,r10,9
	cr6.compare<int32_t>(ctx.r10.s32, 9, xer);
	// ble cr6,0x826810f0
	if (!cr6.gt) goto loc_826810F0;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r8,r11,16
	ctx.r8.u64 = r11.u32 & 0xFFFF;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// sth r10,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r10.u16);
	// stbx r10,r7,r9
	PPC_STORE_U8(ctx.r7.u32 + ctx.r9.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// stbx r9,r7,r10
	PPC_STORE_U8(ctx.r7.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r9.u32);
	// addi r9,r10,-9
	ctx.r9.s64 = ctx.r10.s64 + -9;
	// subfic r10,r10,16
	xer.ca = ctx.r10.u32 <= 16;
	ctx.r10.s64 = 16 - ctx.r10.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
	// srw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
	// b 0x82681100
	goto loc_82681100;
loc_826810F0:
	// addi r8,r10,7
	ctx.r8.s64 = ctx.r10.s64 + 7;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r8,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r8.u32);
loc_82681100:
	// sth r10,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r10.u16);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// cmpwi cr6,r10,16
	cr6.compare<int32_t>(ctx.r10.s32, 16, xer);
	// bne cr6,0x82681140
	if (!cr6.eq) goto loc_82681140;
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stbx r10,r8,r9
	PPC_STORE_U8(ctx.r8.u32 + ctx.r9.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// stbx r9,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, ctx.r9.u8);
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// b 0x82681168
	goto loc_82681168;
loc_82681140:
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// blt cr6,0x82681178
	if (cr6.lt) goto loc_82681178;
	// lhz r11,5808(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lbz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// sth r9,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, ctx.r9.u16);
loc_82681168:
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stw r11,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, r11.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
loc_82681178:
	// li r11,7
	r11.s64 = 7;
	// stw r11,5804(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5804, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681184"))) PPC_WEAK_FUNC(sub_82681184);
PPC_FUNC_IMPL(__imp__sub_82681184) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82681188"))) PPC_WEAK_FUNC(sub_82681188);
PPC_FUNC_IMPL(__imp__sub_82681188) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,124(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	// li r27,0
	r27.s64 = 0;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// addi r28,r11,15708
	r28.s64 = r11.s64 + 15708;
	// ble cr6,0x826812c8
	if (!cr6.gt) goto loc_826812C8;
	// lbz r11,28(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 28);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82681238
	if (!cr6.eq) goto loc_82681238;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// addi r10,r3,140
	ctx.r10.s64 = ctx.r3.s64 + 140;
	// li r11,7
	r11.s64 = 7;
loc_826811D8:
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// bne 0x826811d8
	if (!cr0.eq) goto loc_826811D8;
	// addi r10,r3,168
	ctx.r10.s64 = ctx.r3.s64 + 168;
	// li r11,121
	r11.s64 = 121;
loc_826811F4:
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// bne 0x826811f4
	if (!cr0.eq) goto loc_826811F4;
	// addi r10,r3,652
	ctx.r10.s64 = ctx.r3.s64 + 652;
	// li r11,128
	r11.s64 = 128;
loc_82681210:
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// bne 0x82681210
	if (!cr0.eq) goto loc_82681210;
	// rlwinm r11,r7,30,2,31
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// subfc r11,r9,r11
	xer.ca = r11.u32 >= ctx.r9.u32;
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r11,28(r3)
	PPC_STORE_U8(ctx.r3.u32 + 28, r11.u8);
loc_82681238:
	// addi r4,r3,2832
	ctx.r4.s64 = ctx.r3.s64 + 2832;
	// bl 0x82680ae8
	sub_82680AE8(ctx, base);
	// addi r4,r3,2844
	ctx.r4.s64 = ctx.r3.s64 + 2844;
	// bl 0x82680ae8
	sub_82680AE8(ctx, base);
	// addi r4,r3,140
	ctx.r4.s64 = ctx.r3.s64 + 140;
	// lwz r5,2836(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2836);
	// bl 0x8267fbb8
	sub_8267FBB8(ctx, base);
	// addi r4,r3,2432
	ctx.r4.s64 = ctx.r3.s64 + 2432;
	// lwz r5,2848(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2848);
	// bl 0x8267fbb8
	sub_8267FBB8(ctx, base);
	// addi r4,r3,2856
	ctx.r4.s64 = ctx.r3.s64 + 2856;
	// bl 0x82680ae8
	sub_82680AE8(ctx, base);
	// li r11,18
	r11.s64 = 18;
loc_8268126C:
	// lbzx r10,r11,r28
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r28.u32);
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// lhz r10,2678(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2678);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x82681290
	if (!cr0.eq) goto loc_82681290;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bge cr6,0x8268126c
	if (!cr6.lt) goto loc_8268126C;
loc_82681290:
	// mulli r10,r11,3
	ctx.r10.s64 = r11.s64 * 3;
	// lwz r9,5796(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5796);
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// addi r9,r9,10
	ctx.r9.s64 = ctx.r9.s64 + 10;
	// lwz r11,5792(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5792);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r9,r9,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFF;
	// addi r11,r11,17
	r11.s64 = r11.s64 + 17;
	// addi r10,r11,10
	ctx.r10.s64 = r11.s64 + 10;
	// rlwinm r10,r10,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
	// stw r11,5792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5792, r11.u32);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bgt cr6,0x826812d0
	if (cr6.gt) goto loc_826812D0;
	// b 0x826812cc
	goto loc_826812CC;
loc_826812C8:
	// addi r9,r31,5
	ctx.r9.s64 = r31.s64 + 5;
loc_826812CC:
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_826812D0:
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x826812f8
	if (cr6.gt) goto loc_826812F8;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826812f8
	if (cr6.eq) goto loc_826812F8;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82680dc0
	sub_82680DC0(ctx, base);
	// b 0x82681444
	goto loc_82681444;
loc_826812F8:
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// lwz r10,5812(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// bne cr6,0x82681398
	if (!cr6.eq) goto loc_82681398;
	// cmpwi cr6,r10,13
	cr6.compare<int32_t>(ctx.r10.s32, 13, xer);
	// addi r11,r29,2
	r11.s64 = r29.s64 + 2;
	// ble cr6,0x82681374
	if (!cr6.gt) goto loc_82681374;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// clrlwi r7,r11,16
	ctx.r7.u64 = r11.u32 & 0xFFFF;
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// or r11,r10,r9
	r11.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r8
	PPC_STORE_U8(ctx.r10.u32 + ctx.r8.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-13
	ctx.r10.s64 = r11.s64 + -13;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r7,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (r11.u8 & 0x3F));
	// b 0x82681388
	goto loc_82681388;
loc_82681374:
	// addi r9,r10,3
	ctx.r9.s64 = ctx.r10.s64 + 3;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
loc_82681388:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// addi r5,r28,1372
	ctx.r5.s64 = r28.s64 + 1372;
	// addi r4,r28,220
	ctx.r4.s64 = r28.s64 + 220;
	// b 0x82681440
	goto loc_82681440;
loc_82681398:
	// cmpwi cr6,r10,13
	cr6.compare<int32_t>(ctx.r10.s32, 13, xer);
	// addi r11,r29,4
	r11.s64 = r29.s64 + 4;
	// ble cr6,0x82681408
	if (!cr6.gt) goto loc_82681408;
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// clrlwi r6,r11,16
	ctx.r6.u64 = r11.u32 & 0xFFFF;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// or r11,r10,r9
	r11.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// stbx r11,r10,r7
	PPC_STORE_U8(ctx.r10.u32 + ctx.r7.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r10,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r10.u8);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// addi r10,r11,-13
	ctx.r10.s64 = r11.s64 + -13;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r10,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r10.u32);
	// srw r11,r6,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 >> (r11.u8 & 0x3F));
	// b 0x8268141c
	goto loc_8268141C;
loc_82681408:
	// addi r9,r10,3
	ctx.r9.s64 = ctx.r10.s64 + 3;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// lhz r10,5808(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r9,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, ctx.r9.u32);
loc_8268141C:
	// sth r11,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r11.u16);
	// addi r6,r8,1
	ctx.r6.s64 = ctx.r8.s64 + 1;
	// lwz r10,2848(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2848);
	// lwz r11,2836(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2836);
	// addi r5,r10,1
	ctx.r5.s64 = ctx.r10.s64 + 1;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82680238
	sub_82680238(ctx, base);
	// addi r5,r3,2432
	ctx.r5.s64 = ctx.r3.s64 + 2432;
	// addi r4,r3,140
	ctx.r4.s64 = ctx.r3.s64 + 140;
loc_82681440:
	// bl 0x826804d8
	sub_826804D8(ctx, base);
loc_82681444:
	// addi r10,r3,140
	ctx.r10.s64 = ctx.r3.s64 + 140;
	// li r11,286
	r11.s64 = 286;
loc_8268144C:
	// sth r27,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r27.u16);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne 0x8268144c
	if (!cr0.eq) goto loc_8268144C;
	// addi r10,r3,2432
	ctx.r10.s64 = ctx.r3.s64 + 2432;
	// li r11,30
	r11.s64 = 30;
loc_82681464:
	// sth r27,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r27.u16);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne 0x82681464
	if (!cr0.eq) goto loc_82681464;
	// addi r10,r3,2676
	ctx.r10.s64 = ctx.r3.s64 + 2676;
	// li r11,19
	r11.s64 = 19;
loc_8268147C:
	// sth r27,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r27.u16);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne 0x8268147c
	if (!cr0.eq) goto loc_8268147C;
	// li r11,1
	r11.s64 = 1;
	// stw r27,5796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5796, r27.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// stw r27,5792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5792, r27.u32);
	// stw r27,5800(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5800, r27.u32);
	// stw r27,5784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5784, r27.u32);
	// sth r11,1164(r3)
	PPC_STORE_U16(ctx.r3.u32 + 1164, r11.u16);
	// beq cr6,0x82681510
	if (cr6.eq) goto loc_82681510;
	// lwz r11,5812(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5812);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// ble cr6,0x826814e4
	if (!cr6.gt) goto loc_826814E4;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lhz r11,5808(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stbx r11,r9,r10
	PPC_STORE_U8(ctx.r9.u32 + ctx.r10.u32, r11.u8);
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbz r9,5808(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5808);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
	// b 0x826814fc
	goto loc_826814FC;
loc_826814E4:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82681508
	if (!cr6.gt) goto loc_82681508;
	// lhz r11,5808(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5808);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
loc_826814FC:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
loc_82681508:
	// sth r27,5808(r3)
	PPC_STORE_U16(ctx.r3.u32 + 5808, r27.u16);
	// stw r27,5812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5812, r27.u32);
loc_82681510:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82681518"))) PPC_WEAK_FUNC(sub_82681518);
PPC_FUNC_IMPL(__imp__sub_82681518) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18216(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18216);
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,-13828(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13828);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r30,r9,-13832
	r30.s64 = ctx.r9.s64 + -13832;
	// bne 0x82681564
	if (!cr0.eq) goto loc_82681564;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13828(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13828, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,-22488
	ctx.r4.s64 = r11.s64 + -22488;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
loc_82681564:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82681520"))) PPC_WEAK_FUNC(sub_82681520);
PPC_FUNC_IMPL(__imp__sub_82681520) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,-13828(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13828);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r30,r9,-13832
	r30.s64 = ctx.r9.s64 + -13832;
	// bne 0x82681564
	if (!cr0.eq) goto loc_82681564;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13828(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13828, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,-22488
	ctx.r4.s64 = r11.s64 + -22488;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
loc_82681564:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82681578"))) PPC_WEAK_FUNC(sub_82681578);
PPC_FUNC_IMPL(__imp__sub_82681578) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	uint32_t ea{};
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r11,r11,-13828
	r11.s64 = r11.s64 + -13828;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r10,r10,-13828
	ctx.r10.s64 = ctx.r10.s64 + -13828;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826815A0"))) PPC_WEAK_FUNC(sub_826815A0);
PPC_FUNC_IMPL(__imp__sub_826815A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18272(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18272);
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r31,80
	ctx.r3.s64 = r31.s64 + 80;
	// bl 0x822e5828
	sub_822E5828(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r3,80
	ctx.r3.s64 = 80;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82354930
	sub_82354930(ctx, base);
	// stw r3,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x826815ec
	if (cr0.eq) goto loc_826815EC;
	// bl 0x82684768
	sub_82684768(ctx, base);
	// b 0x826815f0
	goto loc_826815F0;
loc_826815EC:
	// li r3,0
	ctx.r3.s64 = 0;
loc_826815F0:
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826815A8"))) PPC_WEAK_FUNC(sub_826815A8);
PPC_FUNC_IMPL(__imp__sub_826815A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r31,80
	ctx.r3.s64 = r31.s64 + 80;
	// bl 0x822e5828
	sub_822E5828(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r3,80
	ctx.r3.s64 = 80;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82354930
	sub_82354930(ctx, base);
	// stw r3,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x826815ec
	if (cr0.eq) goto loc_826815EC;
	// bl 0x82684768
	sub_82684768(ctx, base);
	// b 0x826815f0
	goto loc_826815F0;
loc_826815EC:
	// li r3,0
	ctx.r3.s64 = 0;
loc_826815F0:
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681604"))) PPC_WEAK_FUNC(sub_82681604);
PPC_FUNC_IMPL(__imp__sub_82681604) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// addi r31,r12,-112
	r31.s64 = r12.s64 + -112;
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// bl 0x821e7b68
	sub_821E7B68(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8268162C"))) PPC_WEAK_FUNC(sub_8268162C);
PPC_FUNC_IMPL(__imp__sub_8268162C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82681630"))) PPC_WEAK_FUNC(sub_82681630);
PPC_FUNC_IMPL(__imp__sub_82681630) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18328(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18328);
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,-13820(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13820);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r30,r9,-13824
	r30.s64 = ctx.r9.s64 + -13824;
	// bne 0x8268167c
	if (!cr0.eq) goto loc_8268167C;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13820(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13820, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,-22416
	ctx.r4.s64 = r11.s64 + -22416;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
loc_8268167C:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82681638"))) PPC_WEAK_FUNC(sub_82681638);
PPC_FUNC_IMPL(__imp__sub_82681638) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,-13820(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13820);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r30,r9,-13824
	r30.s64 = ctx.r9.s64 + -13824;
	// bne 0x8268167c
	if (!cr0.eq) goto loc_8268167C;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13820(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13820, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,-22416
	ctx.r4.s64 = r11.s64 + -22416;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
loc_8268167C:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82681690"))) PPC_WEAK_FUNC(sub_82681690);
PPC_FUNC_IMPL(__imp__sub_82681690) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	uint32_t ea{};
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r11,r11,-13820
	r11.s64 = r11.s64 + -13820;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r10,r10,-13820
	ctx.r10.s64 = ctx.r10.s64 + -13820;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826816B8"))) PPC_WEAK_FUNC(sub_826816B8);
PPC_FUNC_IMPL(__imp__sub_826816B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18384(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18384);
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r31,80
	ctx.r3.s64 = r31.s64 + 80;
	// bl 0x822e58b0
	sub_822E58B0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r3,84
	ctx.r3.s64 = 84;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82354930
	sub_82354930(ctx, base);
	// stw r3,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82681704
	if (cr0.eq) goto loc_82681704;
	// bl 0x82685590
	sub_82685590(ctx, base);
	// b 0x82681708
	goto loc_82681708;
loc_82681704:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681708:
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826816C0"))) PPC_WEAK_FUNC(sub_826816C0);
PPC_FUNC_IMPL(__imp__sub_826816C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r31,80
	ctx.r3.s64 = r31.s64 + 80;
	// bl 0x822e58b0
	sub_822E58B0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r3,84
	ctx.r3.s64 = 84;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82354930
	sub_82354930(ctx, base);
	// stw r3,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82681704
	if (cr0.eq) goto loc_82681704;
	// bl 0x82685590
	sub_82685590(ctx, base);
	// b 0x82681708
	goto loc_82681708;
loc_82681704:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681708:
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8268171C"))) PPC_WEAK_FUNC(sub_8268171C);
PPC_FUNC_IMPL(__imp__sub_8268171C) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// addi r31,r12,-112
	r31.s64 = r12.s64 + -112;
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// bl 0x821e7b68
	sub_821E7B68(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681744"))) PPC_WEAK_FUNC(sub_82681744);
PPC_FUNC_IMPL(__imp__sub_82681744) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82681748"))) PPC_WEAK_FUNC(sub_82681748);
PPC_FUNC_IMPL(__imp__sub_82681748) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18440(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18440);
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,-13812(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13812);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r30,r9,-13816
	r30.s64 = ctx.r9.s64 + -13816;
	// bne 0x82681794
	if (!cr0.eq) goto loc_82681794;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13812(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13812, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,-22344
	ctx.r4.s64 = r11.s64 + -22344;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
loc_82681794:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82681750"))) PPC_WEAK_FUNC(sub_82681750);
PPC_FUNC_IMPL(__imp__sub_82681750) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,-13812(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13812);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r30,r9,-13816
	r30.s64 = ctx.r9.s64 + -13816;
	// bne 0x82681794
	if (!cr0.eq) goto loc_82681794;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13812(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13812, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,-22344
	ctx.r4.s64 = r11.s64 + -22344;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
loc_82681794:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_826817A8"))) PPC_WEAK_FUNC(sub_826817A8);
PPC_FUNC_IMPL(__imp__sub_826817A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	uint32_t ea{};
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r11,r11,-13812
	r11.s64 = r11.s64 + -13812;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r10,r10,-13812
	ctx.r10.s64 = ctx.r10.s64 + -13812;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826817D0"))) PPC_WEAK_FUNC(sub_826817D0);
PPC_FUNC_IMPL(__imp__sub_826817D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18496(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18496);
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r31,80
	ctx.r3.s64 = r31.s64 + 80;
	// bl 0x822e5938
	sub_822E5938(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r3,100
	ctx.r3.s64 = 100;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82354930
	sub_82354930(ctx, base);
	// stw r3,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x8268181c
	if (cr0.eq) goto loc_8268181C;
	// bl 0x826858e8
	sub_826858E8(ctx, base);
	// b 0x82681820
	goto loc_82681820;
loc_8268181C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681820:
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826817D8"))) PPC_WEAK_FUNC(sub_826817D8);
PPC_FUNC_IMPL(__imp__sub_826817D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r31,80
	ctx.r3.s64 = r31.s64 + 80;
	// bl 0x822e5938
	sub_822E5938(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r3,100
	ctx.r3.s64 = 100;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82354930
	sub_82354930(ctx, base);
	// stw r3,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x8268181c
	if (cr0.eq) goto loc_8268181C;
	// bl 0x826858e8
	sub_826858E8(ctx, base);
	// b 0x82681820
	goto loc_82681820;
loc_8268181C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681820:
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681834"))) PPC_WEAK_FUNC(sub_82681834);
PPC_FUNC_IMPL(__imp__sub_82681834) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// addi r31,r12,-112
	r31.s64 = r12.s64 + -112;
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// bl 0x821e7b68
	sub_821E7B68(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8268185C"))) PPC_WEAK_FUNC(sub_8268185C);
PPC_FUNC_IMPL(__imp__sub_8268185C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82681860"))) PPC_WEAK_FUNC(sub_82681860);
PPC_FUNC_IMPL(__imp__sub_82681860) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18552(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18552);
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,-13804(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13804);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r30,r9,-13808
	r30.s64 = ctx.r9.s64 + -13808;
	// bne 0x826818ac
	if (!cr0.eq) goto loc_826818AC;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13804(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13804, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,-22272
	ctx.r4.s64 = r11.s64 + -22272;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
loc_826818AC:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82681868"))) PPC_WEAK_FUNC(sub_82681868);
PPC_FUNC_IMPL(__imp__sub_82681868) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,-13804(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13804);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r30,r9,-13808
	r30.s64 = ctx.r9.s64 + -13808;
	// bne 0x826818ac
	if (!cr0.eq) goto loc_826818AC;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13804(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13804, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,-22272
	ctx.r4.s64 = r11.s64 + -22272;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
loc_826818AC:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_826818C0"))) PPC_WEAK_FUNC(sub_826818C0);
PPC_FUNC_IMPL(__imp__sub_826818C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	uint32_t ea{};
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r11,r11,-13804
	r11.s64 = r11.s64 + -13804;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r10,r10,-13804
	ctx.r10.s64 = ctx.r10.s64 + -13804;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826818E8"))) PPC_WEAK_FUNC(sub_826818E8);
PPC_FUNC_IMPL(__imp__sub_826818E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18608(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18608);
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r31,80
	ctx.r3.s64 = r31.s64 + 80;
	// bl 0x822e59c0
	sub_822E59C0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r3,104
	ctx.r3.s64 = 104;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82354930
	sub_82354930(ctx, base);
	// stw r3,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82681934
	if (cr0.eq) goto loc_82681934;
	// bl 0x82685cb8
	sub_82685CB8(ctx, base);
	// b 0x82681938
	goto loc_82681938;
loc_82681934:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681938:
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826818F0"))) PPC_WEAK_FUNC(sub_826818F0);
PPC_FUNC_IMPL(__imp__sub_826818F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r31,80
	ctx.r3.s64 = r31.s64 + 80;
	// bl 0x822e59c0
	sub_822E59C0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r3,104
	ctx.r3.s64 = 104;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82354930
	sub_82354930(ctx, base);
	// stw r3,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82681934
	if (cr0.eq) goto loc_82681934;
	// bl 0x82685cb8
	sub_82685CB8(ctx, base);
	// b 0x82681938
	goto loc_82681938;
loc_82681934:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681938:
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8268194C"))) PPC_WEAK_FUNC(sub_8268194C);
PPC_FUNC_IMPL(__imp__sub_8268194C) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// addi r31,r12,-112
	r31.s64 = r12.s64 + -112;
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// bl 0x821e7b68
	sub_821E7B68(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681974"))) PPC_WEAK_FUNC(sub_82681974);
PPC_FUNC_IMPL(__imp__sub_82681974) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82681978"))) PPC_WEAK_FUNC(sub_82681978);
PPC_FUNC_IMPL(__imp__sub_82681978) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18664(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18664);
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,-13796(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13796);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r30,r9,-13800
	r30.s64 = ctx.r9.s64 + -13800;
	// bne 0x826819c4
	if (!cr0.eq) goto loc_826819C4;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13796(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13796, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,-26016
	ctx.r4.s64 = r11.s64 + -26016;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
loc_826819C4:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82681980"))) PPC_WEAK_FUNC(sub_82681980);
PPC_FUNC_IMPL(__imp__sub_82681980) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,-13796(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13796);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// addi r30,r9,-13800
	r30.s64 = ctx.r9.s64 + -13800;
	// bne 0x826819c4
	if (!cr0.eq) goto loc_826819C4;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13796(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13796, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,-26016
	ctx.r4.s64 = r11.s64 + -26016;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
loc_826819C4:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_826819D8"))) PPC_WEAK_FUNC(sub_826819D8);
PPC_FUNC_IMPL(__imp__sub_826819D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	uint32_t ea{};
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r11,r11,-13796
	r11.s64 = r11.s64 + -13796;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r10,r10,-13796
	ctx.r10.s64 = ctx.r10.s64 + -13796;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681A00"))) PPC_WEAK_FUNC(sub_82681A00);
PPC_FUNC_IMPL(__imp__sub_82681A00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x822eb800
	sub_822EB800(ctx, base);
	// bl 0x826a9a18
	sub_826A9A18(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681A24"))) PPC_WEAK_FUNC(sub_82681A24);
PPC_FUNC_IMPL(__imp__sub_82681A24) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82681A28"))) PPC_WEAK_FUNC(sub_82681A28);
PPC_FUNC_IMPL(__imp__sub_82681A28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x822e5a48
	sub_822E5A48(ctx, base);
	// bl 0x826a9b18
	sub_826A9B18(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681A4C"))) PPC_WEAK_FUNC(sub_82681A4C);
PPC_FUNC_IMPL(__imp__sub_82681A4C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82681A50"))) PPC_WEAK_FUNC(sub_82681A50);
PPC_FUNC_IMPL(__imp__sub_82681A50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18720(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18720);
	// mflr r12
	// bl 0x8239bcf4
	// stfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, f31.u64);
	// addi r31,r1,-144
	r31.s64 = ctx.r1.s64 + -144;
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r3,264
	ctx.r3.s64 = 264;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// bl 0x82354fd8
	sub_82354FD8(ctx, base);
	// mr. r28,r3
	r28.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// stw r28,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r28.u32);
	// beq 0x82681ae8
	if (cr0.eq) goto loc_82681AE8;
	// li r3,20
	ctx.r3.s64 = 20;
	// bl 0x82354fd8
	sub_82354FD8(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82681ac8
	if (cr0.eq) goto loc_82681AC8;
	// lis r11,-32251
	r11.s64 = -2113601536;
	// stw r30,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r30.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r29,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r29.u32);
	// addi r11,r11,-23396
	r11.s64 = r11.s64 + -23396;
	// stw r30,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r30.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// stw r10,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r10.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// b 0x82681acc
	goto loc_82681ACC;
loc_82681AC8:
	// li r4,0
	ctx.r4.s64 = 0;
loc_82681ACC:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lfs f2,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x826a3b48
	sub_826A3B48(ctx, base);
	// b 0x82681aec
	goto loc_82681AEC;
loc_82681AE8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681AEC:
	// addi r1,r31,144
	ctx.r1.s64 = r31.s64 + 144;
	// lfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82681A58"))) PPC_WEAK_FUNC(sub_82681A58);
PPC_FUNC_IMPL(__imp__sub_82681A58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, f31.u64);
	// addi r31,r1,-144
	r31.s64 = ctx.r1.s64 + -144;
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r3,264
	ctx.r3.s64 = 264;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// bl 0x82354fd8
	sub_82354FD8(ctx, base);
	// mr. r28,r3
	r28.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// stw r28,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r28.u32);
	// beq 0x82681ae8
	if (cr0.eq) goto loc_82681AE8;
	// li r3,20
	ctx.r3.s64 = 20;
	// bl 0x82354fd8
	sub_82354FD8(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82681ac8
	if (cr0.eq) goto loc_82681AC8;
	// lis r11,-32251
	r11.s64 = -2113601536;
	// stw r30,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r30.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r29,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r29.u32);
	// addi r11,r11,-23396
	r11.s64 = r11.s64 + -23396;
	// stw r30,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r30.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// stw r10,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r10.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// b 0x82681acc
	goto loc_82681ACC;
loc_82681AC8:
	// li r4,0
	ctx.r4.s64 = 0;
loc_82681ACC:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lfs f2,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x826a3b48
	sub_826A3B48(ctx, base);
	// b 0x82681aec
	goto loc_82681AEC;
loc_82681AE8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681AEC:
	// addi r1,r31,144
	ctx.r1.s64 = r31.s64 + 144;
	// lfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82681AF8"))) PPC_WEAK_FUNC(sub_82681AF8);
PPC_FUNC_IMPL(__imp__sub_82681AF8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// addi r31,r12,-144
	r31.s64 = r12.s64 + -144;
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// bl 0x821e7b68
	sub_821E7B68(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681B20"))) PPC_WEAK_FUNC(sub_82681B20);
PPC_FUNC_IMPL(__imp__sub_82681B20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18880(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18880);
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r30,132(r31)
	PPC_STORE_U32(r31.u32 + 132, r30.u32);
	// bl 0x822f81f8
	sub_822F81F8(ctx, base);
	// addi r29,r30,60
	r29.s64 = r30.s64 + 60;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826860a0
	sub_826860A0(ctx, base);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// addi r11,r11,18796
	r11.s64 = r11.s64 + 18796;
	// addi r10,r10,18772
	ctx.r10.s64 = ctx.r10.s64 + 18772;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// stw r10,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r10.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82681B28"))) PPC_WEAK_FUNC(sub_82681B28);
PPC_FUNC_IMPL(__imp__sub_82681B28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r30,132(r31)
	PPC_STORE_U32(r31.u32 + 132, r30.u32);
	// bl 0x822f81f8
	sub_822F81F8(ctx, base);
	// addi r29,r30,60
	r29.s64 = r30.s64 + 60;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826860a0
	sub_826860A0(ctx, base);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// addi r11,r11,18796
	r11.s64 = r11.s64 + 18796;
	// addi r10,r10,18772
	ctx.r10.s64 = ctx.r10.s64 + 18772;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// stw r10,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r10.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82681B78"))) PPC_WEAK_FUNC(sub_82681B78);
PPC_FUNC_IMPL(__imp__sub_82681B78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// addi r31,r12,-112
	r31.s64 = r12.s64 + -112;
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r3,132(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 132);
	// bl 0x822f8270
	sub_822F8270(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681BA0"))) PPC_WEAK_FUNC(sub_82681BA0);
PPC_FUNC_IMPL(__imp__sub_82681BA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82681980
	sub_82681980(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681BD0"))) PPC_WEAK_FUNC(sub_82681BD0);
PPC_FUNC_IMPL(__imp__sub_82681BD0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,60
	ctx.r3.s64 = ctx.r3.s64 + 60;
	// b 0x826860c8
	sub_826860C8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82681BD8"))) PPC_WEAK_FUNC(sub_82681BD8);
PPC_FUNC_IMPL(__imp__sub_82681BD8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,60
	ctx.r3.s64 = ctx.r3.s64 + 60;
	// b 0x826863d0
	sub_826863D0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82681BE0"))) PPC_WEAK_FUNC(sub_82681BE0);
PPC_FUNC_IMPL(__imp__sub_82681BE0) {
	PPC_FUNC_PROLOGUE();
	// li r3,7
	ctx.r3.s64 = 7;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681BE8"))) PPC_WEAK_FUNC(sub_82681BE8);
PPC_FUNC_IMPL(__imp__sub_82681BE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,18936(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 18936);
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r30,132(r31)
	PPC_STORE_U32(r31.u32 + 132, r30.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r3,r30,60
	ctx.r3.s64 = r30.s64 + 60;
	// bne cr6,0x82681c20
	if (!cr6.eq) goto loc_82681C20;
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681C20:
	// bl 0x82686118
	sub_82686118(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822ffb28
	sub_822FFB28(ctx, base);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681BF0"))) PPC_WEAK_FUNC(sub_82681BF0);
PPC_FUNC_IMPL(__imp__sub_82681BF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r31,r1,-112
	r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r30,132(r31)
	PPC_STORE_U32(r31.u32 + 132, r30.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r3,r30,60
	ctx.r3.s64 = r30.s64 + 60;
	// bne cr6,0x82681c20
	if (!cr6.eq) goto loc_82681C20;
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681C20:
	// bl 0x82686118
	sub_82686118(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822ffb28
	sub_822FFB28(ctx, base);
	// addi r1,r31,112
	ctx.r1.s64 = r31.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681C44"))) PPC_WEAK_FUNC(sub_82681C44);
PPC_FUNC_IMPL(__imp__sub_82681C44) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// addi r31,r12,-112
	r31.s64 = r12.s64 + -112;
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r3,132(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 132);
	// bl 0x822f8270
	sub_822F8270(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681C6C"))) PPC_WEAK_FUNC(sub_82681C6C);
PPC_FUNC_IMPL(__imp__sub_82681C6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82681C70"))) PPC_WEAK_FUNC(sub_82681C70);
PPC_FUNC_IMPL(__imp__sub_82681C70) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-60
	ctx.r3.s64 = ctx.r3.s64 + -60;
	// b 0x82681db0
	sub_82681DB0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82681C78"))) PPC_WEAK_FUNC(sub_82681C78);
PPC_FUNC_IMPL(__imp__sub_82681C78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,19000(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 19000);
	// mflr r12
	// bl 0x8239bcf0
	// addi r31,r1,-160
	r31.s64 = ctx.r1.s64 + -160;
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lis r28,-32126
	r28.s64 = -2105409536;
	// lwz r11,-13788(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13788);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x82681cfc
	if (!cr0.eq) goto loc_82681CFC;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13788(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13788, r11.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r3,r31,80
	ctx.r3.s64 = r31.s64 + 80;
	// addi r4,r11,2988
	ctx.r4.s64 = r11.s64 + 2988;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r3,r31,84
	ctx.r3.s64 = r31.s64 + 84;
	// bl 0x82681980
	sub_82681980(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r4,r11,2964
	ctx.r4.s64 = r11.s64 + 2964;
	// addi r3,r31,88
	ctx.r3.s64 = r31.s64 + 88;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// bl 0x82270d80
	sub_82270D80(ctx, base);
	// stw r3,-13792(r28)
	PPC_STORE_U32(r28.u32 + -13792, ctx.r3.u32);
	// b 0x82681d00
	goto loc_82681D00;
loc_82681CFC:
	// lwz r3,-13792(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + -13792);
loc_82681D00:
	// lis r11,-32140
	r11.s64 = -2106327040;
	// lwz r11,18972(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 18972);
	// subf r11,r27,r11
	r11.s64 = r11.s64 - r27.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82681d30
	if (cr0.eq) goto loc_82681D30;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// mtctr r11
	ctr.u64 = r11.u64;
	// b 0x82681d78
	goto loc_82681D78;
loc_82681D30:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82319530
	sub_82319530(ctx, base);
	// mr. r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bne 0x82681d6c
	if (!cr0.eq) goto loc_82681D6C;
	// bl 0x8231c9e0
	sub_8231C9E0(ctx, base);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r31,88
	ctx.r3.s64 = r31.s64 + 88;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
loc_82681D6C:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82681D78:
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r31,160
	ctx.r1.s64 = r31.s64 + 160;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_82681C80"))) PPC_WEAK_FUNC(sub_82681C80);
PPC_FUNC_IMPL(__imp__sub_82681C80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// addi r31,r1,-160
	r31.s64 = ctx.r1.s64 + -160;
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lis r28,-32126
	r28.s64 = -2105409536;
	// lwz r11,-13788(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -13788);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x82681cfc
	if (!cr0.eq) goto loc_82681CFC;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,-13788(r10)
	PPC_STORE_U32(ctx.r10.u32 + -13788, r11.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r3,r31,80
	ctx.r3.s64 = r31.s64 + 80;
	// addi r4,r11,2988
	ctx.r4.s64 = r11.s64 + 2988;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r3,r31,84
	ctx.r3.s64 = r31.s64 + 84;
	// bl 0x82681980
	sub_82681980(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r4,r11,2964
	ctx.r4.s64 = r11.s64 + 2964;
	// addi r3,r31,88
	ctx.r3.s64 = r31.s64 + 88;
	// bl 0x82355da8
	sub_82355DA8(ctx, base);
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// bl 0x82270d80
	sub_82270D80(ctx, base);
	// stw r3,-13792(r28)
	PPC_STORE_U32(r28.u32 + -13792, ctx.r3.u32);
	// b 0x82681d00
	goto loc_82681D00;
loc_82681CFC:
	// lwz r3,-13792(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + -13792);
loc_82681D00:
	// lis r11,-32140
	r11.s64 = -2106327040;
	// lwz r11,18972(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 18972);
	// subf r11,r27,r11
	r11.s64 = r11.s64 - r27.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82681d30
	if (cr0.eq) goto loc_82681D30;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// mtctr r11
	ctr.u64 = r11.u64;
	// b 0x82681d78
	goto loc_82681D78;
loc_82681D30:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82319530
	sub_82319530(ctx, base);
	// mr. r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bne 0x82681d6c
	if (!cr0.eq) goto loc_82681D6C;
	// bl 0x8231c9e0
	sub_8231C9E0(ctx, base);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r31,88
	ctx.r3.s64 = r31.s64 + 88;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
loc_82681D6C:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82681D78:
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r31,160
	ctx.r1.s64 = r31.s64 + 160;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_82681D84"))) PPC_WEAK_FUNC(sub_82681D84);
PPC_FUNC_IMPL(__imp__sub_82681D84) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	uint32_t ea{};
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r11,r11,-13788
	r11.s64 = r11.s64 + -13788;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// addi r10,r10,-13788
	ctx.r10.s64 = ctx.r10.s64 + -13788;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681DAC"))) PPC_WEAK_FUNC(sub_82681DAC);
PPC_FUNC_IMPL(__imp__sub_82681DAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82681DB0"))) PPC_WEAK_FUNC(sub_82681DB0);
PPC_FUNC_IMPL(__imp__sub_82681DB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82681bf0
	sub_82681BF0(ctx, base);
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82681de0
	if (cr0.eq) goto loc_82681DE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82354b48
	sub_82354B48(ctx, base);
loc_82681DE0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681DFC"))) PPC_WEAK_FUNC(sub_82681DFC);
PPC_FUNC_IMPL(__imp__sub_82681DFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82681E00"))) PPC_WEAK_FUNC(sub_82681E00);
PPC_FUNC_IMPL(__imp__sub_82681E00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r31{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// lwz r17,-18368(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + -18368);
	// lwz r16,19080(r13)
	r16.u64 = PPC_LOAD_U32(ctx.r13.u32 + 19080);
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f30,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, f30.u64);
	// stfd f31,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, f31.u64);
	// addi r31,r1,-128
	r31.s64 = ctx.r1.s64 + -128;
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32140
	r11.s64 = -2106327040;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// addi r6,r31,80
	ctx.r6.s64 = r31.s64 + 80;
	// fmr f30,f2
	f30.f64 = ctx.f2.f64;
	// addi r5,r31,84
	ctx.r5.s64 = r31.s64 + 84;
	// lwz r11,18972(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 18972);
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
	// lwz r11,160(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 160);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82681e90
	if (cr6.eq) goto loc_82681E90;
	// li r3,264
	ctx.r3.s64 = 264;
	// bl 0x82354fd8
	sub_82354FD8(ctx, base);
	// stw r3,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82681e88
	if (cr0.eq) goto loc_82681E88;
	// lwz r7,80(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// fmr f2,f30
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f30.f64;
	// lwz r4,84(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x826a3b48
	sub_826A3B48(ctx, base);
	// b 0x82681e8c
	goto loc_82681E8C;
loc_82681E88:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681E8C:
	// b 0x82681eb4
	goto loc_82681EB4;
loc_82681E90:
	// li r3,104
	ctx.r3.s64 = 104;
	// bl 0x82354fd8
	sub_82354FD8(ctx, base);
	// stw r3,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82681eb0
	if (cr0.eq) goto loc_82681EB0;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x822fadd0
	sub_822FADD0(ctx, base);
	// b 0x82681eb4
	goto loc_82681EB4;
loc_82681EB0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681EB4:
	// addi r1,r31,128
	ctx.r1.s64 = r31.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// lfd f30,-32(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// lfd f31,-24(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681E08"))) PPC_WEAK_FUNC(sub_82681E08);
PPC_FUNC_IMPL(__imp__sub_82681E08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f30,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, f30.u64);
	// stfd f31,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, f31.u64);
	// addi r31,r1,-128
	r31.s64 = ctx.r1.s64 + -128;
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32140
	r11.s64 = -2106327040;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// addi r6,r31,80
	ctx.r6.s64 = r31.s64 + 80;
	// fmr f30,f2
	f30.f64 = ctx.f2.f64;
	// addi r5,r31,84
	ctx.r5.s64 = r31.s64 + 84;
	// lwz r11,18972(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 18972);
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
	// lwz r11,160(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 160);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82681e90
	if (cr6.eq) goto loc_82681E90;
	// li r3,264
	ctx.r3.s64 = 264;
	// bl 0x82354fd8
	sub_82354FD8(ctx, base);
	// stw r3,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82681e88
	if (cr0.eq) goto loc_82681E88;
	// lwz r7,80(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// fmr f2,f30
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f30.f64;
	// lwz r4,84(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x826a3b48
	sub_826A3B48(ctx, base);
	// b 0x82681e8c
	goto loc_82681E8C;
loc_82681E88:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681E8C:
	// b 0x82681eb4
	goto loc_82681EB4;
loc_82681E90:
	// li r3,104
	ctx.r3.s64 = 104;
	// bl 0x82354fd8
	sub_82354FD8(ctx, base);
	// stw r3,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82681eb0
	if (cr0.eq) goto loc_82681EB0;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x822fadd0
	sub_822FADD0(ctx, base);
	// b 0x82681eb4
	goto loc_82681EB4;
loc_82681EB0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82681EB4:
	// addi r1,r31,128
	ctx.r1.s64 = r31.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// lfd f30,-32(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// lfd f31,-24(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681ED0"))) PPC_WEAK_FUNC(sub_82681ED0);
PPC_FUNC_IMPL(__imp__sub_82681ED0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// addi r31,r12,-128
	r31.s64 = r12.s64 + -128;
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r3,88(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// bl 0x821e7b68
	sub_821E7B68(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82681EF8"))) PPC_WEAK_FUNC(sub_82681EF8);
PPC_FUNC_IMPL(__imp__sub_82681EF8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// addi r31,r12,-128
	r31.s64 = r12.s64 + -128;
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r3,88(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// bl 0x821e7b68
	sub_821E7B68(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

