#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_825E2CD4"))) PPC_WEAK_FUNC(sub_825E2CD4);
PPC_FUNC_IMPL(__imp__sub_825E2CD4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E2CD8"))) PPC_WEAK_FUNC(sub_825E2CD8);
PPC_FUNC_IMPL(__imp__sub_825E2CD8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825e2d20
	if (cr6.eq) goto loc_825E2D20;
	// lwz r11,19768(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19768);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e2d20
	if (!cr6.eq) goto loc_825E2D20;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826a8cc8
	sub_826A8CC8(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r11,19768(r31)
	PPC_STORE_U32(r31.u32 + 19768, r11.u32);
	// ld r11,19744(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 19744);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// std r11,19744(r31)
	PPC_STORE_U64(r31.u32 + 19744, r11.u64);
loc_825E2D20:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E2D34"))) PPC_WEAK_FUNC(sub_825E2D34);
PPC_FUNC_IMPL(__imp__sub_825E2D34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E2D38"))) PPC_WEAK_FUNC(sub_825E2D38);
PPC_FUNC_IMPL(__imp__sub_825E2D38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825e2d80
	if (cr6.eq) goto loc_825E2D80;
	// lwz r11,19768(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19768);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e2d80
	if (cr6.eq) goto loc_825E2D80;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826a8cc8
	sub_826A8CC8(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r11,19768(r31)
	PPC_STORE_U32(r31.u32 + 19768, r11.u32);
	// ld r11,19744(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 19744);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// std r11,19744(r31)
	PPC_STORE_U64(r31.u32 + 19744, r11.u64);
loc_825E2D80:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E2D94"))) PPC_WEAK_FUNC(sub_825E2D94);
PPC_FUNC_IMPL(__imp__sub_825E2D94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E2D98"))) PPC_WEAK_FUNC(sub_825E2D98);
PPC_FUNC_IMPL(__imp__sub_825E2D98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825e2de0
	if (cr6.eq) goto loc_825E2DE0;
	// lwz r11,19772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e2de0
	if (!cr6.eq) goto loc_825E2DE0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826a8cc8
	sub_826A8CC8(ctx, base);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r9,1
	ctx.r9.s64 = 1;
	// neg r10,r11
	ctx.r10.s64 = -r11.s64;
	// std r11,19760(r31)
	PPC_STORE_U64(r31.u32 + 19760, r11.u64);
	// stw r9,19772(r31)
	PPC_STORE_U32(r31.u32 + 19772, ctx.r9.u32);
	// std r10,19752(r31)
	PPC_STORE_U64(r31.u32 + 19752, ctx.r10.u64);
loc_825E2DE0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E2DF4"))) PPC_WEAK_FUNC(sub_825E2DF4);
PPC_FUNC_IMPL(__imp__sub_825E2DF4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E2DF8"))) PPC_WEAK_FUNC(sub_825E2DF8);
PPC_FUNC_IMPL(__imp__sub_825E2DF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825e2e58
	if (cr6.eq) goto loc_825E2E58;
	// lwz r11,19772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e2e58
	if (cr6.eq) goto loc_825E2E58;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826a8cc8
	sub_826A8CC8(ctx, base);
	// ld r10,19744(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 19744);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r7,0
	ctx.r7.s64 = 0;
	// ld r8,19752(r31)
	ctx.r8.u64 = PPC_LOAD_U64(r31.u32 + 19752);
	// ld r9,19760(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 19760);
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r7,19772(r31)
	PPC_STORE_U32(r31.u32 + 19772, ctx.r7.u32);
	// std r7,19760(r31)
	PPC_STORE_U64(r31.u32 + 19760, ctx.r7.u64);
	// std r11,19752(r31)
	PPC_STORE_U64(r31.u32 + 19752, r11.u64);
	// std r10,19744(r31)
	PPC_STORE_U64(r31.u32 + 19744, ctx.r10.u64);
loc_825E2E58:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E2E6C"))) PPC_WEAK_FUNC(sub_825E2E6C);
PPC_FUNC_IMPL(__imp__sub_825E2E6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E2E70"))) PPC_WEAK_FUNC(sub_825E2E70);
PPC_FUNC_IMPL(__imp__sub_825E2E70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x825e2e7c
	if (!cr6.eq) goto loc_825E2E7C;
	// blr 
	return;
loc_825E2E7C:
	// lwz r11,15300(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15300);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e2e98
	if (!cr6.eq) goto loc_825E2E98;
	// lwz r11,15368(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15368);
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
loc_825E2E98:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E2EA0"))) PPC_WEAK_FUNC(sub_825E2EA0);
PPC_FUNC_IMPL(__imp__sub_825E2EA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x825e2ecc
	if (!cr6.eq) goto loc_825E2ECC;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E2ECC:
	// lwz r11,15472(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// beq cr6,0x825e2f10
	if (cr6.eq) goto loc_825E2F10;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// beq cr6,0x825e2f10
	if (cr6.eq) goto loc_825E2F10;
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x825e2efc
	if (!cr6.eq) goto loc_825E2EFC;
	// bl 0x825ec188
	sub_825EC188(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E2EFC:
	// bl 0x825fc738
	sub_825FC738(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E2F10:
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// bl 0x825f0c50
	sub_825F0C50(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E2F34"))) PPC_WEAK_FUNC(sub_825E2F34);
PPC_FUNC_IMPL(__imp__sub_825E2F34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E2F38"))) PPC_WEAK_FUNC(sub_825E2F38);
PPC_FUNC_IMPL(__imp__sub_825E2F38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc4
	// addi r12,r1,-144
	r12.s64 = ctx.r1.s64 + -144;
	// bl 0x8239d5e8
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// mr r17,r4
	r17.u64 = ctx.r4.u64;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// mr r19,r6
	r19.u64 = ctx.r6.u64;
	// mr r21,r7
	r21.u64 = ctx.r7.u64;
	// mr r16,r8
	r16.u64 = ctx.r8.u64;
	// mr r18,r9
	r18.u64 = ctx.r9.u64;
	// mr r15,r10
	r15.u64 = ctx.r10.u64;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x825e2f88
	if (!cr6.eq) goto loc_825E2F88;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// addi r12,r1,-144
	r12.s64 = ctx.r1.s64 + -144;
	// bl 0x8239d634
	// b 0x8239bd14
	return;
loc_825E2F88:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825e2ffc
	if (!cr6.lt) goto loc_825E2FFC;
loc_825E2FA4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e2ffc
	if (cr6.eq) goto loc_825E2FFC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e2fec
	if (!cr0.lt) goto loc_825E2FEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E2FEC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e2fa4
	if (cr6.gt) goto loc_825E2FA4;
loc_825E2FFC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3038
	if (!cr0.lt) goto loc_825E3038;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3038:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stw r30,15364(r20)
	PPC_STORE_U32(r20.u32 + 15364, r30.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// li r29,0
	r29.s64 = 0;
	// lfs f29,2480(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2480);
	f29.f64 = double(temp.f32);
	// lfs f28,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f28.f64 = double(temp.f32);
	// bne cr6,0x825e3384
	if (!cr6.eq) goto loc_825E3384;
	// stfs f28,0(r17)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r17.u32 + 0, temp.u32);
	// li r30,15
	r30.s64 = 15;
	// stfs f29,0(r22)
	temp.f32 = float(f29.f64);
	PPC_STORE_U32(r22.u32 + 0, temp.u32);
	// stfs f29,0(r21)
	temp.f32 = float(f29.f64);
	PPC_STORE_U32(r21.u32 + 0, temp.u32);
	// stfs f28,0(r16)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r16.u32 + 0, temp.u32);
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e30d8
	if (!cr6.lt) goto loc_825E30D8;
loc_825E3080:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e30d8
	if (cr6.eq) goto loc_825E30D8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e30c8
	if (!cr0.lt) goto loc_825E30C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E30C8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3080
	if (cr6.gt) goto loc_825E3080;
loc_825E30D8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3114
	if (!cr0.lt) goto loc_825E3114;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3114:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e318c
	if (!cr6.lt) goto loc_825E318C;
loc_825E3134:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e318c
	if (cr6.eq) goto loc_825E318C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e317c
	if (!cr0.lt) goto loc_825E317C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E317C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3134
	if (cr6.gt) goto loc_825E3134;
loc_825E318C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e31c8
	if (!cr0.lt) goto loc_825E31C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E31C8:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r27,r30,r28
	r27.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3240
	if (!cr6.lt) goto loc_825E3240;
loc_825E31E8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3240
	if (cr6.eq) goto loc_825E3240;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3230
	if (!cr0.lt) goto loc_825E3230;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3230:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e31e8
	if (cr6.gt) goto loc_825E31E8;
loc_825E3240:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e327c
	if (!cr0.lt) goto loc_825E327C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E327C:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e32f4
	if (!cr6.lt) goto loc_825E32F4;
loc_825E329C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e32f4
	if (cr6.eq) goto loc_825E32F4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e32e4
	if (!cr0.lt) goto loc_825E32E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E32E4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e329c
	if (cr6.gt) goto loc_825E329C;
loc_825E32F4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3330
	if (!cr0.lt) goto loc_825E3330;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3330:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// add r10,r30,r28
	ctx.r10.u64 = r30.u64 + r28.u64;
	// addi r11,r11,27480
	r11.s64 = r11.s64 + 27480;
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// lfs f31,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f31.f64 = double(temp.f32);
	// clrldi r11,r27,32
	r11.u64 = r27.u64 & 0xFFFFFFFF;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lis r11,-32245
	r11.s64 = -2113208320;
	// addi r11,r11,-2072
	r11.s64 = r11.s64 + -2072;
	// lfs f30,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f30.f64 = double(temp.f32);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f0,80(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmsubs f0,f0,f31,f30
	f0.f64 = double(float(f0.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r19)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r19.u32 + 0, temp.u32);
	// fmsubs f0,f13,f31,f30
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 - f30.f64));
	// b 0x825e4794
	goto loc_825E4794;
loc_825E3384:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825e383c
	if (!cr6.eq) goto loc_825E383C;
	// stfs f29,0(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f29.f64);
	PPC_STORE_U32(r22.u32 + 0, temp.u32);
	// li r30,15
	r30.s64 = 15;
	// stfs f29,0(r21)
	temp.f32 = float(f29.f64);
	PPC_STORE_U32(r21.u32 + 0, temp.u32);
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3404
	if (!cr6.lt) goto loc_825E3404;
loc_825E33AC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3404
	if (cr6.eq) goto loc_825E3404;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e33f4
	if (!cr0.lt) goto loc_825E33F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E33F4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e33ac
	if (cr6.gt) goto loc_825E33AC;
loc_825E3404:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3440
	if (!cr0.lt) goto loc_825E3440;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3440:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e34b8
	if (!cr6.lt) goto loc_825E34B8;
loc_825E3460:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e34b8
	if (cr6.eq) goto loc_825E34B8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e34a8
	if (!cr0.lt) goto loc_825E34A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E34A8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3460
	if (cr6.gt) goto loc_825E3460;
loc_825E34B8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e34f4
	if (!cr0.lt) goto loc_825E34F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E34F4:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r26,r30,r28
	r26.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e356c
	if (!cr6.lt) goto loc_825E356C;
loc_825E3514:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e356c
	if (cr6.eq) goto loc_825E356C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e355c
	if (!cr0.lt) goto loc_825E355C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E355C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3514
	if (cr6.gt) goto loc_825E3514;
loc_825E356C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e35a8
	if (!cr0.lt) goto loc_825E35A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E35A8:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3620
	if (!cr6.lt) goto loc_825E3620;
loc_825E35C8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3620
	if (cr6.eq) goto loc_825E3620;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3610
	if (!cr0.lt) goto loc_825E3610;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3610:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e35c8
	if (cr6.gt) goto loc_825E35C8;
loc_825E3620:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e365c
	if (!cr0.lt) goto loc_825E365C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E365C:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r27,r30,r28
	r27.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e36d4
	if (!cr6.lt) goto loc_825E36D4;
loc_825E367C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e36d4
	if (cr6.eq) goto loc_825E36D4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e36c4
	if (!cr0.lt) goto loc_825E36C4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E36C4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e367c
	if (cr6.gt) goto loc_825E367C;
loc_825E36D4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3710
	if (!cr0.lt) goto loc_825E3710;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3710:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3788
	if (!cr6.lt) goto loc_825E3788;
loc_825E3730:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3788
	if (cr6.eq) goto loc_825E3788;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3778
	if (!cr0.lt) goto loc_825E3778;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3778:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3730
	if (cr6.gt) goto loc_825E3730;
loc_825E3788:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e37c4
	if (!cr0.lt) goto loc_825E37C4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E37C4:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// clrldi r10,r26,32
	ctx.r10.u64 = r26.u64 & 0xFFFFFFFF;
	// addi r11,r11,27480
	r11.s64 = r11.s64 + 27480;
	// clrldi r9,r27,32
	ctx.r9.u64 = r27.u64 & 0xFFFFFFFF;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfs f31,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f31.f64 = double(temp.f32);
	// add r11,r30,r28
	r11.u64 = r30.u64 + r28.u64;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lis r11,-32245
	r11.s64 = -2113208320;
	// addi r11,r11,-2072
	r11.s64 = r11.s64 + -2072;
	// lfs f30,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f30.f64 = double(temp.f32);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f12,96(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmsubs f0,f0,f31,f30
	f0.f64 = double(float(f0.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r16)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r16.u32 + 0, temp.u32);
	// stfs f0,0(r17)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r17.u32 + 0, temp.u32);
	// fmsubs f13,f13,f31,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * f31.f64 - f30.f64));
	// stfs f13,0(r19)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r19.u32 + 0, temp.u32);
	// fmsubs f12,f12,f31,f30
	ctx.f12.f64 = double(float(ctx.f12.f64 * f31.f64 - f30.f64));
	// stfs f12,0(r18)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r18.u32 + 0, temp.u32);
	// b 0x825e4798
	goto loc_825E4798;
loc_825E383C:
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// li r30,15
	r30.s64 = 15;
	// bne cr6,0x825e3e70
	if (!cr6.eq) goto loc_825E3E70;
	// stfs f29,0(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f29.f64);
	PPC_STORE_U32(r22.u32 + 0, temp.u32);
	// stfs f29,0(r21)
	temp.f32 = float(f29.f64);
	PPC_STORE_U32(r21.u32 + 0, temp.u32);
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e38bc
	if (!cr6.lt) goto loc_825E38BC;
loc_825E3864:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e38bc
	if (cr6.eq) goto loc_825E38BC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e38ac
	if (!cr0.lt) goto loc_825E38AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E38AC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3864
	if (cr6.gt) goto loc_825E3864;
loc_825E38BC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e38f8
	if (!cr0.lt) goto loc_825E38F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E38F8:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3970
	if (!cr6.lt) goto loc_825E3970;
loc_825E3918:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3970
	if (cr6.eq) goto loc_825E3970;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3960
	if (!cr0.lt) goto loc_825E3960;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3960:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3918
	if (cr6.gt) goto loc_825E3918;
loc_825E3970:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e39ac
	if (!cr0.lt) goto loc_825E39AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E39AC:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r25,r30,r28
	r25.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3a24
	if (!cr6.lt) goto loc_825E3A24;
loc_825E39CC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3a24
	if (cr6.eq) goto loc_825E3A24;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3a14
	if (!cr0.lt) goto loc_825E3A14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3A14:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e39cc
	if (cr6.gt) goto loc_825E39CC;
loc_825E3A24:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3a60
	if (!cr0.lt) goto loc_825E3A60;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3A60:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3ad8
	if (!cr6.lt) goto loc_825E3AD8;
loc_825E3A80:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3ad8
	if (cr6.eq) goto loc_825E3AD8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3ac8
	if (!cr0.lt) goto loc_825E3AC8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3AC8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3a80
	if (cr6.gt) goto loc_825E3A80;
loc_825E3AD8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3b14
	if (!cr0.lt) goto loc_825E3B14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3B14:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r26,r30,r28
	r26.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3b8c
	if (!cr6.lt) goto loc_825E3B8C;
loc_825E3B34:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3b8c
	if (cr6.eq) goto loc_825E3B8C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3b7c
	if (!cr0.lt) goto loc_825E3B7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3B7C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3b34
	if (cr6.gt) goto loc_825E3B34;
loc_825E3B8C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3bc8
	if (!cr0.lt) goto loc_825E3BC8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3BC8:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3c40
	if (!cr6.lt) goto loc_825E3C40;
loc_825E3BE8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3c40
	if (cr6.eq) goto loc_825E3C40;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3c30
	if (!cr0.lt) goto loc_825E3C30;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3C30:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3be8
	if (cr6.gt) goto loc_825E3BE8;
loc_825E3C40:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3c7c
	if (!cr0.lt) goto loc_825E3C7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3C7C:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r27,r30,r28
	r27.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3cf4
	if (!cr6.lt) goto loc_825E3CF4;
loc_825E3C9C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3cf4
	if (cr6.eq) goto loc_825E3CF4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3ce4
	if (!cr0.lt) goto loc_825E3CE4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3CE4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3c9c
	if (cr6.gt) goto loc_825E3C9C;
loc_825E3CF4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3d30
	if (!cr0.lt) goto loc_825E3D30;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3D30:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3da8
	if (!cr6.lt) goto loc_825E3DA8;
loc_825E3D50:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3da8
	if (cr6.eq) goto loc_825E3DA8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3d98
	if (!cr0.lt) goto loc_825E3D98;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3D98:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3d50
	if (cr6.gt) goto loc_825E3D50;
loc_825E3DA8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3de4
	if (!cr0.lt) goto loc_825E3DE4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3DE4:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// clrldi r10,r25,32
	ctx.r10.u64 = r25.u64 & 0xFFFFFFFF;
	// addi r11,r11,27480
	r11.s64 = r11.s64 + 27480;
	// clrldi r9,r26,32
	ctx.r9.u64 = r26.u64 & 0xFFFFFFFF;
	// clrldi r8,r27,32
	ctx.r8.u64 = r27.u64 & 0xFFFFFFFF;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfs f31,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f31.f64 = double(temp.f32);
	// add r11,r30,r28
	r11.u64 = r30.u64 + r28.u64;
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// lis r11,-32245
	r11.s64 = -2113208320;
	// addi r11,r11,-2072
	r11.s64 = r11.s64 + -2072;
	// lfs f30,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f30.f64 = double(temp.f32);
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f11,104(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// fmsubs f0,f0,f31,f30
	f0.f64 = double(float(f0.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r17)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r17.u32 + 0, temp.u32);
	// fmsubs f0,f13,f31,f30
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r19)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r19.u32 + 0, temp.u32);
	// fmsubs f0,f12,f31,f30
	f0.f64 = double(float(ctx.f12.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r16)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r16.u32 + 0, temp.u32);
	// fmsubs f0,f11,f31,f30
	f0.f64 = double(float(ctx.f11.f64 * f31.f64 - f30.f64));
	// b 0x825e4794
	goto loc_825E4794;
loc_825E3E70:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3edc
	if (!cr6.lt) goto loc_825E3EDC;
loc_825E3E84:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3edc
	if (cr6.eq) goto loc_825E3EDC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3ecc
	if (!cr0.lt) goto loc_825E3ECC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3ECC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3e84
	if (cr6.gt) goto loc_825E3E84;
loc_825E3EDC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3f18
	if (!cr0.lt) goto loc_825E3F18;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3F18:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e3f90
	if (!cr6.lt) goto loc_825E3F90;
loc_825E3F38:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e3f90
	if (cr6.eq) goto loc_825E3F90;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e3f80
	if (!cr0.lt) goto loc_825E3F80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3F80:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3f38
	if (cr6.gt) goto loc_825E3F38;
loc_825E3F90:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3fcc
	if (!cr0.lt) goto loc_825E3FCC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E3FCC:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r23,r30,r28
	r23.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e4044
	if (!cr6.lt) goto loc_825E4044;
loc_825E3FEC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e4044
	if (cr6.eq) goto loc_825E4044;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e4034
	if (!cr0.lt) goto loc_825E4034;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4034:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e3fec
	if (cr6.gt) goto loc_825E3FEC;
loc_825E4044:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4080
	if (!cr0.lt) goto loc_825E4080;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4080:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e40f8
	if (!cr6.lt) goto loc_825E40F8;
loc_825E40A0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e40f8
	if (cr6.eq) goto loc_825E40F8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e40e8
	if (!cr0.lt) goto loc_825E40E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E40E8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e40a0
	if (cr6.gt) goto loc_825E40A0;
loc_825E40F8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4134
	if (!cr0.lt) goto loc_825E4134;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4134:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r24,r30,r28
	r24.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e41ac
	if (!cr6.lt) goto loc_825E41AC;
loc_825E4154:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e41ac
	if (cr6.eq) goto loc_825E41AC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e419c
	if (!cr0.lt) goto loc_825E419C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E419C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e4154
	if (cr6.gt) goto loc_825E4154;
loc_825E41AC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e41e8
	if (!cr0.lt) goto loc_825E41E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E41E8:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e4260
	if (!cr6.lt) goto loc_825E4260;
loc_825E4208:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e4260
	if (cr6.eq) goto loc_825E4260;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e4250
	if (!cr0.lt) goto loc_825E4250;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4250:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e4208
	if (cr6.gt) goto loc_825E4208;
loc_825E4260:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e429c
	if (!cr0.lt) goto loc_825E429C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E429C:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r25,r30,r28
	r25.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e4314
	if (!cr6.lt) goto loc_825E4314;
loc_825E42BC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e4314
	if (cr6.eq) goto loc_825E4314;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e4304
	if (!cr0.lt) goto loc_825E4304;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4304:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e42bc
	if (cr6.gt) goto loc_825E42BC;
loc_825E4314:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4350
	if (!cr0.lt) goto loc_825E4350;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4350:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e43c8
	if (!cr6.lt) goto loc_825E43C8;
loc_825E4370:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e43c8
	if (cr6.eq) goto loc_825E43C8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e43b8
	if (!cr0.lt) goto loc_825E43B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E43B8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e4370
	if (cr6.gt) goto loc_825E4370;
loc_825E43C8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4404
	if (!cr0.lt) goto loc_825E4404;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4404:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r26,r30,r28
	r26.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e447c
	if (!cr6.lt) goto loc_825E447C;
loc_825E4424:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e447c
	if (cr6.eq) goto loc_825E447C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e446c
	if (!cr0.lt) goto loc_825E446C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E446C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e4424
	if (cr6.gt) goto loc_825E4424;
loc_825E447C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e44b8
	if (!cr0.lt) goto loc_825E44B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E44B8:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e4530
	if (!cr6.lt) goto loc_825E4530;
loc_825E44D8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e4530
	if (cr6.eq) goto loc_825E4530;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e4520
	if (!cr0.lt) goto loc_825E4520;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4520:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e44d8
	if (cr6.gt) goto loc_825E44D8;
loc_825E4530:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e456c
	if (!cr0.lt) goto loc_825E456C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E456C:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// add r27,r30,r28
	r27.u64 = r30.u64 + r28.u64;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e45e4
	if (!cr6.lt) goto loc_825E45E4;
loc_825E458C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e45e4
	if (cr6.eq) goto loc_825E45E4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e45d4
	if (!cr0.lt) goto loc_825E45D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E45D4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e458c
	if (cr6.gt) goto loc_825E458C;
loc_825E45E4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4620
	if (!cr0.lt) goto loc_825E4620;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4620:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e4698
	if (!cr6.lt) goto loc_825E4698;
loc_825E4640:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e4698
	if (cr6.eq) goto loc_825E4698;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e4688
	if (!cr0.lt) goto loc_825E4688;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4688:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e4640
	if (cr6.gt) goto loc_825E4640;
loc_825E4698:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e46d4
	if (!cr0.lt) goto loc_825E46D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E46D4:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// clrldi r10,r23,32
	ctx.r10.u64 = r23.u64 & 0xFFFFFFFF;
	// addi r11,r11,27480
	r11.s64 = r11.s64 + 27480;
	// clrldi r9,r25,32
	ctx.r9.u64 = r25.u64 & 0xFFFFFFFF;
	// clrldi r7,r26,32
	ctx.r7.u64 = r26.u64 & 0xFFFFFFFF;
	// add r8,r30,r28
	ctx.r8.u64 = r30.u64 + r28.u64;
	// std r10,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r10.u64);
	// clrldi r6,r27,32
	ctx.r6.u64 = r27.u64 & 0xFFFFFFFF;
	// lfs f31,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f31.f64 = double(temp.f32);
	// clrldi r11,r24,32
	r11.u64 = r24.u64 & 0xFFFFFFFF;
	// clrldi r10,r8,32
	ctx.r10.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// std r6,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r6.u64);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lis r11,-32245
	r11.s64 = -2113208320;
	// std r10,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r10.u64);
	// addi r11,r11,-2072
	r11.s64 = r11.s64 + -2072;
	// lfs f30,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f30.f64 = double(temp.f32);
	// lfd f0,104(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f12,88(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// lfd f10,112(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f9,120(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// fmsubs f0,f0,f31,f30
	f0.f64 = double(float(f0.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r17)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r17.u32 + 0, temp.u32);
	// fmsubs f0,f13,f31,f30
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r22)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r22.u32 + 0, temp.u32);
	// fmsubs f0,f12,f31,f30
	f0.f64 = double(float(ctx.f12.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r19)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r19.u32 + 0, temp.u32);
	// fmsubs f0,f11,f31,f30
	f0.f64 = double(float(ctx.f11.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r21)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r21.u32 + 0, temp.u32);
	// fmsubs f0,f10,f31,f30
	f0.f64 = double(float(ctx.f10.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r16)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r16.u32 + 0, temp.u32);
	// fmsubs f0,f9,f31,f30
	f0.f64 = double(float(ctx.f9.f64 * f31.f64 - f30.f64));
loc_825E4794:
	// stfs f0,0(r18)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r18.u32 + 0, temp.u32);
loc_825E4798:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e480c
	if (!cr6.lt) goto loc_825E480C;
loc_825E47B4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e480c
	if (cr6.eq) goto loc_825E480C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e47fc
	if (!cr0.lt) goto loc_825E47FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E47FC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e47b4
	if (cr6.gt) goto loc_825E47B4;
loc_825E480C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4848
	if (!cr0.lt) goto loc_825E4848;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4848:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825e49d8
	if (cr6.eq) goto loc_825E49D8;
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e48c4
	if (!cr6.lt) goto loc_825E48C4;
loc_825E486C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e48c4
	if (cr6.eq) goto loc_825E48C4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e48b4
	if (!cr0.lt) goto loc_825E48B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E48B4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e486c
	if (cr6.gt) goto loc_825E486C;
loc_825E48C4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4900
	if (!cr0.lt) goto loc_825E4900;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4900:
	// lwz r31,84(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// rlwinm r28,r30,15,0,16
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0xFFFF8000;
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x825e4978
	if (!cr6.lt) goto loc_825E4978;
loc_825E4920:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e4978
	if (cr6.eq) goto loc_825E4978;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e4968
	if (!cr0.lt) goto loc_825E4968;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E4968:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e4920
	if (cr6.gt) goto loc_825E4920;
loc_825E4978:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e49b4
	if (!cr0.lt) goto loc_825E49B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E49B4:
	// add r11,r30,r28
	r11.u64 = r30.u64 + r28.u64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// std r11,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, r11.u64);
	// lfd f0,120(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmsubs f0,f0,f31,f30
	f0.f64 = double(float(f0.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r15)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r15.u32 + 0, temp.u32);
	// b 0x825e49dc
	goto loc_825E49DC;
loc_825E49D8:
	// stfs f28,0(r15)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r15.u32 + 0, temp.u32);
loc_825E49DC:
	// lwz r11,84(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e4a08
	if (!cr6.eq) goto loc_825E4A08;
	// lfs f0,0(r17)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r17.u32 + 0);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f29
	cr6.compare(f0.f64, f29.f64);
	// blt cr6,0x825e4a08
	if (cr6.lt) goto loc_825E4A08;
	// lfs f0,0(r16)
	temp.u32 = PPC_LOAD_U32(r16.u32 + 0);
	f0.f64 = double(temp.f32);
	// li r3,0
	ctx.r3.s64 = 0;
	// fcmpu cr6,f0,f29
	cr6.compare(f0.f64, f29.f64);
	// bge cr6,0x825e4a0c
	if (!cr6.lt) goto loc_825E4A0C;
loc_825E4A08:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825E4A0C:
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// addi r12,r1,-144
	r12.s64 = ctx.r1.s64 + -144;
	// bl 0x8239d634
	// b 0x8239bd14
	return;
}

__attribute__((alias("__imp__sub_825E4A1C"))) PPC_WEAK_FUNC(sub_825E4A1C);
PPC_FUNC_IMPL(__imp__sub_825E4A1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E4A20"))) PPC_WEAK_FUNC(sub_825E4A20);
PPC_FUNC_IMPL(__imp__sub_825E4A20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,80(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825e4ab8
	if (cr6.eq) goto loc_825E4AB8;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// beq cr6,0x825e4ab8
	if (cr6.eq) goto loc_825E4AB8;
loc_825E4A58:
	// lwz r31,80(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// bl 0x82491030
	sub_82491030(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// lwz r11,15472(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825e4aac
	if (!cr6.eq) goto loc_825E4AAC;
	// lwz r4,88(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// addi r6,r1,92
	ctx.r6.s64 = ctx.r1.s64 + 92;
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r4,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r4.u32);
	// bl 0x825fee98
	sub_825FEE98(ctx, base);
loc_825E4AAC:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e4a58
	if (!cr6.eq) goto loc_825E4A58;
loc_825E4AB8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E4AD4"))) PPC_WEAK_FUNC(sub_825E4AD4);
PPC_FUNC_IMPL(__imp__sub_825E4AD4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E4AD8"))) PPC_WEAK_FUNC(sub_825E4AD8);
PPC_FUNC_IMPL(__imp__sub_825E4AD8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,21184(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21184);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e4b70
	if (cr6.eq) goto loc_825E4B70;
	// lwz r11,14824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e4b18
	if (cr6.eq) goto loc_825E4B18;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
loc_825E4B18:
	// li r30,1
	r30.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r30,3676(r31)
	PPC_STORE_U32(r31.u32 + 3676, r30.u32);
	// bl 0x82605538
	sub_82605538(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,160(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// lwz r4,156(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// bl 0x82606168
	sub_82606168(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e4c00
	if (!cr6.eq) goto loc_825E4C00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,160(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// lwz r4,156(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// bl 0x82605dd0
	sub_82605DD0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825e4b60
	if (cr6.eq) goto loc_825E4B60;
	// stw r30,3676(r31)
	PPC_STORE_U32(r31.u32 + 3676, r30.u32);
	// b 0x825e4c00
	goto loc_825E4C00;
loc_825E4B60:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82602170
	sub_82602170(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f3bf0
	sub_825F3BF0(ctx, base);
loc_825E4B70:
	// lwz r11,212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// mullw r5,r10,r11
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r10,212(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// lwz r3,3776(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r3,3780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r3,3784(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_825E4C00:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E4C18"))) PPC_WEAK_FUNC(sub_825E4C18);
PPC_FUNC_IMPL(__imp__sub_825E4C18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r6,3720(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r9,3776(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// add r29,r9,r10
	r29.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r3,3780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r4,3784(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// add r30,r6,r10
	r30.u64 = ctx.r6.u64 + ctx.r10.u64;
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r25,r3,r11
	r25.u64 = ctx.r3.u64 + r11.u64;
	// lwz r8,3724(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// add r24,r4,r11
	r24.u64 = ctx.r4.u64 + r11.u64;
	// lwz r7,3728(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 + ctx.r9.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// add r28,r8,r11
	r28.u64 = ctx.r8.u64 + r11.u64;
	// add r26,r10,r11
	r26.u64 = ctx.r10.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// add r29,r5,r29
	r29.u64 = ctx.r5.u64 + r29.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r27,0
	r27.s64 = 0;
	// add r9,r11,r29
	ctx.r9.u64 = r11.u64 + r29.u64;
	// lwz r11,188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze. r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x825e4d3c
	if (!cr0.gt) goto loc_825E4D3C;
loc_825E4CB0:
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825e4cfc
	if (!cr6.gt) goto loc_825E4CFC;
	// subf r6,r9,r30
	ctx.r6.s64 = r30.s64 - ctx.r9.s64;
loc_825E4CC4:
	// lwz r8,204(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lbzx r7,r6,r10
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbzx r8,r8,r30
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r30.u32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825e4cc4
	if (cr6.lt) goto loc_825E4CC4;
loc_825E4CFC:
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r29,r11,r9
	r29.u64 = r11.u64 + ctx.r9.u64;
	// add r30,r10,r30
	r30.u64 = ctx.r10.u64 + r30.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r9,r11,r29
	ctx.r9.u64 = r11.u64 + r29.u64;
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r11.s64 = temp.s64;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x825e4cb0
	if (cr6.lt) goto loc_825E4CB0;
loc_825E4D3C:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r30,r5,r25
	r30.u64 = ctx.r5.u64 + r25.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r29,0
	r29.s64 = 0;
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze. r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x825e4e08
	if (!cr0.gt) goto loc_825E4E08;
loc_825E4D7C:
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825e4dc8
	if (!cr6.gt) goto loc_825E4DC8;
	// subf r6,r9,r28
	ctx.r6.s64 = r28.s64 - ctx.r9.s64;
loc_825E4D90:
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lbzx r7,r6,r10
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbzx r8,r8,r28
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r28.u32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825e4d90
	if (cr6.lt) goto loc_825E4D90;
loc_825E4DC8:
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r11,r9
	r30.u64 = r11.u64 + ctx.r9.u64;
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r11.s64 = temp.s64;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x825e4d7c
	if (cr6.lt) goto loc_825E4D7C;
loc_825E4E08:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r30,r5,r24
	r30.u64 = ctx.r5.u64 + r24.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r29,0
	r29.s64 = 0;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// addze. r11,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r11.s64 = temp.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x825e4ed4
	if (!cr0.gt) goto loc_825E4ED4;
loc_825E4E48:
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825e4e94
	if (!cr6.gt) goto loc_825E4E94;
	// subf r6,r9,r26
	ctx.r6.s64 = r26.s64 - ctx.r9.s64;
loc_825E4E5C:
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lbzx r7,r6,r10
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbzx r8,r8,r26
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r26.u32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825e4e5c
	if (cr6.lt) goto loc_825E4E5C;
loc_825E4E94:
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r11,r9
	r30.u64 = r11.u64 + ctx.r9.u64;
	// add r26,r10,r26
	r26.u64 = ctx.r10.u64 + r26.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r11.s64 = temp.s64;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x825e4e48
	if (cr6.lt) goto loc_825E4E48;
loc_825E4ED4:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825E4EDC"))) PPC_WEAK_FUNC(sub_825E4EDC);
PPC_FUNC_IMPL(__imp__sub_825E4EDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E4EE0"))) PPC_WEAK_FUNC(sub_825E4EE0);
PPC_FUNC_IMPL(__imp__sub_825E4EE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825e4f04
	if (!cr6.eq) goto loc_825E4F04;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_825E4F04:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e4f1c
	if (cr6.eq) goto loc_825E4F1C;
	// li r3,13
	ctx.r3.s64 = 13;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_825E4F1C:
	// li r11,0
	r11.s64 = 0;
	// lwz r10,15472(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r10,6
	cr6.compare<int32_t>(ctx.r10.s32, 6, xer);
	// stw r11,3668(r31)
	PPC_STORE_U32(r31.u32 + 3668, r11.u32);
	// sth r11,3684(r31)
	PPC_STORE_U16(r31.u32 + 3684, r11.u16);
	// stw r11,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r11.u32);
	// bne cr6,0x825e5008
	if (!cr6.eq) goto loc_825E5008;
	// li r9,-3
	ctx.r9.s64 = -3;
	// lwz r10,15300(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15300);
	// li r8,1
	ctx.r8.s64 = 1;
	// std r11,3576(r31)
	PPC_STORE_U64(r31.u32 + 3576, r11.u64);
	// stw r11,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,3380(r31)
	PPC_STORE_U32(r31.u32 + 3380, r11.u32);
	// stw r11,3396(r31)
	PPC_STORE_U32(r31.u32 + 3396, r11.u32);
	// stw r9,3376(r31)
	PPC_STORE_U32(r31.u32 + 3376, ctx.r9.u32);
	// stw r8,14788(r31)
	PPC_STORE_U32(r31.u32 + 14788, ctx.r8.u32);
	// stw r11,3384(r31)
	PPC_STORE_U32(r31.u32 + 3384, r11.u32);
	// stw r11,3400(r31)
	PPC_STORE_U32(r31.u32 + 3400, r11.u32);
	// stw r11,3452(r31)
	PPC_STORE_U32(r31.u32 + 3452, r11.u32);
	// beq cr6,0x825e5008
	if (cr6.eq) goto loc_825E5008;
	// lwz r11,19696(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19696);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r6,188(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,180(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r10,19700(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19700);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// lwz r8,200(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// lwz r9,192(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// mullw r29,r6,r11
	r29.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// add r11,r8,r10
	r11.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mullw r30,r11,r10
	r30.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,128
	ctx.r4.s64 = 128;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,128
	ctx.r4.s64 = 128;
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,3732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,128
	ctx.r4.s64 = 128;
	// lwz r3,3736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,128
	ctx.r4.s64 = 128;
	// lwz r3,3740(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_825E5008:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825E5014"))) PPC_WEAK_FUNC(sub_825E5014);
PPC_FUNC_IMPL(__imp__sub_825E5014) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E5018"))) PPC_WEAK_FUNC(sub_825E5018);
PPC_FUNC_IMPL(__imp__sub_825E5018) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,3412(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3412);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e5030
	if (cr6.eq) goto loc_825E5030;
	// lwz r11,3708(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3708);
	// stw r11,3716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3716, r11.u32);
	// blr 
	return;
loc_825E5030:
	// lwz r11,15564(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e5054
	if (cr6.eq) goto loc_825E5054;
	// lwz r11,3396(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e5054
	if (cr6.eq) goto loc_825E5054;
	// lwz r11,3704(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3704);
	// stw r11,3716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3716, r11.u32);
	// blr 
	return;
loc_825E5054:
	// lwz r11,3396(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e506c
	if (cr6.eq) goto loc_825E506C;
	// lwz r11,3688(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3688);
	// stw r11,3716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3716, r11.u32);
	// blr 
	return;
loc_825E506C:
	// lwz r11,3692(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3692);
	// stw r11,3716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3716, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E5078"))) PPC_WEAK_FUNC(sub_825E5078);
PPC_FUNC_IMPL(__imp__sub_825E5078) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// mr r29,r28
	r29.u64 = r28.u64;
	// lwz r11,21580(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21580);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e50a0
	if (cr6.eq) goto loc_825E50A0;
	// bl 0x825ff830
	sub_825FF830(ctx, base);
loc_825E50A0:
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// beq cr6,0x825e50cc
	if (cr6.eq) goto loc_825E50CC;
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// stw r11,596(r10)
	PPC_STORE_U32(ctx.r10.u32 + 596, r11.u32);
	// lwz r11,3704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// lwz r10,160(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// stw r10,600(r11)
	PPC_STORE_U32(r11.u32 + 600, ctx.r10.u32);
	// lwz r11,3704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// b 0x825e50e4
	goto loc_825E50E4;
loc_825E50CC:
	// lwz r10,3688(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// stw r11,596(r10)
	PPC_STORE_U32(ctx.r10.u32 + 596, r11.u32);
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// lwz r10,160(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// stw r10,600(r11)
	PPC_STORE_U32(r11.u32 + 600, ctx.r10.u32);
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
loc_825E50E4:
	// lwz r10,20972(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20972);
	// stw r10,604(r11)
	PPC_STORE_U32(r11.u32 + 604, ctx.r10.u32);
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825e511c
	if (!cr6.gt) goto loc_825E511C;
	// lwz r11,3408(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3408);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e5138
	if (!cr6.eq) goto loc_825E5138;
	// lwz r11,3384(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3384);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e5138
	if (!cr6.eq) goto loc_825E5138;
	// lwz r11,3452(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3452);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e5138
	if (!cr6.eq) goto loc_825E5138;
loc_825E511C:
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e5130
	if (cr6.eq) goto loc_825E5130;
	// lwz r11,3704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// b 0x825e5134
	goto loc_825E5134;
loc_825E5130:
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
loc_825E5134:
	// stw r11,3716(r31)
	PPC_STORE_U32(r31.u32 + 3716, r11.u32);
loc_825E5138:
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// li r30,1
	r30.s64 = 1;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825e5180
	if (cr6.eq) goto loc_825E5180;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825e5180
	if (cr6.eq) goto loc_825E5180;
	// ld r9,3576(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 3576);
	// cmpdi cr6,r9,1
	cr6.compare<int64_t>(ctx.r9.s64, 1, xer);
	// ble cr6,0x825e5178
	if (!cr6.gt) goto loc_825E5178;
	// lwz r9,3404(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3404);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825e5178
	if (!cr6.eq) goto loc_825E5178;
	// lwz r9,15564(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bne cr6,0x825e5178
	if (!cr6.eq) goto loc_825E5178;
	// mr r29,r30
	r29.u64 = r30.u64;
loc_825E5178:
	// lwz r9,15564(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// stw r9,3404(r31)
	PPC_STORE_U32(r31.u32 + 3404, ctx.r9.u32);
loc_825E5180:
	// lwz r9,15472(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r9,7
	cr6.compare<int32_t>(ctx.r9.s32, 7, xer);
	// beq cr6,0x825e5190
	if (cr6.eq) goto loc_825E5190;
	// stw r28,21184(r31)
	PPC_STORE_U32(r31.u32 + 21184, r28.u32);
loc_825E5190:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825e52cc
	if (cr6.eq) goto loc_825E52CC;
	// ld r9,3576(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 3576);
	// cmpdi cr6,r9,1
	cr6.compare<int64_t>(ctx.r9.s64, 1, xer);
	// bne cr6,0x825e51dc
	if (!cr6.eq) goto loc_825E51DC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e51b4
	if (cr6.eq) goto loc_825E51B4;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e51dc
	if (!cr6.eq) goto loc_825E51DC;
loc_825E51B4:
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e51d4
	if (cr6.eq) goto loc_825E51D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825ee6a0
	sub_825EE6A0(ctx, base);
	// lwz r11,3708(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// stw r30,3412(r31)
	PPC_STORE_U32(r31.u32 + 3412, r30.u32);
	// b 0x825e52c8
	goto loc_825E52C8;
loc_825E51D4:
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// b 0x825e52c8
	goto loc_825E52C8;
loc_825E51DC:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825e52cc
	if (cr6.eq) goto loc_825E52CC;
	// lwz r9,3408(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3408);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825e5208
	if (!cr6.eq) goto loc_825E5208;
	// lwz r9,3384(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3384);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825e5208
	if (!cr6.eq) goto loc_825E5208;
	// lwz r9,3452(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3452);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825e523c
	if (cr6.eq) goto loc_825E523C;
loc_825E5208:
	// lwz r10,3384(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3384);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825e52cc
	if (cr6.eq) goto loc_825E52CC;
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x825e52c4
	if (!cr6.eq) goto loc_825E52C4;
	// lwz r11,3396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e52c4
	if (cr6.eq) goto loc_825E52C4;
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e51d4
	if (cr6.eq) goto loc_825E51D4;
	// lwz r11,3704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// b 0x825e52c8
	goto loc_825E52C8;
loc_825E523C:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825e52cc
	if (cr6.eq) goto loc_825E52CC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e525c
	if (cr6.eq) goto loc_825E525C;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825e525c
	if (cr6.eq) goto loc_825E525C;
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x825e52cc
	if (!cr6.eq) goto loc_825E52CC;
loc_825E525C:
	// lwz r10,3412(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3412);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,15564(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// beq cr6,0x825e5290
	if (cr6.eq) goto loc_825E5290;
	// lwz r11,3708(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r28,3412(r31)
	PPC_STORE_U32(r31.u32 + 3412, r28.u32);
	// stw r11,3716(r31)
	PPC_STORE_U32(r31.u32 + 3716, r11.u32);
	// beq cr6,0x825e52cc
	if (cr6.eq) goto loc_825E52CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825ee6a0
	sub_825EE6A0(ctx, base);
	// stw r30,3412(r31)
	PPC_STORE_U32(r31.u32 + 3412, r30.u32);
	// b 0x825e52cc
	goto loc_825E52CC;
loc_825E5290:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825e52bc
	if (cr6.eq) goto loc_825E52BC;
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// beq cr6,0x825e52cc
	if (cr6.eq) goto loc_825E52CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825ee6a0
	sub_825EE6A0(ctx, base);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// stw r30,3412(r31)
	PPC_STORE_U32(r31.u32 + 3412, r30.u32);
	// bne cr6,0x825e52c4
	if (!cr6.eq) goto loc_825E52C4;
	// lwz r11,3704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// b 0x825e52c8
	goto loc_825E52C8;
loc_825E52BC:
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// beq cr6,0x825e52cc
	if (cr6.eq) goto loc_825E52CC;
loc_825E52C4:
	// lwz r11,3696(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3696);
loc_825E52C8:
	// stw r11,3716(r31)
	PPC_STORE_U32(r31.u32 + 3716, r11.u32);
loc_825E52CC:
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e5304
	if (!cr6.gt) goto loc_825E5304;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825e5304
	if (cr6.eq) goto loc_825E5304;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825e5304
	if (cr6.eq) goto loc_825E5304;
	// lwz r10,20980(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20980);
	// lwz r11,20972(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20972);
	// stw r10,20976(r31)
	PPC_STORE_U32(r31.u32 + 20976, ctx.r10.u32);
	// stw r11,20980(r31)
	PPC_STORE_U32(r31.u32 + 20980, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_825E5304:
	// lwz r11,20972(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20972);
	// stw r11,20976(r31)
	PPC_STORE_U32(r31.u32 + 20976, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825E5314"))) PPC_WEAK_FUNC(sub_825E5314);
PPC_FUNC_IMPL(__imp__sub_825E5314) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E5318"))) PPC_WEAK_FUNC(sub_825E5318);
PPC_FUNC_IMPL(__imp__sub_825E5318) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r4,21236(r31)
	PPC_STORE_U32(r31.u32 + 21236, ctx.r4.u32);
	// bne cr6,0x825e53ac
	if (!cr6.eq) goto loc_825E53AC;
	// lwz r10,21240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// li r11,1
	r11.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r30.u32);
	// lwz r10,21276(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21276);
	// lwz r3,21240(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,21244(r31)
	PPC_STORE_U32(r31.u32 + 21244, r11.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,21276(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21276);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,21252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21252);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,21276(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21276);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,21268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,21268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r30,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r30.u32);
	// stw r30,21256(r31)
	PPC_STORE_U32(r31.u32 + 21256, r30.u32);
	// stw r30,21260(r31)
	PPC_STORE_U32(r31.u32 + 21260, r30.u32);
	// stw r11,21264(r31)
	PPC_STORE_U32(r31.u32 + 21264, r11.u32);
	// b 0x825e55b0
	goto loc_825E55B0;
loc_825E53AC:
	// lwz r11,188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// srawi r10,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r10.s64 = r11.s32 >> 4;
	// lwz r11,21244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21244);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825e53c8
	if (cr6.lt) goto loc_825E53C8;
	// li r3,4
	ctx.r3.s64 = 4;
	// b 0x825e55b0
	goto loc_825E55B0;
loc_825E53C8:
	// lwz r10,21240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bgt cr6,0x825e53ec
	if (cr6.gt) goto loc_825E53EC;
	// lwz r11,21276(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21276);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
loc_825E53EC:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// lwz r10,21276(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21276);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825e552c
	if (cr6.lt) goto loc_825E552C;
	// lwz r9,21244(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21244);
	// li r3,4
	ctx.r3.s64 = 4;
	// cmplwi cr6,r9,2
	cr6.compare<uint32_t>(ctx.r9.u32, 2, xer);
	// ble cr6,0x825e5464
	if (!cr6.gt) goto loc_825E5464;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// cmpwi cr6,r9,2
	cr6.compare<int32_t>(ctx.r9.s32, 2, xer);
	// ble cr6,0x825e5448
	if (!cr6.gt) goto loc_825E5448;
	// lwz r11,21240(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// rotlwi r10,r9,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
loc_825E5428:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r30,-4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// bne cr6,0x825e5428
	if (!cr6.eq) goto loc_825E5428;
loc_825E5448:
	// lwz r10,21240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// divwu r11,r8,r9
	r11.u32 = ctx.r8.u32 / ctx.r9.u32;
	// twllei r9,0
	// b 0x825e54f0
	goto loc_825E54F0;
loc_825E5464:
	// bne cr6,0x825e5478
	if (!cr6.eq) goto loc_825E5478;
	// lwz r11,21240(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x825e54f8
	goto loc_825E54F8;
loc_825E5478:
	// lwz r11,21280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21280);
	// lwz r10,21288(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21288);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x825e54c8
	if (!cr6.gt) goto loc_825E54C8;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x825e54c8
	if (!cr6.gt) goto loc_825E54C8;
	// lwz r8,92(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r4,21240(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// srawi r30,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	r30.s64 = ctx.r8.s32 >> 4;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r11,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mullw r10,r30,r10
	ctx.r10.s64 = int64_t(r30.s32) * int64_t(ctx.r10.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// twllei r11,0
	// divwu r11,r10,r11
	r11.u32 = ctx.r10.u32 / r11.u32;
	// lwz r10,-4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	// b 0x825e54f4
	goto loc_825E54F4;
loc_825E54C8:
	// lwz r8,21284(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 21284);
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// ble cr6,0x825e54fc
	if (!cr6.gt) goto loc_825E54FC;
	// lwz r10,21240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,92(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// twllei r8,0
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	r11.s64 = ctx.r9.s32 >> 4;
	// divwu r11,r11,r8
	r11.u32 = r11.u32 / ctx.r8.u32;
loc_825E54F0:
	// lwz r10,-4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
loc_825E54F4:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_825E54F8:
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
loc_825E54FC:
	// lwz r11,21276(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21276);
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x825e552c
	if (!cr6.gt) goto loc_825E552C;
	// lwz r10,21244(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21244);
	// lwz r9,21240(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x825e5578
	if (!cr6.gt) goto loc_825E5578;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
loc_825E552C:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r9,21268(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r10,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r10.u32);
	// lwz r11,21244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21244);
	// lwz r10,21240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// beq cr6,0x825e5580
	if (cr6.eq) goto loc_825E5580;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// lwz r11,21244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21244);
	// lwz r10,21252(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21252);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r6,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r6.u32);
	// b 0x825e5598
	goto loc_825E5598;
loc_825E5578:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x825e55b0
	goto loc_825E55B0;
loc_825E5580:
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// lwz r11,21244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21244);
	// lwz r10,21252(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21252);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r6,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, ctx.r6.u32);
loc_825E5598:
	// lwz r11,21244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21244);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,21244(r31)
	PPC_STORE_U32(r31.u32 + 21244, r11.u32);
	// lwz r11,21280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21280);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,21280(r31)
	PPC_STORE_U32(r31.u32 + 21280, r11.u32);
loc_825E55B0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E55C8"))) PPC_WEAK_FUNC(sub_825E55C8);
PPC_FUNC_IMPL(__imp__sub_825E55C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x825e55d8
	if (!cr6.eq) goto loc_825E55D8;
	// li r3,7
	ctx.r3.s64 = 7;
	// blr 
	return;
loc_825E55D8:
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e55ec
	if (cr6.eq) goto loc_825E55EC;
	// li r3,13
	ctx.r3.s64 = 13;
	// blr 
	return;
loc_825E55EC:
	// lwz r11,15472(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bge cr6,0x825e5610
	if (!cr6.lt) goto loc_825E5610;
	// lwz r11,14824(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14824);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e5664
	if (cr6.eq) goto loc_825E5664;
	// lwz r9,14848(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14848);
	// lwz r11,14852(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14852);
	// b 0x825e566c
	goto loc_825E566C;
loc_825E5610:
	// lwz r11,21184(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21184);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e5664
	if (!cr6.eq) goto loc_825E5664;
	// lwz r11,14772(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e5664
	if (!cr6.gt) goto loc_825E5664;
	// ld r11,3576(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 3576);
	// cmpdi cr6,r11,1
	cr6.compare<int64_t>(r11.s64, 1, xer);
	// ble cr6,0x825e5664
	if (!cr6.gt) goto loc_825E5664;
	// lwz r10,21352(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21352);
	// lwz r11,21380(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21380);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825e564c
	if (cr6.gt) goto loc_825E564C;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_825E564C:
	// lwz r11,21384(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21384);
	// lwz r10,21356(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21356);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825e566c
	if (!cr6.gt) goto loc_825E566C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// b 0x825e566c
	goto loc_825E566C;
loc_825E5664:
	// lwz r11,160(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// lwz r9,156(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
loc_825E566C:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x825e5678
	if (cr6.eq) goto loc_825E5678;
	// stw r9,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r9.u32);
loc_825E5678:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x825e5684
	if (cr6.eq) goto loc_825E5684;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
loc_825E5684:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E568C"))) PPC_WEAK_FUNC(sub_825E568C);
PPC_FUNC_IMPL(__imp__sub_825E568C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E5690"))) PPC_WEAK_FUNC(sub_825E5690);
PPC_FUNC_IMPL(__imp__sub_825E5690) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// mr r27,r8
	r27.u64 = ctx.r8.u64;
	// mr r26,r9
	r26.u64 = ctx.r9.u64;
	// mr r25,r10
	r25.u64 = ctx.r10.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x825e56c8
	if (!cr6.eq) goto loc_825E56C8;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd3c
	return;
loc_825E56C8:
	// li r11,40
	r11.s64 = 40;
	// stw r30,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r30.u32);
	// stw r29,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r29.u32);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r4,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r4.u32);
	// sth r5,190(r1)
	PPC_STORE_U16(ctx.r1.u32 + 190, ctx.r5.u16);
	// stw r11,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// sth r11,188(r1)
	PPC_STORE_U16(ctx.r1.u32 + 188, r11.u16);
	// beq cr6,0x825e5710
	if (cr6.eq) goto loc_825E5710;
	// cmplwi cr6,r4,3
	cr6.compare<uint32_t>(ctx.r4.u32, 3, xer);
	// beq cr6,0x825e5710
	if (cr6.eq) goto loc_825E5710;
	// clrlwi r11,r5,16
	r11.u64 = ctx.r5.u32 & 0xFFFF;
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// b 0x825e573c
	goto loc_825E573C;
loc_825E5710:
	// clrlwi r11,r5,16
	r11.u64 = ctx.r5.u32 & 0xFFFF;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
loc_825E573C:
	// li r31,0
	r31.s64 = 0;
	// stw r11,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r11.u32);
	// addi r5,r1,152
	ctx.r5.s64 = ctx.r1.s64 + 152;
	// addi r4,r1,148
	ctx.r4.s64 = ctx.r1.s64 + 148;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r31,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, r31.u32);
	// stw r31,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r31.u32);
	// stw r31,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r31.u32);
	// stw r31,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r31.u32);
	// bl 0x825f3c40
	sub_825F3C40(ctx, base);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r26,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r26.u32);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// stw r27,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r27.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// stw r31,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r31.u32);
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// stw r31,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r31.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r31.u32);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// bl 0x8260c6a8
	sub_8260C6A8(ctx, base);
	// lhz r11,190(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 190);
	// addi r9,r1,156
	ctx.r9.s64 = ctx.r1.s64 + 156;
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// lwz r7,372(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mullw r11,r11,r27
	r11.s64 = int64_t(r11.s32) * int64_t(r27.s32);
	// lwz r5,196(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r3,144(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// mullw r11,r11,r26
	r11.s64 = int64_t(r11.s32) * int64_t(r26.s32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// addze r8,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r8.s64 = temp.s64;
	// bl 0x8260c518
	sub_8260C518(ctx, base);
	// lwz r3,144(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// bl 0x8260afe0
	sub_8260AFE0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825E57FC"))) PPC_WEAK_FUNC(sub_825E57FC);
PPC_FUNC_IMPL(__imp__sub_825E57FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E5800"))) PPC_WEAK_FUNC(sub_825E5800);
PPC_FUNC_IMPL(__imp__sub_825E5800) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stfd f30,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, f30.u64);
	// stfd f31,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fmr f30,f2
	f30.f64 = ctx.f2.f64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// li r29,0
	r29.s64 = 0;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// lwz r8,80(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r11,3340(r31)
	PPC_STORE_U32(r31.u32 + 3340, r11.u32);
	// stw r29,21428(r31)
	PPC_STORE_U32(r31.u32 + 21428, r29.u32);
	// fctiwz f0,f31
	f0.s64 = (f31.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f31.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x82606b40
	sub_82606B40(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e5950
	if (!cr6.eq) goto loc_825E5950;
	// stfs f31,3648(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r31.u32 + 3648, temp.u32);
	// cmpwi cr6,r28,4
	cr6.compare<int32_t>(r28.s32, 4, xer);
	// stfs f30,3652(r31)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r31.u32 + 3652, temp.u32);
	// sth r29,3684(r31)
	PPC_STORE_U16(r31.u32 + 3684, r29.u16);
	// stw r30,3640(r31)
	PPC_STORE_U32(r31.u32 + 3640, r30.u32);
	// stw r28,3644(r31)
	PPC_STORE_U32(r31.u32 + 3644, r28.u32);
	// ble cr6,0x825e5888
	if (!cr6.gt) goto loc_825E5888;
	// li r11,4
	r11.s64 = 4;
	// b 0x825e5894
	goto loc_825E5894;
loc_825E5888:
	// cmpwi cr6,r28,-1
	cr6.compare<int32_t>(r28.s32, -1, xer);
	// bge cr6,0x825e5898
	if (!cr6.lt) goto loc_825E5898;
	// li r11,-1
	r11.s64 = -1;
loc_825E5894:
	// stw r11,3644(r31)
	PPC_STORE_U32(r31.u32 + 3644, r11.u32);
loc_825E5898:
	// lwz r11,228(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,15504(r31)
	PPC_STORE_U32(r31.u32 + 15504, r11.u32);
	// beq cr6,0x825e58b4
	if (cr6.eq) goto loc_825E58B4;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825e58b4
	if (cr6.eq) goto loc_825E58B4;
	// stw r29,15504(r31)
	PPC_STORE_U32(r31.u32 + 15504, r29.u32);
loc_825E58B4:
	// lwz r6,216(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r10,156(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// lwz r8,160(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// rlwinm r5,r9,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r6,116(r31)
	PPC_STORE_U32(r31.u32 + 116, ctx.r6.u32);
	// rlwinm r6,r11,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r7,212(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
	// stw r10,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r10.u32);
	// stw r9,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r9.u32);
	// stw r8,92(r31)
	PPC_STORE_U32(r31.u32 + 92, ctx.r8.u32);
	// stw r7,104(r31)
	PPC_STORE_U32(r31.u32 + 104, ctx.r7.u32);
	// stw r6,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r6.u32);
	// stw r5,112(r31)
	PPC_STORE_U32(r31.u32 + 112, ctx.r5.u32);
	// bne cr6,0x825e5908
	if (!cr6.eq) goto loc_825E5908;
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// li r8,1
	ctx.r8.s64 = 1;
	// beq cr6,0x825e590c
	if (cr6.eq) goto loc_825E590C;
loc_825E5908:
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
loc_825E590C:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r7,188(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r9,r11,28728
	ctx.r9.s64 = r11.s64 + 28728;
	// stw r8,120(r31)
	PPC_STORE_U32(r31.u32 + 120, ctx.r8.u32);
	// srawi r11,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	r11.s64 = ctx.r10.s32 >> 4;
	// stw r29,3456(r31)
	PPC_STORE_U32(r31.u32 + 3456, r29.u32);
	// addi r9,r9,384
	ctx.r9.s64 = ctx.r9.s64 + 384;
	// stw r29,3460(r31)
	PPC_STORE_U32(r31.u32 + 3460, r29.u32);
	// srawi r10,r7,4
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 4;
	// stw r29,3464(r31)
	PPC_STORE_U32(r31.u32 + 3464, r29.u32);
	// stw r11,128(r31)
	PPC_STORE_U32(r31.u32 + 128, r11.u32);
	// stw r9,256(r31)
	PPC_STORE_U32(r31.u32 + 256, ctx.r9.u32);
	// mullw r9,r10,r11
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r10,132(r31)
	PPC_STORE_U32(r31.u32 + 132, ctx.r10.u32);
	// stw r9,124(r31)
	PPC_STORE_U32(r31.u32 + 124, ctx.r9.u32);
loc_825E5950:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f30,-56(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f31,-48(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825E5960"))) PPC_WEAK_FUNC(sub_825E5960);
PPC_FUNC_IMPL(__imp__sub_825E5960) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stfd f31,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, f31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// li r11,1
	r11.s64 = 1;
	// li r29,-1
	r29.s64 = -1;
	// std r30,19744(r31)
	PPC_STORE_U64(r31.u32 + 19744, r30.u64);
	// std r30,19752(r31)
	PPC_STORE_U64(r31.u32 + 19752, r30.u64);
	// stw r30,19768(r31)
	PPC_STORE_U32(r31.u32 + 19768, r30.u32);
	// std r30,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r30.u64);
	// stw r30,19772(r31)
	PPC_STORE_U32(r31.u32 + 19772, r30.u32);
	// stw r30,19824(r31)
	PPC_STORE_U32(r31.u32 + 19824, r30.u32);
	// stw r30,3420(r31)
	PPC_STORE_U32(r31.u32 + 3420, r30.u32);
	// stw r30,19828(r31)
	PPC_STORE_U32(r31.u32 + 19828, r30.u32);
	// stw r29,19924(r31)
	PPC_STORE_U32(r31.u32 + 19924, r29.u32);
	// stw r11,19724(r31)
	PPC_STORE_U32(r31.u32 + 19724, r11.u32);
	// bl 0x826a8cb8
	sub_826A8CB8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r10,r11,-150
	ctx.r10.s64 = r11.s64 + -150;
	// stw r11,19936(r31)
	PPC_STORE_U32(r31.u32 + 19936, r11.u32);
	// stw r10,19932(r31)
	PPC_STORE_U32(r31.u32 + 19932, ctx.r10.u32);
	// bl 0x823b5460
	sub_823B5460(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e59d0
	if (!cr6.eq) goto loc_825E59D0;
	// stw r30,19724(r31)
	PPC_STORE_U32(r31.u32 + 19724, r30.u32);
loc_825E59D0:
	// lis r11,-7341
	r11.s64 = -481099776;
	// ld r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lis r8,8388
	ctx.r8.s64 = 549715968;
	// lwz r10,3656(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3656);
	// ori r11,r11,63439
	r11.u64 = r11.u64 | 63439;
	// ori r8,r8,39845
	ctx.r8.u64 = ctx.r8.u64 | 39845;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldimi r11,r8,32,0
	r11.u64 = (__builtin_rotateleft64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000) | (r11.u64 & 0xFFFFFFFF);
	// mulhd r11,r9,r11
	// sradi r11,r11,7
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7F) != 0);
	r11.s64 = r11.s64 >> 7;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// stw r11,19928(r31)
	PPC_STORE_U32(r31.u32 + 19928, r11.u32);
	// bgt cr6,0x825e5a0c
	if (cr6.gt) goto loc_825E5A0C;
	// li r10,30
	ctx.r10.s64 = 30;
loc_825E5A0C:
	// li r11,1000
	r11.s64 = 1000;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// twllei r10,0
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// tdllei r8,0
	// divw. r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotldi r10,r9,1
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u64, 1);
	// divd r9,r9,r8
	ctx.r9.s64 = ctx.r9.s64 / ctx.r8.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// andc r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 & ~ctx.r10.u64;
	// stw r11,19820(r31)
	PPC_STORE_U32(r31.u32 + 19820, r11.u32);
	// tdlgei r10,-1
	// stw r9,19832(r31)
	PPC_STORE_U32(r31.u32 + 19832, ctx.r9.u32);
	// bgt 0x825e5a4c
	if (cr0.gt) goto loc_825E5A4C;
	// li r11,33
	r11.s64 = 33;
	// stw r11,19820(r31)
	PPC_STORE_U32(r31.u32 + 19820, r11.u32);
loc_825E5A4C:
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,114
	ctx.r4.s64 = 114;
	// li r3,6
	ctx.r3.s64 = 6;
	// bl 0x82278448
	sub_82278448(ctx, base);
	// cmpwi cr6,r3,100
	cr6.compare<int32_t>(ctx.r3.s32, 100, xer);
	// blt cr6,0x825e5a6c
	if (cr6.lt) goto loc_825E5A6C;
	// cmpwi cr6,r3,32000
	cr6.compare<int32_t>(ctx.r3.s32, 32000, xer);
	// ble cr6,0x825e5a70
	if (!cr6.gt) goto loc_825E5A70;
loc_825E5A6C:
	// li r3,100
	ctx.r3.s64 = 100;
loc_825E5A70:
	// lwz r11,19928(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19928);
	// extsw r10,r3
	ctx.r10.s64 = ctx.r3.s32;
	// li r5,0
	ctx.r5.s64 = 0;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// li r4,114
	ctx.r4.s64 = 114;
	// li r3,5
	ctx.r3.s64 = 5;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lwz r11,188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfd f13,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f0
	ctx.f12.f64 = double(float(f0.f64));
	// lfs f0,27500(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 27500);
	f0.f64 = double(temp.f32);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64));
	// lfs f0,27496(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 27496);
	f0.f64 = double(temp.f32);
	// fdivs f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 / ctx.f13.f64));
	// lfd f0,104(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// frsp f10,f0
	ctx.f10.f64 = double(float(f0.f64));
	// lfs f0,-4904(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4904);
	f0.f64 = double(temp.f32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * f0.f64));
	// lfs f0,1932(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 1932);
	f0.f64 = double(temp.f32);
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fmuls f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fmuls f31,f13,f0
	f31.f64 = double(float(ctx.f13.f64 * f0.f64));
	// bl 0x82278448
	sub_82278448(ctx, base);
	// stw r29,19736(r31)
	PPC_STORE_U32(r31.u32 + 19736, r29.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,19732(r31)
	PPC_STORE_U32(r31.u32 + 19732, ctx.r3.u32);
	// bne cr6,0x825e5b30
	if (!cr6.eq) goto loc_825E5B30;
	// stw r30,19724(r31)
	PPC_STORE_U32(r31.u32 + 19724, r30.u32);
	// b 0x825e5b7c
	goto loc_825E5B7C;
loc_825E5B30:
	// cmpwi cr6,r3,15
	cr6.compare<int32_t>(ctx.r3.s32, 15, xer);
	// ble cr6,0x825e5b7c
	if (!cr6.gt) goto loc_825E5B7C;
	// lwz r11,19832(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19832);
	// srawi r10,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r3.s32 >> 4;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// lis r11,-32248
	r11.s64 = -2113404928;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// lfd f0,-26592(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -26592);
	// fmul f0,f12,f0
	f0.f64 = ctx.f12.f64 * f0.f64;
	// fmul f0,f0,f13
	f0.f64 = f0.f64 * ctx.f13.f64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// li r12,19832
	r12.s64 = 19832;
	// stfiwx f0,r31,r12
	PPC_STORE_U32(r31.u32 + r12.u32, f0.u32);
loc_825E5B7C:
	// lwz r11,19832(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19832);
	// addi r10,r31,19836
	ctx.r10.s64 = r31.s64 + 19836;
	// stw r30,19812(r31)
	PPC_STORE_U32(r31.u32 + 19812, r30.u32);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// lfd f0,27488(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 27488);
	// fmul f0,f13,f0
	f0.f64 = ctx.f13.f64 * f0.f64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
loc_825E5BAC:
	// lwz r11,19812(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19812);
	// lwz r10,19832(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19832);
	// addi r11,r11,4944
	r11.s64 = r11.s64 + 4944;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r10,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r10.u32);
	// lwz r11,19812(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19812);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,19812(r31)
	PPC_STORE_U32(r31.u32 + 19812, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// blt cr6,0x825e5bac
	if (cr6.lt) goto loc_825E5BAC;
	// lwz r11,19832(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19832);
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// addi r9,r31,19864
	ctx.r9.s64 = r31.s64 + 19864;
	// stw r30,19812(r31)
	PPC_STORE_U32(r31.u32 + 19812, r30.u32);
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r31,19904
	r11.s64 = r31.s64 + 19904;
	// subfic r7,r31,-19904
	xer.ca = r31.u32 <= 4294947392;
	ctx.r7.s64 = -19904 - r31.s64;
	// addi r6,r8,27400
	ctx.r6.s64 = ctx.r8.s64 + 27400;
	// stw r10,19808(r31)
	PPC_STORE_U32(r31.u32 + 19808, ctx.r10.u32);
	// li r10,5
	ctx.r10.s64 = 5;
loc_825E5C00:
	// lwz r8,3924(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r8,r6,20
	ctx.r8.s64 = ctx.r6.s64 + 20;
	// bne cr6,0x825e5c14
	if (!cr6.eq) goto loc_825E5C14;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_825E5C14:
	// add r5,r7,r11
	ctx.r5.u64 = ctx.r7.u64 + r11.u64;
	// addi r4,r11,-64
	ctx.r4.s64 = r11.s64 + -64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lwzx r8,r5,r8
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r8.u32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r4
	PPC_STORE_U32(ctx.r4.u32, f0.u32);
	// std r30,0(r9)
	PPC_STORE_U64(ctx.r9.u32 + 0, r30.u64);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825e5c00
	if (!cr6.eq) goto loc_825E5C00;
	// lwz r11,19856(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19856);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,188(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// lwz r4,180(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// stw r11,19860(r31)
	PPC_STORE_U32(r31.u32 + 19860, r11.u32);
	// bl 0x8260d1d8
	sub_8260D1D8(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lfd f31,-40(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825E5C80"))) PPC_WEAK_FUNC(sub_825E5C80);
PPC_FUNC_IMPL(__imp__sub_825E5C80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,1
	r30.s64 = 1;
loc_825E5C9C:
	// lwz r11,15548(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15548);
	// addi r11,r11,4976
	r11.s64 = r11.s64 + 4976;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// cmpwi cr6,r11,50
	cr6.compare<int32_t>(r11.s32, 50, xer);
	// ble cr6,0x825e5d34
	if (!cr6.gt) goto loc_825E5D34;
	// lwz r9,180(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// lwz r11,15548(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15548);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// addi r9,r11,4976
	ctx.r9.s64 = r11.s64 + 4976;
	// addi r11,r11,2483
	r11.s64 = r11.s64 + 2483;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// ldx r8,r11,r31
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + r31.u32);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// rotldi r11,r8,1
	r11.u64 = __builtin_rotateleft64(ctx.r8.u64, 1);
	// divd r8,r8,r9
	ctx.r8.s64 = ctx.r8.s64 / ctx.r9.s64;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// tdllei r9,0
	// extsw r11,r8
	r11.s64 = ctx.r8.s32;
	// andc r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 & ~ctx.r7.u64;
	// cmpwi cr6,r11,50
	cr6.compare<int32_t>(r11.s32, 50, xer);
	// tdlgei r9,-1
	// blt cr6,0x825e5d34
	if (cr6.lt) goto loc_825E5D34;
	// cmpwi cr6,r10,50
	cr6.compare<int32_t>(ctx.r10.s32, 50, xer);
	// blt cr6,0x825e5d34
	if (cr6.lt) goto loc_825E5D34;
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// divw r5,r11,r10
	ctx.r5.s32 = r11.s32 / ctx.r10.s32;
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// li r4,119
	ctx.r4.s64 = 119;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// addi r3,r30,6
	ctx.r3.s64 = r30.s64 + 6;
	// twllei r10,0
	// twlgei r11,-1
	// bl 0x82278448
	sub_82278448(ctx, base);
loc_825E5D34:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// ble cr6,0x825e5c9c
	if (!cr6.gt) goto loc_825E5C9C;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E5D58"))) PPC_WEAK_FUNC(sub_825E5D58);
PPC_FUNC_IMPL(__imp__sub_825E5D58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r24,0
	r24.s64 = 0;
	// mr r25,r24
	r25.u64 = r24.u64;
	// lwz r11,19824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19824);
	// lwz r10,3924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// lwz r8,15556(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15556);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// subfic r11,r10,0
	xer.ca = ctx.r10.u32 <= 0;
	r11.s64 = 0 - ctx.r10.s64;
	// rlwinm r10,r8,28,0,3
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0xF0000000;
	// subfe r8,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	ctx.r8.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// srawi. r11,r10,28
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFFFF) != 0);
	r11.s64 = ctx.r10.s32 >> 28;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r10,r8,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,19824(r31)
	PPC_STORE_U32(r31.u32 + 19824, ctx.r9.u32);
	// addi r27,r10,4
	r27.s64 = ctx.r10.s64 + 4;
	// blt 0x825e5da8
	if (cr0.lt) goto loc_825E5DA8;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// ble cr6,0x825e5db0
	if (!cr6.gt) goto loc_825E5DB0;
loc_825E5DA8:
	// cmpwi cr6,r11,-2
	cr6.compare<int32_t>(r11.s32, -2, xer);
	// bne cr6,0x825e5e64
	if (!cr6.eq) goto loc_825E5E64;
loc_825E5DB0:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x825e5dc0
	if (!cr6.lt) goto loc_825E5DC0;
	// mr r11,r24
	r11.u64 = r24.u64;
	// b 0x825e5dcc
	goto loc_825E5DCC;
loc_825E5DC0:
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// ble cr6,0x825e5dcc
	if (!cr6.gt) goto loc_825E5DCC;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_825E5DCC:
	// lwz r10,15548(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15548);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// beq cr6,0x825e5dfc
	if (cr6.eq) goto loc_825E5DFC;
	// bge cr6,0x825e5df8
	if (!cr6.lt) goto loc_825E5DF8;
	// lwz r10,19832(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19832);
	// mulli r9,r10,218
	ctx.r9.s64 = ctx.r10.s64 * 218;
	// mulli r10,r10,243
	ctx.r10.s64 = ctx.r10.s64 * 243;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stw r9,19836(r31)
	PPC_STORE_U32(r31.u32 + 19836, ctx.r9.u32);
	// stw r10,19832(r31)
	PPC_STORE_U32(r31.u32 + 19832, ctx.r10.u32);
loc_825E5DF8:
	// stw r11,15548(r31)
	PPC_STORE_U32(r31.u32 + 15548, r11.u32);
loc_825E5DFC:
	// lwz r11,19768(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19768);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e5e24
	if (cr6.eq) goto loc_825E5E24;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826a8cc8
	sub_826A8CC8(ctx, base);
	// ld r11,19744(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 19744);
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r24,19768(r31)
	PPC_STORE_U32(r31.u32 + 19768, r24.u32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// std r11,19744(r31)
	PPC_STORE_U64(r31.u32 + 19744, r11.u64);
loc_825E5E24:
	// lwz r11,19768(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19768);
	// std r24,19752(r31)
	PPC_STORE_U64(r31.u32 + 19752, r24.u64);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r24,3420(r31)
	PPC_STORE_U32(r31.u32 + 3420, r24.u32);
	// std r24,19744(r31)
	PPC_STORE_U64(r31.u32 + 19744, r24.u64);
	// bne cr6,0x825e62dc
	if (!cr6.eq) goto loc_825E62DC;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826a8cc8
	sub_826A8CC8(ctx, base);
	// ld r11,19744(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 19744);
	// ld r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,1
	ctx.r10.s64 = 1;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// stw r10,19768(r31)
	PPC_STORE_U32(r31.u32 + 19768, ctx.r10.u32);
	// std r11,19744(r31)
	PPC_STORE_U64(r31.u32 + 19744, r11.u64);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825E5E64:
	// cmpwi cr6,r9,5
	cr6.compare<int32_t>(ctx.r9.s32, 5, xer);
	// bge cr6,0x825e5ea4
	if (!cr6.lt) goto loc_825E5EA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2d38
	sub_825E2D38(ctx, base);
	// lwz r11,19824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19824);
	// std r24,19744(r31)
	PPC_STORE_U64(r31.u32 + 19744, r24.u64);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// std r24,19752(r31)
	PPC_STORE_U64(r31.u32 + 19752, r24.u64);
	// stw r24,3420(r31)
	PPC_STORE_U32(r31.u32 + 3420, r24.u32);
	// bgt cr6,0x825e62d4
	if (cr6.gt) goto loc_825E62D4;
	// bl 0x826a8cb8
	sub_826A8CB8(ctx, base);
	// stw r3,19932(r31)
	PPC_STORE_U32(r31.u32 + 19932, ctx.r3.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2cd8
	sub_825E2CD8(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825E5EA4:
	// bl 0x826a8cb8
	sub_826A8CB8(ctx, base);
	// lwz r11,19936(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19936);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// addi r11,r11,8000
	r11.s64 = r11.s64 + 8000;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// ble cr6,0x825e5f18
	if (!cr6.gt) goto loc_825E5F18;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r26,19936(r31)
	PPC_STORE_U32(r31.u32 + 19936, r26.u32);
	// li r4,114
	ctx.r4.s64 = 114;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x82278448
	sub_82278448(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,114
	ctx.r4.s64 = 114;
	// li r3,5
	ctx.r3.s64 = 5;
	// bl 0x82278448
	sub_82278448(ctx, base);
	// cmplwi cr6,r30,4
	cr6.compare<uint32_t>(r30.u32, 4, xer);
	// stw r3,19732(r31)
	PPC_STORE_U32(r31.u32 + 19732, ctx.r3.u32);
	// bgt cr6,0x825e5f18
	if (cr6.gt) goto loc_825E5F18;
	// li r11,1
	r11.s64 = 1;
	// stw r30,15548(r31)
	PPC_STORE_U32(r31.u32 + 15548, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,15544(r31)
	PPC_STORE_U32(r31.u32 + 15544, r11.u32);
	// bl 0x8260cf90
	sub_8260CF90(ctx, base);
	// lwz r11,19732(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19732);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,19724(r31)
	PPC_STORE_U32(r31.u32 + 19724, r11.u32);
loc_825E5F18:
	// lwz r11,19724(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19724);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e62dc
	if (cr6.eq) goto loc_825E62DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2d38
	sub_825E2D38(ctx, base);
	// lwz r11,19728(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19728);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r28,r11
	r28.u64 = r11.u64;
	// bgt cr6,0x825e5f40
	if (cr6.gt) goto loc_825E5F40;
	// lwz r28,15548(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 15548);
loc_825E5F40:
	// lwz r11,19812(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19812);
	// ld r10,19744(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 19744);
	// addi r11,r11,4944
	r11.s64 = r11.s64 + 4944;
	// lwz r9,19808(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19808);
	// extsw r5,r10
	ctx.r5.s64 = ctx.r10.s32;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// stw r10,19808(r31)
	PPC_STORE_U32(r31.u32 + 19808, ctx.r10.u32);
	// stwx r5,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r5.u32);
	// lwz r10,19812(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19812);
	// ld r3,19752(r31)
	ctx.r3.u64 = PPC_LOAD_U64(r31.u32 + 19752);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r11,3420(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3420);
	// lwz r9,19808(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19808);
	// extsw r30,r3
	r30.s64 = ctx.r3.s32;
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// srawi r4,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r4.s64 = ctx.r9.s32 >> 3;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,19812(r31)
	PPC_STORE_U32(r31.u32 + 19812, ctx.r10.u32);
	// bgt cr6,0x825e5fac
	if (cr6.gt) goto loc_825E5FAC;
	// lwz r10,15556(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15556);
	// lis r9,12288
	ctx.r9.s64 = 805306368;
	// rlwinm r10,r10,16,0,3
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xF0000000;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825e6028
	if (cr6.lt) goto loc_825E6028;
loc_825E5FAC:
	// lwz r10,19828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19828);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,19828(r31)
	PPC_STORE_U32(r31.u32 + 19828, r11.u32);
	// ble cr6,0x825e5ff0
	if (!cr6.gt) goto loc_825E5FF0;
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825e5fd0
	if (!cr6.gt) goto loc_825E5FD0;
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
loc_825E5FD0:
	// lwz r11,19832(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19832);
	// mulli r11,r11,230
	r11.s64 = r11.s64 * 230;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// mulli r10,r11,230
	ctx.r10.s64 = r11.s64 * 230;
	// stw r11,19832(r31)
	PPC_STORE_U32(r31.u32 + 19832, r11.u32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stw r10,19836(r31)
	PPC_STORE_U32(r31.u32 + 19836, ctx.r10.u32);
	// b 0x825e5ff8
	goto loc_825E5FF8;
loc_825E5FF0:
	// stw r26,19932(r31)
	PPC_STORE_U32(r31.u32 + 19932, r26.u32);
	// stw r24,19924(r31)
	PPC_STORE_U32(r31.u32 + 19924, r24.u32);
loc_825E5FF8:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825e6024
	if (!cr6.gt) goto loc_825E6024;
	// addi r11,r28,4961
	r11.s64 = r28.s64 + 4961;
	// addi r10,r28,4960
	ctx.r10.s64 = r28.s64 + 4960;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x825e6024
	if (!cr6.eq) goto loc_825E6024;
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
loc_825E6024:
	// stw r24,3420(r31)
	PPC_STORE_U32(r31.u32 + 3420, r24.u32);
loc_825E6028:
	// lwz r7,15556(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 15556);
	// rlwinm r11,r7,16,0,15
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 16) & 0xFFFF0000;
	// srawi r6,r11,28
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFFFF) != 0);
	ctx.r6.s64 = r11.s32 >> 28;
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// bne cr6,0x825e6068
	if (!cr6.eq) goto loc_825E6068;
	// lwz r11,19820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19820);
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// srawi r9,r7,16
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 16;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// ble cr6,0x825e6068
	if (!cr6.gt) goto loc_825E6068;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825e6064
	if (!cr6.gt) goto loc_825E6064;
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
loc_825E6064:
	// stw r26,19932(r31)
	PPC_STORE_U32(r31.u32 + 19932, r26.u32);
loc_825E6068:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825e60e4
	if (!cr6.gt) goto loc_825E60E4;
	// addi r11,r28,4960
	r11.s64 = r28.s64 + 4960;
	// lwz r8,19832(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 19832);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r31
	ctx.r9.u64 = r11.u64 + r31.u64;
loc_825E6080:
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r10,r11,r5
	ctx.r10.u64 = r11.u64 + ctx.r5.u64;
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// ble cr6,0x825e60e4
	if (!cr6.gt) goto loc_825E60E4;
	// lwz r29,19816(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 19816);
	// cmpwi cr6,r29,7
	cr6.compare<int32_t>(r29.s32, 7, xer);
	// ble cr6,0x825e60b4
	if (!cr6.gt) goto loc_825E60B4;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x825e60b4
	if (!cr6.gt) goto loc_825E60B4;
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// b 0x825e60dc
	goto loc_825E60DC;
loc_825E60B4:
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x825e60e4
	if (!cr6.gt) goto loc_825E60E4;
	// lwz r11,19728(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19728);
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r25,r24
	r25.u64 = r24.u64;
	// bgt cr6,0x825e60dc
	if (cr6.gt) goto loc_825E60DC;
	// lwz r25,15548(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 15548);
loc_825E60DC:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bgt cr6,0x825e6080
	if (cr6.gt) goto loc_825E6080;
loc_825E60E4:
	// lwz r8,15548(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15548);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// cmpw cr6,r28,r8
	cr6.compare<int32_t>(r28.s32, ctx.r8.s32, xer);
	// addi r29,r11,27460
	r29.s64 = r11.s64 + 27460;
	// bne cr6,0x825e61bc
	if (!cr6.eq) goto loc_825E61BC;
	// cmpw cr6,r28,r27
	cr6.compare<int32_t>(r28.s32, r27.s32, xer);
	// bge cr6,0x825e61bc
	if (!cr6.lt) goto loc_825E61BC;
	// cmpwi cr6,r6,1
	cr6.compare<int32_t>(ctx.r6.s32, 1, xer);
	// ble cr6,0x825e6128
	if (!cr6.gt) goto loc_825E6128;
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// bne cr6,0x825e61bc
	if (!cr6.eq) goto loc_825E61BC;
	// lwz r11,19820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19820);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r10,r7,16
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 16;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x825e61bc
	if (!cr6.gt) goto loc_825E61BC;
loc_825E6128:
	// lwz r6,19924(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 19924);
	// lwz r11,19932(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19932);
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// subf r7,r11,r26
	ctx.r7.s64 = r26.s64 - r11.s64;
	// bge cr6,0x825e6144
	if (!cr6.lt) goto loc_825E6144;
	// lwz r10,19832(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19832);
	// b 0x825e6148
	goto loc_825E6148;
loc_825E6144:
	// lwz r10,19836(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19836);
loc_825E6148:
	// addi r9,r28,1
	ctx.r9.s64 = r28.s64 + 1;
	// addi r11,r9,4960
	r11.s64 = ctx.r9.s64 + 4960;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// cmpw cr6,r5,r10
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r10.s32, xer);
	// bge cr6,0x825e61ac
	if (!cr6.lt) goto loc_825E61AC;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825e61ac
	if (!cr6.lt) goto loc_825E61AC;
	// addi r11,r29,-20
	r11.s64 = r29.s64 + -20;
	// rlwinm r10,r28,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmplw cr6,r7,r11
	cr6.compare<uint32_t>(ctx.r7.u32, r11.u32, xer);
	// bgt cr6,0x825e618c
	if (cr6.gt) goto loc_825E618C;
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// bge cr6,0x825e61bc
	if (!cr6.lt) goto loc_825E61BC;
loc_825E618C:
	// addi r11,r29,-20
	r11.s64 = r29.s64 + -20;
	// rlwinm r10,r27,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmplw cr6,r7,r11
	cr6.compare<uint32_t>(ctx.r7.u32, r11.u32, xer);
	// ble cr6,0x825e61ac
	if (!cr6.gt) goto loc_825E61AC;
	// addi r11,r26,-500
	r11.s64 = r26.s64 + -500;
	// stw r11,19932(r31)
	PPC_STORE_U32(r31.u32 + 19932, r11.u32);
loc_825E61AC:
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// bge cr6,0x825e61bc
	if (!cr6.lt) goto loc_825E61BC;
	// addi r11,r6,-1
	r11.s64 = ctx.r6.s64 + -1;
	// stw r11,19924(r31)
	PPC_STORE_U32(r31.u32 + 19924, r11.u32);
loc_825E61BC:
	// cmpdi cr6,r3,0
	cr6.compare<int64_t>(ctx.r3.s64, 0, xer);
	// ble cr6,0x825e6204
	if (!cr6.gt) goto loc_825E6204;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x825e6204
	if (!cr6.gt) goto loc_825E6204;
	// cmpw cr6,r8,r27
	cr6.compare<int32_t>(ctx.r8.s32, r27.s32, xer);
	// bgt cr6,0x825e6204
	if (cr6.gt) goto loc_825E6204;
	// addi r11,r8,2483
	r11.s64 = ctx.r8.s64 + 2483;
	// extsw r10,r30
	ctx.r10.s64 = r30.s32;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// ldx r9,r11,r31
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + r31.u32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stdx r10,r11,r31
	PPC_STORE_U64(r11.u32 + r31.u32, ctx.r10.u64);
	// lwz r11,15548(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15548);
	// addi r11,r11,4976
	r11.s64 = r11.s64 + 4976;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r10,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r10.u32);
loc_825E6204:
	// lwz r11,15548(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15548);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// bne cr6,0x825e622c
	if (!cr6.eq) goto loc_825E622C;
	// lwz r10,19728(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19728);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bgt cr6,0x825e622c
	if (cr6.gt) goto loc_825E622C;
	// lwz r11,19816(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19816);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,19816(r31)
	PPC_STORE_U32(r31.u32 + 19816, r11.u32);
	// b 0x825e62cc
	goto loc_825E62CC;
loc_825E622C:
	// rlwinm r30,r28,2,0,29
	r30.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r30,r29
	r11.u64 = PPC_LOAD_U32(r30.u32 + r29.u32);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// stw r11,15508(r31)
	PPC_STORE_U32(r31.u32 + 15508, r11.u32);
	// lwzx r11,r30,r29
	r11.u64 = PPC_LOAD_U32(r30.u32 + r29.u32);
	// rlwinm r11,r11,31,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x1;
	// stw r11,15512(r31)
	PPC_STORE_U32(r31.u32 + 15512, r11.u32);
	// lwzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r29.u32);
	// lwzx r11,r30,r29
	r11.u64 = PPC_LOAD_U32(r30.u32 + r29.u32);
	// xor r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 ^ r11.u64;
	// rlwinm r10,r10,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825e6270
	if (cr6.eq) goto loc_825E6270;
	// rlwinm r3,r11,30,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x1;
	// lwz r4,3924(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// bl 0x8260f190
	sub_8260F190(ctx, base);
loc_825E6270:
	// lwz r11,15548(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15548);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// bge cr6,0x825e62bc
	if (!cr6.lt) goto loc_825E62BC;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// stw r26,19932(r31)
	PPC_STORE_U32(r31.u32 + 19932, r26.u32);
	// ble cr6,0x825e62a8
	if (!cr6.gt) goto loc_825E62A8;
	// addi r10,r29,-20
	ctx.r10.s64 = r29.s64 + -20;
	// addi r9,r29,-20
	ctx.r9.s64 = r29.s64 + -20;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r30,r9
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + ctx.r9.u32);
	// lwzx r10,r8,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// add r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 + r26.u64;
	// stw r10,19932(r31)
	PPC_STORE_U32(r31.u32 + 19932, ctx.r10.u32);
loc_825E62A8:
	// lwz r10,19924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19924);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825e62bc
	if (!cr6.lt) goto loc_825E62BC;
	// stw r11,19924(r31)
	PPC_STORE_U32(r31.u32 + 19924, r11.u32);
loc_825E62BC:
	// stw r28,15548(r31)
	PPC_STORE_U32(r31.u32 + 15548, r28.u32);
	// stw r28,3644(r31)
	PPC_STORE_U32(r31.u32 + 3644, r28.u32);
	// stw r25,19728(r31)
	PPC_STORE_U32(r31.u32 + 19728, r25.u32);
	// stw r24,19816(r31)
	PPC_STORE_U32(r31.u32 + 19816, r24.u32);
loc_825E62CC:
	// std r24,19744(r31)
	PPC_STORE_U64(r31.u32 + 19744, r24.u64);
	// std r24,19752(r31)
	PPC_STORE_U64(r31.u32 + 19752, r24.u64);
loc_825E62D4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2cd8
	sub_825E2CD8(ctx, base);
loc_825E62DC:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825E62E4"))) PPC_WEAK_FUNC(sub_825E62E4);
PPC_FUNC_IMPL(__imp__sub_825E62E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E62E8"))) PPC_WEAK_FUNC(sub_825E62E8);
PPC_FUNC_IMPL(__imp__sub_825E62E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e6314
	if (!cr6.eq) goto loc_825E6314;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E6314:
	// lwz r10,21332(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21332);
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825e6338
	if (!cr6.eq) goto loc_825E6338;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E6338:
	// lwz r9,21328(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 21328);
	// lfd f0,20856(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 20856);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r10,r1,104
	ctx.r10.s64 = ctx.r1.s64 + 104;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// lwz r9,3908(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 3908);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// lwz r9,21228(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 21228);
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// lwz r9,21160(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 21160);
	// lwz r11,3660(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 3660);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// bl 0x825ff6b8
	sub_825FF6B8(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E6394"))) PPC_WEAK_FUNC(sub_825E6394);
PPC_FUNC_IMPL(__imp__sub_825E6394) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E6398"))) PPC_WEAK_FUNC(sub_825E6398);
PPC_FUNC_IMPL(__imp__sub_825E6398) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825e63cc
	if (!cr6.eq) goto loc_825E63CC;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825E63CC:
	// lwz r11,21332(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21332);
	// rlwinm r11,r11,0,30,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e63f4
	if (!cr6.eq) goto loc_825E63F4;
	// li r3,12
	ctx.r3.s64 = 12;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825E63F4:
	// lwz r11,21328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21328);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// li r5,92
	ctx.r5.s64 = 92;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21184(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21184);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,21352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21352);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// lwz r10,21356(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21356);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// bl 0x825ff6b8
	sub_825FF6B8(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E6454"))) PPC_WEAK_FUNC(sub_825E6454);
PPC_FUNC_IMPL(__imp__sub_825E6454) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E6458"))) PPC_WEAK_FUNC(sub_825E6458);
PPC_FUNC_IMPL(__imp__sub_825E6458) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e6484
	if (!cr6.eq) goto loc_825E6484;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E6484:
	// lwz r10,21332(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21332);
	// rlwinm r10,r10,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825e64a8
	if (!cr6.eq) goto loc_825E64A8;
	// li r3,12
	ctx.r3.s64 = 12;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E64A8:
	// li r8,2
	ctx.r8.s64 = 2;
	// lwz r10,21184(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21184);
	// lwz r9,14772(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14772);
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r4,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r4.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// stw r8,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
	// lwz r8,21328(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 21328);
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// lwz r8,21208(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 21208);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// lwz r8,156(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 156);
	// stw r8,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r8.u32);
	// lwz r8,160(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 160);
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// bgt cr6,0x825e6500
	if (cr6.gt) goto loc_825E6500;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825E6500:
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,21364(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21364);
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// lwz r10,21368(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21368);
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// lwz r10,21372(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21372);
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// lwz r10,21376(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21376);
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// lwz r10,21212(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21212);
	// stw r10,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r10.u32);
	// lwz r10,21220(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21220);
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
	// lwz r10,21216(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21216);
	// lwz r11,21224(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 21224);
	// stw r10,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r10.u32);
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r11.u32);
	// bl 0x825ff6b8
	sub_825FF6B8(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E655C"))) PPC_WEAK_FUNC(sub_825E655C);
PPC_FUNC_IMPL(__imp__sub_825E655C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E6560"))) PPC_WEAK_FUNC(sub_825E6560);
PPC_FUNC_IMPL(__imp__sub_825E6560) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e658c
	if (!cr6.eq) goto loc_825E658C;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E658C:
	// lwz r10,21332(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21332);
	// rlwinm r10,r10,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825e65b0
	if (!cr6.eq) goto loc_825E65B0;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E65B0:
	// lwz r9,21328(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 21328);
	// lwz r10,19976(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 19976);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// li r9,3
	ctx.r9.s64 = 3;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// beq cr6,0x825e6608
	if (cr6.eq) goto loc_825E6608;
	// lwz r10,19980(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 19980);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825e6608
	if (cr6.eq) goto loc_825E6608;
	// lwz r10,21076(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21076);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// lwz r8,21072(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 21072);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,27256
	ctx.r9.s64 = ctx.r9.s64 + 27256;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// b 0x825e660c
	goto loc_825E660C;
loc_825E6608:
	// lwz r10,284(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 284);
loc_825E660C:
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,21160(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21160);
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// lwz r10,20836(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20836);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// lwz r10,20840(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20840);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// lwz r10,21164(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21164);
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// lwz r10,20976(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20976);
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// lwz r10,3444(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 3444);
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// lwz r10,3448(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 3448);
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// lwz r10,20868(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20868);
	// stw r10,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r10.u32);
	// lwz r10,20872(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20872);
	// lwz r11,21436(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 21436);
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// bl 0x825ff6b8
	sub_825FF6B8(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E6680"))) PPC_WEAK_FUNC(sub_825E6680);
PPC_FUNC_IMPL(__imp__sub_825E6680) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e66ac
	if (!cr6.eq) goto loc_825E66AC;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E66AC:
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r9,21332(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 21332);
	// slw r10,r10,r4
	ctx.r10.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r4.u8 & 0x3F));
	// and r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ctx.r10.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825e66d8
	if (!cr6.eq) goto loc_825E66D8;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825E66D8:
	// lwz r10,21328(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21328);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// stw r5,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r5.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// bl 0x825ff6b8
	sub_825FF6B8(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E670C"))) PPC_WEAK_FUNC(sub_825E670C);
PPC_FUNC_IMPL(__imp__sub_825E670C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E6710"))) PPC_WEAK_FUNC(sub_825E6710);
PPC_FUNC_IMPL(__imp__sub_825E6710) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// lwz r24,23968(r27)
	r24.u64 = PPC_LOAD_U32(r27.u32 + 23968);
	// ble cr6,0x825e677c
	if (!cr6.gt) goto loc_825E677C;
	// lis r11,21845
	r11.s64 = 1431633920;
	// addi r31,r24,17696
	r31.s64 = r24.s64 + 17696;
	// mr r30,r25
	r30.u64 = r25.u64;
	// li r26,-1
	r26.s64 = -1;
	// ori r28,r11,21845
	r28.u64 = r11.u64 | 21845;
loc_825E6744:
	// addi r29,r31,264
	r29.s64 = r31.s64 + 264;
	// lwz r3,15204(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 15204);
	// li r5,-1
	ctx.r5.s64 = -1;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r26,612(r11)
	PPC_STORE_U32(r11.u32 + 612, r26.u32);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// stw r28,616(r11)
	PPC_STORE_U32(r11.u32 + 616, r28.u32);
	// bne cr6,0x825e6744
	if (!cr6.eq) goto loc_825E6744;
loc_825E677C:
	// li r11,0
	r11.s64 = 0;
	// stw r25,18216(r24)
	PPC_STORE_U32(r24.u32 + 18216, r25.u32);
	// stw r25,17956(r24)
	PPC_STORE_U32(r24.u32 + 17956, r25.u32);
	// stw r11,17952(r24)
	PPC_STORE_U32(r24.u32 + 17952, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825E6794"))) PPC_WEAK_FUNC(sub_825E6794);
PPC_FUNC_IMPL(__imp__sub_825E6794) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E6798"))) PPC_WEAK_FUNC(sub_825E6798);
PPC_FUNC_IMPL(__imp__sub_825E6798) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -96, f30.u64);
	// stfd f31,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, f31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// lis r3,1
	ctx.r3.s64 = 65536;
	// fmr f30,f2
	f30.f64 = ctx.f2.f64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// li r28,0
	r28.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r3,r3,33760
	ctx.r3.u64 = ctx.r3.u64 | 33760;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// mr r24,r28
	r24.u64 = r28.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825e6808
	if (!cr6.eq) goto loc_825E6808;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r28,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r28.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f31,-88(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x8239bd34
	return;
loc_825E6808:
	// lis r5,1
	ctx.r5.s64 = 65536;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r5,r5,33760
	ctx.r5.u64 = ctx.r5.u64 | 33760;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,56
	ctx.r3.s64 = 56;
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
	// stw r31,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r31.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,80(r31)
	PPC_STORE_U32(r31.u32 + 80, ctx.r3.u32);
	// bne cr6,0x825e6854
	if (!cr6.eq) goto loc_825E6854;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f31,-88(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x8239bd34
	return;
loc_825E6854:
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x825eb280
	sub_825EB280(ctx, base);
	// lis r11,22358
	r11.s64 = 1465253888;
	// lwz r9,284(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// ori r11,r11,17201
	r11.u64 = r11.u64 | 17201;
	// lwz r10,276(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x825e6904
	if (!cr6.eq) goto loc_825E6904;
	// lis r30,22349
	r30.s64 = 1464664064;
	// li r24,1
	r24.s64 = 1;
	// ori r30,r30,22081
	r30.u64 = r30.u64 | 22081;
loc_825E6880:
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// stw r10,19704(r31)
	PPC_STORE_U32(r31.u32 + 19704, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r9,19708(r31)
	PPC_STORE_U32(r31.u32 + 19708, ctx.r9.u32);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// fmr f2,f30
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f30.f64;
	// stw r8,15236(r31)
	PPC_STORE_U32(r31.u32 + 15236, ctx.r8.u32);
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// stw r31,40(r11)
	PPC_STORE_U32(r11.u32 + 40, r31.u32);
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// stw r26,21412(r31)
	PPC_STORE_U32(r31.u32 + 21412, r26.u32);
	// stw r25,21416(r31)
	PPC_STORE_U32(r31.u32 + 21416, r25.u32);
	// stw r28,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r28.u32);
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// bl 0x825e5800
	sub_825E5800(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x825e68e0
	if (cr6.eq) goto loc_825E68E0;
	// li r11,1
	r11.s64 = 1;
	// stw r11,21480(r31)
	PPC_STORE_U32(r31.u32 + 21480, r11.u32);
loc_825E68E0:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825e68f0
	if (!cr6.eq) goto loc_825E68F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e5960
	sub_825E5960(ctx, base);
loc_825E68F0:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f31,-88(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x8239bd34
	return;
loc_825E6904:
	// lis r11,22349
	r11.s64 = 1464664064;
	// ori r11,r11,22067
	r11.u64 = r11.u64 | 22067;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x825e6944
	if (cr6.eq) goto loc_825E6944;
	// lis r11,30573
	r11.s64 = 2003632128;
	// ori r11,r11,30259
	r11.u64 = r11.u64 | 30259;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x825e6944
	if (cr6.eq) goto loc_825E6944;
	// lis r11,22349
	r11.s64 = 1464664064;
	// ori r11,r11,22096
	r11.u64 = r11.u64 | 22096;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x825e6944
	if (cr6.eq) goto loc_825E6944;
	// lis r11,30573
	r11.s64 = 2003632128;
	// ori r11,r11,30320
	r11.u64 = r11.u64 | 30320;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x825e6880
	if (!cr6.eq) goto loc_825E6880;
loc_825E6944:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825e6880
	if (cr6.eq) goto loc_825E6880;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x825e6880
	if (cr6.eq) goto loc_825E6880;
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,3924(r31)
	PPC_STORE_U32(r31.u32 + 3924, r11.u32);
	// stw r11,3936(r31)
	PPC_STORE_U32(r31.u32 + 3936, r11.u32);
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lwz r8,3924(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// rlwinm r11,r11,28,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r11,3940(r31)
	PPC_STORE_U32(r31.u32 + 3940, r11.u32);
	// stw r11,15300(r31)
	PPC_STORE_U32(r31.u32 + 15300, r11.u32);
	// beq cr6,0x825e6880
	if (cr6.eq) goto loc_825E6880;
	// li r3,6
	ctx.r3.s64 = 6;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f31,-88(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_825E6994"))) PPC_WEAK_FUNC(sub_825E6994);
PPC_FUNC_IMPL(__imp__sub_825E6994) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E6998"))) PPC_WEAK_FUNC(sub_825E6998);
PPC_FUNC_IMPL(__imp__sub_825E6998) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825e69c0
	if (!cr6.eq) goto loc_825E69C0;
	// li r3,7
	ctx.r3.s64 = 7;
	// b 0x825e6a14
	goto loc_825E6A14;
loc_825E69C0:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e69d4
	if (cr6.eq) goto loc_825E69D4;
	// li r3,13
	ctx.r3.s64 = 13;
	// b 0x825e6a14
	goto loc_825E6A14;
loc_825E69D4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e5c80
	sub_825E5C80(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604728
	sub_82604728(ctx, base);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// ori r5,r5,33760
	ctx.r5.u64 = ctx.r5.u64 | 33760;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e6a10
	if (!cr6.eq) goto loc_825E6A10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_825E6A10:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_825E6A14:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E6A2C"))) PPC_WEAK_FUNC(sub_825E6A2C);
PPC_FUNC_IMPL(__imp__sub_825E6A2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E6A30"))) PPC_WEAK_FUNC(sub_825E6A30);
PPC_FUNC_IMPL(__imp__sub_825E6A30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-512(r1)
	ea = -512 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r21,0
	r21.s64 = 0;
	// li r28,1
	r28.s64 = 1;
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// stw r21,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r21.u32);
	// stw r28,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r28.u32);
	// bne cr6,0x825e6a64
	if (!cr6.eq) goto loc_825E6A64;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239bd28
	return;
loc_825E6A64:
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e6a80
	if (cr6.eq) goto loc_825E6A80;
	// li r3,13
	ctx.r3.s64 = 13;
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239bd28
	return;
loc_825E6A80:
	// lwz r20,3340(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// mr r30,r21
	r30.u64 = r21.u64;
	// bl 0x82491030
	sub_82491030(ctx, base);
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e6bc4
	if (cr6.eq) goto loc_825E6BC4;
loc_825E6AB0:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825e6b08
	if (!cr6.eq) goto loc_825E6B08;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r29,r5,r30
	r29.u64 = ctx.r5.u64 + r30.u64;
	// cmplwi cr6,r29,256
	cr6.compare<uint32_t>(r29.u32, 256, xer);
	// bge cr6,0x825e6b0c
	if (!cr6.lt) goto loc_825E6B0C;
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// add r3,r30,r11
	ctx.r3.u64 = r30.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r3,3340(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r30,r29
	r30.u64 = r29.u64;
	// bl 0x82491030
	sub_82491030(ctx, base);
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e6ab0
	if (!cr6.eq) goto loc_825E6AB0;
loc_825E6B08:
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
loc_825E6B0C:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825e6b98
	if (cr6.eq) goto loc_825E6B98;
	// lwz r11,23248(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23248);
	// add r29,r5,r30
	r29.u64 = ctx.r5.u64 + r30.u64;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// ble cr6,0x825e6b60
	if (!cr6.gt) goto loc_825E6B60;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e6b34
	if (cr6.eq) goto loc_825E6B34;
	// lwz r3,23252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 23252);
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_825E6B34:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,23252(r31)
	PPC_STORE_U32(r31.u32 + 23252, ctx.r3.u32);
	// bne cr6,0x825e6b5c
	if (!cr6.eq) goto loc_825E6B5C;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r21,23248(r31)
	PPC_STORE_U32(r31.u32 + 23248, r21.u32);
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239bd28
	return;
loc_825E6B5C:
	// stw r29,23248(r31)
	PPC_STORE_U32(r31.u32 + 23248, r29.u32);
loc_825E6B60:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r3,23252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 23252);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,23252(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23252);
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r5,r11,r30
	ctx.r5.u64 = r11.u64 + r30.u64;
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// lwz r11,23252(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23252);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
loc_825E6B98:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e6bc4
	if (cr6.eq) goto loc_825E6BC4;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x825e6bb8
	if (cr6.eq) goto loc_825E6BB8;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e6bc4
	if (!cr6.eq) goto loc_825E6BC4;
loc_825E6BB8:
	// li r3,11
	ctx.r3.s64 = 11;
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239bd28
	return;
loc_825E6BC4:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// stw r21,21396(r31)
	PPC_STORE_U32(r31.u32 + 21396, r21.u32);
	// stw r21,21400(r31)
	PPC_STORE_U32(r31.u32 + 21400, r21.u32);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// stw r21,21392(r31)
	PPC_STORE_U32(r31.u32 + 21392, r21.u32);
	// stw r21,21408(r31)
	PPC_STORE_U32(r31.u32 + 21408, r21.u32);
	// bne cr6,0x825e6c88
	if (!cr6.eq) goto loc_825E6C88;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825e6c00
	if (cr6.eq) goto loc_825E6C00;
	// stw r28,21408(r31)
	PPC_STORE_U32(r31.u32 + 21408, r28.u32);
	// stw r21,14772(r31)
	PPC_STORE_U32(r31.u32 + 14772, r21.u32);
	// b 0x825e6c08
	goto loc_825E6C08;
loc_825E6C00:
	// stw r21,21408(r31)
	PPC_STORE_U32(r31.u32 + 21408, r21.u32);
	// stw r28,14772(r31)
	PPC_STORE_U32(r31.u32 + 14772, r28.u32);
loc_825E6C08:
	// rlwinm r10,r11,30,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x1;
	// rlwinm r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r10,21392(r31)
	PPC_STORE_U32(r31.u32 + 21392, ctx.r10.u32);
	// beq cr6,0x825e6c24
	if (cr6.eq) goto loc_825E6C24;
	// stw r28,21404(r31)
	PPC_STORE_U32(r31.u32 + 21404, r28.u32);
	// b 0x825e6c28
	goto loc_825E6C28;
loc_825E6C24:
	// stw r21,21404(r31)
	PPC_STORE_U32(r31.u32 + 21404, r21.u32);
loc_825E6C28:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r5,r11,-1
	ctx.r5.s64 = r11.s64 + -1;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// bne cr6,0x825e6c6c
	if (!cr6.eq) goto loc_825E6C6C;
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// lwz r3,3340(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82491030
	sub_82491030(ctx, base);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// addi r11,r11,-7
	r11.s64 = r11.s64 + -7;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x825e6c8c
	goto loc_825E6C8C;
loc_825E6C6C:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r11,r11,-7
	r11.s64 = r11.s64 + -7;
	// addi r4,r10,1
	ctx.r4.s64 = ctx.r10.s64 + 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x825e6c94
	goto loc_825E6C94;
loc_825E6C88:
	// li r7,0
	ctx.r7.s64 = 0;
loc_825E6C8C:
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
loc_825E6C94:
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// bl 0x825eb668
	sub_825EB668(ctx, base);
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// bl 0x825e2ea0
	sub_825E2EA0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825e6e88
	if (!cr6.eq) goto loc_825E6E88;
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825e6e80
	if (cr6.eq) goto loc_825E6E80;
	// lwz r11,15300(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15300);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e6cec
	if (!cr6.eq) goto loc_825E6CEC;
	// lwz r11,15368(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15368);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e6e80
	if (cr6.eq) goto loc_825E6E80;
loc_825E6CEC:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// mr r25,r21
	r25.u64 = r21.u64;
	// beq cr6,0x825e6d00
	if (cr6.eq) goto loc_825E6D00;
	// lwz r25,156(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// bne cr6,0x825e6d08
	if (!cr6.eq) goto loc_825E6D08;
loc_825E6D00:
	// mr r26,r21
	r26.u64 = r21.u64;
	// b 0x825e6d0c
	goto loc_825E6D0C;
loc_825E6D08:
	// lwz r26,160(r3)
	r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
loc_825E6D0C:
	// lwz r24,88(r3)
	r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// lwz r23,92(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 92);
	// lwz r29,19704(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19704);
	// lwz r28,19708(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19708);
	// lwz r27,15236(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15236);
	// beq cr6,0x825e6d48
	if (cr6.eq) goto loc_825E6D48;
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e6d3c
	if (cr6.eq) goto loc_825E6D3C;
	// li r30,13
	r30.s64 = 13;
	// b 0x825e6d4c
	goto loc_825E6D4C;
loc_825E6D3C:
	// lwz r30,3656(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3656);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bgt cr6,0x825e6d4c
	if (cr6.gt) goto loc_825E6D4C;
loc_825E6D48:
	// li r30,30
	r30.s64 = 30;
loc_825E6D4C:
	// lwz r11,15368(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15368);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e6dac
	if (cr6.eq) goto loc_825E6DAC;
	// bl 0x825e6998
	sub_825E6998(ctx, base);
	// extsw r7,r30
	ctx.r7.s64 = r30.s32;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stw r27,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r27.u32);
	// lis r5,22358
	ctx.r5.s64 = 1465253888;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// stw r21,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r21.u32);
	// std r7,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r7.u64);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// ori r5,r5,20530
	ctx.r5.u64 = ctx.r5.u64 | 20530;
	// lfs f2,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f2.f64 = double(temp.f32);
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lfd f0,128(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f1,f0
	ctx.f1.f64 = double(float(f0.f64));
	// bl 0x825e6798
	sub_825E6798(ctx, base);
	// b 0x825e6dfc
	goto loc_825E6DFC;
loc_825E6DAC:
	// bl 0x825e6998
	sub_825E6998(ctx, base);
	// extsw r7,r30
	ctx.r7.s64 = r30.s32;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stw r27,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r27.u32);
	// lis r5,22349
	ctx.r5.s64 = 1464664064;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// stw r21,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r21.u32);
	// std r7,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r7.u64);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// ori r5,r5,22067
	ctx.r5.u64 = ctx.r5.u64 | 22067;
	// lfs f2,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f2.f64 = double(temp.f32);
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lfd f0,128(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f1,f0
	ctx.f1.f64 = double(float(f0.f64));
	// bl 0x825e6798
	sub_825E6798(ctx, base);
loc_825E6DFC:
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e6e8c
	if (!cr6.eq) goto loc_825E6E8C;
	// stw r24,15308(r11)
	PPC_STORE_U32(r11.u32 + 15308, r24.u32);
	// lwz r10,0(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// stw r23,15312(r10)
	PPC_STORE_U32(ctx.r10.u32 + 15312, r23.u32);
	// lwz r10,15472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15472);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r10,7
	cr6.compare<int32_t>(ctx.r10.s32, 7, xer);
	// bne cr6,0x825e6e50
	if (!cr6.eq) goto loc_825E6E50;
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r5,r10,-1
	ctx.r5.s64 = ctx.r10.s64 + -1;
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// lwz r10,15472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15472);
	// lwz r3,80(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// addi r11,r10,-7
	r11.s64 = ctx.r10.s64 + -7;
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// addi r4,r10,1
	ctx.r4.s64 = ctx.r10.s64 + 1;
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x825e6e60
	goto loc_825E6E60;
loc_825E6E50:
	// lwz r3,80(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
loc_825E6E60:
	// bl 0x825eb668
	sub_825EB668(ctx, base);
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x825e2ea0
	sub_825E2EA0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239bd28
	return;
loc_825E6E80:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e5960
	sub_825E5960(ctx, base);
loc_825E6E88:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_825E6E8C:
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_825E6E94"))) PPC_WEAK_FUNC(sub_825E6E94);
PPC_FUNC_IMPL(__imp__sub_825E6E94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E6E98"))) PPC_WEAK_FUNC(sub_825E6E98);
PPC_FUNC_IMPL(__imp__sub_825E6E98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x825e6ea8
	if (!cr6.eq) goto loc_825E6EA8;
	// li r3,7
	ctx.r3.s64 = 7;
	// blr 
	return;
loc_825E6EA8:
	// b 0x825e4ee0
	sub_825E4EE0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825E6EAC"))) PPC_WEAK_FUNC(sub_825E6EAC);
PPC_FUNC_IMPL(__imp__sub_825E6EAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E6EB0"))) PPC_WEAK_FUNC(sub_825E6EB0);
PPC_FUNC_IMPL(__imp__sub_825E6EB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r24,0
	r24.s64 = 0;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// li r30,1
	r30.s64 = 1;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e6f40
	if (!cr6.lt) goto loc_825E6F40;
loc_825E6EE8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e6f40
	if (cr6.eq) goto loc_825E6F40;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e6f30
	if (!cr0.lt) goto loc_825E6F30;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E6F30:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e6ee8
	if (cr6.gt) goto loc_825E6EE8;
loc_825E6F40:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6f7c
	if (!cr0.lt) goto loc_825E6F7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E6F7C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,21364(r27)
	PPC_STORE_U32(r27.u32 + 21364, r28.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e6ff4
	if (!cr6.lt) goto loc_825E6FF4;
loc_825E6F9C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e6ff4
	if (cr6.eq) goto loc_825E6FF4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e6fe4
	if (!cr0.lt) goto loc_825E6FE4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E6FE4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e6f9c
	if (cr6.gt) goto loc_825E6F9C;
loc_825E6FF4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7030
	if (!cr0.lt) goto loc_825E7030;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7030:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,21368(r27)
	PPC_STORE_U32(r27.u32 + 21368, r28.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e70a8
	if (!cr6.lt) goto loc_825E70A8;
loc_825E7050:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e70a8
	if (cr6.eq) goto loc_825E70A8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7098
	if (!cr0.lt) goto loc_825E7098;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7098:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7050
	if (cr6.gt) goto loc_825E7050;
loc_825E70A8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e70e4
	if (!cr0.lt) goto loc_825E70E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E70E4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,21372(r27)
	PPC_STORE_U32(r27.u32 + 21372, r28.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e715c
	if (!cr6.lt) goto loc_825E715C;
loc_825E7104:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e715c
	if (cr6.eq) goto loc_825E715C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e714c
	if (!cr0.lt) goto loc_825E714C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E714C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7104
	if (cr6.gt) goto loc_825E7104;
loc_825E715C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7198
	if (!cr0.lt) goto loc_825E7198;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7198:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,21376(r27)
	PPC_STORE_U32(r27.u32 + 21376, r28.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e7210
	if (!cr6.lt) goto loc_825E7210;
loc_825E71B8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7210
	if (cr6.eq) goto loc_825E7210;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7200
	if (!cr0.lt) goto loc_825E7200;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7200:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e71b8
	if (cr6.gt) goto loc_825E71B8;
loc_825E7210:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e724c
	if (!cr0.lt) goto loc_825E724C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E724C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,3892(r27)
	PPC_STORE_U32(r27.u32 + 3892, r28.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e72c4
	if (!cr6.lt) goto loc_825E72C4;
loc_825E726C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e72c4
	if (cr6.eq) goto loc_825E72C4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e72b4
	if (!cr0.lt) goto loc_825E72B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E72B4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e726c
	if (cr6.gt) goto loc_825E726C;
loc_825E72C4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7300
	if (!cr0.lt) goto loc_825E7300;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7300:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,1792(r27)
	PPC_STORE_U32(r27.u32 + 1792, r28.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e7378
	if (!cr6.lt) goto loc_825E7378;
loc_825E7320:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7378
	if (cr6.eq) goto loc_825E7378;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7368
	if (!cr0.lt) goto loc_825E7368;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7368:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7320
	if (cr6.gt) goto loc_825E7320;
loc_825E7378:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e73b4
	if (!cr0.lt) goto loc_825E73B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E73B4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// stw r28,20864(r27)
	PPC_STORE_U32(r27.u32 + 20864, r28.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825e742c
	if (!cr6.lt) goto loc_825E742C;
loc_825E73D4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e742c
	if (cr6.eq) goto loc_825E742C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e741c
	if (!cr0.lt) goto loc_825E741C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E741C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e73d4
	if (cr6.gt) goto loc_825E73D4;
loc_825E742C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7468
	if (!cr0.lt) goto loc_825E7468;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7468:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,3980(r27)
	PPC_STORE_U32(r27.u32 + 3980, r28.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e74e0
	if (!cr6.lt) goto loc_825E74E0;
loc_825E7488:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e74e0
	if (cr6.eq) goto loc_825E74E0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e74d0
	if (!cr0.lt) goto loc_825E74D0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E74D0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7488
	if (cr6.gt) goto loc_825E7488;
loc_825E74E0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e751c
	if (!cr0.lt) goto loc_825E751C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E751C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,436(r27)
	PPC_STORE_U32(r27.u32 + 436, r28.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e7594
	if (!cr6.lt) goto loc_825E7594;
loc_825E753C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7594
	if (cr6.eq) goto loc_825E7594;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7584
	if (!cr0.lt) goto loc_825E7584;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7584:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e753c
	if (cr6.gt) goto loc_825E753C;
loc_825E7594:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e75d0
	if (!cr0.lt) goto loc_825E75D0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E75D0:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,2972(r27)
	PPC_STORE_U32(r27.u32 + 2972, r28.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e7648
	if (!cr6.lt) goto loc_825E7648;
loc_825E75F0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7648
	if (cr6.eq) goto loc_825E7648;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7638
	if (!cr0.lt) goto loc_825E7638;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7638:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e75f0
	if (cr6.gt) goto loc_825E75F0;
loc_825E7648:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7684
	if (!cr0.lt) goto loc_825E7684;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7684:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,3436(r27)
	PPC_STORE_U32(r27.u32 + 3436, r30.u32);
	// mr r29,r24
	r29.u64 = r24.u64;
	// li r30,1
	r30.s64 = 1;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825e7750
	if (cr6.eq) goto loc_825E7750;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e7704
	if (!cr6.lt) goto loc_825E7704;
loc_825E76AC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7704
	if (cr6.eq) goto loc_825E7704;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e76f4
	if (!cr0.lt) goto loc_825E76F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E76F4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e76ac
	if (cr6.gt) goto loc_825E76AC;
loc_825E7704:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7740
	if (!cr0.lt) goto loc_825E7740;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7740:
	// addi r11,r30,2
	r11.s64 = r30.s64 + 2;
	// stw r30,3428(r27)
	PPC_STORE_U32(r27.u32 + 3428, r30.u32);
	// stw r11,21524(r27)
	PPC_STORE_U32(r27.u32 + 21524, r11.u32);
	// b 0x825e77f4
	goto loc_825E77F4;
loc_825E7750:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e77b0
	if (!cr6.lt) goto loc_825E77B0;
loc_825E7758:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e77b0
	if (cr6.eq) goto loc_825E77B0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e77a0
	if (!cr0.lt) goto loc_825E77A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E77A0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7758
	if (cr6.gt) goto loc_825E7758;
loc_825E77B0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e77ec
	if (!cr0.lt) goto loc_825E77EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E77EC:
	// stw r30,3440(r27)
	PPC_STORE_U32(r27.u32 + 3440, r30.u32);
	// stw r30,21524(r27)
	PPC_STORE_U32(r27.u32 + 21524, r30.u32);
loc_825E77F4:
	// lwz r11,21204(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21204);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e78dc
	if (cr6.eq) goto loc_825E78DC;
	// lwz r11,21208(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21208);
	// mr r28,r24
	r28.u64 = r24.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e78dc
	if (!cr6.gt) goto loc_825E78DC;
loc_825E7810:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825e7888
	if (!cr6.lt) goto loc_825E7888;
loc_825E782C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7888
	if (cr6.eq) goto loc_825E7888;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7878
	if (!cr0.lt) goto loc_825E7878;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7878:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e782c
	if (cr6.gt) goto loc_825E782C;
loc_825E7888:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e78c4
	if (!cr0.lt) goto loc_825E78C4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E78C4:
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r10,21208(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 21208);
	// stbx r30,r28,r9
	PPC_STORE_U8(r28.u32 + ctx.r9.u32, r30.u8);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// blt cr6,0x825e7810
	if (cr6.lt) goto loc_825E7810;
loc_825E78DC:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e7950
	if (!cr6.lt) goto loc_825E7950;
loc_825E78F8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7950
	if (cr6.eq) goto loc_825E7950;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7940
	if (!cr0.lt) goto loc_825E7940;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7940:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e78f8
	if (cr6.gt) goto loc_825E78F8;
loc_825E7950:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e798c
	if (!cr0.lt) goto loc_825E798C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E798C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825e7b4c
	if (cr6.eq) goto loc_825E7B4C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,12
	r30.s64 = 12;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bge cr6,0x825e7a08
	if (!cr6.lt) goto loc_825E7A08;
loc_825E79B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7a08
	if (cr6.eq) goto loc_825E7A08;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e79f8
	if (!cr0.lt) goto loc_825E79F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E79F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e79b0
	if (cr6.gt) goto loc_825E79B0;
loc_825E7A08:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7a44
	if (!cr0.lt) goto loc_825E7A44;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7A44:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// li r30,12
	r30.s64 = 12;
	// rlwinm r28,r11,1,0,30
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bge cr6,0x825e7ac0
	if (!cr6.lt) goto loc_825E7AC0;
loc_825E7A68:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7ac0
	if (cr6.eq) goto loc_825E7AC0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7ab0
	if (!cr0.lt) goto loc_825E7AB0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7AB0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7a68
	if (cr6.gt) goto loc_825E7A68;
loc_825E7AC0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7afc
	if (!cr0.lt) goto loc_825E7AFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7AFC:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// beq cr6,0x825e7b1c
	if (cr6.eq) goto loc_825E7B1C;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x825e7b1c
	if (cr6.eq) goto loc_825E7B1C;
	// stw r28,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r28.u32);
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
loc_825E7B1C:
	// lwz r10,21352(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 21352);
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825e7b40
	if (cr6.gt) goto loc_825E7B40;
	// lwz r10,21356(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 21356);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825e7b40
	if (cr6.gt) goto loc_825E7B40;
	// stw r28,156(r27)
	PPC_STORE_U32(r27.u32 + 156, r28.u32);
	// stw r11,160(r27)
	PPC_STORE_U32(r27.u32 + 160, r11.u32);
	// b 0x825e7b5c
	goto loc_825E7B5C;
loc_825E7B40:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd38
	return;
loc_825E7B4C:
	// lwz r11,21352(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21352);
	// lwz r10,21356(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 21356);
	// stw r11,156(r27)
	PPC_STORE_U32(r27.u32 + 156, r11.u32);
	// stw r10,160(r27)
	PPC_STORE_U32(r27.u32 + 160, ctx.r10.u32);
loc_825E7B5C:
	// lwz r11,20864(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20864);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e7c1c
	if (cr6.eq) goto loc_825E7C1C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e7bdc
	if (!cr6.lt) goto loc_825E7BDC;
loc_825E7B84:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7bdc
	if (cr6.eq) goto loc_825E7BDC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7bcc
	if (!cr0.lt) goto loc_825E7BCC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7BCC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7b84
	if (cr6.gt) goto loc_825E7B84;
loc_825E7BDC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7c18
	if (!cr0.lt) goto loc_825E7C18;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7C18:
	// stw r30,20956(r27)
	PPC_STORE_U32(r27.u32 + 20956, r30.u32);
loc_825E7C1C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e7c90
	if (!cr6.lt) goto loc_825E7C90;
loc_825E7C38:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7c90
	if (cr6.eq) goto loc_825E7C90;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7c80
	if (!cr0.lt) goto loc_825E7C80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7C80:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7c38
	if (cr6.gt) goto loc_825E7C38;
loc_825E7C90:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7ccc
	if (!cr0.lt) goto loc_825E7CCC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7CCC:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,21212(r27)
	PPC_STORE_U32(r27.u32 + 21212, r30.u32);
	// beq cr6,0x825e7d94
	if (cr6.eq) goto loc_825E7D94;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825e7d4c
	if (!cr6.lt) goto loc_825E7D4C;
loc_825E7CF4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7d4c
	if (cr6.eq) goto loc_825E7D4C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7d3c
	if (!cr0.lt) goto loc_825E7D3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7D3C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7cf4
	if (cr6.gt) goto loc_825E7CF4;
loc_825E7D4C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7d88
	if (!cr0.lt) goto loc_825E7D88;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7D88:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// stw r11,21220(r27)
	PPC_STORE_U32(r27.u32 + 21220, r11.u32);
	// b 0x825e7d98
	goto loc_825E7D98;
loc_825E7D94:
	// stw r24,21220(r27)
	PPC_STORE_U32(r27.u32 + 21220, r24.u32);
loc_825E7D98:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e7e0c
	if (!cr6.lt) goto loc_825E7E0C;
loc_825E7DB4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7e0c
	if (cr6.eq) goto loc_825E7E0C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7dfc
	if (!cr0.lt) goto loc_825E7DFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7DFC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7db4
	if (cr6.gt) goto loc_825E7DB4;
loc_825E7E0C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7e48
	if (!cr0.lt) goto loc_825E7E48;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7E48:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,21216(r27)
	PPC_STORE_U32(r27.u32 + 21216, r30.u32);
	// beq cr6,0x825e7f10
	if (cr6.eq) goto loc_825E7F10;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825e7ec8
	if (!cr6.lt) goto loc_825E7EC8;
loc_825E7E70:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e7ec8
	if (cr6.eq) goto loc_825E7EC8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825e7eb8
	if (!cr0.lt) goto loc_825E7EB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7EB8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e7e70
	if (cr6.gt) goto loc_825E7E70;
loc_825E7EC8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7f04
	if (!cr0.lt) goto loc_825E7F04;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E7F04:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// stw r11,21224(r27)
	PPC_STORE_U32(r27.u32 + 21224, r11.u32);
	// b 0x825e7f14
	goto loc_825E7F14;
loc_825E7F10:
	// stw r24,21224(r27)
	PPC_STORE_U32(r27.u32 + 21224, r24.u32);
loc_825E7F14:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e7b40
	if (!cr6.eq) goto loc_825E7B40;
	// lwz r10,14772(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 14772);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r9,3436(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 3436);
	// addi r11,r11,28552
	r11.s64 = r11.s64 + 28552;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,14776(r27)
	PPC_STORE_U32(r27.u32 + 14776, r11.u32);
	// bne cr6,0x825e7f58
	if (!cr6.eq) goto loc_825E7F58;
	// lwz r11,3440(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3440);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r24
	r11.u64 = r24.u64;
	// beq cr6,0x825e7f5c
	if (cr6.eq) goto loc_825E7F5C;
loc_825E7F58:
	// li r11,1
	r11.s64 = 1;
loc_825E7F5C:
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,3432(r27)
	PPC_STORE_U32(r27.u32 + 3432, r11.u32);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r24,284(r27)
	PPC_STORE_U32(r27.u32 + 284, r24.u32);
	// bl 0x82603a38
	sub_82603A38(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825e6458
	sub_825E6458(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825e7fa0
	if (cr6.eq) goto loc_825E7FA0;
	// cmpwi cr6,r3,12
	cr6.compare<int32_t>(ctx.r3.s32, 12, xer);
	// bne cr6,0x825e7fa8
	if (!cr6.eq) goto loc_825E7FA8;
	// li r11,1
	r11.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,21336(r27)
	PPC_STORE_U32(r27.u32 + 21336, r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd38
	return;
loc_825E7FA0:
	// stw r24,21336(r27)
	PPC_STORE_U32(r27.u32 + 21336, r24.u32);
	// li r3,0
	ctx.r3.s64 = 0;
loc_825E7FA8:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825E7FB0"))) PPC_WEAK_FUNC(sub_825E7FB0);
PPC_FUNC_IMPL(__imp__sub_825E7FB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-1392(r1)
	ea = -1392 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r20,r4
	r20.u64 = ctx.r4.u64;
	// mr r21,r5
	r21.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// mr r17,r8
	r17.u64 = ctx.r8.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825e7fe8
	if (!cr6.eq) goto loc_825E7FE8;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239bd18
	return;
loc_825E7FE8:
	// lwz r11,3668(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3668);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8000
	if (!cr6.eq) goto loc_825E8000;
	// li r3,3
	ctx.r3.s64 = 3;
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239bd18
	return;
loc_825E8000:
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// li r19,0
	r19.s64 = 0;
	// stw r19,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r19.u32);
	// lwz r11,3924(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e808c
	if (cr6.eq) goto loc_825E808C;
	// lis r11,12889
	r11.s64 = 844693504;
	// ori r11,r11,21849
	r11.u64 = r11.u64 | 21849;
	// cmplw cr6,r20,r11
	cr6.compare<uint32_t>(r20.u32, r11.u32, xer);
	// beq cr6,0x825e8064
	if (cr6.eq) goto loc_825E8064;
	// lis r11,22870
	r11.s64 = 1498808320;
	// ori r11,r11,22869
	r11.u64 = r11.u64 | 22869;
	// cmplw cr6,r20,r11
	cr6.compare<uint32_t>(r20.u32, r11.u32, xer);
	// beq cr6,0x825e8064
	if (cr6.eq) goto loc_825E8064;
	// lis r11,21849
	r11.s64 = 1431896064;
	// ori r11,r11,22105
	r11.u64 = r11.u64 | 22105;
	// cmplw cr6,r20,r11
	cr6.compare<uint32_t>(r20.u32, r11.u32, xer);
	// beq cr6,0x825e8064
	if (cr6.eq) goto loc_825E8064;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x825e806c
	if (cr6.eq) goto loc_825E806C;
	// cmplwi cr6,r20,3
	cr6.compare<uint32_t>(r20.u32, 3, xer);
	// beq cr6,0x825e8064
	if (cr6.eq) goto loc_825E8064;
loc_825E8058:
	// li r3,5
	ctx.r3.s64 = 5;
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239bd18
	return;
loc_825E8064:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x825e8078
	if (!cr6.eq) goto loc_825E8078;
loc_825E806C:
	// clrlwi r11,r21,16
	r11.u64 = r21.u32 & 0xFFFF;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// beq cr6,0x825e8058
	if (cr6.eq) goto loc_825E8058;
loc_825E8078:
	// cmplwi cr6,r20,3
	cr6.compare<uint32_t>(r20.u32, 3, xer);
	// bne cr6,0x825e808c
	if (!cr6.eq) goto loc_825E808C;
	// clrlwi r11,r21,16
	r11.u64 = r21.u32 & 0xFFFF;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// beq cr6,0x825e8058
	if (cr6.eq) goto loc_825E8058;
loc_825E808C:
	// lwz r11,21428(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21428);
	// li r18,1
	r18.s64 = 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e80a8
	if (!cr6.eq) goto loc_825E80A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e5018
	sub_825E5018(ctx, base);
	// b 0x825e89e8
	goto loc_825E89E8;
loc_825E80A8:
	// lwz r11,3424(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3424);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e80d0
	if (!cr6.eq) goto loc_825E80D0;
	// lwz r11,21388(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21388);
	// stw r19,3424(r31)
	PPC_STORE_U32(r31.u32 + 3424, r19.u32);
	// sth r18,3684(r31)
	PPC_STORE_U16(r31.u32 + 3684, r18.u16);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e89e8
	if (!cr6.eq) goto loc_825E89E8;
	// stw r19,21388(r31)
	PPC_STORE_U32(r31.u32 + 21388, r19.u32);
	// b 0x825e85d8
	goto loc_825E85D8;
loc_825E80D0:
	// lhz r11,3684(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 3684);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e80e8
	if (!cr6.eq) goto loc_825E80E8;
	// li r3,9
	ctx.r3.s64 = 9;
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239bd18
	return;
loc_825E80E8:
	// lwz r11,15300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15300);
	// cmplwi cr6,r17,0
	cr6.compare<uint32_t>(r17.u32, 0, xer);
	// bne cr6,0x825e833c
	if (!cr6.eq) goto loc_825E833C;
	// li r18,1
	r18.s64 = 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e810c
	if (!cr6.eq) goto loc_825E810C;
	// lwz r11,15368(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15368);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e8114
	if (cr6.eq) goto loc_825E8114;
loc_825E810C:
	// stw r18,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r18.u32);
	// b 0x825e8118
	goto loc_825E8118;
loc_825E8114:
	// stw r19,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r19.u32);
loc_825E8118:
	// lwz r11,14792(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14792);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e81ac
	if (cr6.eq) goto loc_825E81AC;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e8138
	if (cr6.eq) goto loc_825E8138;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825e8180
	if (!cr6.eq) goto loc_825E8180;
loc_825E8138:
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825e8150
	if (!cr6.eq) goto loc_825E8150;
	// lwz r11,14796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14796);
	// stw r11,14800(r31)
	PPC_STORE_U32(r31.u32 + 14800, r11.u32);
	// b 0x825e8180
	goto loc_825E8180;
loc_825E8150:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825e8180
	if (cr6.eq) goto loc_825E8180;
	// lwz r11,14800(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14800);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8178
	if (!cr6.eq) goto loc_825E8178;
	// lwz r11,14796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14796);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e8178
	if (!cr6.eq) goto loc_825E8178;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f3a70
	sub_825F3A70(ctx, base);
loc_825E8178:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261ab70
	sub_8261AB70(ctx, base);
loc_825E8180:
	// lwz r11,14796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14796);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e81ac
	if (cr6.eq) goto loc_825E81AC;
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e81a4
	if (!cr6.eq) goto loc_825E81A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260ca00
	sub_8260CA00(ctx, base);
	// stw r18,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r18.u32);
loc_825E81A4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261acb0
	sub_8261ACB0(ctx, base);
loc_825E81AC:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825e81dc
	if (!cr6.eq) goto loc_825E81DC;
	// lwz r11,21212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21212);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e81d0
	if (!cr6.eq) goto loc_825E81D0;
	// lwz r11,21216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21216);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e81dc
	if (cr6.eq) goto loc_825E81DC;
loc_825E81D0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261b0c0
	sub_8261B0C0(ctx, base);
	// stw r18,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r18.u32);
loc_825E81DC:
	// lwz r11,14820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14820);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e82cc
	if (cr6.eq) goto loc_825E82CC;
	// lwz r11,14824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e82cc
	if (cr6.eq) goto loc_825E82CC;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
	// lwz r29,14824(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rotlwi r10,r29,0
	ctx.r10.u64 = __builtin_rotateleft32(r29.u32, 0);
	// lwz r28,14828(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r30,220(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// stw r19,14824(r31)
	PPC_STORE_U32(r31.u32 + 14824, r19.u32);
	// stw r29,14828(r31)
	PPC_STORE_U32(r31.u32 + 14828, r29.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// beq cr6,0x825e8284
	if (cr6.eq) goto loc_825E8284;
	// lwz r9,3752(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3752);
	// lwz r8,3748(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3748);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwz r7,3744(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3744);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lwz r11,14904(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// lwz r6,3784(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// lwz r5,3780(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r4,3776(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// add r4,r10,r4
	ctx.r4.u64 = ctx.r10.u64 + ctx.r4.u64;
	// bl 0x82608e60
	sub_82608E60(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f3978
	sub_825F3978(ctx, base);
	// b 0x825e82c0
	goto loc_825E82C0;
loc_825E8284:
	// lwz r9,3784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r8,3780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwz r7,3776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lwz r11,14904(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// add r4,r10,r4
	ctx.r4.u64 = ctx.r10.u64 + ctx.r4.u64;
	// bl 0x82608e60
	sub_82608E60(ctx, base);
loc_825E82C0:
	// stw r28,14828(r31)
	PPC_STORE_U32(r31.u32 + 14828, r28.u32);
	// stw r29,14824(r31)
	PPC_STORE_U32(r31.u32 + 14824, r29.u32);
	// stw r18,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r18.u32);
loc_825E82CC:
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e832c
	if (!cr6.gt) goto loc_825E832C;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825e82ec
	if (cr6.eq) goto loc_825E82EC;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825e830c
	if (!cr6.eq) goto loc_825E830C;
loc_825E82EC:
	// lwz r11,3452(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3452);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e8324
	if (cr6.eq) goto loc_825E8324;
	// lwz r11,292(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 292);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825e8324
	if (cr6.eq) goto loc_825E8324;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825e8324
	if (cr6.eq) goto loc_825E8324;
loc_825E830C:
	// mr r11,r18
	r11.u64 = r18.u64;
	// stw r18,15500(r31)
	PPC_STORE_U32(r31.u32 + 15500, r18.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,3396(r31)
	PPC_STORE_U32(r31.u32 + 3396, r11.u32);
	// bl 0x825e5078
	sub_825E5078(ctx, base);
	// b 0x825e8e40
	goto loc_825E8E40;
loc_825E8324:
	// mr r11,r19
	r11.u64 = r19.u64;
	// stw r11,3396(r31)
	PPC_STORE_U32(r31.u32 + 3396, r11.u32);
loc_825E832C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r18,15500(r31)
	PPC_STORE_U32(r31.u32 + 15500, r18.u32);
	// bl 0x825e5078
	sub_825E5078(ctx, base);
	// b 0x825e8e40
	goto loc_825E8E40;
loc_825E833C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,2
	r11.s64 = 2;
	// stw r11,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r11.u32);
	// bne cr6,0x825e8358
	if (!cr6.eq) goto loc_825E8358;
	// lwz r11,15368(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15368);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e84f0
	if (cr6.eq) goto loc_825E84F0;
loc_825E8358:
	// lwz r11,15308(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r10,15312(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// lwz r4,3356(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// srawi r8,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r8.s64 = r11.s32 >> 4;
	// stw r19,220(r31)
	PPC_STORE_U32(r31.u32 + 220, r19.u32);
	// srawi r3,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r10.s32 >> 4;
	// stw r19,224(r31)
	PPC_STORE_U32(r31.u32 + 224, r19.u32);
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
	// stw r10,92(r31)
	PPC_STORE_U32(r31.u32 + 92, ctx.r10.u32);
	// stw r9,208(r31)
	PPC_STORE_U32(r31.u32 + 208, ctx.r9.u32);
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r8,136(r31)
	PPC_STORE_U32(r31.u32 + 136, ctx.r8.u32);
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,204(r31)
	PPC_STORE_U32(r31.u32 + 204, r11.u32);
	// stw r3,140(r31)
	PPC_STORE_U32(r31.u32 + 140, ctx.r3.u32);
	// stw r9,228(r31)
	PPC_STORE_U32(r31.u32 + 228, ctx.r9.u32);
	// stw r8,232(r31)
	PPC_STORE_U32(r31.u32 + 232, ctx.r8.u32);
	// bl 0x8263a9f8
	sub_8263A9F8(ctx, base);
	// lwz r6,216(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// li r18,1
	r18.s64 = 1;
	// lwz r7,3356(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// twllei r7,0
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// stw r6,116(r31)
	PPC_STORE_U32(r31.u32 + 116, ctx.r6.u32);
	// rlwinm r4,r10,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,212(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// divwu r7,r8,r7
	ctx.r7.u32 = ctx.r8.u32 / ctx.r7.u32;
	// rlwinm r6,r11,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r5,88(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// stw r3,3812(r31)
	PPC_STORE_U32(r31.u32 + 3812, ctx.r3.u32);
	// stw r3,3848(r31)
	PPC_STORE_U32(r31.u32 + 3848, ctx.r3.u32);
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
	// stw r7,3816(r31)
	PPC_STORE_U32(r31.u32 + 3816, ctx.r7.u32);
	// stw r10,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r10.u32);
	// stw r9,104(r31)
	PPC_STORE_U32(r31.u32 + 104, ctx.r9.u32);
	// stw r6,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r6.u32);
	// stw r4,112(r31)
	PPC_STORE_U32(r31.u32 + 112, ctx.r4.u32);
	// bne cr6,0x825e8410
	if (!cr6.eq) goto loc_825E8410;
	// lwz r11,92(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// mr r11,r18
	r11.u64 = r18.u64;
	// beq cr6,0x825e8414
	if (cr6.eq) goto loc_825E8414;
loc_825E8410:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_825E8414:
	// lwz r9,15308(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// rlwinm r8,r8,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r7,15312(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// stw r11,120(r31)
	PPC_STORE_U32(r31.u32 + 120, r11.u32);
	// addi r11,r9,15
	r11.s64 = ctx.r9.s64 + 15;
	// addi r10,r7,15
	ctx.r10.s64 = ctx.r7.s64 + 15;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// srawi r10,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 4;
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// mullw r8,r10,r11
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,128(r31)
	PPC_STORE_U32(r31.u32 + 128, r11.u32);
	// stw r10,132(r31)
	PPC_STORE_U32(r31.u32 + 132, ctx.r10.u32);
	// stw r8,124(r31)
	PPC_STORE_U32(r31.u32 + 124, ctx.r8.u32);
	// bne cr6,0x825e8460
	if (!cr6.eq) goto loc_825E8460;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// cmplw cr6,r7,r11
	cr6.compare<uint32_t>(ctx.r7.u32, r11.u32, xer);
	// mr r11,r18
	r11.u64 = r18.u64;
	// beq cr6,0x825e8464
	if (cr6.eq) goto loc_825E8464;
loc_825E8460:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_825E8464:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r11,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r11.u32);
	// stw r20,15488(r31)
	PPC_STORE_U32(r31.u32 + 15488, r20.u32);
	// sth r21,15492(r31)
	PPC_STORE_U16(r31.u32 + 15492, r21.u16);
	// stw r30,15504(r31)
	PPC_STORE_U32(r31.u32 + 15504, r30.u32);
	// beq cr6,0x825e8488
	if (cr6.eq) goto loc_825E8488;
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// beq cr6,0x825e8488
	if (cr6.eq) goto loc_825E8488;
	// stw r19,15504(r31)
	PPC_STORE_U32(r31.u32 + 15504, r19.u32);
loc_825E8488:
	// lwz r11,15504(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15504);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825e84b0
	if (cr6.eq) goto loc_825E84B0;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x825e84b0
	if (cr6.eq) goto loc_825E84B0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// mr r11,r27
	r11.u64 = r27.u64;
	// bne cr6,0x825e84c0
	if (!cr6.eq) goto loc_825E84C0;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// b 0x825e84c0
	goto loc_825E84C0;
loc_825E84B0:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// mr r11,r27
	r11.u64 = r27.u64;
	// bne cr6,0x825e84c0
	if (!cr6.eq) goto loc_825E84C0;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_825E84C0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,15496(r31)
	PPC_STORE_U32(r31.u32 + 15496, r11.u32);
	// bl 0x82638e48
	sub_82638E48(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e8e54
	if (!cr6.eq) goto loc_825E8E54;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// stw r18,15484(r31)
	PPC_STORE_U32(r31.u32 + 15484, r18.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82638ce0
	sub_82638CE0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e8e54
	if (!cr6.eq) goto loc_825E8E54;
	// b 0x825e8e40
	goto loc_825E8E40;
loc_825E84F0:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// stw r20,15488(r31)
	PPC_STORE_U32(r31.u32 + 15488, r20.u32);
	// sth r21,15492(r31)
	PPC_STORE_U16(r31.u32 + 15492, r21.u16);
	// mr r11,r27
	r11.u64 = r27.u64;
	// stw r30,15504(r31)
	PPC_STORE_U32(r31.u32 + 15504, r30.u32);
	// bne cr6,0x825e850c
	if (!cr6.eq) goto loc_825E850C;
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
loc_825E850C:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r11,15496(r31)
	PPC_STORE_U32(r31.u32 + 15496, r11.u32);
	// beq cr6,0x825e8524
	if (cr6.eq) goto loc_825E8524;
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// beq cr6,0x825e8524
	if (cr6.eq) goto loc_825E8524;
	// stw r19,15504(r31)
	PPC_STORE_U32(r31.u32 + 15504, r19.u32);
loc_825E8524:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82638e48
	sub_82638E48(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e8e54
	if (!cr6.eq) goto loc_825E8E54;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r18,15484(r31)
	PPC_STORE_U32(r31.u32 + 15484, r18.u32);
	// bl 0x825e5d58
	sub_825E5D58(ctx, base);
	// lwz r11,3644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3644);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// bne cr6,0x825e856c
	if (!cr6.eq) goto loc_825E856C;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8588
	if (!cr6.eq) goto loc_825E8588;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260cf90
	sub_8260CF90(ctx, base);
	// lwz r11,15548(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15548);
	// stw r11,3644(r31)
	PPC_STORE_U32(r31.u32 + 3644, r11.u32);
	// b 0x825e8588
	goto loc_825E8588;
loc_825E856C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825e8580
	if (cr6.lt) goto loc_825E8580;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bgt cr6,0x825e8580
	if (cr6.gt) goto loc_825E8580;
	// stw r11,15548(r31)
	PPC_STORE_U32(r31.u32 + 15548, r11.u32);
loc_825E8580:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260cf90
	sub_8260CF90(ctx, base);
loc_825E8588:
	// lwz r11,15508(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15508);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e85d8
	if (cr6.eq) goto loc_825E85D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2d98
	sub_825E2D98(ctx, base);
	// lwz r11,3924(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// lwz r5,140(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r4,0
	ctx.r4.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x825e85c4
	if (cr6.eq) goto loc_825E85C4;
	// lwz r11,15860(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15860);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x825e85cc
	goto loc_825E85CC;
loc_825E85C4:
	// li r6,0
	ctx.r6.s64 = 0;
	// bl 0x8260cb60
	sub_8260CB60(ctx, base);
loc_825E85CC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r18,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r18.u32);
	// bl 0x825e2df8
	sub_825E2DF8(ctx, base);
loc_825E85D8:
	// lwz r11,14792(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14792);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e866c
	if (cr6.eq) goto loc_825E866C;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e85f8
	if (cr6.eq) goto loc_825E85F8;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825e8640
	if (!cr6.eq) goto loc_825E8640;
loc_825E85F8:
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825e8610
	if (!cr6.eq) goto loc_825E8610;
	// lwz r11,14796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14796);
	// stw r11,14800(r31)
	PPC_STORE_U32(r31.u32 + 14800, r11.u32);
	// b 0x825e8640
	goto loc_825E8640;
loc_825E8610:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825e8640
	if (cr6.eq) goto loc_825E8640;
	// lwz r11,14800(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14800);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8638
	if (!cr6.eq) goto loc_825E8638;
	// lwz r11,14796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14796);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e8638
	if (!cr6.eq) goto loc_825E8638;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f3a70
	sub_825F3A70(ctx, base);
loc_825E8638:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261ab70
	sub_8261AB70(ctx, base);
loc_825E8640:
	// lwz r11,14796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14796);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e866c
	if (cr6.eq) goto loc_825E866C;
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8664
	if (!cr6.eq) goto loc_825E8664;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260ca00
	sub_8260CA00(ctx, base);
	// stw r18,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r18.u32);
loc_825E8664:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82610720
	sub_82610720(ctx, base);
loc_825E866C:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825e869c
	if (!cr6.eq) goto loc_825E869C;
	// lwz r11,21212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21212);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8690
	if (!cr6.eq) goto loc_825E8690;
	// lwz r11,21216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21216);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e869c
	if (cr6.eq) goto loc_825E869C;
loc_825E8690:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261b0c0
	sub_8261B0C0(ctx, base);
	// stw r18,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r18.u32);
loc_825E869C:
	// lwz r11,14824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e898c
	if (cr6.eq) goto loc_825E898C;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
	// lwz r29,14824(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// lwz r28,14828(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,15900(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15900);
	// stw r19,14824(r31)
	PPC_STORE_U32(r31.u32 + 14824, r19.u32);
	// stw r29,14828(r31)
	PPC_STORE_U32(r31.u32 + 14828, r29.u32);
	// beq cr6,0x825e8830
	if (cr6.eq) goto loc_825E8830;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e87d0
	if (cr6.eq) goto loc_825E87D0;
	// lwz r11,19712(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19712);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e87d0
	if (!cr6.eq) goto loc_825E87D0;
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// li r8,8
	ctx.r8.s64 = 8;
	// lwz r9,584(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 584);
	// mulli r11,r9,68
	r11.s64 = ctx.r9.s64 * 68;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,584(r10)
	PPC_STORE_U32(ctx.r10.u32 + 584, ctx.r9.u32);
	// stw r8,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r8.u32);
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// stw r10,64(r11)
	PPC_STORE_U32(r11.u32 + 64, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,14904(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// stw r10,68(r11)
	PPC_STORE_U32(r11.u32 + 68, ctx.r10.u32);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// stw r10,72(r11)
	PPC_STORE_U32(r11.u32 + 72, ctx.r10.u32);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// stw r10,76(r11)
	PPC_STORE_U32(r11.u32 + 76, ctx.r10.u32);
	// lwz r10,14824(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// stw r10,80(r11)
	PPC_STORE_U32(r11.u32 + 80, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// stw r10,84(r11)
	PPC_STORE_U32(r11.u32 + 84, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,14884(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14884);
	// stw r10,88(r11)
	PPC_STORE_U32(r11.u32 + 88, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,14888(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14888);
	// stw r10,92(r11)
	PPC_STORE_U32(r11.u32 + 92, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,14872(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14872);
	// stw r10,104(r11)
	PPC_STORE_U32(r11.u32 + 104, ctx.r10.u32);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// stw r10,96(r11)
	PPC_STORE_U32(r11.u32 + 96, ctx.r10.u32);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// stw r10,100(r11)
	PPC_STORE_U32(r11.u32 + 100, ctx.r10.u32);
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// stw r10,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r10.u32);
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// stw r10,60(r11)
	PPC_STORE_U32(r11.u32 + 60, ctx.r10.u32);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// stw r10,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r10.u32);
	// b 0x825e8980
	goto loc_825E8980;
loc_825E87D0:
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// lwz r9,3752(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3752);
	// lwz r8,3748(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3748);
	// lwz r7,3744(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3744);
	// lwz r30,220(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r6,3784(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r5,3780(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r4,3776(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// lwz r11,14904(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// add r4,r10,r4
	ctx.r4.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// bl 0x82608e60
	sub_82608E60(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f3978
	sub_825F3978(ctx, base);
	// b 0x825e8980
	goto loc_825E8980;
loc_825E8830:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e892c
	if (cr6.eq) goto loc_825E892C;
	// lwz r11,19712(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19712);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e892c
	if (!cr6.eq) goto loc_825E892C;
	// lwz r11,3704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// li r8,8
	ctx.r8.s64 = 8;
	// stw r19,584(r11)
	PPC_STORE_U32(r11.u32 + 584, r19.u32);
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// lwz r9,584(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 584);
	// mulli r11,r9,68
	r11.s64 = ctx.r9.s64 * 68;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,584(r10)
	PPC_STORE_U32(ctx.r10.u32 + 584, ctx.r9.u32);
	// stw r8,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r8.u32);
	// lwz r10,3688(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// stw r10,64(r11)
	PPC_STORE_U32(r11.u32 + 64, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,14904(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// stw r10,68(r11)
	PPC_STORE_U32(r11.u32 + 68, ctx.r10.u32);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// stw r10,72(r11)
	PPC_STORE_U32(r11.u32 + 72, ctx.r10.u32);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// stw r10,76(r11)
	PPC_STORE_U32(r11.u32 + 76, ctx.r10.u32);
	// lwz r10,14824(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// stw r10,80(r11)
	PPC_STORE_U32(r11.u32 + 80, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// stw r10,84(r11)
	PPC_STORE_U32(r11.u32 + 84, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,14884(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14884);
	// stw r10,88(r11)
	PPC_STORE_U32(r11.u32 + 88, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,14888(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14888);
	// stw r10,92(r11)
	PPC_STORE_U32(r11.u32 + 92, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,14872(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14872);
	// stw r10,104(r11)
	PPC_STORE_U32(r11.u32 + 104, ctx.r10.u32);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// stw r10,96(r11)
	PPC_STORE_U32(r11.u32 + 96, ctx.r10.u32);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// stw r10,100(r11)
	PPC_STORE_U32(r11.u32 + 100, ctx.r10.u32);
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// stw r10,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r10.u32);
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// stw r10,60(r11)
	PPC_STORE_U32(r11.u32 + 60, ctx.r10.u32);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// stw r10,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r10.u32);
	// b 0x825e8980
	goto loc_825E8980;
loc_825E892C:
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// lwz r9,3784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r8,3780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r7,3776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r30,220(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// lwz r11,14904(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// add r4,r10,r4
	ctx.r4.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// bl 0x82608e60
	sub_82608E60(ctx, base);
loc_825E8980:
	// stw r28,14828(r31)
	PPC_STORE_U32(r31.u32 + 14828, r28.u32);
	// stw r29,14824(r31)
	PPC_STORE_U32(r31.u32 + 14824, r29.u32);
	// stw r18,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r18.u32);
loc_825E898C:
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// stw r19,21432(r31)
	PPC_STORE_U32(r31.u32 + 21432, r19.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e89e0
	if (!cr6.gt) goto loc_825E89E0;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825e89b0
	if (cr6.eq) goto loc_825E89B0;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825e89d0
	if (!cr6.eq) goto loc_825E89D0;
loc_825E89B0:
	// lwz r11,3452(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3452);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e89d8
	if (cr6.eq) goto loc_825E89D8;
	// lwz r11,292(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 292);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825e89d8
	if (cr6.eq) goto loc_825E89D8;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825e89d8
	if (cr6.eq) goto loc_825E89D8;
loc_825E89D0:
	// mr r11,r18
	r11.u64 = r18.u64;
	// b 0x825e89dc
	goto loc_825E89DC;
loc_825E89D8:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_825E89DC:
	// stw r11,3396(r31)
	PPC_STORE_U32(r31.u32 + 3396, r11.u32);
loc_825E89E0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e5078
	sub_825E5078(ctx, base);
loc_825E89E8:
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r1,184
	ctx.r4.s64 = ctx.r1.s64 + 184;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e55c8
	sub_825E55C8(ctx, base);
	// lwz r25,15472(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r25,7
	cr6.compare<int32_t>(r25.s32, 7, xer);
	// beq cr6,0x825e8a0c
	if (cr6.eq) goto loc_825E8A0C;
	// cmpwi cr6,r25,6
	cr6.compare<int32_t>(r25.s32, 6, xer);
	// bne cr6,0x825e8e28
	if (!cr6.eq) goto loc_825E8E28;
loc_825E8A0C:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8e24
	if (!cr6.eq) goto loc_825E8E24;
	// lwz r11,3716(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3716);
	// li r29,64
	r29.s64 = 64;
	// lwz r10,21184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21184);
	// stw r19,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r19.u32);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// lwz r24,0(r11)
	r24.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r23,4(r11)
	r23.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r22,8(r11)
	r22.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// bne cr6,0x825e8a8c
	if (!cr6.eq) goto loc_825E8A8C;
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e8a8c
	if (!cr6.gt) goto loc_825E8A8C;
	// ld r11,3576(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 3576);
	// cmpdi cr6,r11,1
	cr6.compare<int64_t>(r11.s64, 1, xer);
	// ble cr6,0x825e8a8c
	if (!cr6.gt) goto loc_825E8A8C;
	// lwz r10,21352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21352);
	// lwz r11,21380(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21380);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825e8a6c
	if (cr6.gt) goto loc_825E8A6C;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_825E8A6C:
	// lwz r11,21384(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21384);
	// lwz r10,21356(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21356);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825e8a84
	if (!cr6.gt) goto loc_825E8A84;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// b 0x825e8a94
	goto loc_825E8A94;
loc_825E8A84:
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// b 0x825e8a94
	goto loc_825E8A94;
loc_825E8A8C:
	// lwz r8,156(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// lwz r4,160(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 160);
loc_825E8A94:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// bne cr6,0x825e8aa4
	if (!cr6.eq) goto loc_825E8AA4;
	// mr r5,r8
	ctx.r5.u64 = ctx.r8.u64;
loc_825E8AA4:
	// lwz r9,15300(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15300);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// stw r5,15496(r31)
	PPC_STORE_U32(r31.u32 + 15496, ctx.r5.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825e8ac8
	if (!cr6.eq) goto loc_825E8AC8;
	// addi r11,r8,15
	r11.s64 = ctx.r8.s64 + 15;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// rlwinm r8,r11,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
loc_825E8AC8:
	// lwz r30,3924(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825e8af8
	if (cr6.eq) goto loc_825E8AF8;
	// srawi r11,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r11.s64 = ctx.r8.s32 >> 2;
	// addi r28,r8,64
	r28.s64 = ctx.r8.s64 + 64;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// mr r27,r11
	r27.u64 = r11.u64;
	// mr r26,r11
	r26.u64 = r11.u64;
	// b 0x825e8b0c
	goto loc_825E8B0C;
loc_825E8AF8:
	// mr r28,r19
	r28.u64 = r19.u64;
	// mr r27,r19
	r27.u64 = r19.u64;
	// mr r26,r19
	r26.u64 = r19.u64;
	// li r6,32
	ctx.r6.s64 = 32;
	// li r7,32
	ctx.r7.s64 = 32;
loc_825E8B0C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825e8b20
	if (!cr6.eq) goto loc_825E8B20;
	// lwz r11,15368(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15368);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e8b2c
	if (cr6.eq) goto loc_825E8B2C;
loc_825E8B20:
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// mr r29,r19
	r29.u64 = r19.u64;
loc_825E8B2C:
	// cmplwi cr6,r20,3
	cr6.compare<uint32_t>(r20.u32, 3, xer);
	// bne cr6,0x825e8ba4
	if (!cr6.eq) goto loc_825E8BA4;
	// clrlwi r11,r21,16
	r11.u64 = r21.u32 & 0xFFFF;
	// li r9,31
	ctx.r9.s64 = 31;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bne cr6,0x825e8b5c
	if (!cr6.eq) goto loc_825E8B5C;
	// lis r3,0
	ctx.r3.s64 = 0;
	// stw r9,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r9.u32);
	// ori r3,r3,63488
	ctx.r3.u64 = ctx.r3.u64 | 63488;
	// stw r3,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r3.u32);
	// li r3,2016
	ctx.r3.s64 = 2016;
	// stw r3,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r3.u32);
loc_825E8B5C:
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bne cr6,0x825e8b7c
	if (!cr6.eq) goto loc_825E8B7C;
	// li r11,31744
	r11.s64 = 31744;
	// stw r9,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r9.u32);
	// li r21,16
	r21.s64 = 16;
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r11.u32);
	// li r11,992
	r11.s64 = 992;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
loc_825E8B7C:
	// clrlwi r11,r21,16
	r11.u64 = r21.u32 & 0xFFFF;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// bne cr6,0x825e8ba4
	if (!cr6.eq) goto loc_825E8BA4;
	// lis r11,255
	r11.s64 = 16711680;
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r11.u32);
	// lis r11,0
	r11.s64 = 0;
	// ori r11,r11,65280
	r11.u64 = r11.u64 | 65280;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r11.u32);
	// li r11,255
	r11.s64 = 255;
	// stw r11,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r11.u32);
loc_825E8BA4:
	// li r9,40
	ctx.r9.s64 = 40;
	// lwz r11,21336(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21336);
	// stw r5,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r5.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// stw r9,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r9.u32);
	// bne cr6,0x825e8bc4
	if (!cr6.eq) goto loc_825E8BC4;
	// lwz r11,21416(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21416);
loc_825E8BC4:
	// clrlwi r3,r21,16
	ctx.r3.u64 = r21.u32 & 0xFFFF;
	// lwz r16,15504(r31)
	r16.u64 = PPC_LOAD_U32(r31.u32 + 15504);
	// stw r11,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, r11.u32);
	// mullw r5,r3,r5
	ctx.r5.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r5.s32);
	// sth r18,204(r1)
	PPC_STORE_U16(ctx.r1.u32 + 204, r18.u16);
	// sth r21,206(r1)
	PPC_STORE_U16(ctx.r1.u32 + 206, r21.u16);
	// stw r20,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r20.u32);
	// stw r19,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r19.u32);
	// stw r19,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r19.u32);
	// stw r19,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r19.u32);
	// stw r19,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r19.u32);
	// mullw r5,r5,r4
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// rlwinm r5,r5,29,3,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0x1FFFFFFF;
	// cmpwi cr6,r16,2
	cr6.compare<int32_t>(r16.s32, 2, xer);
	// stw r5,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r5.u32);
	// bne cr6,0x825e8c0c
	if (!cr6.eq) goto loc_825E8C0C;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// stw r11,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, r11.u32);
loc_825E8C0C:
	// add r11,r8,r29
	r11.u64 = ctx.r8.u64 + r29.u64;
	// stw r9,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r9.u32);
	// add r9,r4,r29
	ctx.r9.u64 = ctx.r4.u64 + r29.u64;
	// lwz r3,20952(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20952);
	// li r8,12
	ctx.r8.s64 = 12;
	// sth r18,156(r1)
	PPC_STORE_U16(ctx.r1.u32 + 156, r18.u16);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r11.u32);
	// stw r9,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r9.u32);
	// sth r8,158(r1)
	PPC_STORE_U16(ctx.r1.u32 + 158, ctx.r8.u16);
	// bne cr6,0x825e8ce8
	if (!cr6.eq) goto loc_825E8CE8;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825e8c4c
	if (cr6.eq) goto loc_825E8C4C;
	// lis r8,12593
	ctx.r8.s64 = 825294848;
	// ori r8,r8,13392
	ctx.r8.u64 = ctx.r8.u64 | 13392;
	// b 0x825e8c54
	goto loc_825E8C54;
loc_825E8C4C:
	// lis r8,12338
	ctx.r8.s64 = 808583168;
	// ori r8,r8,13385
	ctx.r8.u64 = ctx.r8.u64 | 13385;
loc_825E8C54:
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// stw r8,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r8.u32);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r25,7
	cr6.compare<int32_t>(r25.s32, 7, xer);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// bne cr6,0x825e8c9c
	if (!cr6.eq) goto loc_825E8C9C;
	// ld r11,3576(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 3576);
	// cmpdi cr6,r11,1
	cr6.compare<int64_t>(r11.s64, 1, xer);
	// ble cr6,0x825e8c8c
	if (!cr6.gt) goto loc_825E8C8C;
	// lwz r11,20976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20976);
	// b 0x825e8c90
	goto loc_825E8C90;
loc_825E8C8C:
	// lwz r11,20972(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20972);
loc_825E8C90:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,2
	r11.s64 = 2;
	// beq cr6,0x825e8ca0
	if (cr6.eq) goto loc_825E8CA0;
loc_825E8C9C:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_825E8CA0:
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r26,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r26.u32);
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// stw r27,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r27.u32);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x82634d98
	sub_82634D98(ctx, base);
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// stw r3,20952(r31)
	PPC_STORE_U32(r31.u32 + 20952, ctx.r3.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e8d84
	if (cr6.eq) goto loc_825E8D84;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239bd18
	return;
loc_825E8CE8:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825e8cfc
	if (cr6.eq) goto loc_825E8CFC;
	// lis r8,12593
	ctx.r8.s64 = 825294848;
	// ori r8,r8,13392
	ctx.r8.u64 = ctx.r8.u64 | 13392;
	// b 0x825e8d04
	goto loc_825E8D04;
loc_825E8CFC:
	// lis r8,12338
	ctx.r8.s64 = 808583168;
	// ori r8,r8,13385
	ctx.r8.u64 = ctx.r8.u64 | 13385;
loc_825E8D04:
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// stw r8,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r8.u32);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r25,7
	cr6.compare<int32_t>(r25.s32, 7, xer);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// bne cr6,0x825e8d4c
	if (!cr6.eq) goto loc_825E8D4C;
	// ld r11,3576(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 3576);
	// cmpdi cr6,r11,1
	cr6.compare<int64_t>(r11.s64, 1, xer);
	// ble cr6,0x825e8d3c
	if (!cr6.gt) goto loc_825E8D3C;
	// lwz r11,20976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20976);
	// b 0x825e8d40
	goto loc_825E8D40;
loc_825E8D3C:
	// lwz r11,20972(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20972);
loc_825E8D40:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,2
	r11.s64 = 2;
	// beq cr6,0x825e8d50
	if (cr6.eq) goto loc_825E8D50;
loc_825E8D4C:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_825E8D50:
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r26,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r26.u32);
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// stw r27,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r27.u32);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x826350d8
	sub_826350D8(ctx, base);
	// stw r3,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r3.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e8e54
	if (!cr6.eq) goto loc_825E8E54;
loc_825E8D84:
	// lwz r8,20952(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 20952);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x825e8058
	if (cr6.eq) goto loc_825E8058;
	// lwz r11,3924(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e8dc0
	if (cr6.eq) goto loc_825E8DC0;
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
	// addi r10,r27,1
	ctx.r10.s64 = r27.s64 + 1;
	// addi r7,r26,1
	ctx.r7.s64 = r26.s64 + 1;
	// rlwinm r9,r11,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r11,r7,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// add r24,r9,r24
	r24.u64 = ctx.r9.u64 + r24.u64;
	// add r23,r10,r23
	r23.u64 = ctx.r10.u64 + r23.u64;
	// add r22,r11,r22
	r22.u64 = r11.u64 + r22.u64;
loc_825E8DC0:
	// lwz r11,21440(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21440);
	// stw r11,52(r8)
	PPC_STORE_U32(ctx.r8.u32 + 52, r11.u32);
	// lwz r11,21440(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21440);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e8e04
	if (!cr6.eq) goto loc_825E8E04;
	// lwz r11,21444(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21444);
	// stw r11,40(r8)
	PPC_STORE_U32(ctx.r8.u32 + 40, r11.u32);
	// lwz r11,21448(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21448);
	// stw r11,44(r8)
	PPC_STORE_U32(ctx.r8.u32 + 44, r11.u32);
	// lwz r11,21452(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21452);
	// stw r11,48(r8)
	PPC_STORE_U32(ctx.r8.u32 + 48, r11.u32);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// stw r11,14624(r8)
	PPC_STORE_U32(ctx.r8.u32 + 14624, r11.u32);
	// lwz r11,21460(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21460);
	// stw r11,14628(r8)
	PPC_STORE_U32(ctx.r8.u32 + 14628, r11.u32);
	// lwz r11,21464(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21464);
	// stw r11,14632(r8)
	PPC_STORE_U32(ctx.r8.u32 + 14632, r11.u32);
loc_825E8E04:
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// lwz r3,20952(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20952);
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// bl 0x826350a8
	sub_826350A8(ctx, base);
	// stw r19,21184(r31)
	PPC_STORE_U32(r31.u32 + 21184, r19.u32);
	// b 0x825e8e44
	goto loc_825E8E44;
loc_825E8E24:
	// cmpwi cr6,r25,6
	cr6.compare<int32_t>(r25.s32, 6, xer);
loc_825E8E28:
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82638ce0
	sub_82638CE0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e8e54
	if (!cr6.eq) goto loc_825E8E54;
	// stw r19,21184(r31)
	PPC_STORE_U32(r31.u32 + 21184, r19.u32);
loc_825E8E40:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825E8E44:
	// lhz r11,3684(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 3684);
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// sth r11,3684(r31)
	PPC_STORE_U16(r31.u32 + 3684, r11.u16);
loc_825E8E54:
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_825E8E5C"))) PPC_WEAK_FUNC(sub_825E8E5C);
PPC_FUNC_IMPL(__imp__sub_825E8E5C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E8E60"))) PPC_WEAK_FUNC(sub_825E8E60);
PPC_FUNC_IMPL(__imp__sub_825E8E60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -104, f30.u64);
	// stfd f31,-96(r1)
	PPC_STORE_U64(ctx.r1.u32 + -96, f31.u64);
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r27,0
	r27.s64 = 0;
	// li r23,1
	r23.s64 = 1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r27.u32);
	// stw r27,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r27.u32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stw r23,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r23.u32);
	// bne cr6,0x825e8eb4
	if (!cr6.eq) goto loc_825E8EB4;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239bd30
	return;
loc_825E8EB4:
	// lwz r11,15300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15300);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8ee0
	if (!cr6.eq) goto loc_825E8EE0;
	// lwz r11,15368(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15368);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8ee0
	if (!cr6.eq) goto loc_825E8EE0;
loc_825E8ECC:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239bd30
	return;
loc_825E8EE0:
	// lwz r11,15552(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15552);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825e8ef8
	if (!cr6.gt) goto loc_825E8EF8;
	// lwz r11,3420(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3420);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,3420(r31)
	PPC_STORE_U32(r31.u32 + 3420, r11.u32);
loc_825E8EF8:
	// lhz r11,3684(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 3684);
	// stw r27,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r27.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e8f28
	if (cr6.eq) goto loc_825E8F28;
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r7,15504(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 15504);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r6,15496(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 15496);
	// lhz r5,15492(r31)
	ctx.r5.u64 = PPC_LOAD_U16(r31.u32 + 15492);
	// lwz r4,15488(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15488);
	// bl 0x825e7fb0
	sub_825E7FB0(ctx, base);
	// sth r27,3684(r31)
	PPC_STORE_U16(r31.u32 + 3684, r27.u16);
loc_825E8F28:
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// lwz r3,3340(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r7,r1,132
	ctx.r7.s64 = ctx.r1.s64 + 132;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,136
	ctx.r5.s64 = ctx.r1.s64 + 136;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82491030
	sub_82491030(ctx, base);
	// lwz r6,128(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r5,132(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r4,136(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x825e8f7c
	if (cr6.eq) goto loc_825E8F7C;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x825e8f68
	if (cr6.eq) goto loc_825E8F68;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x825e8f7c
	if (!cr6.eq) goto loc_825E8F7C;
loc_825E8F68:
	// li r3,11
	ctx.r3.s64 = 11;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239bd30
	return;
loc_825E8F7C:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x825e8f98
	if (!cr6.eq) goto loc_825E8F98;
loc_825E8F84:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239bd30
	return;
loc_825E8F98:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825e8fec
	if (!cr6.eq) goto loc_825E8FEC;
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// addi r7,r1,132
	ctx.r7.s64 = ctx.r1.s64 + 132;
	// addi r6,r1,136
	ctx.r6.s64 = ctx.r1.s64 + 136;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825fe4d8
	sub_825FE4D8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e96b4
	if (!cr6.eq) goto loc_825E96B4;
	// lwz r11,3672(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3672);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8fe0
	if (!cr6.eq) goto loc_825E8FE0;
	// li r3,3
	ctx.r3.s64 = 3;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239bd30
	return;
loc_825E8FE0:
	// lwz r5,132(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r4,136(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r6,128(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
loc_825E8FEC:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x825e8ffc
	if (!cr6.eq) goto loc_825E8FFC;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x825e8f84
	if (cr6.eq) goto loc_825E8F84;
loc_825E8FFC:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// addi r11,r11,-7
	r11.s64 = r11.s64 + -7;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bl 0x825eb668
	sub_825EB668(ctx, base);
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r10,128(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mr r29,r23
	r29.u64 = r23.u64;
	// mr r28,r27
	r28.u64 = r27.u64;
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// ld r10,3576(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 3576);
	// ld r11,3584(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 3584);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r27,3424(r31)
	PPC_STORE_U32(r31.u32 + 3424, r27.u32);
	// stw r23,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r23.u32);
	// std r10,3576(r31)
	PPC_STORE_U64(r31.u32 + 3576, ctx.r10.u64);
	// std r11,3584(r31)
	PPC_STORE_U64(r31.u32 + 3584, r11.u64);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e90b4
	if (!cr6.lt) goto loc_825E90B4;
loc_825E905C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e90b4
	if (cr6.eq) goto loc_825E90B4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r9,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825e90a4
	if (!cr0.lt) goto loc_825E90A4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E90A4:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825e905c
	if (cr6.gt) goto loc_825E905C;
loc_825E90B4:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r26,r11,r28
	r26.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x825e90f0
	if (!cr0.lt) goto loc_825E90F0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E90F0:
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// mr r29,r23
	r29.u64 = r23.u64;
	// stw r26,288(r31)
	PPC_STORE_U32(r31.u32 + 288, r26.u32);
	// mr r28,r27
	r28.u64 = r27.u64;
	// lwz r26,15304(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 15304);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e916c
	if (!cr6.lt) goto loc_825E916C;
loc_825E9114:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e916c
	if (cr6.eq) goto loc_825E916C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r9,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825e915c
	if (!cr0.lt) goto loc_825E915C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E915C:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825e9114
	if (cr6.gt) goto loc_825E9114;
loc_825E916C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x825e91a8
	if (!cr0.lt) goto loc_825E91A8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E91A8:
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// lwz r10,288(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 288);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,15304(r31)
	PPC_STORE_U32(r31.u32 + 15304, r11.u32);
	// bne cr6,0x825e94a8
	if (!cr6.eq) goto loc_825E94A8;
	// lwz r4,180(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r11,188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// lwz r9,200(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// srawi r8,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r4.s32 >> 4;
	// srawi r30,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r30.s64 = r11.s32 >> 4;
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,160(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// stw r23,1944(r31)
	PPC_STORE_U32(r31.u32 + 1944, r23.u32);
	// stw r27,220(r31)
	PPC_STORE_U32(r31.u32 + 220, r27.u32);
	// stw r9,216(r31)
	PPC_STORE_U32(r31.u32 + 216, ctx.r9.u32);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r8,136(r31)
	PPC_STORE_U32(r31.u32 + 136, ctx.r8.u32);
	// rlwinm r8,r4,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r30,140(r31)
	PPC_STORE_U32(r31.u32 + 140, r30.u32);
	// addi r9,r9,-8
	ctx.r9.s64 = ctx.r9.s64 + -8;
	// rlwinm r30,r10,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r27,224(r31)
	PPC_STORE_U32(r31.u32 + 224, r27.u32);
	// stw r4,204(r31)
	PPC_STORE_U32(r31.u32 + 204, ctx.r4.u32);
	// stw r11,212(r31)
	PPC_STORE_U32(r31.u32 + 212, r11.u32);
	// stw r10,208(r31)
	PPC_STORE_U32(r31.u32 + 208, ctx.r10.u32);
	// stw r9,236(r31)
	PPC_STORE_U32(r31.u32 + 236, ctx.r9.u32);
	// stw r8,228(r31)
	PPC_STORE_U32(r31.u32 + 228, ctx.r8.u32);
	// stw r30,232(r31)
	PPC_STORE_U32(r31.u32 + 232, r30.u32);
	// bl 0x82642518
	sub_82642518(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e96b4
	if (!cr6.eq) goto loc_825E96B4;
	// addi r11,r25,15
	r11.s64 = r25.s64 + 15;
	// stw r25,15308(r31)
	PPC_STORE_U32(r31.u32 + 15308, r25.u32);
	// addi r10,r24,15
	ctx.r10.s64 = r24.s64 + 15;
	// stw r24,15312(r31)
	PPC_STORE_U32(r31.u32 + 15312, r24.u32);
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// stw r23,3356(r31)
	PPC_STORE_U32(r31.u32 + 3356, r23.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r10,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 4;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r11,15316(r31)
	PPC_STORE_U32(r31.u32 + 15316, r11.u32);
	// stw r10,15320(r31)
	PPC_STORE_U32(r31.u32 + 15320, ctx.r10.u32);
	// bl 0x82648b58
	sub_82648B58(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e96b4
	if (!cr6.eq) goto loc_825E96B4;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x825e92ac
	if (cr6.lt) goto loc_825E92AC;
	// li r11,8
	r11.s64 = 8;
	// stw r27,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r27.u32);
	// addi r10,r31,2640
	ctx.r10.s64 = r31.s64 + 2640;
	// addi r9,r31,2680
	ctx.r9.s64 = r31.s64 + 2680;
	// stw r11,344(r31)
	PPC_STORE_U32(r31.u32 + 344, r11.u32);
	// stw r10,2904(r31)
	PPC_STORE_U32(r31.u32 + 2904, ctx.r10.u32);
	// stw r9,2916(r31)
	PPC_STORE_U32(r31.u32 + 2916, ctx.r9.u32);
	// stw r11,348(r31)
	PPC_STORE_U32(r31.u32 + 348, r11.u32);
	// stw r11,20004(r31)
	PPC_STORE_U32(r31.u32 + 20004, r11.u32);
	// stw r11,14804(r31)
	PPC_STORE_U32(r31.u32 + 14804, r11.u32);
	// stw r11,20940(r31)
	PPC_STORE_U32(r31.u32 + 20940, r11.u32);
loc_825E92AC:
	// lwz r11,15396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e92cc
	if (!cr6.eq) goto loc_825E92CC;
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r4,r11,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bl 0x825f50e0
	sub_825F50E0(ctx, base);
loc_825E92CC:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// beq cr6,0x825e92f8
	if (cr6.eq) goto loc_825E92F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f7da8
	sub_825F7DA8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825e93a0
	if (cr6.eq) goto loc_825E93A0;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239bd30
	return;
loc_825E92F8:
	// lwz r11,21160(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e8f84
	if (!cr6.eq) goto loc_825E8F84;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// stw r27,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r27.u32);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// stw r27,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r27.u32);
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// lis r7,-32138
	ctx.r7.s64 = -2106195968;
	// lis r6,-32138
	ctx.r6.s64 = -2106195968;
	// lis r5,-32138
	ctx.r5.s64 = -2106195968;
	// lis r4,-32138
	ctx.r4.s64 = -2106195968;
	// addi r11,r11,6272
	r11.s64 = r11.s64 + 6272;
	// addi r10,r10,6200
	ctx.r10.s64 = ctx.r10.s64 + 6200;
	// addi r9,r9,5628
	ctx.r9.s64 = ctx.r9.s64 + 5628;
	// addi r8,r8,6372
	ctx.r8.s64 = ctx.r8.s64 + 6372;
	// addi r7,r7,6336
	ctx.r7.s64 = ctx.r7.s64 + 6336;
	// addi r6,r6,6408
	ctx.r6.s64 = ctx.r6.s64 + 6408;
	// stw r11,1824(r31)
	PPC_STORE_U32(r31.u32 + 1824, r11.u32);
	// addi r5,r5,6444
	ctx.r5.s64 = ctx.r5.s64 + 6444;
	// stw r10,1828(r31)
	PPC_STORE_U32(r31.u32 + 1828, ctx.r10.u32);
	// addi r4,r4,6464
	ctx.r4.s64 = ctx.r4.s64 + 6464;
	// stw r9,1836(r31)
	PPC_STORE_U32(r31.u32 + 1836, ctx.r9.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r8,1840(r31)
	PPC_STORE_U32(r31.u32 + 1840, ctx.r8.u32);
	// stw r7,1844(r31)
	PPC_STORE_U32(r31.u32 + 1844, ctx.r7.u32);
	// stw r6,1848(r31)
	PPC_STORE_U32(r31.u32 + 1848, ctx.r6.u32);
	// stw r5,1864(r31)
	PPC_STORE_U32(r31.u32 + 1864, ctx.r5.u32);
	// stw r4,1868(r31)
	PPC_STORE_U32(r31.u32 + 1868, ctx.r4.u32);
	// bl 0x825f30e8
	sub_825F30E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r27,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r27.u32);
	// stw r27,19984(r31)
	PPC_STORE_U32(r31.u32 + 19984, r27.u32);
	// bl 0x82603700
	sub_82603700(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f60b0
	sub_825F60B0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e96b4
	if (!cr6.eq) goto loc_825E96B4;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// beq cr6,0x825e8f84
	if (cr6.eq) goto loc_825E8F84;
loc_825E93A0:
	// clrlwi r11,r22,28
	r11.u64 = r22.u32 & 0xF;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e93d0
	if (cr6.eq) goto loc_825E93D0;
	// rlwinm r11,r22,16,0,15
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 16) & 0xFFFF0000;
	// srawi r11,r11,28
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFFFF) != 0);
	r11.s64 = r11.s32 >> 28;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x825e93d0
	if (cr6.eq) goto loc_825E93D0;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825e93d0
	if (cr6.eq) goto loc_825E93D0;
	// li r11,2
	r11.s64 = 2;
	// stw r11,15548(r31)
	PPC_STORE_U32(r31.u32 + 15548, r11.u32);
	// b 0x825e93d4
	goto loc_825E93D4;
loc_825E93D0:
	// stw r27,15548(r31)
	PPC_STORE_U32(r31.u32 + 15548, r27.u32);
loc_825E93D4:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// beq cr6,0x825e93f4
	if (cr6.eq) goto loc_825E93F4;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825e9444
	if (!cr6.eq) goto loc_825E9444;
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e9444
	if (!cr6.eq) goto loc_825E9444;
loc_825E93F4:
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e9438
	if (cr6.eq) goto loc_825E9438;
	// lis r11,-32157
	r11.s64 = -2107441152;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// addi r11,r11,5376
	r11.s64 = r11.s64 + 5376;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x825e9438
	if (cr6.eq) goto loc_825E9438;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x825e9438
	if (cr6.eq) goto loc_825E9438;
	// lwz r11,3364(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3364);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825e9438
	if (cr6.gt) goto loc_825E9438;
	// lis r11,-32142
	r11.s64 = -2106458112;
	// addi r11,r11,-22960
	r11.s64 = r11.s64 + -22960;
	// b 0x825e9440
	goto loc_825E9440;
loc_825E9438:
	// lis r11,-32161
	r11.s64 = -2107703296;
	// addi r11,r11,15504
	r11.s64 = r11.s64 + 15504;
loc_825E9440:
	// stw r11,15772(r31)
	PPC_STORE_U32(r31.u32 + 15772, r11.u32);
loc_825E9444:
	// lwz r11,15772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15772);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,21552(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21552);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// lwz r10,3688(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// stw r11,608(r10)
	PPC_STORE_U32(ctx.r10.u32 + 608, r11.u32);
	// bne cr6,0x825e96b4
	if (!cr6.eq) goto loc_825E96B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260cf90
	sub_8260CF90(ctx, base);
	// lwz r11,15508(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15508);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e94a0
	if (cr6.eq) goto loc_825E94A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2d98
	sub_825E2D98(ctx, base);
	// li r6,1
	ctx.r6.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,3812(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3812);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260cb60
	sub_8260CB60(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2df8
	sub_825E2DF8(ctx, base);
loc_825E94A0:
	// stw r23,3668(r31)
	PPC_STORE_U32(r31.u32 + 3668, r23.u32);
	// b 0x825e94e0
	goto loc_825E94E0;
loc_825E94A8:
	// lwz r11,3668(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3668);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e8ecc
	if (cr6.eq) goto loc_825E8ECC;
	// li r9,2
	ctx.r9.s64 = 2;
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21552);
	// rlwinm r11,r22,16,0,15
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 16) & 0xFFFF0000;
	// srawi r11,r11,28
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFFFF) != 0);
	r11.s64 = r11.s32 >> 28;
	// stw r9,284(r31)
	PPC_STORE_U32(r31.u32 + 284, ctx.r9.u32);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// lwz r9,3688(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// stw r10,608(r9)
	PPC_STORE_U32(ctx.r9.u32 + 608, ctx.r10.u32);
	// beq cr6,0x825e96a8
	if (cr6.eq) goto loc_825E96A8;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825e96a8
	if (cr6.eq) goto loc_825E96A8;
loc_825E94E0:
	// lwz r11,15304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15304);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825e94f4
	if (cr6.eq) goto loc_825E94F4;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825e8ecc
	if (!cr6.eq) goto loc_825E8ECC;
loc_825E94F4:
	// lwz r11,15368(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15368);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e9534
	if (cr6.eq) goto loc_825E9534;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82648000
	sub_82648000(ctx, base);
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// lwz r11,608(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 608);
	// stw r11,608(r10)
	PPC_STORE_U32(ctx.r10.u32 + 608, r11.u32);
	// beq cr6,0x825e9694
	if (cr6.eq) goto loc_825E9694;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239bd30
	return;
loc_825E9534:
	// addi r10,r1,140
	ctx.r10.s64 = ctx.r1.s64 + 140;
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// addi r8,r1,156
	ctx.r8.s64 = ctx.r1.s64 + 156;
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// addi r6,r1,148
	ctx.r6.s64 = ctx.r1.s64 + 148;
	// addi r5,r1,164
	ctx.r5.s64 = ctx.r1.s64 + 164;
	// addi r4,r1,152
	ctx.r4.s64 = ctx.r1.s64 + 152;
	// bl 0x825e2f38
	sub_825E2F38(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e96b4
	if (!cr6.eq) goto loc_825E96B4;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// lwz r11,15364(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15364);
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lfd f31,-31368(r9)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r9.u32 + -31368);
	// lfd f30,-31360(r10)
	f30.u64 = PPC_LOAD_U64(ctx.r10.u32 + -31360);
	// bne cr6,0x825e958c
	if (!cr6.eq) goto loc_825E958C;
	// fmr f5,f30
	ctx.f5.f64 = f30.f64;
	// fmr f4,f31
	ctx.f4.f64 = f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// b 0x825e95b8
	goto loc_825E95B8;
loc_825E958C:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e95a8
	if (!cr6.eq) goto loc_825E95A8;
	// lfs f5,152(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f5.f64 = double(temp.f32);
	// fmr f4,f31
	ctx.f4.f64 = f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fmr f1,f5
	ctx.f1.f64 = ctx.f5.f64;
	// b 0x825e95b8
	goto loc_825E95B8;
loc_825E95A8:
	// lfs f5,156(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f1.f64 = double(temp.f32);
loc_825E95B8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f7,140(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f6.f64 = double(temp.f32);
	// lwz r9,3784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lfs f3,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f3.f64 = double(temp.f32);
	// lwz r8,3780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r7,3776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// bl 0x826429f8
	sub_826429F8(ctx, base);
	// lwz r11,15304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15304);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825e9694
	if (!cr6.eq) goto loc_825E9694;
	// addi r10,r1,148
	ctx.r10.s64 = ctx.r1.s64 + 148;
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// addi r8,r1,164
	ctx.r8.s64 = ctx.r1.s64 + 164;
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// addi r6,r1,140
	ctx.r6.s64 = ctx.r1.s64 + 140;
	// addi r5,r1,156
	ctx.r5.s64 = ctx.r1.s64 + 156;
	// addi r4,r1,152
	ctx.r4.s64 = ctx.r1.s64 + 152;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2f38
	sub_825E2F38(ctx, base);
	// lwz r11,15364(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15364);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e9634
	if (!cr6.eq) goto loc_825E9634;
	// fmr f5,f30
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = f30.f64;
	// fmr f4,f31
	ctx.f4.f64 = f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// b 0x825e9660
	goto loc_825E9660;
loc_825E9634:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e9650
	if (!cr6.eq) goto loc_825E9650;
	// lfs f5,152(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f5.f64 = double(temp.f32);
	// fmr f4,f31
	ctx.f4.f64 = f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fmr f1,f5
	ctx.f1.f64 = ctx.f5.f64;
	// b 0x825e9660
	goto loc_825E9660;
loc_825E9650:
	// lfs f5,164(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f1.f64 = double(temp.f32);
loc_825E9660:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f7,148(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f6.f64 = double(temp.f32);
	// lwz r9,3796(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3796);
	// lfs f3,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f3.f64 = double(temp.f32);
	// lwz r8,3792(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3792);
	// lwz r7,3788(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3788);
	// lwz r6,3740(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r5,3736(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// bl 0x826429f8
	sub_826429F8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82641c20
	sub_82641C20(ctx, base);
loc_825E9694:
	// lwz r11,3704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// li r10,3
	ctx.r10.s64 = 3;
	// stw r11,3716(r31)
	PPC_STORE_U32(r31.u32 + 3716, r11.u32);
	// stw r10,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, ctx.r10.u32);
	// b 0x825e96ac
	goto loc_825E96AC;
loc_825E96A8:
	// stw r23,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r23.u32);
loc_825E96AC:
	// sth r23,3684(r31)
	PPC_STORE_U16(r31.u32 + 3684, r23.u16);
	// li r3,0
	ctx.r3.s64 = 0;
loc_825E96B4:
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825E96C4"))) PPC_WEAK_FUNC(sub_825E96C4);
PPC_FUNC_IMPL(__imp__sub_825E96C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E96C8"))) PPC_WEAK_FUNC(sub_825E96C8);
PPC_FUNC_IMPL(__imp__sub_825E96C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r28,1
	r28.s64 = 1;
	// stw r28,21360(r31)
	PPC_STORE_U32(r31.u32 + 21360, r28.u32);
	// bl 0x826035b8
	sub_826035B8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r30,156(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r29,160(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e6eb0
	sub_825E6EB0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e97bc
	if (!cr6.eq) goto loc_825E97BC;
	// lwz r4,156(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// lwz r11,21352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21352);
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// bgt cr6,0x825e97b8
	if (cr6.gt) goto loc_825E97B8;
	// lwz r5,160(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// lwz r11,21356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21356);
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// bgt cr6,0x825e97b8
	if (cr6.gt) goto loc_825E97B8;
	// li r27,0
	r27.s64 = 0;
	// cmpw cr6,r4,r30
	cr6.compare<int32_t>(ctx.r4.s32, r30.s32, xer);
	// bne cr6,0x825e9740
	if (!cr6.eq) goto loc_825E9740;
	// cmpw cr6,r5,r29
	cr6.compare<int32_t>(ctx.r5.s32, r29.s32, xer);
	// bne cr6,0x825e9740
	if (!cr6.eq) goto loc_825E9740;
	// stw r27,21184(r31)
	PPC_STORE_U32(r31.u32 + 21184, r27.u32);
	// b 0x825e9758
	goto loc_825E9758;
loc_825E9740:
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// stw r28,21184(r31)
	PPC_STORE_U32(r31.u32 + 21184, r28.u32);
	// addi r11,r11,-31828
	r11.s64 = r11.s64 + -31828;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_825E9758:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82606a70
	sub_82606A70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825e97c4
	if (!cr6.eq) goto loc_825E97C4;
	// lwz r11,21288(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21288);
	// stw r28,21360(r31)
	PPC_STORE_U32(r31.u32 + 21360, r28.u32);
	// stw r28,3672(r31)
	PPC_STORE_U32(r31.u32 + 3672, r28.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e979c
	if (!cr6.gt) goto loc_825E979C;
	// lwz r10,21280(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21280);
	// twllei r11,0
	// stw r27,21288(r31)
	PPC_STORE_U32(r31.u32 + 21288, r27.u32);
	// divwu r11,r10,r11
	r11.u32 = ctx.r10.u32 / r11.u32;
	// stw r27,21280(r31)
	PPC_STORE_U32(r31.u32 + 21280, r27.u32);
	// stw r11,21284(r31)
	PPC_STORE_U32(r31.u32 + 21284, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_825E979C:
	// mr r11,r28
	r11.u64 = r28.u64;
	// stw r27,21288(r31)
	PPC_STORE_U32(r31.u32 + 21288, r27.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r27,21280(r31)
	PPC_STORE_U32(r31.u32 + 21280, r27.u32);
	// stw r11,21284(r31)
	PPC_STORE_U32(r31.u32 + 21284, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_825E97B8:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825E97BC:
	// stw r29,160(r31)
	PPC_STORE_U32(r31.u32 + 160, r29.u32);
	// stw r30,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r30.u32);
loc_825E97C4:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825E97CC"))) PPC_WEAK_FUNC(sub_825E97CC);
PPC_FUNC_IMPL(__imp__sub_825E97CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E97D0"))) PPC_WEAK_FUNC(sub_825E97D0);
PPC_FUNC_IMPL(__imp__sub_825E97D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r27,0
	r27.s64 = 0;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// li r24,1
	r24.s64 = 1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r22,r27
	r22.u64 = r27.u64;
	// stw r27,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r27.u32);
	// stw r27,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r27.u32);
	// mr r28,r27
	r28.u64 = r27.u64;
	// stw r24,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r24.u32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// sth r27,0(r23)
	PPC_STORE_U16(r23.u32 + 0, r27.u16);
	// bne cr6,0x825e981c
	if (!cr6.eq) goto loc_825E981C;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825E981C:
	// lwz r11,15552(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15552);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825e9834
	if (!cr6.gt) goto loc_825E9834;
	// lwz r11,3420(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3420);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,3420(r31)
	PPC_STORE_U32(r31.u32 + 3420, r11.u32);
loc_825E9834:
	// lhz r11,3684(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 3684);
	// stw r27,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r27.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e9864
	if (cr6.eq) goto loc_825E9864;
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r7,15504(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 15504);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r6,15496(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 15496);
	// lhz r5,15492(r31)
	ctx.r5.u64 = PPC_LOAD_U16(r31.u32 + 15492);
	// lwz r4,15488(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15488);
	// bl 0x825e7fb0
	sub_825E7FB0(ctx, base);
	// sth r27,3684(r31)
	PPC_STORE_U16(r31.u32 + 3684, r27.u16);
loc_825E9864:
	// lwz r11,15556(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15556);
	// rlwinm r11,r11,28,0,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xF0000000;
	// srawi. r11,r11,28
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFFFF) != 0);
	r11.s64 = r11.s32 >> 28;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x825e9880
	if (cr0.lt) goto loc_825E9880;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bgt cr6,0x825e9880
	if (cr6.gt) goto loc_825E9880;
	// stw r11,3644(r31)
	PPC_STORE_U32(r31.u32 + 3644, r11.u32);
loc_825E9880:
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// lwz r3,3340(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r30,r27
	r30.u64 = r27.u64;
	// bl 0x82491030
	sub_82491030(ctx, base);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x825e99c8
	if (cr6.eq) goto loc_825E99C8;
loc_825E98AC:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825e9904
	if (!cr6.eq) goto loc_825E9904;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r29,r30,r5
	r29.u64 = r30.u64 + ctx.r5.u64;
	// cmplwi cr6,r29,64
	cr6.compare<uint32_t>(r29.u32, 64, xer);
	// bge cr6,0x825e9908
	if (!cr6.lt) goto loc_825E9908;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// add r3,r30,r11
	ctx.r3.u64 = r30.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r3,3340(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r30,r29
	r30.u64 = r29.u64;
	// bl 0x82491030
	sub_82491030(ctx, base);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x825e98ac
	if (!cr6.eq) goto loc_825E98AC;
loc_825E9904:
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
loc_825E9908:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825e9998
	if (cr6.eq) goto loc_825E9998;
	// lwz r11,23248(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23248);
	// add r29,r30,r5
	r29.u64 = r30.u64 + ctx.r5.u64;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// ble cr6,0x825e995c
	if (!cr6.gt) goto loc_825E995C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e9930
	if (cr6.eq) goto loc_825E9930;
	// lwz r3,23252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 23252);
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_825E9930:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,23252(r31)
	PPC_STORE_U32(r31.u32 + 23252, ctx.r3.u32);
	// bne cr6,0x825e9958
	if (!cr6.eq) goto loc_825E9958;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r27,23248(r31)
	PPC_STORE_U32(r31.u32 + 23248, r27.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825E9958:
	// stw r29,23248(r31)
	PPC_STORE_U32(r31.u32 + 23248, r29.u32);
loc_825E995C:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r3,23252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 23252);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,23252(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23252);
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,23252(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23252);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r5,r30,r11
	ctx.r5.u64 = r30.u64 + r11.u64;
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
loc_825E9998:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x825e99cc
	if (cr6.eq) goto loc_825E99CC;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x825e99b4
	if (cr6.eq) goto loc_825E99B4;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e99cc
	if (!cr6.eq) goto loc_825E99CC;
loc_825E99B4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e4ad8
	sub_825E4AD8(ctx, base);
	// li r3,11
	ctx.r3.s64 = 11;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825E99C8:
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
loc_825E99CC:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x825e9a00
	if (!cr6.eq) goto loc_825E9A00;
loc_825E99D4:
	// lwz r11,3668(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3668);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e99ec
	if (!cr6.eq) goto loc_825E99EC;
loc_825E99E0:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825E99EC:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r24,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r24.u32);
	// stw r24,3424(r31)
	PPC_STORE_U32(r31.u32 + 3424, r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825E9A00:
	// cmplwi cr6,r5,1
	cr6.compare<uint32_t>(ctx.r5.u32, 1, xer);
	// bne cr6,0x825e9a60
	if (!cr6.eq) goto loc_825E9A60;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x825e9a60
	if (!cr6.eq) goto loc_825E9A60;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x825e99e0
	if (cr6.lt) goto loc_825E99E0;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// beq cr6,0x825e9a60
	if (cr6.eq) goto loc_825E9A60;
	// lwz r11,3668(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3668);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e99e0
	if (cr6.eq) goto loc_825E99E0;
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e9a4c
	if (cr6.eq) goto loc_825E9A4C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f3a40
	sub_825F3A40(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825ea7e0
	if (!cr6.eq) goto loc_825EA7E0;
loc_825E9A4C:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r24,3424(r31)
	PPC_STORE_U32(r31.u32 + 3424, r24.u32);
	// stw r24,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825E9A60:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825e9ab0
	if (!cr6.eq) goto loc_825E9AB0;
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// addi r6,r1,116
	ctx.r6.s64 = ctx.r1.s64 + 116;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825fe4d8
	sub_825FE4D8(ctx, base);
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// bne cr6,0x825e9a94
	if (!cr6.eq) goto loc_825E9A94;
	// li r22,4
	r22.s64 = 4;
	// b 0x825e9a9c
	goto loc_825E9A9C;
loc_825E9A94:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825ea7e0
	if (!cr6.eq) goto loc_825EA7E0;
loc_825E9A9C:
	// lwz r11,3672(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3672);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea250
	if (cr6.eq) goto loc_825EA250;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
loc_825E9AB0:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x825e9ac0
	if (!cr6.eq) goto loc_825E9AC0;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x825e99d4
	if (cr6.eq) goto loc_825E99D4;
loc_825E9AC0:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r11,r11,-7
	r11.s64 = r11.s64 + -7;
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bl 0x825eb668
	sub_825EB668(ctx, base);
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,3676(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3676);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ea250
	if (!cr6.eq) goto loc_825EA250;
	// ld r10,3576(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 3576);
	// ld r11,3584(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 3584);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,3452(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3452);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r27,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r27.u32);
	// stw r27,3424(r31)
	PPC_STORE_U32(r31.u32 + 3424, r27.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r27,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r27.u32);
	// stw r27,3408(r31)
	PPC_STORE_U32(r31.u32 + 3408, r27.u32);
	// std r10,3576(r31)
	PPC_STORE_U64(r31.u32 + 3576, ctx.r10.u64);
	// std r11,3584(r31)
	PPC_STORE_U64(r31.u32 + 3584, r11.u64);
	// bne cr6,0x825e9b34
	if (!cr6.eq) goto loc_825E9B34;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// stw r11,292(r31)
	PPC_STORE_U32(r31.u32 + 292, r11.u32);
	// b 0x825e9b38
	goto loc_825E9B38;
loc_825E9B34:
	// stw r27,3452(r31)
	PPC_STORE_U32(r31.u32 + 3452, r27.u32);
loc_825E9B38:
	// lwz r8,15472(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r8,6
	cr6.compare<int32_t>(ctx.r8.s32, 6, xer);
	// beq cr6,0x825e9b58
	if (cr6.eq) goto loc_825E9B58;
	// cmpwi cr6,r8,7
	cr6.compare<int32_t>(ctx.r8.s32, 7, xer);
	// bne cr6,0x825e9bdc
	if (!cr6.eq) goto loc_825E9BDC;
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825e9bdc
	if (!cr6.eq) goto loc_825E9BDC;
loc_825E9B58:
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e9bb8
	if (cr6.eq) goto loc_825E9BB8;
	// lis r11,-32157
	r11.s64 = -2107441152;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// addi r11,r11,5376
	r11.s64 = r11.s64 + 5376;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x825e9bb8
	if (cr6.eq) goto loc_825E9BB8;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x825e9bb8
	if (cr6.eq) goto loc_825E9BB8;
	// lwz r11,3364(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3364);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825e9bb8
	if (cr6.gt) goto loc_825E9BB8;
	// lis r9,-32159
	ctx.r9.s64 = -2107572224;
	// lis r10,-32142
	ctx.r10.s64 = -2106458112;
	// lis r11,-32142
	r11.s64 = -2106458112;
	// addi r9,r9,24920
	ctx.r9.s64 = ctx.r9.s64 + 24920;
	// addi r10,r10,-3152
	ctx.r10.s64 = ctx.r10.s64 + -3152;
	// addi r11,r11,-22960
	r11.s64 = r11.s64 + -22960;
	// stw r9,15776(r31)
	PPC_STORE_U32(r31.u32 + 15776, ctx.r9.u32);
	// stw r10,3056(r31)
	PPC_STORE_U32(r31.u32 + 3056, ctx.r10.u32);
	// stw r11,15772(r31)
	PPC_STORE_U32(r31.u32 + 15772, r11.u32);
	// b 0x825e9bdc
	goto loc_825E9BDC;
loc_825E9BB8:
	// lis r9,-32161
	ctx.r9.s64 = -2107703296;
	// lis r10,-32157
	ctx.r10.s64 = -2107441152;
	// lis r11,-32158
	r11.s64 = -2107506688;
	// addi r9,r9,15504
	ctx.r9.s64 = ctx.r9.s64 + 15504;
	// addi r10,r10,672
	ctx.r10.s64 = ctx.r10.s64 + 672;
	// addi r11,r11,30968
	r11.s64 = r11.s64 + 30968;
	// stw r9,15772(r31)
	PPC_STORE_U32(r31.u32 + 15772, ctx.r9.u32);
	// stw r10,15776(r31)
	PPC_STORE_U32(r31.u32 + 15776, ctx.r10.u32);
	// stw r11,3056(r31)
	PPC_STORE_U32(r31.u32 + 3056, r11.u32);
loc_825E9BDC:
	// cmpwi cr6,r8,6
	cr6.compare<int32_t>(ctx.r8.s32, 6, xer);
	// blt cr6,0x825e9c10
	if (cr6.lt) goto loc_825E9C10;
	// li r11,8
	r11.s64 = 8;
	// stw r27,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r27.u32);
	// addi r10,r31,2640
	ctx.r10.s64 = r31.s64 + 2640;
	// addi r9,r31,2680
	ctx.r9.s64 = r31.s64 + 2680;
	// stw r11,344(r31)
	PPC_STORE_U32(r31.u32 + 344, r11.u32);
	// stw r10,2904(r31)
	PPC_STORE_U32(r31.u32 + 2904, ctx.r10.u32);
	// stw r9,2916(r31)
	PPC_STORE_U32(r31.u32 + 2916, ctx.r9.u32);
	// stw r11,348(r31)
	PPC_STORE_U32(r31.u32 + 348, r11.u32);
	// stw r11,20004(r31)
	PPC_STORE_U32(r31.u32 + 20004, r11.u32);
	// stw r11,14804(r31)
	PPC_STORE_U32(r31.u32 + 14804, r11.u32);
	// stw r11,20940(r31)
	PPC_STORE_U32(r31.u32 + 20940, r11.u32);
loc_825E9C10:
	// cmpwi cr6,r8,7
	cr6.compare<int32_t>(ctx.r8.s32, 7, xer);
	// beq cr6,0x825e9c40
	if (cr6.eq) goto loc_825E9C40;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f7da8
	sub_825F7DA8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825ea1fc
	if (cr6.eq) goto loc_825EA1FC;
loc_825E9C2C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e4ad8
	sub_825E4AD8(ctx, base);
loc_825E9C34:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825E9C40:
	// lwz r11,21160(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21160);
	// li r26,2
	r26.s64 = 2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e9dd0
	if (cr6.eq) goto loc_825E9DD0;
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// mr r29,r24
	r29.u64 = r24.u64;
	// mr r28,r27
	r28.u64 = r27.u64;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e9cc4
	if (!cr6.lt) goto loc_825E9CC4;
loc_825E9C6C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e9cc4
	if (cr6.eq) goto loc_825E9CC4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825e9cb4
	if (!cr0.lt) goto loc_825E9CB4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E9CB4:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825e9c6c
	if (cr6.gt) goto loc_825E9C6C;
loc_825E9CC4:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x825e9d00
	if (!cr0.lt) goto loc_825E9D00;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E9D00:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x825e9dd0
	if (cr6.eq) goto loc_825E9DD0;
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// mr r29,r24
	r29.u64 = r24.u64;
	// mr r28,r27
	r28.u64 = r27.u64;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825e9d7c
	if (!cr6.lt) goto loc_825E9D7C;
loc_825E9D24:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825e9d7c
	if (cr6.eq) goto loc_825E9D7C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825e9d6c
	if (!cr0.lt) goto loc_825E9D6C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E9D6C:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825e9d24
	if (cr6.gt) goto loc_825E9D24;
loc_825E9D7C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x825e9db8
	if (!cr0.lt) goto loc_825E9DB8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825E9DB8:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825e9dc8
	if (!cr6.eq) goto loc_825E9DC8;
	// stw r26,21436(r31)
	PPC_STORE_U32(r31.u32 + 21436, r26.u32);
	// b 0x825e9dd4
	goto loc_825E9DD4;
loc_825E9DC8:
	// stw r24,21436(r31)
	PPC_STORE_U32(r31.u32 + 21436, r24.u32);
	// b 0x825e9dd4
	goto loc_825E9DD4;
loc_825E9DD0:
	// stw r27,21436(r31)
	PPC_STORE_U32(r31.u32 + 21436, r27.u32);
loc_825E9DD4:
	// lwz r11,21436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21436);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r27,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r27.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r27,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r27.u32);
	// bne cr6,0x825ea00c
	if (!cr6.eq) goto loc_825EA00C;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// lis r7,-32138
	ctx.r7.s64 = -2106195968;
	// lis r6,-32138
	ctx.r6.s64 = -2106195968;
	// lis r5,-32138
	ctx.r5.s64 = -2106195968;
	// lis r4,-32138
	ctx.r4.s64 = -2106195968;
	// addi r11,r11,6272
	r11.s64 = r11.s64 + 6272;
	// addi r10,r10,6200
	ctx.r10.s64 = ctx.r10.s64 + 6200;
	// addi r9,r9,5628
	ctx.r9.s64 = ctx.r9.s64 + 5628;
	// addi r8,r8,6372
	ctx.r8.s64 = ctx.r8.s64 + 6372;
	// addi r7,r7,6336
	ctx.r7.s64 = ctx.r7.s64 + 6336;
	// addi r6,r6,6408
	ctx.r6.s64 = ctx.r6.s64 + 6408;
	// stw r11,1824(r31)
	PPC_STORE_U32(r31.u32 + 1824, r11.u32);
	// addi r5,r5,6444
	ctx.r5.s64 = ctx.r5.s64 + 6444;
	// stw r10,1828(r31)
	PPC_STORE_U32(r31.u32 + 1828, ctx.r10.u32);
	// addi r4,r4,6464
	ctx.r4.s64 = ctx.r4.s64 + 6464;
	// stw r9,1836(r31)
	PPC_STORE_U32(r31.u32 + 1836, ctx.r9.u32);
	// stw r8,1840(r31)
	PPC_STORE_U32(r31.u32 + 1840, ctx.r8.u32);
	// stw r7,1844(r31)
	PPC_STORE_U32(r31.u32 + 1844, ctx.r7.u32);
	// stw r6,1848(r31)
	PPC_STORE_U32(r31.u32 + 1848, ctx.r6.u32);
	// stw r5,1864(r31)
	PPC_STORE_U32(r31.u32 + 1864, ctx.r5.u32);
	// stw r4,1868(r31)
	PPC_STORE_U32(r31.u32 + 1868, ctx.r4.u32);
	// bl 0x825f30e8
	sub_825F30E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r27,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r27.u32);
	// mr r28,r27
	r28.u64 = r27.u64;
	// stw r27,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r27.u32);
	// stw r27,19984(r31)
	PPC_STORE_U32(r31.u32 + 19984, r27.u32);
	// bl 0x82603700
	sub_82603700(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f60b0
	sub_825F60B0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825e9c2c
	if (!cr6.eq) goto loc_825E9C2C;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825e9e9c
	if (cr6.eq) goto loc_825E9E9C;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825e9e9c
	if (cr6.eq) goto loc_825E9E9C;
	// lwz r10,21516(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21516);
	// stw r27,21516(r31)
	PPC_STORE_U32(r31.u32 + 21516, r27.u32);
	// stw r10,21520(r31)
	PPC_STORE_U32(r31.u32 + 21520, ctx.r10.u32);
loc_825E9E9C:
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x825ea1fc
	if (!cr6.eq) goto loc_825EA1FC;
	// lwz r11,21480(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21480);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e9fe8
	if (cr6.eq) goto loc_825E9FE8;
	// lwz r11,14788(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e9fe8
	if (cr6.eq) goto loc_825E9FE8;
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e9fe8
	if (!cr6.gt) goto loc_825E9FE8;
	// lwz r11,3376(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3376);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825e9ed8
	if (!cr6.eq) goto loc_825E9ED8;
	// stw r27,3376(r31)
	PPC_STORE_U32(r31.u32 + 3376, r27.u32);
loc_825E9ED8:
	// lwz r11,3396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3396);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e9ef0
	if (cr6.eq) goto loc_825E9EF0;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_825E9EF0:
	// bl 0x82627880
	sub_82627880(ctx, base);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r11,212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r5,r10,r11
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,216(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,144(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,144(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,1780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r10,276(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 276);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e9fe8
	if (!cr6.gt) goto loc_825E9FE8;
loc_825E9F78:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r11,r27
	r11.u64 = r27.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825e9fd8
	if (!cr6.gt) goto loc_825E9FD8;
loc_825E9F88:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r6,3048(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3048);
	// mullw r9,r9,r7
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// sthx r27,r8,r6
	PPC_STORE_U16(ctx.r8.u32 + ctx.r6.u32, r27.u16);
	// lwz r9,3048(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3048);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// sth r27,2(r9)
	PPC_STORE_U16(ctx.r9.u32 + 2, r27.u16);
	// lwz r9,3052(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3052);
	// stwx r27,r5,r9
	PPC_STORE_U32(ctx.r5.u32 + ctx.r9.u32, r27.u32);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// oris r9,r9,2
	ctx.r9.u64 = ctx.r9.u64 | 131072;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825e9f88
	if (cr6.lt) goto loc_825E9F88;
loc_825E9FD8:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// blt cr6,0x825e9f78
	if (cr6.lt) goto loc_825E9F78;
loc_825E9FE8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,19984(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// bl 0x8261b310
	sub_8261B310(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e4a20
	sub_825E4A20(ctx, base);
	// stw r24,3424(r31)
	PPC_STORE_U32(r31.u32 + 3424, r24.u32);
	// stw r24,21388(r31)
	PPC_STORE_U32(r31.u32 + 21388, r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825EA00C:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// lis r7,-32138
	ctx.r7.s64 = -2106195968;
	// lis r6,-32138
	ctx.r6.s64 = -2106195968;
	// lis r5,-32138
	ctx.r5.s64 = -2106195968;
	// lis r4,-32138
	ctx.r4.s64 = -2106195968;
	// addi r11,r11,6560
	r11.s64 = r11.s64 + 6560;
	// addi r10,r10,6488
	ctx.r10.s64 = ctx.r10.s64 + 6488;
	// addi r9,r9,6624
	ctx.r9.s64 = ctx.r9.s64 + 6624;
	// addi r8,r8,6696
	ctx.r8.s64 = ctx.r8.s64 + 6696;
	// addi r7,r7,6660
	ctx.r7.s64 = ctx.r7.s64 + 6660;
	// addi r6,r6,6732
	ctx.r6.s64 = ctx.r6.s64 + 6732;
	// stw r11,1824(r31)
	PPC_STORE_U32(r31.u32 + 1824, r11.u32);
	// addi r5,r5,6768
	ctx.r5.s64 = ctx.r5.s64 + 6768;
	// stw r10,1828(r31)
	PPC_STORE_U32(r31.u32 + 1828, ctx.r10.u32);
	// addi r4,r4,6788
	ctx.r4.s64 = ctx.r4.s64 + 6788;
	// stw r9,1836(r31)
	PPC_STORE_U32(r31.u32 + 1836, ctx.r9.u32);
	// stw r8,1840(r31)
	PPC_STORE_U32(r31.u32 + 1840, ctx.r8.u32);
	// stw r7,1844(r31)
	PPC_STORE_U32(r31.u32 + 1844, ctx.r7.u32);
	// stw r6,1848(r31)
	PPC_STORE_U32(r31.u32 + 1848, ctx.r6.u32);
	// stw r5,1864(r31)
	PPC_STORE_U32(r31.u32 + 1864, ctx.r5.u32);
	// stw r4,1868(r31)
	PPC_STORE_U32(r31.u32 + 1868, ctx.r4.u32);
	// bne cr6,0x825eaa14
	if (!cr6.eq) goto loc_825EAA14;
	// bl 0x825f30e8
	sub_825F30E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// stw r24,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r24.u32);
	// stw r27,19984(r31)
	PPC_STORE_U32(r31.u32 + 19984, r27.u32);
	// bl 0x82603700
	sub_82603700(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261e1c8
	sub_8261E1C8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825e9c2c
	if (!cr6.eq) goto loc_825E9C2C;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825ea0c0
	if (cr6.eq) goto loc_825EA0C0;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825ea0c0
	if (cr6.eq) goto loc_825EA0C0;
	// lwz r10,21516(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21516);
	// stw r26,21516(r31)
	PPC_STORE_U32(r31.u32 + 21516, r26.u32);
	// stw r10,21520(r31)
	PPC_STORE_U32(r31.u32 + 21520, ctx.r10.u32);
loc_825EA0C0:
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x825ea1fc
	if (!cr6.eq) goto loc_825EA1FC;
	// lwz r11,21480(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21480);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea160
	if (cr6.eq) goto loc_825EA160;
	// lwz r11,14788(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea160
	if (cr6.eq) goto loc_825EA160;
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ea160
	if (!cr6.gt) goto loc_825EA160;
	// lwz r11,3376(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3376);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ea0fc
	if (!cr6.eq) goto loc_825EA0FC;
	// stw r27,3376(r31)
	PPC_STORE_U32(r31.u32 + 3376, r27.u32);
loc_825EA0FC:
	// lwz r11,3396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3396);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea114
	if (cr6.eq) goto loc_825EA114;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_825EA114:
	// bl 0x82627880
	sub_82627880(ctx, base);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r11,212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r5,r10,r11
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,216(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_825EA160:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// lwz r10,276(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 276);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825ea1b4
	if (!cr6.gt) goto loc_825EA1B4;
loc_825EA174:
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r11,r27
	r11.u64 = r27.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// ble cr6,0x825ea1a4
	if (!cr6.gt) goto loc_825EA1A4;
loc_825EA184:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// oris r8,r8,2
	ctx.r8.u64 = ctx.r8.u64 | 131072;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x825ea184
	if (cr6.lt) goto loc_825EA184;
loc_825EA1A4:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x825ea174
	if (cr6.lt) goto loc_825EA174;
loc_825EA1B4:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r3,3052(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3052);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,19984(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// bl 0x8261b310
	sub_8261B310(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e4a20
	sub_825E4A20(ctx, base);
	// stw r24,3424(r31)
	PPC_STORE_U32(r31.u32 + 3424, r24.u32);
	// stw r24,21388(r31)
	PPC_STORE_U32(r31.u32 + 21388, r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825EA1FC:
	// lwz r11,21580(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21580);
	// stw r24,1944(r31)
	PPC_STORE_U32(r31.u32 + 1944, r24.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r27,20060(r31)
	PPC_STORE_U32(r31.u32 + 20060, r27.u32);
	// beq cr6,0x825ea220
	if (cr6.eq) goto loc_825EA220;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825ff718
	sub_825FF718(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825ea7e0
	if (!cr6.eq) goto loc_825EA7E0;
loc_825EA220:
	// lwz r10,284(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825ea25c
	if (cr6.eq) goto loc_825EA25C;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// beq cr6,0x825ea25c
	if (cr6.eq) goto loc_825EA25C;
	// lwz r11,3668(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3668);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ea358
	if (!cr6.eq) goto loc_825EA358;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e4a20
	sub_825E4A20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825ea7e0
	if (!cr6.eq) goto loc_825EA7E0;
loc_825EA250:
	// li r3,3
	ctx.r3.s64 = 3;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825EA25C:
	// lwz r11,14788(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea358
	if (cr6.eq) goto loc_825EA358;
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ea348
	if (!cr6.gt) goto loc_825EA348;
	// lwz r11,3376(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3376);
	// cmpwi cr6,r11,-3
	cr6.compare<int32_t>(r11.s32, -3, xer);
	// bne cr6,0x825ea2b8
	if (!cr6.eq) goto loc_825EA2B8;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// beq cr6,0x825e99e0
	if (cr6.eq) goto loc_825E99E0;
	// lwz r11,21368(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21368);
	// lwz r10,3396(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3396);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,3376(r31)
	PPC_STORE_U32(r31.u32 + 3376, r11.u32);
	// beq cr6,0x825ea2ac
	if (cr6.eq) goto loc_825EA2AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
loc_825EA2AC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82627880
	sub_82627880(ctx, base);
	// b 0x825ea358
	goto loc_825EA358;
loc_825EA2B8:
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// beq cr6,0x825ea2fc
	if (cr6.eq) goto loc_825EA2FC;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// beq cr6,0x825ea2fc
	if (cr6.eq) goto loc_825EA2FC;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ea2d8
	if (!cr6.eq) goto loc_825EA2D8;
	// stw r27,3376(r31)
	PPC_STORE_U32(r31.u32 + 3376, r27.u32);
	// b 0x825ea2e4
	goto loc_825EA2E4;
loc_825EA2D8:
	// lwz r11,3396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea2ec
	if (cr6.eq) goto loc_825EA2EC;
loc_825EA2E4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
loc_825EA2EC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82627880
	sub_82627880(ctx, base);
	// stw r27,3384(r31)
	PPC_STORE_U32(r31.u32 + 3384, r27.u32);
	// b 0x825ea358
	goto loc_825EA358;
loc_825EA2FC:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ea330
	if (!cr6.eq) goto loc_825EA330;
	// lwz r11,21368(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21368);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ea330
	if (!cr6.eq) goto loc_825EA330;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r24,3408(r31)
	PPC_STORE_U32(r31.u32 + 3408, r24.u32);
	// sth r24,3684(r31)
	PPC_STORE_U16(r31.u32 + 3684, r24.u16);
	// bl 0x825e4a20
	sub_825E4A20(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r24,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825EA330:
	// lwz r11,3396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea358
	if (cr6.eq) goto loc_825EA358;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
	// b 0x825ea358
	goto loc_825EA358;
loc_825EA348:
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r4,r11,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bl 0x825f50e0
	sub_825F50E0(ctx, base);
loc_825EA358:
	// lwz r11,248(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// stw r11,-25088(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25088, r11.u32);
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea7e8
	if (cr6.eq) goto loc_825EA7E8;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825ea7e8
	if (cr6.eq) goto loc_825EA7E8;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,-25092(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25092, r11.u32);
	// bl 0x825ebb40
	sub_825EBB40(ctx, base);
	// lwz r11,14788(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea550
	if (cr6.eq) goto loc_825EA550;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825ea550
	if (cr6.eq) goto loc_825EA550;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825ea708
	if (!cr6.eq) goto loc_825EA708;
	// lwz r11,3376(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3376);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ea3e0
	if (!cr6.eq) goto loc_825EA3E0;
	// lwz r11,21368(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21368);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ea3e0
	if (!cr6.eq) goto loc_825EA3E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r24,3408(r31)
	PPC_STORE_U32(r31.u32 + 3408, r24.u32);
	// sth r24,3684(r31)
	PPC_STORE_U16(r31.u32 + 3684, r24.u16);
	// bl 0x825e4a20
	sub_825E4A20(ctx, base);
	// stw r24,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825EA3E0:
	// lwz r11,3384(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3384);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea460
	if (cr6.eq) goto loc_825EA460;
	// lwz r11,3396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3396);
	// stw r27,3384(r31)
	PPC_STORE_U32(r31.u32 + 3384, r27.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea404
	if (cr6.eq) goto loc_825EA404;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
loc_825EA404:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82627880
	sub_82627880(ctx, base);
	// lwz r11,212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r5,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r5.s64 = r11.s32 >> 2;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// srawi r5,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r5.s64 = r11.s32 >> 2;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// stw r24,3396(r31)
	PPC_STORE_U32(r31.u32 + 3396, r24.u32);
loc_825EA460:
	// rlwinm r11,r25,16,0,15
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 16) & 0xFFFF0000;
	// srawi r11,r11,28
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFFFF) != 0);
	r11.s64 = r11.s32 >> 28;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x825ea51c
	if (cr6.eq) goto loc_825EA51C;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825ea51c
	if (cr6.eq) goto loc_825EA51C;
	// lwz r11,3396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea48c
	if (cr6.eq) goto loc_825EA48C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
loc_825EA48C:
	// lwz r11,3924(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ea4d0
	if (!cr6.eq) goto loc_825EA4D0;
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ea4d0
	if (!cr6.eq) goto loc_825EA4D0;
	// lwz r11,3056(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3056);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21552);
	// stw r10,608(r11)
	PPC_STORE_U32(r11.u32 + 608, ctx.r10.u32);
	// stw r24,15560(r31)
	PPC_STORE_U32(r31.u32 + 15560, r24.u32);
	// stw r24,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r24.u32);
	// b 0x825ea708
	goto loc_825EA708;
loc_825EA4D0:
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea500
	if (cr6.eq) goto loc_825EA500;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8271cf58
	sub_8271CF58(ctx, base);
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21552);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r10,608(r11)
	PPC_STORE_U32(r11.u32 + 608, ctx.r10.u32);
	// stw r24,15560(r31)
	PPC_STORE_U32(r31.u32 + 15560, r24.u32);
	// stw r24,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r24.u32);
	// b 0x825ea708
	goto loc_825EA708;
loc_825EA500:
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// li r30,6
	r30.s64 = 6;
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21552);
	// stw r10,608(r11)
	PPC_STORE_U32(r11.u32 + 608, ctx.r10.u32);
	// stw r24,15560(r31)
	PPC_STORE_U32(r31.u32 + 15560, r24.u32);
	// stw r24,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r24.u32);
	// b 0x825ea708
	goto loc_825EA708;
loc_825EA51C:
	// lwz r11,3924(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea538
	if (cr6.eq) goto loc_825EA538;
	// stw r24,15560(r31)
	PPC_STORE_U32(r31.u32 + 15560, r24.u32);
	// li r30,6
	r30.s64 = 6;
	// stw r24,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r24.u32);
	// b 0x825ea708
	goto loc_825EA708;
loc_825EA538:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82625d78
	sub_82625D78(ctx, base);
	// stw r24,15560(r31)
	PPC_STORE_U32(r31.u32 + 15560, r24.u32);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r24,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r24.u32);
	// b 0x825ea708
	goto loc_825EA708;
loc_825EA550:
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ea618
	if (!cr6.eq) goto loc_825EA618;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f50e0
	sub_825F50E0(ctx, base);
	// lwz r11,14820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14820);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea584
	if (cr6.eq) goto loc_825EA584;
	// lwz r11,14824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x825ea648
	if (!cr6.eq) goto loc_825EA648;
loc_825EA584:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bge cr6,0x825ea648
	if (!cr6.lt) goto loc_825EA648;
	// lwz r11,19696(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19696);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea648
	if (cr6.eq) goto loc_825EA648;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 184);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r9,164(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r6,220(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r5,172(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// lwz r3,3732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// lwz r30,15856(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 15856);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mtctr r30
	ctr.u64 = r30.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r30,196(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 196);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,168(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 168);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r7,224(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r6,176(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r3,3736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r29,15852(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 15852);
	// stw r24,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r24.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// mtctr r29
	ctr.u64 = r29.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x825ea648
	goto loc_825EA648;
loc_825EA618:
	// lwz r11,3376(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3376);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ea62c
	if (!cr6.eq) goto loc_825EA62C;
	// stw r27,3376(r31)
	PPC_STORE_U32(r31.u32 + 3376, r27.u32);
	// b 0x825ea638
	goto loc_825EA638;
loc_825EA62C:
	// lwz r11,3396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea640
	if (cr6.eq) goto loc_825EA640;
loc_825EA638:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
loc_825EA640:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82627880
	sub_82627880(ctx, base);
loc_825EA648:
	// lwz r11,15900(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15900);
	// stw r27,3384(r31)
	PPC_STORE_U32(r31.u32 + 3384, r27.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea65c
	if (cr6.eq) goto loc_825EA65C;
	// stw r27,15908(r31)
	PPC_STORE_U32(r31.u32 + 15908, r27.u32);
loc_825EA65C:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x825ea6cc
	if (cr6.lt) goto loc_825EA6CC;
	// lwz r11,14820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14820);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea67c
	if (cr6.eq) goto loc_825EA67C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f7ab8
	sub_825F7AB8(ctx, base);
loc_825EA67C:
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea690
	if (cr6.eq) goto loc_825EA690;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f3a70
	sub_825F3A70(ctx, base);
loc_825EA690:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82610690
	sub_82610690(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82625230
	sub_82625230(ctx, base);
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea6b8
	if (cr6.eq) goto loc_825EA6B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8271ce50
	sub_8271CE50(ctx, base);
	// b 0x825ea6dc
	goto loc_825EA6DC;
loc_825EA6B8:
	// lwz r11,3924(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea6cc
	if (cr6.eq) goto loc_825EA6CC;
	// li r30,6
	r30.s64 = 6;
	// b 0x825ea6e0
	goto loc_825EA6E0;
loc_825EA6CC:
	// lwz r11,15776(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15776);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_825EA6DC:
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_825EA6E0:
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21552);
	// stw r10,608(r11)
	PPC_STORE_U32(r11.u32 + 608, ctx.r10.u32);
	// bne cr6,0x825ea700
	if (!cr6.eq) goto loc_825EA700;
	// li r22,4
	r22.s64 = 4;
	// mr r30,r27
	r30.u64 = r27.u64;
	// b 0x825ea708
	goto loc_825EA708;
loc_825EA700:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825e9c2c
	if (!cr6.eq) goto loc_825E9C2C;
loc_825EA708:
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ea718
	if (!cr6.eq) goto loc_825EA718;
	// stw r27,3380(r31)
	PPC_STORE_U32(r31.u32 + 3380, r27.u32);
loc_825EA718:
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// bne cr6,0x825eaab0
	if (!cr6.eq) goto loc_825EAAB0;
loc_825EA720:
	// li r22,4
	r22.s64 = 4;
loc_825EA724:
	// sth r24,3684(r31)
	PPC_STORE_U16(r31.u32 + 3684, r24.u16);
	// sth r24,0(r23)
	PPC_STORE_U16(r23.u32 + 0, r24.u16);
	// lwz r11,3452(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3452);
	// lwz r10,19976(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// cmpw cr6,r10,r28
	cr6.compare<int32_t>(ctx.r10.s32, r28.s32, xer);
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// stw r11,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r11.u32);
	// bne cr6,0x825ea7c8
	if (!cr6.eq) goto loc_825EA7C8;
	// lwz r9,19980(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpw cr6,r9,r27
	cr6.compare<int32_t>(ctx.r9.s32, r27.s32, xer);
	// bne cr6,0x825ea7c8
	if (!cr6.eq) goto loc_825EA7C8;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825ea770
	if (cr6.eq) goto loc_825EA770;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ea78c
	if (!cr6.eq) goto loc_825EA78C;
loc_825EA770:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825ea780
	if (cr6.eq) goto loc_825EA780;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825ea78c
	if (!cr6.eq) goto loc_825EA78C;
loc_825EA780:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,19984(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// bl 0x8261b310
	sub_8261B310(ctx, base);
loc_825EA78C:
	// lwz r11,21436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21436);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ea7c8
	if (!cr6.eq) goto loc_825EA7C8;
	// lwz r11,21072(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21072);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ea7c8
	if (!cr6.eq) goto loc_825EA7C8;
	// lwz r11,21076(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21076);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ea7c8
	if (!cr6.eq) goto loc_825EA7C8;
	// lwz r11,21432(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21432);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea7c8
	if (cr6.eq) goto loc_825EA7C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e4c18
	sub_825E4C18(ctx, base);
	// stw r24,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r24.u32);
loc_825EA7C8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e4a20
	sub_825E4A20(ctx, base);
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// bne cr6,0x825ea7dc
	if (!cr6.eq) goto loc_825EA7DC;
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
loc_825EA7DC:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
loc_825EA7E0:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
loc_825EA7E8:
	// lwz r10,15900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15900);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825ea800
	if (cr6.eq) goto loc_825EA800;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825ea800
	if (cr6.eq) goto loc_825EA800;
	// stw r27,15908(r31)
	PPC_STORE_U32(r31.u32 + 15908, r27.u32);
loc_825EA800:
	// lwz r11,14820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14820);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea814
	if (cr6.eq) goto loc_825EA814;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f7ab8
	sub_825F7AB8(ctx, base);
loc_825EA814:
	// mr r11,r24
	r11.u64 = r24.u64;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// stw r11,-25092(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25092, r11.u32);
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea838
	if (cr6.eq) goto loc_825EA838;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8271ccb0
	sub_8271CCB0(ctx, base);
	// b 0x825ea874
	goto loc_825EA874;
loc_825EA838:
	// lwz r11,3948(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3948);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea850
	if (cr6.eq) goto loc_825EA850;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82623628
	sub_82623628(ctx, base);
	// b 0x825ea874
	goto loc_825EA874;
loc_825EA850:
	// lwz r11,3924(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ea864
	if (cr6.eq) goto loc_825EA864;
	// li r30,6
	r30.s64 = 6;
	// b 0x825ea878
	goto loc_825EA878;
loc_825EA864:
	// lwz r11,15772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15772);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_825EA874:
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_825EA878:
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21552);
	// stw r10,608(r11)
	PPC_STORE_U32(r11.u32 + 608, ctx.r10.u32);
	// bne cr6,0x825ea898
	if (!cr6.eq) goto loc_825EA898;
	// li r22,4
	r22.s64 = 4;
	// mr r30,r27
	r30.u64 = r27.u64;
	// b 0x825ea8a0
	goto loc_825EA8A0;
loc_825EA898:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825e9c2c
	if (!cr6.eq) goto loc_825E9C2C;
loc_825EA8A0:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825eaa0c
	if (cr6.eq) goto loc_825EAA0C;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825ea8bc
	if (cr6.eq) goto loc_825EA8BC;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x825ea8c8
	if (!cr6.eq) goto loc_825EA8C8;
loc_825EA8BC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825ebe10
	sub_825EBE10(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_825EA8C8:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825e9c34
	if (!cr6.eq) goto loc_825E9C34;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x825ea98c
	if (cr6.lt) goto loc_825EA98C;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825ea98c
	if (cr6.eq) goto loc_825EA98C;
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ea98c
	if (!cr6.gt) goto loc_825EA98C;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825ea968
	if (!cr6.gt) goto loc_825EA968;
loc_825EA904:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r11,r27
	r11.u64 = r27.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// ble cr6,0x825ea958
	if (!cr6.gt) goto loc_825EA958;
loc_825EA914:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r8,3048(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3048);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// sthx r27,r9,r8
	PPC_STORE_U16(ctx.r9.u32 + ctx.r8.u32, r27.u16);
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r8,3048(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3048);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r27,2(r9)
	PPC_STORE_U16(ctx.r9.u32 + 2, r27.u16);
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x825ea914
	if (cr6.lt) goto loc_825EA914;
loc_825EA958:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x825ea904
	if (cr6.lt) goto loc_825EA904;
loc_825EA968:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r3,3052(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3052);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_825EA98C:
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825eaa0c
	if (!cr6.gt) goto loc_825EAA0C;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825eaa0c
	if (cr6.eq) goto loc_825EAA0C;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825eaa0c
	if (!cr6.gt) goto loc_825EAA0C;
loc_825EA9B4:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r11,r27
	r11.u64 = r27.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// ble cr6,0x825ea9fc
	if (!cr6.gt) goto loc_825EA9FC;
loc_825EA9C4:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r9,276(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 276);
	// mullw r10,r10,r7
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r10,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// rlwinm r8,r8,0,15,13
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825ea9c4
	if (cr6.lt) goto loc_825EA9C4;
loc_825EA9FC:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// cmplw cr6,r7,r11
	cr6.compare<uint32_t>(ctx.r7.u32, r11.u32, xer);
	// blt cr6,0x825ea9b4
	if (cr6.lt) goto loc_825EA9B4;
loc_825EAA0C:
	// stw r24,3668(r31)
	PPC_STORE_U32(r31.u32 + 3668, r24.u32);
	// b 0x825ea724
	goto loc_825EA724;
loc_825EAA14:
	// bl 0x825f30e8
	sub_825F30E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// stw r24,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r24.u32);
	// mr r27,r24
	r27.u64 = r24.u64;
	// stw r24,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r24.u32);
	// bl 0x82603700
	sub_82603700(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82620780
	sub_82620780(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825e9c2c
	if (!cr6.eq) goto loc_825E9C2C;
	// lwz r11,21072(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21072);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825eaa68
	if (cr6.eq) goto loc_825EAA68;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825eaa68
	if (cr6.eq) goto loc_825EAA68;
	// lwz r11,21516(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21516);
	// stw r24,21516(r31)
	PPC_STORE_U32(r31.u32 + 21516, r24.u32);
	// stw r11,21520(r31)
	PPC_STORE_U32(r31.u32 + 21520, r11.u32);
	// b 0x825eaa6c
	goto loc_825EAA6C;
loc_825EAA68:
	// stw r26,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r26.u32);
loc_825EAA6C:
	// rlwinm r11,r25,16,0,15
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 16) & 0xFFFF0000;
	// srawi r11,r11,28
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFFFF) != 0);
	r11.s64 = r11.s32 >> 28;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x825eaa90
	if (cr6.eq) goto loc_825EAA90;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825eaa90
	if (cr6.eq) goto loc_825EAA90;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261d270
	sub_8261D270(ctx, base);
	// b 0x825eaa98
	goto loc_825EAA98;
loc_825EAA90:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82625d78
	sub_82625D78(ctx, base);
loc_825EAA98:
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21552);
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// stw r10,608(r11)
	PPC_STORE_U32(r11.u32 + 608, ctx.r10.u32);
	// beq cr6,0x825ea720
	if (cr6.eq) goto loc_825EA720;
loc_825EAAB0:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825ea724
	if (cr6.eq) goto loc_825EA724;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e4ad8
	sub_825E4AD8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825EAACC"))) PPC_WEAK_FUNC(sub_825EAACC);
PPC_FUNC_IMPL(__imp__sub_825EAACC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EAAD0"))) PPC_WEAK_FUNC(sub_825EAAD0);
PPC_FUNC_IMPL(__imp__sub_825EAAD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// mr r24,r10
	r24.u64 = ctx.r10.u64;
	// li r22,1
	r22.s64 = 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825eab30
	if (cr6.eq) goto loc_825EAB30;
	// lwz r11,19768(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19768);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825eab30
	if (!cr6.eq) goto loc_825EAB30;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x826a8cc8
	sub_826A8CC8(ctx, base);
	// ld r11,19744(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 19744);
	// ld r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// stw r22,19768(r31)
	PPC_STORE_U32(r31.u32 + 19768, r22.u32);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// std r11,19744(r31)
	PPC_STORE_U64(r31.u32 + 19744, r11.u64);
loc_825EAB30:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825eab48
	if (cr6.eq) goto loc_825EAB48;
	// li r3,13
	ctx.r3.s64 = 13;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd30
	return;
loc_825EAB48:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825ead4c
	if (cr6.eq) goto loc_825EAD4C;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x825ead4c
	if (cr6.eq) goto loc_825EAD4C;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x825ead4c
	if (cr6.eq) goto loc_825EAD4C;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x825ead4c
	if (cr6.eq) goto loc_825EAD4C;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x825ead4c
	if (cr6.eq) goto loc_825EAD4C;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825ead4c
	if (cr6.eq) goto loc_825EAD4C;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x825ead4c
	if (cr6.eq) goto loc_825EAD4C;
	// lwz r11,21336(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21336);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825eabe0
	if (!cr6.eq) goto loc_825EABE0;
	// lis r4,12338
	ctx.r4.s64 = 808583168;
	// stw r22,21440(r31)
	PPC_STORE_U32(r31.u32 + 21440, r22.u32);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// stw r30,21444(r31)
	PPC_STORE_U32(r31.u32 + 21444, r30.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r28,21448(r31)
	PPC_STORE_U32(r31.u32 + 21448, r28.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r27,21452(r31)
	PPC_STORE_U32(r31.u32 + 21452, r27.u32);
	// li r5,12
	ctx.r5.s64 = 12;
	// stw r26,21456(r31)
	PPC_STORE_U32(r31.u32 + 21456, r26.u32);
	// ori r4,r4,13385
	ctx.r4.u64 = ctx.r4.u64 | 13385;
	// stw r25,21460(r31)
	PPC_STORE_U32(r31.u32 + 21460, r25.u32);
	// stw r24,21464(r31)
	PPC_STORE_U32(r31.u32 + 21464, r24.u32);
	// bl 0x825e7fb0
	sub_825E7FB0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2d38
	sub_825E2D38(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd30
	return;
loc_825EABE0:
	// addi r5,r1,104
	ctx.r5.s64 = ctx.r1.s64 + 104;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x825e55c8
	sub_825E55C8(ctx, base);
	// lwz r11,21412(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21412);
	// lwz r23,96(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r29,104(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// cmpw cr6,r23,r11
	cr6.compare<int32_t>(r23.s32, r11.s32, xer);
	// bne cr6,0x825eac60
	if (!cr6.eq) goto loc_825EAC60;
	// lwz r10,21416(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21416);
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// bne cr6,0x825eac60
	if (!cr6.eq) goto loc_825EAC60;
	// lis r4,12338
	ctx.r4.s64 = 808583168;
	// stw r22,21440(r31)
	PPC_STORE_U32(r31.u32 + 21440, r22.u32);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// stw r30,21444(r31)
	PPC_STORE_U32(r31.u32 + 21444, r30.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r28,21448(r31)
	PPC_STORE_U32(r31.u32 + 21448, r28.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r27,21452(r31)
	PPC_STORE_U32(r31.u32 + 21452, r27.u32);
	// li r5,12
	ctx.r5.s64 = 12;
	// stw r26,21456(r31)
	PPC_STORE_U32(r31.u32 + 21456, r26.u32);
	// ori r4,r4,13385
	ctx.r4.u64 = ctx.r4.u64 | 13385;
	// stw r25,21460(r31)
	PPC_STORE_U32(r31.u32 + 21460, r25.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r24,21464(r31)
	PPC_STORE_U32(r31.u32 + 21464, r24.u32);
	// bl 0x825e7fb0
	sub_825E7FB0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2d38
	sub_825E2D38(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd30
	return;
loc_825EAC60:
	// lwz r10,21420(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21420);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825eacc4
	if (!cr6.eq) goto loc_825EACC4;
	// cmpw cr6,r23,r11
	cr6.compare<int32_t>(r23.s32, r11.s32, xer);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// bgt cr6,0x825eac7c
	if (cr6.gt) goto loc_825EAC7C;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_825EAC7C:
	// lwz r11,21416(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21416);
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// ble cr6,0x825eac8c
	if (!cr6.gt) goto loc_825EAC8C;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_825EAC8C:
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// addi r11,r3,127
	r11.s64 = ctx.r3.s64 + 127;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21420(r31)
	PPC_STORE_U32(r31.u32 + 21420, ctx.r3.u32);
	// rlwinm r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	// stw r11,21424(r31)
	PPC_STORE_U32(r31.u32 + 21424, r11.u32);
	// bne cr6,0x825eacc4
	if (!cr6.eq) goto loc_825EACC4;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd30
	return;
loc_825EACC4:
	// li r11,0
	r11.s64 = 0;
	// lwz r8,21424(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 21424);
	// lis r4,12338
	ctx.r4.s64 = 808583168;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,12
	ctx.r5.s64 = 12;
	// ori r4,r4,13385
	ctx.r4.u64 = ctx.r4.u64 | 13385;
	// stw r11,21440(r31)
	PPC_STORE_U32(r31.u32 + 21440, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e7fb0
	sub_825E7FB0(ctx, base);
	// lis r4,12338
	ctx.r4.s64 = 808583168;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r10,21424(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21424);
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// lwz r9,21416(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21416);
	// li r5,12
	ctx.r5.s64 = 12;
	// lwz r8,21412(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 21412);
	// ori r4,r4,13385
	ctx.r4.u64 = ctx.r4.u64 | 13385;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r22,21440(r31)
	PPC_STORE_U32(r31.u32 + 21440, r22.u32);
	// stw r30,21444(r31)
	PPC_STORE_U32(r31.u32 + 21444, r30.u32);
	// stw r28,21448(r31)
	PPC_STORE_U32(r31.u32 + 21448, r28.u32);
	// stw r27,21452(r31)
	PPC_STORE_U32(r31.u32 + 21452, r27.u32);
	// stw r26,21456(r31)
	PPC_STORE_U32(r31.u32 + 21456, r26.u32);
	// stw r25,21460(r31)
	PPC_STORE_U32(r31.u32 + 21460, r25.u32);
	// stw r24,21464(r31)
	PPC_STORE_U32(r31.u32 + 21464, r24.u32);
	// bl 0x825e5690
	sub_825E5690(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2d38
	sub_825E2D38(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd30
	return;
loc_825EAD4C:
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825EAD58"))) PPC_WEAK_FUNC(sub_825EAD58);
PPC_FUNC_IMPL(__imp__sub_825EAD58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825ead84
	if (!cr6.eq) goto loc_825EAD84;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_825EAD84:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ead9c
	if (cr6.eq) goto loc_825EAD9C;
	// li r3,13
	ctx.r3.s64 = 13;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_825EAD9C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2cd8
	sub_825E2CD8(ctx, base);
	// cmpwi cr6,r30,-1
	cr6.compare<int32_t>(r30.s32, -1, xer);
	// beq cr6,0x825eadb4
	if (cr6.eq) goto loc_825EADB4;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825eadbc
	if (!cr6.eq) goto loc_825EADBC;
loc_825EADB4:
	// rlwinm r11,r30,0,20,27
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFF0;
	// ori r30,r11,15
	r30.u64 = r11.u64 | 15;
loc_825EADBC:
	// lwz r11,15300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15300);
	// li r28,1
	r28.s64 = 1;
	// stw r30,15556(r31)
	PPC_STORE_U32(r31.u32 + 15556, r30.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825eaddc
	if (!cr6.eq) goto loc_825EADDC;
	// lwz r11,15368(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15368);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825eadf8
	if (cr6.eq) goto loc_825EADF8;
loc_825EADDC:
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r5,15312(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,15308(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// bl 0x825e8e60
	sub_825E8E60(ctx, base);
	// sth r28,0(r29)
	PPC_STORE_U16(r29.u32 + 0, r28.u16);
	// b 0x825eae08
	goto loc_825EAE08;
loc_825EADF8:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e97d0
	sub_825E97D0(ctx, base);
loc_825EAE08:
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// bne cr6,0x825eae18
	if (!cr6.eq) goto loc_825EAE18;
	// stw r28,3676(r31)
	PPC_STORE_U32(r31.u32 + 3676, r28.u32);
loc_825EAE18:
	// lwz r11,3680(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3680);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825eae30
	if (cr6.eq) goto loc_825EAE30;
	// li r11,0
	r11.s64 = 0;
	// stw r28,3676(r31)
	PPC_STORE_U32(r31.u32 + 3676, r28.u32);
	// stw r11,3680(r31)
	PPC_STORE_U32(r31.u32 + 3680, r11.u32);
loc_825EAE30:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2d38
	sub_825E2D38(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825EAE44"))) PPC_WEAK_FUNC(sub_825EAE44);
PPC_FUNC_IMPL(__imp__sub_825EAE44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EAE48"))) PPC_WEAK_FUNC(sub_825EAE48);
PPC_FUNC_IMPL(__imp__sub_825EAE48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r21,0
	r21.s64 = 0;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// lwz r28,28(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// lwz r31,8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lis r11,9
	r11.s64 = 589824;
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// ori r23,r11,128
	r23.u64 = r11.u64 | 128;
	// lis r11,-32688
	r11.s64 = -2142240768;
	// mr r26,r9
	r26.u64 = ctx.r9.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// ori r22,r11,183
	r22.u64 = r11.u64 | 183;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825eaed4
	if (cr6.eq) goto loc_825EAED4;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x825eaed4
	if (cr6.eq) goto loc_825EAED4;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825aa0d8
	sub_825AA0D8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge cr6,0x825eaf1c
	if (!cr6.lt) goto loc_825EAF1C;
	// cmplw cr6,r3,r22
	cr6.compare<uint32_t>(ctx.r3.u32, r22.u32, xer);
	// bne cr6,0x825eb0a0
	if (!cr6.eq) goto loc_825EB0A0;
	// li r11,1
	r11.s64 = 1;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_825EAED4:
	// extsb r26,r27
	r26.s64 = r27.s8;
	// mr r27,r21
	r27.u64 = r21.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x825eaf1c
	if (!cr6.gt) goto loc_825EAF1C;
loc_825EAEE4:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// bl 0x825aa0d8
	sub_825AA0D8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge cr6,0x825eaf1c
	if (!cr6.lt) goto loc_825EAF1C;
	// cmplw cr6,r3,r22
	cr6.compare<uint32_t>(ctx.r3.u32, r22.u32, xer);
	// bne cr6,0x825eb0a0
	if (!cr6.eq) goto loc_825EB0A0;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmpw cr6,r27,r26
	cr6.compare<int32_t>(r27.s32, r26.s32, xer);
	// blt cr6,0x825eaee4
	if (cr6.lt) goto loc_825EAEE4;
loc_825EAF1C:
	// addi r11,r31,-1
	r11.s64 = r31.s64 + -1;
	// cmplwi cr6,r11,11
	cr6.compare<uint32_t>(r11.u32, 11, xer);
	// bgt cr6,0x825eb098
	if (cr6.gt) goto loc_825EB098;
	// lis r12,-32161
	r12.s64 = -2107703296;
	// addi r12,r12,-20672
	r12.s64 = r12.s64 + -20672;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825EAFC4;
	case 1:
		goto loc_825EAFE0;
	case 2:
		goto loc_825EAFFC;
	case 3:
		goto loc_825EB018;
	case 4:
		goto loc_825EB02C;
	case 5:
		goto loc_825EB040;
	case 6:
		goto loc_825EB070;
	case 7:
		goto loc_825EB084;
	case 8:
		goto loc_825EAF8C;
	case 9:
		goto loc_825EAFA8;
	case 10:
		goto loc_825EB058;
	case 11:
		goto loc_825EAF70;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-20540(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20540);
	// lwz r18,-20512(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20512);
	// lwz r18,-20484(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20484);
	// lwz r18,-20456(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20456);
	// lwz r18,-20436(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20436);
	// lwz r18,-20416(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20416);
	// lwz r18,-20368(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20368);
	// lwz r18,-20348(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20348);
	// lwz r18,-20596(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20596);
	// lwz r18,-20568(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20568);
	// lwz r18,-20392(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20392);
	// lwz r18,-20624(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + -20624);
loc_825EAF70:
	// lis r11,12849
	r11.s64 = 842072064;
	// li r10,12
	ctx.r10.s64 = 12;
	// ori r11,r11,22105
	r11.u64 = r11.u64 | 22105;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EAF8C:
	// lis r11,22101
	r11.s64 = 1448411136;
	// li r10,12
	ctx.r10.s64 = 12;
	// ori r11,r11,22857
	r11.u64 = r11.u64 | 22857;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EAFA8:
	// lis r11,12338
	r11.s64 = 808583168;
	// li r10,12
	ctx.r10.s64 = 12;
	// ori r11,r11,13385
	r11.u64 = r11.u64 | 13385;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EAFC4:
	// lis r11,12889
	r11.s64 = 844693504;
	// li r10,16
	ctx.r10.s64 = 16;
	// ori r11,r11,21849
	r11.u64 = r11.u64 | 21849;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EAFE0:
	// lis r11,22870
	r11.s64 = 1498808320;
	// li r10,16
	ctx.r10.s64 = 16;
	// ori r11,r11,22869
	r11.u64 = r11.u64 | 22869;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EAFFC:
	// lis r11,21849
	r11.s64 = 1431896064;
	// li r10,16
	ctx.r10.s64 = 16;
	// ori r11,r11,22105
	r11.u64 = r11.u64 | 22105;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EB018:
	// li r11,24
	r11.s64 = 24;
	// stw r21,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r21.u32);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EB02C:
	// li r11,16
	r11.s64 = 16;
	// stw r21,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r21.u32);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EB040:
	// li r11,3
	r11.s64 = 3;
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EB058:
	// li r11,3
	r11.s64 = 3;
	// li r10,12
	ctx.r10.s64 = 12;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EB070:
	// li r11,32
	r11.s64 = 32;
	// stw r21,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r21.u32);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EB084:
	// li r11,8
	r11.s64 = 8;
	// stw r21,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r21.u32);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_825EB098:
	// lis r3,-32688
	ctx.r3.s64 = -2142240768;
	// ori r3,r3,183
	ctx.r3.u64 = ctx.r3.u64 | 183;
loc_825EB0A0:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_825EB0A8"))) PPC_WEAK_FUNC(sub_825EB0A8);
PPC_FUNC_IMPL(__imp__sub_825EB0A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mr r21,r5
	r21.u64 = ctx.r5.u64;
	// lwz r27,28(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mr r23,r6
	r23.u64 = ctx.r6.u64;
	// rlwinm r9,r10,0,30,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r20,r8
	r20.u64 = ctx.r8.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r31,8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r29,12(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// beq cr6,0x825eb0f4
	if (cr6.eq) goto loc_825EB0F4;
	// lwz r30,12(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// b 0x825eb0f8
	goto loc_825EB0F8;
loc_825EB0F4:
	// mr r30,r31
	r30.u64 = r31.u64;
loc_825EB0F8:
	// rlwinm r11,r10,0,29,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eb10c
	if (cr6.eq) goto loc_825EB10C;
	// lwz r26,16(r3)
	r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// b 0x825eb110
	goto loc_825EB110;
loc_825EB10C:
	// mr r26,r29
	r26.u64 = r29.u64;
loc_825EB110:
	// lis r11,-32688
	r11.s64 = -2142240768;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// ori r25,r11,3
	r25.u64 = r11.u64 | 3;
	// lis r11,-32688
	r11.s64 = -2142240768;
	// ori r24,r11,182
	r24.u64 = r11.u64 | 182;
	// ble cr6,0x825eb164
	if (!cr6.gt) goto loc_825EB164;
	// lis r5,9
	ctx.r5.s64 = 589824;
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r30.u32);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r31.u32);
	// ori r5,r5,144
	ctx.r5.u64 = ctx.r5.u64 | 144;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825aa0d8
	sub_825AA0D8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge cr6,0x825eb168
	if (!cr6.lt) goto loc_825EB168;
	// cmplw cr6,r3,r25
	cr6.compare<uint32_t>(ctx.r3.u32, r25.u32, xer);
	// beq cr6,0x825eb160
	if (cr6.eq) goto loc_825EB160;
	// cmplw cr6,r3,r24
	cr6.compare<uint32_t>(ctx.r3.u32, r24.u32, xer);
	// bne cr6,0x825eb278
	if (!cr6.eq) goto loc_825EB278;
loc_825EB160:
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
loc_825EB164:
	// bne cr6,0x825eb270
	if (!cr6.eq) goto loc_825EB270;
loc_825EB168:
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// cmplw cr6,r26,r29
	cr6.compare<uint32_t>(r26.u32, r29.u32, xer);
	// ble cr6,0x825eb1ac
	if (!cr6.gt) goto loc_825EB1AC;
	// lis r5,9
	ctx.r5.s64 = 589824;
	// stw r26,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r26.u32);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// ori r5,r5,160
	ctx.r5.u64 = ctx.r5.u64 | 160;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825aa0d8
	sub_825AA0D8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge cr6,0x825eb1b4
	if (!cr6.lt) goto loc_825EB1B4;
	// cmplw cr6,r3,r25
	cr6.compare<uint32_t>(ctx.r3.u32, r25.u32, xer);
	// beq cr6,0x825eb1ac
	if (cr6.eq) goto loc_825EB1AC;
	// cmplw cr6,r3,r24
	cr6.compare<uint32_t>(ctx.r3.u32, r24.u32, xer);
	// bne cr6,0x825eb278
	if (!cr6.eq) goto loc_825EB278;
loc_825EB1AC:
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// bne cr6,0x825eb270
	if (!cr6.eq) goto loc_825EB270;
loc_825EB1B4:
	// lis r5,9
	ctx.r5.s64 = 589824;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// ori r5,r5,176
	ctx.r5.u64 = ctx.r5.u64 | 176;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825aa0d8
	sub_825AA0D8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825eb278
	if (cr6.lt) goto loc_825EB278;
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// stw r10,0(r22)
	PPC_STORE_U32(r22.u32 + 0, ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// bne cr6,0x825eb270
	if (!cr6.eq) goto loc_825EB270;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825eb270
	if (!cr6.eq) goto loc_825EB270;
	// cmplwi cr6,r21,12
	cr6.compare<uint32_t>(r21.u32, 12, xer);
	// bne cr6,0x825eb270
	if (!cr6.eq) goto loc_825EB270;
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,1
	ctx.r9.s64 = 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lis r5,9
	ctx.r5.s64 = 589824;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,7
	r11.s64 = r11.s64 + 7;
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// ori r5,r5,112
	ctx.r5.u64 = ctx.r5.u64 | 112;
	// rlwinm r11,r11,29,3,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// bl 0x825aa0d8
	sub_825AA0D8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825eb278
	if (cr6.lt) goto loc_825EB278;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,31,3,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x1FFFFFFF;
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd28
	return;
loc_825EB270:
	// lis r3,-32688
	ctx.r3.s64 = -2142240768;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
loc_825EB278:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_825EB280"))) PPC_WEAK_FUNC(sub_825EB280);
PPC_FUNC_IMPL(__imp__sub_825EB280) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_825EB290:
	// std r9,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bdnz 0x825eb290
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_825EB290;
	// stw r4,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, ctx.r4.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EB2A4"))) PPC_WEAK_FUNC(sub_825EB2A4);
PPC_FUNC_IMPL(__imp__sub_825EB2A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EB2A8"))) PPC_WEAK_FUNC(sub_825EB2A8);
PPC_FUNC_IMPL(__imp__sub_825EB2A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825eb2bc
	if (!cr6.eq) goto loc_825EB2BC;
	// li r11,8
	r11.s64 = 8;
loc_825EB2BC:
	// subfic r10,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r10.s64 = 64 - r11.s64;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EB2D8"))) PPC_WEAK_FUNC(sub_825EB2D8);
PPC_FUNC_IMPL(__imp__sub_825EB2D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcec
	// lwz r11,40(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// li r10,-1
	ctx.r10.s64 = -1;
	// lwz r9,36(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lwz r26,0(r5)
	r26.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// lwz r29,21320(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 21320);
	// bne cr6,0x825eb308
	if (!cr6.eq) goto loc_825EB308;
	// li r10,-256
	ctx.r10.s64 = -256;
	// b 0x825eb314
	goto loc_825EB314;
loc_825EB308:
	// cmpwi cr6,r9,2
	cr6.compare<int32_t>(ctx.r9.s32, 2, xer);
	// bne cr6,0x825eb314
	if (!cr6.eq) goto loc_825EB314;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825EB314:
	// clrlwi r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	// li r31,0
	r31.s64 = 0;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eb36c
	if (cr6.eq) goto loc_825EB36C;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x825eb36c
	if (!cr6.gt) goto loc_825EB36C;
	// clrlwi r11,r10,16
	r11.u64 = ctx.r10.u32 & 0xFFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825eb348
	if (!cr6.eq) goto loc_825EB348;
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// beq cr6,0x825eb354
	if (cr6.eq) goto loc_825EB354;
loc_825EB348:
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// stb r10,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r10.u8);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
loc_825EB354:
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// li r31,1
	r31.s64 = 1;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// addi r6,r4,1
	ctx.r6.s64 = ctx.r4.s64 + 1;
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
loc_825EB36C:
	// addi r28,r26,-1
	r28.s64 = r26.s64 + -1;
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
	// bge cr6,0x825eb418
	if (!cr6.lt) goto loc_825EB418;
	// subf r11,r31,r28
	r11.s64 = r28.s64 - r31.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// rlwinm r11,r30,1,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
loc_825EB390:
	// lhz r11,0(r6)
	r11.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// clrlwi r8,r10,16
	ctx.r8.u64 = ctx.r10.u32 & 0xFFFF;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// and r25,r7,r8
	r25.u64 = ctx.r7.u64 & ctx.r8.u64;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x825eb3f0
	if (!cr6.eq) goto loc_825EB3F0;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825eb3cc
	if (!cr6.eq) goto loc_825EB3CC;
	// rlwinm r25,r7,0,16,23
	r25.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFF00;
	// cmplwi cr6,r25,768
	cr6.compare<uint32_t>(r25.u32, 768, xer);
	// bne cr6,0x825eb3cc
	if (!cr6.eq) goto loc_825EB3CC;
	// stb r11,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r11.u8);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// b 0x825eb404
	goto loc_825EB404;
loc_825EB3CC:
	// clrlwi r8,r8,24
	ctx.r8.u64 = ctx.r8.u32 & 0xFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825eb3f0
	if (!cr6.eq) goto loc_825EB3F0;
	// cmplwi cr6,r7,3
	cr6.compare<uint32_t>(ctx.r7.u32, 3, xer);
	// bne cr6,0x825eb3f0
	if (!cr6.eq) goto loc_825EB3F0;
	// rlwinm r11,r11,24,24,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// stb r11,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r11.u8);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// b 0x825eb404
	goto loc_825EB404;
loc_825EB3F0:
	// rlwinm r11,r10,24,24,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFF;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// stb r11,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r11.u8);
	// stb r8,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r8.u8);
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
loc_825EB404:
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825eb390
	if (!cr6.eq) goto loc_825EB390;
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
loc_825EB418:
	// bne cr6,0x825eb4d8
	if (!cr6.eq) goto loc_825EB4D8;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// lbz r11,0(r6)
	r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825eb4ac
	if (!cr6.eq) goto loc_825EB4AC;
	// clrlwi r10,r11,24
	ctx.r10.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// beq cr6,0x825eb440
	if (cr6.eq) goto loc_825EB440;
	// stb r11,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r11.u8);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
loc_825EB440:
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825eb560
	if (!cr6.eq) goto loc_825EB560;
	// add r31,r27,r26
	r31.u64 = r27.u64 + r26.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x825eb560
	if (!cr6.gt) goto loc_825EB560;
	// add r11,r26,r4
	r11.u64 = r26.u64 + ctx.r4.u64;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// lis r11,21845
	r11.s64 = 1431633920;
	// ori r11,r11,21846
	r11.u64 = r11.u64 | 21846;
loc_825EB468:
	// cmpw cr6,r10,r26
	cr6.compare<int32_t>(ctx.r10.s32, r26.s32, xer);
	// bge cr6,0x825eb47c
	if (!cr6.lt) goto loc_825EB47C;
	// lbz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825eb560
	if (!cr6.eq) goto loc_825EB560;
loc_825EB47C:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mulhw r8,r9,r11
	ctx.r8.s64 = (int64_t(ctx.r9.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r6,r8,1,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// cmpw cr6,r10,r31
	cr6.compare<int32_t>(ctx.r10.s32, r31.s32, xer);
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// blt cr6,0x825eb468
	if (cr6.lt) goto loc_825EB468;
	// b 0x825eb560
	goto loc_825EB560;
loc_825EB4AC:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// stb r11,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r11.u8);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// clrlwi r10,r11,24
	ctx.r10.u64 = r11.u32 & 0xFF;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// bne cr6,0x825eb4d0
	if (!cr6.eq) goto loc_825EB4D0;
	// rlwinm r9,r10,28,30,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0x2;
	// b 0x825eb560
	goto loc_825EB560;
loc_825EB4D0:
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// b 0x825eb560
	goto loc_825EB560;
loc_825EB4D8:
	// cmpw cr6,r31,r26
	cr6.compare<int32_t>(r31.s32, r26.s32, xer);
	// bne cr6,0x825eb560
	if (!cr6.eq) goto loc_825EB560;
	// clrlwi r11,r10,16
	r11.u64 = ctx.r10.u32 & 0xFFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825eb554
	if (!cr6.eq) goto loc_825EB554;
	// add r31,r27,r26
	r31.u64 = r27.u64 + r26.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x825eb560
	if (!cr6.gt) goto loc_825EB560;
	// add r11,r26,r4
	r11.u64 = r26.u64 + ctx.r4.u64;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// lis r11,21845
	r11.s64 = 1431633920;
	// ori r11,r11,21846
	r11.u64 = r11.u64 | 21846;
loc_825EB510:
	// cmpw cr6,r10,r26
	cr6.compare<int32_t>(ctx.r10.s32, r26.s32, xer);
	// bge cr6,0x825eb524
	if (!cr6.lt) goto loc_825EB524;
	// lbz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825eb560
	if (!cr6.eq) goto loc_825EB560;
loc_825EB524:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mulhw r8,r9,r11
	ctx.r8.s64 = (int64_t(ctx.r9.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r6,r8,1,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// cmpw cr6,r10,r31
	cr6.compare<int32_t>(ctx.r10.s32, r31.s32, xer);
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// blt cr6,0x825eb510
	if (cr6.lt) goto loc_825EB510;
	// b 0x825eb560
	goto loc_825EB560;
loc_825EB554:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r9,r11,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
loc_825EB560:
	// lwz r11,40(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// lwz r10,21320(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21320);
	// stw r9,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r9.u32);
	// lwz r11,21320(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 21320);
	// subf r10,r10,r29
	ctx.r10.s64 = r29.s64 - ctx.r10.s64;
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// stw r9,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r9.u32);
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825EB58C"))) PPC_WEAK_FUNC(sub_825EB58C);
PPC_FUNC_IMPL(__imp__sub_825EB58C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EB590"))) PPC_WEAK_FUNC(sub_825EB590);
PPC_FUNC_IMPL(__imp__sub_825EB590) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r30,r31,24
	r30.s64 = r31.s64 + 24;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// bl 0x82491030
	sub_82491030(ctx, base);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r11,15472(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825eb634
	if (!cr6.eq) goto loc_825EB634;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825eb634
	if (cr6.eq) goto loc_825EB634;
	// lwz r4,88(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lwz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// bl 0x825fee98
	sub_825FEE98(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825eb610
	if (cr6.eq) goto loc_825EB610;
	// li r11,1
	r11.s64 = 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
loc_825EB610:
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825eb64c
	if (cr6.eq) goto loc_825EB64C;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r6,0(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x825eb2d8
	sub_825EB2D8(ctx, base);
	// b 0x825eb64c
	goto loc_825EB64C;
loc_825EB634:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r10,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r10.u32);
loc_825EB64C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EB664"))) PPC_WEAK_FUNC(sub_825EB664);
PPC_FUNC_IMPL(__imp__sub_825EB664) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EB668"))) PPC_WEAK_FUNC(sub_825EB668);
PPC_FUNC_IMPL(__imp__sub_825EB668) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// add r10,r4,r5
	ctx.r10.u64 = ctx.r4.u64 + ctx.r5.u64;
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r7,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r7.u32);
	// li r9,-16
	ctx.r9.s64 = -16;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, r11.u64);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stw r10,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r10.u32);
	// beq cr6,0x825eb6bc
	if (cr6.eq) goto loc_825EB6BC;
	// addi r5,r1,132
	ctx.r5.s64 = ctx.r1.s64 + 132;
	// stw r11,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, r11.u32);
	// bl 0x825eb2d8
	sub_825EB2D8(ctx, base);
	// lwz r4,12(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
loc_825EB6BC:
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// bgt cr6,0x825eb708
	if (cr6.gt) goto loc_825EB708;
loc_825EB6C8:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmpwi cr6,r11,40
	cr6.compare<int32_t>(r11.s32, 40, xer);
	// bgt cr6,0x825eb708
	if (cr6.gt) goto loc_825EB708;
	// subfic r8,r11,40
	xer.ca = r11.u32 <= 40;
	ctx.r8.s64 = 40 - r11.s64;
	// lbz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// ld r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, r11.u64);
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// ble cr6,0x825eb6c8
	if (!cr6.gt) goto loc_825EB6C8;
loc_825EB708:
	// stw r4,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r4.u32);
	// stw r31,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, r31.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EB724"))) PPC_WEAK_FUNC(sub_825EB724);
PPC_FUNC_IMPL(__imp__sub_825EB724) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EB728"))) PPC_WEAK_FUNC(sub_825EB728);
PPC_FUNC_IMPL(__imp__sub_825EB728) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r30,r29,8
	r30.s64 = r29.s64 + 8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrlwi r11,r10,29
	r11.u64 = ctx.r10.u32 & 0x7;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825eb754
	if (cr6.eq) goto loc_825EB754;
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
loc_825EB754:
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// ble cr6,0x825eb7d4
	if (!cr6.gt) goto loc_825EB7D4;
loc_825EB764:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x825eb7ac
	if (cr6.gt) goto loc_825EB7AC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r10,40
	cr6.compare<int32_t>(ctx.r10.s32, 40, xer);
	// bgt cr6,0x825eb7fc
	if (cr6.gt) goto loc_825EB7FC;
	// subfic r7,r10,40
	xer.ca = ctx.r10.u32 <= 40;
	ctx.r7.s64 = 40 - ctx.r10.s64;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// sld r10,r8,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r7.u8 & 0x7F));
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// b 0x825eb7c4
	goto loc_825EB7C4;
loc_825EB7AC:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825eb7d4
	if (!cr6.eq) goto loc_825EB7D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb590
	sub_825EB590(ctx, base);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
loc_825EB7C4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bgt cr6,0x825eb764
	if (cr6.gt) goto loc_825EB764;
loc_825EB7D4:
	// subfic r11,r30,64
	xer.ca = r30.u32 <= 64;
	r11.s64 = 64 - r30.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
loc_825EB7E4:
	// li r10,-1
	ctx.r10.s64 = -1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// srw r10,r10,r29
	ctx.r10.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r29.u8 & 0x3F));
	// and r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 & r11.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_825EB7FC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r9
	cr6.compare<uint32_t>(r30.u32, ctx.r9.u32, xer);
	// ble cr6,0x825eb7d4
	if (!cr6.gt) goto loc_825EB7D4;
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bgt cr6,0x825eb7d4
	if (cr6.gt) goto loc_825EB7D4;
	// addi r8,r10,248
	ctx.r8.s64 = ctx.r10.s64 + 248;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r9,r30,32
	xer.ca = r30.u32 <= 32;
	ctx.r9.s64 = 32 - r30.s64;
	// clrlwi r8,r8,24
	ctx.r8.u64 = ctx.r8.u32 & 0xFF;
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// srd r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r8.u8 & 0x7F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srd r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// b 0x825eb7e4
	goto loc_825EB7E4;
}

__attribute__((alias("__imp__sub_825EB840"))) PPC_WEAK_FUNC(sub_825EB840);
PPC_FUNC_IMPL(__imp__sub_825EB840) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x825eb8a0
	if (cr6.gt) goto loc_825EB8A0;
loc_825EB85C:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r10,8
	ctx.r9.s64 = ctx.r10.s64 + 8;
	// ld r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// subfic r10,r10,40
	xer.ca = ctx.r10.u32 <= 40;
	ctx.r10.s64 = 40 - ctx.r10.s64;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// sld r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r7.u64 << (ctx.r10.u8 & 0x7F));
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// std r10,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r10.u64);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x825eb85c
	if (!cr6.gt) goto loc_825EB85C;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// bge cr6,0x825eb8ec
	if (!cr6.lt) goto loc_825EB8EC;
loc_825EB8A0:
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825eb8c4
	if (!cr6.eq) goto loc_825EB8C4;
	// bl 0x825eb590
	sub_825EB590(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825EB8C4:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmpwi cr6,r11,-16
	cr6.compare<int32_t>(r11.s32, -16, xer);
	// bge cr6,0x825eb8ec
	if (!cr6.lt) goto loc_825EB8EC;
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825eb8e4
	if (!cr6.eq) goto loc_825EB8E4;
	// li r11,2
	r11.s64 = 2;
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
loc_825EB8E4:
	// li r11,127
	r11.s64 = 127;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
loc_825EB8EC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EB900"))) PPC_WEAK_FUNC(sub_825EB900);
PPC_FUNC_IMPL(__imp__sub_825EB900) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
loc_825EB914:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825eb94c
	if (cr6.lt) goto loc_825EB94C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x825eb914
	if (cr6.eq) goto loc_825EB914;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825EB94C:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r5,r11,6
	ctx.r5.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r10,r10,8,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lbz r9,5(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,3(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// rldicr r11,r10,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r10.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// stw r5,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r5.u32);
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// neg r8,r11
	ctx.r8.s64 = -r11.s64;
	// addi r11,r11,48
	r11.s64 = r11.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EB9CC"))) PPC_WEAK_FUNC(sub_825EB9CC);
PPC_FUNC_IMPL(__imp__sub_825EB9CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EB9D0"))) PPC_WEAK_FUNC(sub_825EB9D0);
PPC_FUNC_IMPL(__imp__sub_825EB9D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// clrldi r10,r4,32
	ctx.r10.u64 = ctx.r4.u64 & 0xFFFFFFFF;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge 0x825ebb28
	if (!cr0.lt) goto loc_825EBB28;
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825eba90
	if (cr6.lt) goto loc_825EBA90;
loc_825EBA18:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x825eba68
	if (cr6.gt) goto loc_825EBA68;
loc_825EBA24:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r10,8
	ctx.r9.s64 = ctx.r10.s64 + 8;
	// ld r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r10,r10,40
	xer.ca = ctx.r10.u32 <= 40;
	ctx.r10.s64 = 40 - ctx.r10.s64;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// sld r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r7.u64 << (ctx.r10.u8 & 0x7F));
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x825eba24
	if (!cr6.gt) goto loc_825EBA24;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// bge cr6,0x825ebb28
	if (!cr6.lt) goto loc_825EBB28;
loc_825EBA68:
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x825ebafc
	if (!cr6.eq) goto loc_825EBAFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb590
	sub_825EB590(ctx, base);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x825eba18
	if (!cr6.lt) goto loc_825EBA18;
loc_825EBA90:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r4,2(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r5,3(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r7,5(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
	// rldicr r8,r9,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// rldicr r8,r8,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// rldicr r8,r8,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// rldicr r8,r8,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r10,r8,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r7.u8 & 0x7F));
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// b 0x825ebb24
	goto loc_825EBB24;
loc_825EBAFC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r10,-16
	cr6.compare<int32_t>(ctx.r10.s32, -16, xer);
	// bge cr6,0x825ebb24
	if (!cr6.lt) goto loc_825EBB24;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825ebb1c
	if (!cr6.eq) goto loc_825EBB1C;
	// li r10,2
	ctx.r10.s64 = 2;
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
loc_825EBB1C:
	// li r10,127
	ctx.r10.s64 = 127;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
loc_825EBB24:
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
loc_825EBB28:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EBB3C"))) PPC_WEAK_FUNC(sub_825EBB3C);
PPC_FUNC_IMPL(__imp__sub_825EBB3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EBB40"))) PPC_WEAK_FUNC(sub_825EBB40);
PPC_FUNC_IMPL(__imp__sub_825EBB40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,452(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 452);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ebb58
	if (!cr6.eq) goto loc_825EBB58;
	// lwz r11,3112(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3112);
	// lwz r10,3116(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3116);
	// b 0x825ebb84
	goto loc_825EBB84;
loc_825EBB58:
	// lwz r11,3900(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3900);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ebb8c
	if (cr6.eq) goto loc_825EBB8C;
	// lwz r11,3904(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3904);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ebb7c
	if (cr6.eq) goto loc_825EBB7C;
	// lwz r11,3092(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3092);
	// lwz r10,3088(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3088);
	// b 0x825ebb84
	goto loc_825EBB84;
loc_825EBB7C:
	// lwz r11,3100(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3100);
	// lwz r10,3096(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3096);
loc_825EBB84:
	// stw r11,3084(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3084, r11.u32);
	// stw r10,3080(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3080, ctx.r10.u32);
loc_825EBB8C:
	// lwz r11,3900(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3900);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r11,3904(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3904);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ebbb8
	if (cr6.eq) goto loc_825EBBB8;
	// lwz r11,3092(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3092);
	// lwz r10,3088(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3088);
	// stw r11,3104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3104, r11.u32);
	// stw r10,3108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3108, ctx.r10.u32);
	// blr 
	return;
loc_825EBBB8:
	// lwz r11,3100(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3100);
	// lwz r10,3096(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3096);
	// stw r11,3104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3104, r11.u32);
	// stw r10,3108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3108, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EBBCC"))) PPC_WEAK_FUNC(sub_825EBBCC);
PPC_FUNC_IMPL(__imp__sub_825EBBCC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EBBD0"))) PPC_WEAK_FUNC(sub_825EBBD0);
PPC_FUNC_IMPL(__imp__sub_825EBBD0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lbz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r11,1(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// lbz r9,5(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5);
	// lbz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
	// lbz r7,3(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 3);
	// stb r10,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r10.u8);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lbz r11,2(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// stb r9,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r9.u8);
	// stb r8,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r8.u8);
	// stb r7,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, ctx.r7.u8);
	// stb r10,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r10.u8);
	// stb r11,3(r3)
	PPC_STORE_U8(ctx.r3.u32 + 3, r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EBC08"))) PPC_WEAK_FUNC(sub_825EBC08);
PPC_FUNC_IMPL(__imp__sub_825EBC08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r8,15472(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15472);
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r4,248(r3)
	PPC_STORE_U32(ctx.r3.u32 + 248, ctx.r4.u32);
	// cmpwi cr6,r8,6
	cr6.compare<int32_t>(ctx.r8.s32, 6, xer);
	// blt cr6,0x825ebcc4
	if (cr6.lt) goto loc_825EBCC4;
	// lwz r11,252(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 252);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r10,6548(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 6548);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r8,320(r3)
	PPC_STORE_U32(ctx.r3.u32 + 320, ctx.r8.u32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -16);
	// stw r10,316(r3)
	PPC_STORE_U32(ctx.r3.u32 + 316, ctx.r10.u32);
	// lwz r10,-20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -20);
	// stw r10,312(r3)
	PPC_STORE_U32(ctx.r3.u32 + 312, ctx.r10.u32);
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r10,2968(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2968);
	// stw r11,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, r11.u32);
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// stw r11,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825ebc98
	if (cr6.eq) goto loc_825EBC98;
	// lwz r11,1900(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// sth r8,16(r11)
	PPC_STORE_U16(r11.u32 + 16, ctx.r8.u16);
	// lwz r11,1900(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// lwz r11,1904(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// sth r8,16(r11)
	PPC_STORE_U16(r11.u32 + 16, ctx.r8.u16);
	// lwz r11,1904(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// blr 
	return;
loc_825EBC98:
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// twllei r11,0
	// addi r10,r10,1024
	ctx.r10.s64 = ctx.r10.s64 + 1024;
	// divw r9,r10,r11
	ctx.r9.s32 = ctx.r10.s32 / r11.s32;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// andc r11,r11,r10
	r11.u64 = r11.u64 & ~ctx.r10.u64;
	// twlgei r11,-1
	// lwz r11,1900(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// sth r9,16(r11)
	PPC_STORE_U16(r11.u32 + 16, ctx.r9.u16);
	// b 0x825ebdc8
	goto loc_825EBDC8;
loc_825EBCC4:
	// not r11,r4
	r11.u64 = ~ctx.r4.u64;
	// stw r9,312(r3)
	PPC_STORE_U32(ctx.r3.u32 + 312, ctx.r9.u32);
	// li r10,8
	ctx.r10.s64 = 8;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r8,3
	cr6.compare<int32_t>(ctx.r8.s32, 3, xer);
	// subf r7,r11,r4
	ctx.r7.s64 = ctx.r4.s64 - r11.s64;
	// stw r10,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r10.u32);
	// stw r10,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r10.u32);
	// stw r11,320(r3)
	PPC_STORE_U32(ctx.r3.u32 + 320, r11.u32);
	// stw r7,316(r3)
	PPC_STORE_U32(ctx.r3.u32 + 316, ctx.r7.u32);
	// bge cr6,0x825ebcf8
	if (!cr6.lt) goto loc_825EBCF8;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bnelr cr6
	if (!cr6.eq) return;
loc_825EBCF8:
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// bgt cr6,0x825ebd28
	if (cr6.gt) goto loc_825EBD28;
	// lwz r11,14756(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14756);
	// stw r10,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r10.u32);
	// beq cr6,0x825ebd94
	if (cr6.eq) goto loc_825EBD94;
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// bgt cr6,0x825ebd94
	if (cr6.gt) goto loc_825EBD94;
	// stw r9,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r9.u32);
	// stw r9,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r9.u32);
	// b 0x825ebd94
	goto loc_825EBD94;
loc_825EBD28:
	// cmpwi cr6,r8,4
	cr6.compare<int32_t>(ctx.r8.s32, 4, xer);
	// blt cr6,0x825ebd40
	if (cr6.lt) goto loc_825EBD40;
	// srawi r11,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	r11.s64 = ctx.r4.s32 >> 1;
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
	// stw r11,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, r11.u32);
	// b 0x825ebd90
	goto loc_825EBD90;
loc_825EBD40:
	// cmpwi cr6,r4,8
	cr6.compare<int32_t>(ctx.r4.s32, 8, xer);
	// bgt cr6,0x825ebd60
	if (cr6.gt) goto loc_825EBD60;
	// addi r11,r4,13
	r11.s64 = ctx.r4.s64 + 13;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stw r10,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r10.u32);
	// stw r11,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, r11.u32);
	// b 0x825ebd94
	goto loc_825EBD94;
loc_825EBD60:
	// cmpwi cr6,r4,24
	cr6.compare<int32_t>(ctx.r4.s32, 24, xer);
	// bgt cr6,0x825ebd80
	if (cr6.gt) goto loc_825EBD80;
	// addi r11,r4,13
	r11.s64 = ctx.r4.s64 + 13;
	// addi r10,r4,8
	ctx.r10.s64 = ctx.r4.s64 + 8;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stw r10,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r10.u32);
	// stw r11,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, r11.u32);
	// b 0x825ebd94
	goto loc_825EBD94;
loc_825EBD80:
	// addi r10,r4,-6
	ctx.r10.s64 = ctx.r4.s64 + -6;
	// addi r11,r4,-8
	r11.s64 = ctx.r4.s64 + -8;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r10.u32);
loc_825EBD90:
	// stw r11,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, r11.u32);
loc_825EBD94:
	// cmpwi cr6,r8,3
	cr6.compare<int32_t>(ctx.r8.s32, 3, xer);
	// bltlr cr6
	if (cr6.lt) return;
	// lwz r11,296(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 296);
	// lwz r9,1900(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// twllei r11,0
	// addi r10,r10,1024
	ctx.r10.s64 = ctx.r10.s64 + 1024;
	// divw r8,r10,r11
	ctx.r8.s32 = ctx.r10.s32 / r11.s32;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// andc r11,r11,r10
	r11.u64 = r11.u64 & ~ctx.r10.u64;
	// sth r8,16(r9)
	PPC_STORE_U16(ctx.r9.u32 + 16, ctx.r8.u16);
	// twlgei r11,-1
loc_825EBDC8:
	// lwz r11,1900(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// lhz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 16);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// lwz r11,300(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 300);
	// lwz r9,1904(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// twllei r11,0
	// addi r10,r10,1024
	ctx.r10.s64 = ctx.r10.s64 + 1024;
	// divw r8,r10,r11
	ctx.r8.s32 = ctx.r10.s32 / r11.s32;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// andc r11,r11,r10
	r11.u64 = r11.u64 & ~ctx.r10.u64;
	// sth r8,16(r9)
	PPC_STORE_U16(ctx.r9.u32 + 16, ctx.r8.u16);
	// twlgei r11,-1
	// lwz r11,1904(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// lhz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 16);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EBE10"))) PPC_WEAK_FUNC(sub_825EBE10);
PPC_FUNC_IMPL(__imp__sub_825EBE10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r11,15472(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15472);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// blt cr6,0x825ebef8
	if (cr6.lt) goto loc_825EBEF8;
	// lwz r11,3884(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3884);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ec17c
	if (cr6.eq) goto loc_825EC17C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ebeac
	if (!cr6.lt) goto loc_825EBEAC;
loc_825EBE54:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ebeac
	if (cr6.eq) goto loc_825EBEAC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ebe9c
	if (!cr0.lt) goto loc_825EBE9C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EBE9C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ebe54
	if (cr6.gt) goto loc_825EBE54;
loc_825EBEAC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ebee8
	if (!cr0.lt) goto loc_825EBEE8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EBEE8:
	// stw r30,3948(r27)
	PPC_STORE_U32(r27.u32 + 3948, r30.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_825EBEF8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r28,0
	r28.s64 = 0;
	// li r30,5
	r30.s64 = 5;
	// mr r29,r28
	r29.u64 = r28.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825ebf70
	if (!cr6.lt) goto loc_825EBF70;
loc_825EBF18:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ebf70
	if (cr6.eq) goto loc_825EBF70;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ebf60
	if (!cr0.lt) goto loc_825EBF60;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EBF60:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ebf18
	if (cr6.gt) goto loc_825EBF18;
loc_825EBF70:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ebfac
	if (!cr0.lt) goto loc_825EBFAC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EBFAC:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ebfdc
	if (cr6.eq) goto loc_825EBFDC;
loc_825EBFBC:
	// li r11,30
	r11.s64 = 30;
	// stw r28,3900(r27)
	PPC_STORE_U32(r27.u32 + 3900, r28.u32);
	// li r10,500
	ctx.r10.s64 = 500;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,3656(r27)
	PPC_STORE_U32(r27.u32 + 3656, r11.u32);
	// stw r10,3660(r27)
	PPC_STORE_U32(r27.u32 + 3660, ctx.r10.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_825EBFDC:
	// lwz r11,3656(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3656);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ebfec
	if (!cr6.eq) goto loc_825EBFEC;
	// stw r30,3656(r27)
	PPC_STORE_U32(r27.u32 + 3656, r30.u32);
loc_825EBFEC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,11
	r30.s64 = 11;
	// mr r29,r28
	r29.u64 = r28.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,11
	cr6.compare<uint32_t>(r11.u32, 11, xer);
	// bge cr6,0x825ec05c
	if (!cr6.lt) goto loc_825EC05C;
loc_825EC004:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec05c
	if (cr6.eq) goto loc_825EC05C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec04c
	if (!cr0.lt) goto loc_825EC04C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC04C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec004
	if (cr6.gt) goto loc_825EC004;
loc_825EC05C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec098
	if (!cr0.lt) goto loc_825EC098;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC098:
	// lis r11,-32126
	r11.s64 = -2105409536;
	// stw r30,3660(r27)
	PPC_STORE_U32(r27.u32 + 3660, r30.u32);
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// stw r30,-25084(r11)
	PPC_STORE_U32(r11.u32 + -25084, r30.u32);
	// lwz r11,3656(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3656);
	// stw r11,-25080(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25080, r11.u32);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ebfbc
	if (!cr6.eq) goto loc_825EBFBC;
	// lwz r11,15472(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15472);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825ec17c
	if (cr6.eq) goto loc_825EC17C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,1
	r30.s64 = 1;
	// mr r29,r28
	r29.u64 = r28.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ec13c
	if (!cr6.lt) goto loc_825EC13C;
loc_825EC0E4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec13c
	if (cr6.eq) goto loc_825EC13C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec12c
	if (!cr0.lt) goto loc_825EC12C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC12C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec0e4
	if (cr6.gt) goto loc_825EC0E4;
loc_825EC13C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec178
	if (!cr0.lt) goto loc_825EC178;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC178:
	// stw r30,3900(r27)
	PPC_STORE_U32(r27.u32 + 3900, r30.u32);
loc_825EC17C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825EC188"))) PPC_WEAK_FUNC(sub_825EC188);
PPC_FUNC_IMPL(__imp__sub_825EC188) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r30,5
	r30.s64 = 5;
	// li r29,0
	r29.s64 = 0;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825ec20c
	if (!cr6.lt) goto loc_825EC20C;
loc_825EC1B4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec20c
	if (cr6.eq) goto loc_825EC20C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec1fc
	if (!cr0.lt) goto loc_825EC1FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC1FC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec1b4
	if (cr6.gt) goto loc_825EC1B4;
loc_825EC20C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec248
	if (!cr0.lt) goto loc_825EC248;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC248:
	// lis r11,-32126
	r11.s64 = -2105409536;
	// stw r28,3656(r27)
	PPC_STORE_U32(r27.u32 + 3656, r28.u32);
	// li r30,11
	r30.s64 = 11;
	// li r29,0
	r29.s64 = 0;
	// stw r28,-25080(r11)
	PPC_STORE_U32(r11.u32 + -25080, r28.u32);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,11
	cr6.compare<uint32_t>(r11.u32, 11, xer);
	// bge cr6,0x825ec2c8
	if (!cr6.lt) goto loc_825EC2C8;
loc_825EC270:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec2c8
	if (cr6.eq) goto loc_825EC2C8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec2b8
	if (!cr0.lt) goto loc_825EC2B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC2B8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec270
	if (cr6.gt) goto loc_825EC270;
loc_825EC2C8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec304
	if (!cr0.lt) goto loc_825EC304;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC304:
	// lis r11,-32126
	r11.s64 = -2105409536;
	// stw r28,3660(r27)
	PPC_STORE_U32(r27.u32 + 3660, r28.u32);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// stw r28,-25084(r11)
	PPC_STORE_U32(r11.u32 + -25084, r28.u32);
	// li r11,1
	r11.s64 = 1;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// stw r11,3900(r27)
	PPC_STORE_U32(r27.u32 + 3900, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ec38c
	if (!cr6.lt) goto loc_825EC38C;
loc_825EC334:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec38c
	if (cr6.eq) goto loc_825EC38C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec37c
	if (!cr0.lt) goto loc_825EC37C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC37C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec334
	if (cr6.gt) goto loc_825EC334;
loc_825EC38C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec3c8
	if (!cr0.lt) goto loc_825EC3C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC3C8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,3888(r27)
	PPC_STORE_U32(r27.u32 + 3888, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ec440
	if (!cr6.lt) goto loc_825EC440;
loc_825EC3E8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec440
	if (cr6.eq) goto loc_825EC440;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec430
	if (!cr0.lt) goto loc_825EC430;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC430:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec3e8
	if (cr6.gt) goto loc_825EC3E8;
loc_825EC440:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec47c
	if (!cr0.lt) goto loc_825EC47C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC47C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,3892(r27)
	PPC_STORE_U32(r27.u32 + 3892, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ec4f4
	if (!cr6.lt) goto loc_825EC4F4;
loc_825EC49C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec4f4
	if (cr6.eq) goto loc_825EC4F4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec4e4
	if (!cr0.lt) goto loc_825EC4E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC4E4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec49c
	if (cr6.gt) goto loc_825EC49C;
loc_825EC4F4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec530
	if (!cr0.lt) goto loc_825EC530;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC530:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,436(r27)
	PPC_STORE_U32(r27.u32 + 436, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ec5a8
	if (!cr6.lt) goto loc_825EC5A8;
loc_825EC550:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec5a8
	if (cr6.eq) goto loc_825EC5A8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec598
	if (!cr0.lt) goto loc_825EC598;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC598:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec550
	if (cr6.gt) goto loc_825EC550;
loc_825EC5A8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec5e4
	if (!cr0.lt) goto loc_825EC5E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC5E4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,3884(r27)
	PPC_STORE_U32(r27.u32 + 3884, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ec65c
	if (!cr6.lt) goto loc_825EC65C;
loc_825EC604:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec65c
	if (cr6.eq) goto loc_825EC65C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec64c
	if (!cr0.lt) goto loc_825EC64C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC64C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec604
	if (cr6.gt) goto loc_825EC604;
loc_825EC65C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec698
	if (!cr0.lt) goto loc_825EC698;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC698:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,444(r27)
	PPC_STORE_U32(r27.u32 + 444, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ec710
	if (!cr6.lt) goto loc_825EC710;
loc_825EC6B8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec710
	if (cr6.eq) goto loc_825EC710;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec700
	if (!cr0.lt) goto loc_825EC700;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC700:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec6b8
	if (cr6.gt) goto loc_825EC6B8;
loc_825EC710:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec74c
	if (!cr0.lt) goto loc_825EC74C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC74C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// stw r28,396(r27)
	PPC_STORE_U32(r27.u32 + 396, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825ec7c4
	if (!cr6.lt) goto loc_825EC7C4;
loc_825EC76C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec7c4
	if (cr6.eq) goto loc_825EC7C4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec7b4
	if (!cr0.lt) goto loc_825EC7B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC7B4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec76c
	if (cr6.gt) goto loc_825EC76C;
loc_825EC7C4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec800
	if (!cr0.lt) goto loc_825EC800;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC800:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r30,15464(r27)
	PPC_STORE_U32(r27.u32 + 15464, r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825EC810"))) PPC_WEAK_FUNC(sub_825EC810);
PPC_FUNC_IMPL(__imp__sub_825EC810) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// li r30,24
	r30.s64 = 24;
	// li r29,0
	r29.s64 = 0;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bge cr6,0x825ec894
	if (!cr6.lt) goto loc_825EC894;
loc_825EC83C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec894
	if (cr6.eq) goto loc_825EC894;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec884
	if (!cr0.lt) goto loc_825EC884;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC884:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec83c
	if (cr6.gt) goto loc_825EC83C;
loc_825EC894:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec8d0
	if (!cr0.lt) goto loc_825EC8D0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC8D0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// beq cr6,0x825ec8e4
	if (cr6.eq) goto loc_825EC8E4;
loc_825EC8D8:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825EC8E4:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825ec958
	if (!cr6.lt) goto loc_825EC958;
loc_825EC900:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ec958
	if (cr6.eq) goto loc_825EC958;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ec948
	if (!cr0.lt) goto loc_825EC948;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC948:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec900
	if (cr6.gt) goto loc_825EC900;
loc_825EC958:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ec994
	if (!cr0.lt) goto loc_825EC994;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EC994:
	// cmplwi cr6,r30,182
	cr6.compare<uint32_t>(r30.u32, 182, xer);
	// bne cr6,0x825ec8d8
	if (!cr6.eq) goto loc_825EC8D8;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825eca10
	if (!cr6.lt) goto loc_825ECA10;
loc_825EC9B8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eca10
	if (cr6.eq) goto loc_825ECA10;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825eca00
	if (!cr0.lt) goto loc_825ECA00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECA00:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ec9b8
	if (cr6.gt) goto loc_825EC9B8;
loc_825ECA10:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825eca4c
	if (!cr0.lt) goto loc_825ECA4C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECA4C:
	// li r28,0
	r28.s64 = 0;
	// stw r30,284(r26)
	PPC_STORE_U32(r26.u32 + 284, r30.u32);
	// li r25,1
	r25.s64 = 1;
loc_825ECA58:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r25
	r30.u64 = r25.u64;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ecad0
	if (!cr6.lt) goto loc_825ECAD0;
loc_825ECA74:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ecad0
	if (cr6.eq) goto loc_825ECAD0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ecac0
	if (!cr0.lt) goto loc_825ECAC0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECAC0:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eca74
	if (cr6.gt) goto loc_825ECA74;
loc_825ECAD0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ecb0c
	if (!cr0.lt) goto loc_825ECB0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECB0C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825ecb1c
	if (cr6.eq) goto loc_825ECB1C;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// b 0x825eca58
	goto loc_825ECA58;
loc_825ECB1C:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// clrldi r11,r28,32
	r11.u64 = r28.u64 & 0xFFFFFFFF;
	// ld r10,3600(r26)
	ctx.r10.u64 = PPC_LOAD_U64(r26.u32 + 3600);
	// mr r30,r25
	r30.u64 = r25.u64;
	// add r27,r11,r10
	r27.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ecb80
	if (!cr6.lt) goto loc_825ECB80;
loc_825ECB40:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ecb80
	if (cr6.eq) goto loc_825ECB80;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825ecb70
	if (!cr0.lt) goto loc_825ECB70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECB70:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ecb40
	if (cr6.gt) goto loc_825ECB40;
loc_825ECB80:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825ecba8
	if (!cr0.lt) goto loc_825ECBA8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECBA8:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r30,3632(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 3632);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825ecbcc
	if (!cr6.eq) goto loc_825ECBCC;
	// li r11,0
	r11.s64 = 0;
	// b 0x825ecc6c
	goto loc_825ECC6C;
loc_825ECBCC:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825ecc2c
	if (!cr6.gt) goto loc_825ECC2C;
loc_825ECBD4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ecc2c
	if (cr6.eq) goto loc_825ECC2C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ecc1c
	if (!cr0.lt) goto loc_825ECC1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECC1C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ecbd4
	if (cr6.gt) goto loc_825ECBD4;
loc_825ECC2C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ecc68
	if (!cr0.lt) goto loc_825ECC68;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECC68:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_825ECC6C:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// clrldi r28,r11,32
	r28.u64 = r11.u64 & 0xFFFFFFFF;
	// mr r30,r25
	r30.u64 = r25.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825eccc8
	if (!cr6.lt) goto loc_825ECCC8;
loc_825ECC88:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eccc8
	if (cr6.eq) goto loc_825ECCC8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825eccb8
	if (!cr0.lt) goto loc_825ECCB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECCB8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ecc88
	if (cr6.gt) goto loc_825ECC88;
loc_825ECCC8:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825eccf0
	if (!cr0.lt) goto loc_825ECCF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECCF0:
	// lwz r10,3568(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 3568);
	// mr r30,r25
	r30.u64 = r25.u64;
	// ld r11,3600(r26)
	r11.u64 = PPC_LOAD_U64(r26.u32 + 3600);
	// li r29,0
	r29.s64 = 0;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// std r27,3600(r26)
	PPC_STORE_U64(r26.u32 + 3600, r27.u64);
	// mulld r10,r10,r27
	ctx.r10.s64 = ctx.r10.s64 * r27.s64;
	// std r11,3616(r26)
	PPC_STORE_U64(r26.u32 + 3616, r11.u64);
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// std r10,3576(r26)
	PPC_STORE_U64(r26.u32 + 3576, ctx.r10.u64);
	// ld r10,3608(r26)
	ctx.r10.u64 = PPC_LOAD_U64(r26.u32 + 3608);
	// std r11,3608(r26)
	PPC_STORE_U64(r26.u32 + 3608, r11.u64);
	// std r10,3624(r26)
	PPC_STORE_U64(r26.u32 + 3624, ctx.r10.u64);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ecd90
	if (!cr6.lt) goto loc_825ECD90;
loc_825ECD38:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ecd90
	if (cr6.eq) goto loc_825ECD90;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ecd80
	if (!cr0.lt) goto loc_825ECD80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECD80:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ecd38
	if (cr6.gt) goto loc_825ECD38;
loc_825ECD90:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ecdcc
	if (!cr0.lt) goto loc_825ECDCC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECDCC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825ec8d8
	if (cr6.eq) goto loc_825EC8D8;
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ece98
	if (!cr6.eq) goto loc_825ECE98;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r25
	r30.u64 = r25.u64;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ece54
	if (!cr6.lt) goto loc_825ECE54;
loc_825ECDFC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ece54
	if (cr6.eq) goto loc_825ECE54;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ece44
	if (!cr0.lt) goto loc_825ECE44;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECE44:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ecdfc
	if (cr6.gt) goto loc_825ECDFC;
loc_825ECE54:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ece90
	if (!cr0.lt) goto loc_825ECE90;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECE90:
	// stw r30,3904(r26)
	PPC_STORE_U32(r26.u32 + 3904, r30.u32);
	// b 0x825ecea0
	goto loc_825ECEA0;
loc_825ECE98:
	// li r11,0
	r11.s64 = 0;
	// stw r11,3904(r26)
	PPC_STORE_U32(r26.u32 + 3904, r11.u32);
loc_825ECEA0:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825ecef8
	if (!cr6.lt) goto loc_825ECEF8;
loc_825ECEB8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ecef8
	if (cr6.eq) goto loc_825ECEF8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825ecee8
	if (!cr0.lt) goto loc_825ECEE8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECEE8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eceb8
	if (cr6.gt) goto loc_825ECEB8;
loc_825ECEF8:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825ecf20
	if (!cr0.lt) goto loc_825ECF20;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECF20:
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ecff0
	if (!cr6.eq) goto loc_825ECFF0;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825ecfa0
	if (!cr6.lt) goto loc_825ECFA0;
loc_825ECF48:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ecfa0
	if (cr6.eq) goto loc_825ECFA0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ecf90
	if (!cr0.lt) goto loc_825ECF90;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECF90:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ecf48
	if (cr6.gt) goto loc_825ECF48;
loc_825ECFA0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ecfdc
	if (!cr0.lt) goto loc_825ECFDC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ECFDC:
	// stw r30,248(r26)
	PPC_STORE_U32(r26.u32 + 248, r30.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r25,3556(r26)
	PPC_STORE_U32(r26.u32 + 3556, r25.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825ECFF0:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ed178
	if (!cr6.eq) goto loc_825ED178;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825ed06c
	if (!cr6.lt) goto loc_825ED06C;
loc_825ED014:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ed06c
	if (cr6.eq) goto loc_825ED06C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ed05c
	if (!cr0.lt) goto loc_825ED05C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED05C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ed014
	if (cr6.gt) goto loc_825ED014;
loc_825ED06C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ed0a8
	if (!cr0.lt) goto loc_825ED0A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED0A8:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// stw r28,248(r26)
	PPC_STORE_U32(r26.u32 + 248, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825ed120
	if (!cr6.lt) goto loc_825ED120;
loc_825ED0C8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ed120
	if (cr6.eq) goto loc_825ED120;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ed110
	if (!cr0.lt) goto loc_825ED110;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED110:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ed0c8
	if (cr6.gt) goto loc_825ED0C8;
loc_825ED120:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ed15c
	if (!cr0.lt) goto loc_825ED15C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED15C:
	// addi r11,r30,-1
	r11.s64 = r30.s64 + -1;
	// stw r30,3556(r26)
	PPC_STORE_U32(r26.u32 + 3556, r30.u32);
	// li r10,16
	ctx.r10.s64 = 16;
	// slw r11,r25,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r25.u32 << (r11.u8 & 0x3F));
	// slw r10,r10,r30
	ctx.r10.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// stw r11,3564(r26)
	PPC_STORE_U32(r26.u32 + 3564, r11.u32);
	// stw r10,3560(r26)
	PPC_STORE_U32(r26.u32 + 3560, ctx.r10.u32);
loc_825ED178:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825ED184"))) PPC_WEAK_FUNC(sub_825ED184);
PPC_FUNC_IMPL(__imp__sub_825ED184) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825ED188"))) PPC_WEAK_FUNC(sub_825ED188);
PPC_FUNC_IMPL(__imp__sub_825ED188) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// clrlwi r29,r30,31
	r29.u64 = r30.u32 & 0x1;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x825ed1e8
	if (cr6.eq) goto loc_825ED1E8;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r28,r11,0
	r28.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x825ed1d8
	if (!cr0.lt) goto loc_825ED1D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED1D8:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwimi r11,r28,31,0,0
	r11.u64 = (__builtin_rotateleft32(r28.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// addi r27,r27,20
	r27.s64 = r27.s64 + 20;
loc_825ED1E8:
	// cmpw cr6,r29,r30
	cr6.compare<int32_t>(r29.s32, r30.s32, xer);
	// bge cr6,0x825ed2fc
	if (!cr6.lt) goto loc_825ED2FC;
	// subf r11,r29,r30
	r11.s64 = r30.s64 - r29.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
loc_825ED200:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x825ed22c
	if (!cr0.lt) goto loc_825ED22C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED22C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825ed2d4
	if (cr6.eq) goto loc_825ED2D4;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x825ed260
	if (!cr0.lt) goto loc_825ED260;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED260:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825ed280
	if (cr6.eq) goto loc_825ED280;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// b 0x825ed2e8
	goto loc_825ED2E8;
loc_825ED280:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x825ed2ac
	if (!cr0.lt) goto loc_825ED2AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED2AC:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825ed2cc
	if (cr6.eq) goto loc_825ED2CC;
	// clrlwi r11,r11,1
	r11.u64 = r11.u32 & 0x7FFFFFFF;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// b 0x825ed2e8
	goto loc_825ED2E8;
loc_825ED2CC:
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// b 0x825ed2dc
	goto loc_825ED2DC;
loc_825ED2D4:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// clrlwi r11,r11,1
	r11.u64 = r11.u32 & 0x7FFFFFFF;
loc_825ED2DC:
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// clrlwi r11,r11,1
	r11.u64 = r11.u32 & 0x7FFFFFFF;
loc_825ED2E8:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// stw r11,20(r27)
	PPC_STORE_U32(r27.u32 + 20, r11.u32);
	// addi r27,r27,40
	r27.s64 = r27.s64 + 40;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825ed200
	if (!cr6.eq) goto loc_825ED200;
loc_825ED2FC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825ED304"))) PPC_WEAK_FUNC(sub_825ED304);
PPC_FUNC_IMPL(__imp__sub_825ED304) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825ED308"))) PPC_WEAK_FUNC(sub_825ED308);
PPC_FUNC_IMPL(__imp__sub_825ED308) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r6
	r24.u64 = ctx.r6.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// li r25,0
	r25.s64 = 0;
	// lwz r27,0(r24)
	r27.u64 = PPC_LOAD_U32(r24.u32 + 0);
loc_825ED32C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x825ed358
	if (!cr0.lt) goto loc_825ED358;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED358:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825ed45c
	if (!cr6.eq) goto loc_825ED45C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,3
	r30.s64 = 3;
	// li r29,0
	r29.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825ed3d0
	if (!cr6.lt) goto loc_825ED3D0;
loc_825ED378:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ed3d0
	if (cr6.eq) goto loc_825ED3D0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ed3c0
	if (!cr0.lt) goto loc_825ED3C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED3C0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ed378
	if (cr6.gt) goto loc_825ED378;
loc_825ED3D0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ed40c
	if (!cr0.lt) goto loc_825ED40C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED40C:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bgt cr6,0x825ed464
	if (cr6.gt) goto loc_825ED464;
	// bne cr6,0x825ed564
	if (!cr6.eq) goto loc_825ED564;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x825ed444
	if (!cr0.lt) goto loc_825ED444;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED444:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825ed484
	if (cr6.eq) goto loc_825ED484;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x825ed474
	if (!cr6.eq) goto loc_825ED474;
	// li r25,1
	r25.s64 = 1;
	// b 0x825ed32c
	goto loc_825ED32C;
loc_825ED45C:
	// li r11,0
	r11.s64 = 0;
	// b 0x825ed630
	goto loc_825ED630;
loc_825ED464:
	// addi r11,r30,-2
	r11.s64 = r30.s64 + -2;
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// b 0x825ed630
	goto loc_825ED630;
loc_825ED474:
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r27,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r27.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825ED484:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,5
	r30.s64 = 5;
	// li r29,0
	r29.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825ed4f4
	if (!cr6.lt) goto loc_825ED4F4;
loc_825ED49C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ed4f4
	if (cr6.eq) goto loc_825ED4F4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ed4e4
	if (!cr0.lt) goto loc_825ED4E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED4E4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ed49c
	if (cr6.gt) goto loc_825ED49C;
loc_825ED4F4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ed530
	if (!cr0.lt) goto loc_825ED530;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED530:
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r30
	r11.u64 = r30.u64;
	// lwzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r28.u32);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bne cr6,0x825ed54c
	if (!cr6.eq) goto loc_825ED54C;
	// ori r11,r30,32
	r11.u64 = r30.u64 | 32;
	// b 0x825ed630
	goto loc_825ED630;
loc_825ED54C:
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// beq cr6,0x825ed630
	if (cr6.eq) goto loc_825ED630;
	// li r3,4
	ctx.r3.s64 = 4;
	// stw r30,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r30.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825ED564:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,4
	r30.s64 = 4;
	// li r29,0
	r29.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x825ed5d4
	if (!cr6.lt) goto loc_825ED5D4;
loc_825ED57C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ed5d4
	if (cr6.eq) goto loc_825ED5D4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ed5c4
	if (!cr0.lt) goto loc_825ED5C4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED5C4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ed57c
	if (cr6.gt) goto loc_825ED57C;
loc_825ED5D4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ed610
	if (!cr0.lt) goto loc_825ED610;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED610:
	// cmpwi cr6,r30,15
	cr6.compare<int32_t>(r30.s32, 15, xer);
	// bne cr6,0x825ed628
	if (!cr6.eq) goto loc_825ED628;
	// li r3,4
	ctx.r3.s64 = 4;
	// stw r27,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r27.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825ED628:
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
loc_825ED630:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x825ed63c
	if (cr6.eq) goto loc_825ED63C;
	// xori r11,r11,63
	r11.u64 = r11.u64 ^ 63;
loc_825ED63C:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825ED64C"))) PPC_WEAK_FUNC(sub_825ED64C);
PPC_FUNC_IMPL(__imp__sub_825ED64C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825ED650"))) PPC_WEAK_FUNC(sub_825ED650);
PPC_FUNC_IMPL(__imp__sub_825ED650) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r4,3
	ctx.r4.s64 = 3;
	// li r24,0
	r24.s64 = 0;
	// lwz r9,144(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 144);
	// lwz r11,19984(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 19984);
	// lwz r10,268(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 268);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// lwz r3,140(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x8263a9e0
	sub_8263A9E0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825ed7d8
	if (!cr6.eq) goto loc_825ED7D8;
	// li r4,3
	ctx.r4.s64 = 3;
	// lwz r3,136(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// bl 0x8263a9e0
	sub_8263A9E0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825ed7d8
	if (cr6.eq) goto loc_825ED7D8;
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// li r26,0
	r26.s64 = 0;
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// clrlwi r25,r11,31
	r25.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825ed91c
	if (!cr6.gt) goto loc_825ED91C;
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// addi r27,r10,4996
	r27.s64 = ctx.r10.s64 + 4996;
loc_825ED6CC:
	// mr r29,r25
	r29.u64 = r25.u64;
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// bge cr6,0x825ed7c4
	if (!cr6.lt) goto loc_825ED7C4;
loc_825ED6D8:
	// mullw r11,r11,r26
	r11.s64 = int64_t(r11.s32) * int64_t(r26.s32);
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// addi r5,r27,60
	ctx.r5.s64 = r27.s64 + 60;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// bl 0x825ed308
	sub_825ED308(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825edb14
	if (!cr6.eq) goto loc_825EDB14;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r28,r11
	r11.u64 = r28.u64 + r11.u64;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwimi r4,r10,31,0,0
	ctx.r4.u64 = (__builtin_rotateleft32(ctx.r10.u32, 31) & 0x80000000) | (ctx.r4.u64 & 0xFFFFFFFF7FFFFFFF);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// rlwimi r10,r8,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r8.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// add r10,r28,r11
	ctx.r10.u64 = r28.u64 + r11.u64;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwimi r8,r7,31,0,0
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r7.u32, 31) & 0x80000000) | (ctx.r8.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,20(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// rlwimi r8,r6,31,0,0
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r6.u32, 31) & 0x80000000) | (ctx.r8.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r8,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r8.u32);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwimi r10,r5,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r5.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// rlwimi r10,r9,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x825ed6d8
	if (cr6.lt) goto loc_825ED6D8;
loc_825ED7C4:
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// addi r26,r26,3
	r26.s64 = r26.s64 + 3;
	// cmpw cr6,r26,r10
	cr6.compare<int32_t>(r26.s32, ctx.r10.s32, xer);
	// blt cr6,0x825ed6cc
	if (cr6.lt) goto loc_825ED6CC;
	// b 0x825ed91c
	goto loc_825ED91C;
loc_825ED7D8:
	// lwz r11,140(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// li r4,3
	ctx.r4.s64 = 3;
	// lwz r3,136(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// clrlwi r24,r11,31
	r24.u64 = r11.u32 & 0x1;
	// bl 0x8263a9e0
	sub_8263A9E0(ctx, base);
	// lwz r11,140(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r26,r24
	r26.u64 = r24.u64;
	// cmpw cr6,r24,r11
	cr6.compare<int32_t>(r24.s32, r11.s32, xer);
	// bge cr6,0x825ed91c
	if (!cr6.lt) goto loc_825ED91C;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r27,r11,4996
	r27.s64 = r11.s64 + 4996;
loc_825ED808:
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// mr r28,r25
	r28.u64 = r25.u64;
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// bge cr6,0x825ed90c
	if (!cr6.lt) goto loc_825ED90C;
loc_825ED818:
	// mullw r11,r11,r26
	r11.s64 = int64_t(r11.s32) * int64_t(r26.s32);
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// addi r5,r27,60
	ctx.r5.s64 = r27.s64 + 60;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// bl 0x825ed308
	sub_825ED308(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825edb14
	if (!cr6.eq) goto loc_825EDB14;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r29,2
	r11.s64 = r29.s64 + 2;
	// addi r28,r28,3
	r28.s64 = r28.s64 + 3;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r9,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r29,r10
	ctx.r10.u64 = r29.u64 + ctx.r10.u64;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r31
	r11.u64 = ctx.r10.u64 + r31.u64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwimi r8,r7,31,0,0
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r7.u32, 31) & 0x80000000) | (ctx.r8.u64 & 0xFFFFFFFF7FFFFFFF);
	// lwzx r7,r10,r31
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// rlwimi r7,r5,31,0,0
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r5.u32, 31) & 0x80000000) | (ctx.r7.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// stwx r7,r10,r31
	PPC_STORE_U32(ctx.r10.u32 + r31.u32, ctx.r7.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// rlwimi r10,r6,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r6.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwzx r7,r10,r31
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// rlwimi r8,r4,31,0,0
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r4.u32, 31) & 0x80000000) | (ctx.r8.u64 & 0xFFFFFFFF7FFFFFFF);
	// rlwimi r7,r9,31,0,0
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r9.u32, 31) & 0x80000000) | (ctx.r7.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// stwx r7,r10,r31
	PPC_STORE_U32(ctx.r10.u32 + r31.u32, ctx.r7.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// rlwimi r10,r3,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r3.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// blt cr6,0x825ed818
	if (cr6.lt) goto loc_825ED818;
loc_825ED90C:
	// lwz r11,140(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// addi r26,r26,2
	r26.s64 = r26.s64 + 2;
	// cmpw cr6,r26,r11
	cr6.compare<int32_t>(r26.s32, r11.s32, xer);
	// blt cr6,0x825ed808
	if (cr6.lt) goto loc_825ED808;
loc_825ED91C:
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x825eda20
	if (!cr6.gt) goto loc_825EDA20;
loc_825ED928:
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825ed954
	if (!cr0.lt) goto loc_825ED954;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED954:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x825ed9d0
	if (cr6.eq) goto loc_825ED9D0;
	// lwz r11,140(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825eda14
	if (!cr6.gt) goto loc_825EDA14;
loc_825ED96C:
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r28,r8,0
	r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825ed998
	if (!cr0.lt) goto loc_825ED998;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825ED998:
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// mullw r11,r29,r11
	r11.s64 = int64_t(r29.s32) * int64_t(r11.s32);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// rlwimi r10,r28,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(r28.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r10.u32);
	// lwz r11,140(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x825ed96c
	if (cr6.lt) goto loc_825ED96C;
	// b 0x825eda14
	goto loc_825EDA14;
loc_825ED9D0:
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825eda14
	if (!cr6.gt) goto loc_825EDA14;
loc_825ED9E0:
	// lwz r10,136(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stwx r9,r10,r31
	PPC_STORE_U32(ctx.r10.u32 + r31.u32, ctx.r9.u32);
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825ed9e0
	if (cr6.lt) goto loc_825ED9E0;
loc_825EDA14:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmpw cr6,r27,r25
	cr6.compare<int32_t>(r27.s32, r25.s32, xer);
	// blt cr6,0x825ed928
	if (cr6.lt) goto loc_825ED928;
loc_825EDA20:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x825edb10
	if (cr6.eq) goto loc_825EDB10;
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825eda54
	if (!cr0.lt) goto loc_825EDA54;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDA54:
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x825edad4
	if (cr6.eq) goto loc_825EDAD4;
	// mr r28,r25
	r28.u64 = r25.u64;
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// bge cr6,0x825edb10
	if (!cr6.lt) goto loc_825EDB10;
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r25,r11
	r11.u64 = r25.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
loc_825EDA7C:
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825edaa8
	if (!cr0.lt) goto loc_825EDAA8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDAA8:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// rlwimi r11,r29,31,0,0
	r11.u64 = (__builtin_rotateleft32(r29.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// addi r31,r31,20
	r31.s64 = r31.s64 + 20;
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// blt cr6,0x825eda7c
	if (cr6.lt) goto loc_825EDA7C;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825EDAD4:
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// bge cr6,0x825edb10
	if (!cr6.lt) goto loc_825EDB10;
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r25,r11
	r11.u64 = r25.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
loc_825EDAF0:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r9,136(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825edaf0
	if (cr6.lt) goto loc_825EDAF0;
loc_825EDB10:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825EDB14:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825EDB1C"))) PPC_WEAK_FUNC(sub_825EDB1C);
PPC_FUNC_IMPL(__imp__sub_825EDB1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EDB20"))) PPC_WEAK_FUNC(sub_825EDB20);
PPC_FUNC_IMPL(__imp__sub_825EDB20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// lwz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r9,268(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// ld r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r11,r9
	r29.u64 = r11.u64 + ctx.r9.u64;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r10,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rldicl r10,r10,1,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u64, 1) & 0x1;
	// rotlwi r25,r10,0
	r25.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825edb80
	if (!cr0.lt) goto loc_825EDB80;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDB80:
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// li r28,2
	r28.s64 = 2;
	// li r27,0
	r27.s64 = 0;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825edbf4
	if (!cr6.lt) goto loc_825EDBF4;
loc_825EDB9C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825edbf4
	if (cr6.eq) goto loc_825EDBF4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r28,r11,r28
	r28.s64 = r28.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r10,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r28.u8 & 0x3F));
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// bge 0x825edbe4
	if (!cr0.lt) goto loc_825EDBE4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDBE4:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bgt cr6,0x825edb9c
	if (cr6.gt) goto loc_825EDB9C;
loc_825EDBF4:
	// subfic r9,r28,64
	xer.ca = r28.u32 <= 64;
	ctx.r9.s64 = 64 - r28.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r28,32
	ctx.r8.u64 = r28.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r28,r10
	ctx.r10.s64 = ctx.r10.s64 - r28.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r27
	r28.u64 = r11.u64 + r27.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x825edc30
	if (!cr0.lt) goto loc_825EDC30;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDC30:
	// cmplwi cr6,r28,1
	cr6.compare<uint32_t>(r28.u32, 1, xer);
	// beq cr6,0x825eddc8
	if (cr6.eq) goto loc_825EDDC8;
	// cmplwi cr6,r28,2
	cr6.compare<uint32_t>(r28.u32, 2, xer);
	// beq cr6,0x825eddb0
	if (cr6.eq) goto loc_825EDDB0;
	// cmplwi cr6,r28,3
	cr6.compare<uint32_t>(r28.u32, 3, xer);
	// beq cr6,0x825edd94
	if (cr6.eq) goto loc_825EDD94;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825edc74
	if (!cr0.lt) goto loc_825EDC74;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDC74:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825edcec
	if (cr6.eq) goto loc_825EDCEC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,84(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r4,144(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// li r26,2
	r26.s64 = 2;
	// bl 0x825ed188
	sub_825ED188(ctx, base);
loc_825EDC90:
	// lwz r9,19984(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,144(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// lwz r10,268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// lwz r9,140(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// ble cr6,0x825edd7c
	if (!cr6.gt) goto loc_825EDD7C;
loc_825EDCC0:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825ee0a0
	if (!cr6.gt) goto loc_825EE0A0;
loc_825EDCD0:
	// add. r10,r6,r7
	ctx.r10.u64 = ctx.r6.u64 + ctx.r7.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x825ee074
	if (cr0.eq) goto loc_825EE074;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x825ee020
	if (!cr6.eq) goto loc_825EE020;
	// lwz r10,-20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -20);
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// b 0x825ee078
	goto loc_825EE078;
loc_825EDCEC:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825edd18
	if (!cr0.lt) goto loc_825EDD18;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDD18:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825edd3c
	if (cr6.eq) goto loc_825EDD3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// li r26,4
	r26.s64 = 4;
	// bl 0x825ed650
	sub_825ED650(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825edc90
	if (cr6.eq) goto loc_825EDC90;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825EDD3C:
	// li r26,0
	r26.s64 = 0;
loc_825EDD40:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x825edd7c
	if (cr6.eq) goto loc_825EDD7C;
	// lwz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825edd7c
	if (!cr6.gt) goto loc_825EDD7C;
loc_825EDD58:
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// rlwimi r9,r10,0,1,31
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x7FFFFFFF) | (ctx.r9.u64 & 0xFFFFFFFF80000000);
	// stw r9,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r9.u32);
	// addi r29,r29,20
	r29.s64 = r29.s64 + 20;
	// lwz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825edd58
	if (cr6.lt) goto loc_825EDD58;
loc_825EDD7C:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x825ee0b4
	if (!cr6.eq) goto loc_825EE0B4;
	// stw r26,344(r31)
	PPC_STORE_U32(r31.u32 + 344, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825EDD94:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// li r26,3
	r26.s64 = 3;
	// bl 0x825ed650
	sub_825ED650(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825edd40
	if (cr6.eq) goto loc_825EDD40;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825EDDB0:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,84(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r4,144(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// li r26,1
	r26.s64 = 1;
	// bl 0x825ed188
	sub_825ED188(ctx, base);
	// b 0x825edd40
	goto loc_825EDD40;
loc_825EDDC8:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825eddf4
	if (!cr0.lt) goto loc_825EDDF4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDDF4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// li r27,0
	r27.s64 = 0;
	// beq cr6,0x825edf10
	if (cr6.eq) goto loc_825EDF10;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r26,6
	r26.s64 = 6;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825edd40
	if (!cr6.gt) goto loc_825EDD40;
loc_825EDE10:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825ede3c
	if (!cr0.lt) goto loc_825EDE3C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDE3C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825edeb8
	if (cr6.eq) goto loc_825EDEB8;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825edefc
	if (!cr6.gt) goto loc_825EDEFC;
loc_825EDE54:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r28,r8,0
	r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825ede80
	if (!cr0.lt) goto loc_825EDE80;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDE80:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r29
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// rlwimi r10,r28,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(r28.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r29
	PPC_STORE_U32(r11.u32 + r29.u32, ctx.r10.u32);
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x825ede54
	if (cr6.lt) goto loc_825EDE54;
	// b 0x825edefc
	goto loc_825EDEFC;
loc_825EDEB8:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825edefc
	if (!cr6.gt) goto loc_825EDEFC;
loc_825EDEC8:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r29.u32);
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stwx r9,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + r29.u32, ctx.r9.u32);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825edec8
	if (cr6.lt) goto loc_825EDEC8;
loc_825EDEFC:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x825ede10
	if (cr6.lt) goto loc_825EDE10;
	// b 0x825edd40
	goto loc_825EDD40;
loc_825EDF10:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r26,5
	r26.s64 = 5;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825edd40
	if (!cr6.gt) goto loc_825EDD40;
loc_825EDF20:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825edf4c
	if (!cr0.lt) goto loc_825EDF4C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDF4C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825edfc8
	if (cr6.eq) goto loc_825EDFC8;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ee00c
	if (!cr6.gt) goto loc_825EE00C;
loc_825EDF64:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r28,r8,0
	r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825edf90
	if (!cr0.lt) goto loc_825EDF90;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EDF90:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r11,r11,r27
	r11.s64 = int64_t(r11.s32) * int64_t(r27.s32);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r29
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// rlwimi r10,r28,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(r28.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r29
	PPC_STORE_U32(r11.u32 + r29.u32, ctx.r10.u32);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x825edf64
	if (cr6.lt) goto loc_825EDF64;
	// b 0x825ee00c
	goto loc_825EE00C;
loc_825EDFC8:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825ee00c
	if (!cr6.gt) goto loc_825EE00C;
loc_825EDFD8:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r10,r10,r27
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r27.s32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r29.u32);
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stwx r9,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + r29.u32, ctx.r9.u32);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825edfd8
	if (cr6.lt) goto loc_825EDFD8;
loc_825EE00C:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x825edf20
	if (cr6.lt) goto loc_825EDF20;
	// b 0x825edd40
	goto loc_825EDD40;
loc_825EE020:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne cr6,0x825ee048
	if (!cr6.eq) goto loc_825EE048;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// b 0x825ee078
	goto loc_825EE078;
loc_825EE048:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r10,-20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -20);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r9,r9,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x825ee078
	if (cr6.eq) goto loc_825EE078;
loc_825EE074:
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_825EE078:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,31,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x80000000;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// rlwimi r10,r9,0,1,31
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0x7FFFFFFF) | (ctx.r10.u64 & 0xFFFFFFFF80000000);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// blt cr6,0x825edcd0
	if (cr6.lt) goto loc_825EDCD0;
loc_825EE0A0:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// blt cr6,0x825edcc0
	if (cr6.lt) goto loc_825EDCC0;
	// b 0x825edd7c
	goto loc_825EDD7C;
loc_825EE0B4:
	// cmpwi cr6,r24,5
	cr6.compare<int32_t>(r24.s32, 5, xer);
	// bne cr6,0x825ee0cc
	if (!cr6.eq) goto loc_825EE0CC;
	// stw r26,20940(r31)
	PPC_STORE_U32(r31.u32 + 20940, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825EE0CC:
	// cmpwi cr6,r24,4
	cr6.compare<int32_t>(r24.s32, 4, xer);
	// bne cr6,0x825ee0e4
	if (!cr6.eq) goto loc_825EE0E4;
	// stw r26,20004(r31)
	PPC_STORE_U32(r31.u32 + 20004, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825EE0E4:
	// cmpwi cr6,r24,3
	cr6.compare<int32_t>(r24.s32, 3, xer);
	// bne cr6,0x825ee0fc
	if (!cr6.eq) goto loc_825EE0FC;
	// stw r26,14804(r31)
	PPC_STORE_U32(r31.u32 + 14804, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825EE0FC:
	// cmpwi cr6,r24,2
	cr6.compare<int32_t>(r24.s32, 2, xer);
	// bne cr6,0x825ee114
	if (!cr6.eq) goto loc_825EE114;
	// stw r26,19988(r31)
	PPC_STORE_U32(r31.u32 + 19988, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825EE114:
	// stw r26,348(r31)
	PPC_STORE_U32(r31.u32 + 348, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825EE124"))) PPC_WEAK_FUNC(sub_825EE124);
PPC_FUNC_IMPL(__imp__sub_825EE124) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EE128"))) PPC_WEAK_FUNC(sub_825EE128);
PPC_FUNC_IMPL(__imp__sub_825EE128) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r11,268(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 268);
	// lwz r10,3944(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 3944);
	// mr r24,r11
	r24.u64 = r11.u64;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// blt cr6,0x825ee57c
	if (cr6.lt) goto loc_825EE57C;
	// beq cr6,0x825ee380
	if (cr6.eq) goto loc_825EE380;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// bge cr6,0x825ee698
	if (!cr6.lt) goto loc_825EE698;
	// mr r26,r11
	r26.u64 = r11.u64;
	// lwz r11,136(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// li r24,0
	r24.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825ee698
	if (!cr6.gt) goto loc_825EE698;
loc_825EE16C:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r28,r26
	r28.u64 = r26.u64;
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ee1e8
	if (!cr6.lt) goto loc_825EE1E8;
loc_825EE18C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ee1e8
	if (cr6.eq) goto loc_825EE1E8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ee1d8
	if (!cr0.lt) goto loc_825EE1D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE1D8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ee18c
	if (cr6.gt) goto loc_825EE18C;
loc_825EE1E8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ee224
	if (!cr0.lt) goto loc_825EE224;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE224:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825ee270
	if (cr6.eq) goto loc_825EE270;
	// lwz r10,140(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// ble cr6,0x825ee364
	if (!cr6.gt) goto loc_825EE364;
loc_825EE23C:
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
	// lwz r9,140(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
	// blt cr6,0x825ee23c
	if (cr6.lt) goto loc_825EE23C;
	// b 0x825ee364
	goto loc_825EE364;
loc_825EE270:
	// lwz r11,140(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825ee364
	if (!cr6.gt) goto loc_825EE364;
loc_825EE280:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ee2f8
	if (!cr6.lt) goto loc_825EE2F8;
loc_825EE29C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ee2f8
	if (cr6.eq) goto loc_825EE2F8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ee2e8
	if (!cr0.lt) goto loc_825EE2E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE2E8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ee29c
	if (cr6.gt) goto loc_825EE29C;
loc_825EE2F8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ee334
	if (!cr0.lt) goto loc_825EE334;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE334:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// rlwimi r11,r30,31,0,0
	r11.u64 = (__builtin_rotateleft32(r30.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// lwz r10,140(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// lwz r11,136(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// cmplw cr6,r27,r10
	cr6.compare<uint32_t>(r27.u32, ctx.r10.u32, xer);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// blt cr6,0x825ee280
	if (cr6.lt) goto loc_825EE280;
loc_825EE364:
	// lwz r11,136(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r26,r26,20
	r26.s64 = r26.s64 + 20;
	// cmplw cr6,r24,r11
	cr6.compare<uint32_t>(r24.u32, r11.u32, xer);
	// blt cr6,0x825ee16c
	if (cr6.lt) goto loc_825EE16C;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825EE380:
	// lwz r11,140(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825ee698
	if (!cr6.gt) goto loc_825EE698;
loc_825EE390:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ee408
	if (!cr6.lt) goto loc_825EE408;
loc_825EE3AC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ee408
	if (cr6.eq) goto loc_825EE408;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ee3f8
	if (!cr0.lt) goto loc_825EE3F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE3F8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ee3ac
	if (cr6.gt) goto loc_825EE3AC;
loc_825EE408:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ee444
	if (!cr0.lt) goto loc_825EE444;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE444:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825ee480
	if (cr6.eq) goto loc_825EE480;
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// ble cr6,0x825ee564
	if (!cr6.gt) goto loc_825EE564;
loc_825EE45C:
	// lwz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// addi r24,r24,20
	r24.s64 = r24.s64 + 20;
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825ee45c
	if (cr6.lt) goto loc_825EE45C;
	// b 0x825ee564
	goto loc_825EE564;
loc_825EE480:
	// lwz r11,136(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825ee564
	if (!cr6.gt) goto loc_825EE564;
loc_825EE490:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ee508
	if (!cr6.lt) goto loc_825EE508;
loc_825EE4AC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ee508
	if (cr6.eq) goto loc_825EE508;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ee4f8
	if (!cr0.lt) goto loc_825EE4F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE4F8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ee4ac
	if (cr6.gt) goto loc_825EE4AC;
loc_825EE508:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ee544
	if (!cr0.lt) goto loc_825EE544;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE544:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// rlwimi r11,r30,31,0,0
	r11.u64 = (__builtin_rotateleft32(r30.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// addi r24,r24,20
	r24.s64 = r24.s64 + 20;
	// lwz r11,136(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// blt cr6,0x825ee490
	if (cr6.lt) goto loc_825EE490;
loc_825EE564:
	// lwz r11,140(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// blt cr6,0x825ee390
	if (cr6.lt) goto loc_825EE390;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825EE57C:
	// lwz r11,140(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// li r26,0
	r26.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825ee698
	if (!cr6.gt) goto loc_825EE698;
loc_825EE58C:
	// lwz r11,136(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825ee674
	if (!cr6.gt) goto loc_825EE674;
	// mr r28,r24
	r28.u64 = r24.u64;
loc_825EE5A0:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ee618
	if (!cr6.lt) goto loc_825EE618;
loc_825EE5BC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ee618
	if (cr6.eq) goto loc_825EE618;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ee608
	if (!cr0.lt) goto loc_825EE608;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE608:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ee5bc
	if (cr6.gt) goto loc_825EE5BC;
loc_825EE618:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ee654
	if (!cr0.lt) goto loc_825EE654;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE654:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// rlwimi r11,r30,31,0,0
	r11.u64 = (__builtin_rotateleft32(r30.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// addi r28,r28,20
	r28.s64 = r28.s64 + 20;
	// lwz r11,136(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// blt cr6,0x825ee5a0
	if (cr6.lt) goto loc_825EE5A0;
loc_825EE674:
	// lwz r11,136(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r9,140(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplw cr6,r26,r9
	cr6.compare<uint32_t>(r26.u32, ctx.r9.u32, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r24,r11,r24
	r24.u64 = r11.u64 + r24.u64;
	// blt cr6,0x825ee58c
	if (cr6.lt) goto loc_825EE58C;
loc_825EE698:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825EE6A0"))) PPC_WEAK_FUNC(sub_825EE6A0);
PPC_FUNC_IMPL(__imp__sub_825EE6A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r10,3708(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3708);
	// lwz r11,3704(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3704);
	// stw r10,3704(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3704, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r11,3708(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3708, r11.u32);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r9,3776(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3776, ctx.r9.u32);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r9,3780(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3780, ctx.r9.u32);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r10,3784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3784, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,3788(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3788, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,3792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3792, ctx.r10.u32);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,3796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3796, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825EE6E8"))) PPC_WEAK_FUNC(sub_825EE6E8);
PPC_FUNC_IMPL(__imp__sub_825EE6E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r11,3980(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3980);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825ee718
	if (!cr6.eq) goto loc_825EE718;
	// li r11,1
	r11.s64 = 1;
	// li r10,15
	ctx.r10.s64 = 15;
	// stw r11,280(r27)
	PPC_STORE_U32(r27.u32 + 280, r11.u32);
	// stw r10,3976(r27)
	PPC_STORE_U32(r27.u32 + 3976, ctx.r10.u32);
	// b 0x825eea08
	goto loc_825EEA08;
loc_825EE718:
	// li r29,0
	r29.s64 = 0;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// stw r29,472(r27)
	PPC_STORE_U32(r27.u32 + 472, r29.u32);
	// stw r29,3976(r27)
	PPC_STORE_U32(r27.u32 + 3976, r29.u32);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825ee750
	if (!cr0.lt) goto loc_825EE750;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE750:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// stw r31,280(r27)
	PPC_STORE_U32(r27.u32 + 280, r31.u32);
	// beq cr6,0x825eea18
	if (cr6.eq) goto loc_825EEA18;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// mr r28,r29
	r28.u64 = r29.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825ee7d0
	if (!cr6.lt) goto loc_825EE7D0;
loc_825EE778:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ee7d0
	if (cr6.eq) goto loc_825EE7D0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825ee7c0
	if (!cr0.lt) goto loc_825EE7C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE7C0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ee778
	if (cr6.gt) goto loc_825EE778;
loc_825EE7D0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r28
	r30.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ee80c
	if (!cr0.lt) goto loc_825EE80C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE80C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825ee820
	if (!cr6.eq) goto loc_825EE820;
	// li r11,15
	r11.s64 = 15;
	// stw r11,3976(r27)
	PPC_STORE_U32(r27.u32 + 3976, r11.u32);
	// b 0x825ee9f0
	goto loc_825EE9F0;
loc_825EE820:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x825ee8f8
	if (!cr6.eq) goto loc_825EE8F8;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825ee898
	if (!cr6.lt) goto loc_825EE898;
loc_825EE840:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ee898
	if (cr6.eq) goto loc_825EE898;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ee888
	if (!cr0.lt) goto loc_825EE888;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE888:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ee840
	if (cr6.gt) goto loc_825EE840;
loc_825EE898:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ee8d4
	if (!cr0.lt) goto loc_825EE8D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE8D4:
	// cmplwi cr6,r30,3
	cr6.compare<uint32_t>(r30.u32, 3, xer);
	// beq cr6,0x825ee8ec
	if (cr6.eq) goto loc_825EE8EC;
	// li r11,3
	r11.s64 = 3;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// stw r11,3976(r27)
	PPC_STORE_U32(r27.u32 + 3976, r11.u32);
	// b 0x825ee9f0
	goto loc_825EE9F0;
loc_825EE8EC:
	// li r11,9
	r11.s64 = 9;
	// stw r11,3976(r27)
	PPC_STORE_U32(r27.u32 + 3976, r11.u32);
	// b 0x825ee9f0
	goto loc_825EE9F0;
loc_825EE8F8:
	// cmplwi cr6,r30,2
	cr6.compare<uint32_t>(r30.u32, 2, xer);
	// bne cr6,0x825ee9b8
	if (!cr6.eq) goto loc_825EE9B8;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825ee96c
	if (!cr6.lt) goto loc_825EE96C;
loc_825EE914:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ee96c
	if (cr6.eq) goto loc_825EE96C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ee95c
	if (!cr0.lt) goto loc_825EE95C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE95C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ee914
	if (cr6.gt) goto loc_825EE914;
loc_825EE96C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ee9a8
	if (!cr0.lt) goto loc_825EE9A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE9A8:
	// li r11,1
	r11.s64 = 1;
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// stw r11,3976(r27)
	PPC_STORE_U32(r27.u32 + 3976, r11.u32);
	// b 0x825ee9f0
	goto loc_825EE9F0;
loc_825EE9B8:
	// cmplwi cr6,r30,3
	cr6.compare<uint32_t>(r30.u32, 3, xer);
	// bne cr6,0x825ee9f0
	if (!cr6.eq) goto loc_825EE9F0;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825ee9ec
	if (!cr0.lt) goto loc_825EE9EC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EE9EC:
	// stw r31,472(r27)
	PPC_STORE_U32(r27.u32 + 472, r31.u32);
loc_825EE9F0:
	// lwz r11,472(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 472);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825eea08
	if (!cr6.eq) goto loc_825EEA08;
	// lwz r11,3976(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825eea18
	if (cr6.eq) goto loc_825EEA18;
loc_825EEA08:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82639298
	sub_82639298(ctx, base);
	// stw r3,3984(r27)
	PPC_STORE_U32(r27.u32 + 3984, ctx.r3.u32);
loc_825EEA18:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825EEA20"))) PPC_WEAK_FUNC(sub_825EEA20);
PPC_FUNC_IMPL(__imp__sub_825EEA20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// li r30,11
	r30.s64 = 11;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,11
	cr6.compare<uint32_t>(r11.u32, 11, xer);
	// bge cr6,0x825eeaa8
	if (!cr6.lt) goto loc_825EEAA8;
loc_825EEA50:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eeaa8
	if (cr6.eq) goto loc_825EEAA8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825eea98
	if (!cr0.lt) goto loc_825EEA98;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEA98:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eea50
	if (cr6.gt) goto loc_825EEA50;
loc_825EEAA8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825eeae4
	if (!cr0.lt) goto loc_825EEAE4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEAE4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,11
	r30.s64 = 11;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,11
	cr6.compare<uint32_t>(r11.u32, 11, xer);
	// bge cr6,0x825eeb58
	if (!cr6.lt) goto loc_825EEB58;
loc_825EEB00:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eeb58
	if (cr6.eq) goto loc_825EEB58;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825eeb48
	if (!cr0.lt) goto loc_825EEB48;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEB48:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eeb00
	if (cr6.gt) goto loc_825EEB00;
loc_825EEB58:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825eeb94
	if (!cr0.lt) goto loc_825EEB94;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEB94:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x825eee68
	if (cr6.eq) goto loc_825EEE68;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x825eee68
	if (cr6.eq) goto loc_825EEE68;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// stw r28,156(r27)
	PPC_STORE_U32(r27.u32 + 156, r28.u32);
	// stw r29,160(r27)
	PPC_STORE_U32(r27.u32 + 160, r29.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825eec04
	if (!cr6.lt) goto loc_825EEC04;
loc_825EEBC4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eec04
	if (cr6.eq) goto loc_825EEC04;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825eebf4
	if (!cr0.lt) goto loc_825EEBF4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEBF4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eebc4
	if (cr6.gt) goto loc_825EEBC4;
loc_825EEC04:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825eec2c
	if (!cr0.lt) goto loc_825EEC2C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEC2C:
	// li r11,1
	r11.s64 = 1;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// stw r26,3888(r27)
	PPC_STORE_U32(r27.u32 + 3888, r26.u32);
	// li r30,1
	r30.s64 = 1;
	// stw r26,436(r27)
	PPC_STORE_U32(r27.u32 + 436, r26.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// stw r26,3892(r27)
	PPC_STORE_U32(r27.u32 + 3892, r26.u32);
	// stw r11,3900(r27)
	PPC_STORE_U32(r27.u32 + 3900, r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825eecb4
	if (!cr6.lt) goto loc_825EECB4;
loc_825EEC5C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eecb4
	if (cr6.eq) goto loc_825EECB4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825eeca4
	if (!cr0.lt) goto loc_825EECA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EECA4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eec5c
	if (cr6.gt) goto loc_825EEC5C;
loc_825EECB4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825eecf0
	if (!cr0.lt) goto loc_825EECF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EECF0:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,3884(r27)
	PPC_STORE_U32(r27.u32 + 3884, r28.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825eed68
	if (!cr6.lt) goto loc_825EED68;
loc_825EED10:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eed68
	if (cr6.eq) goto loc_825EED68;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825eed58
	if (!cr0.lt) goto loc_825EED58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EED58:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eed10
	if (cr6.gt) goto loc_825EED10;
loc_825EED68:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825eeda4
	if (!cr0.lt) goto loc_825EEDA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEDA4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// stw r28,396(r27)
	PPC_STORE_U32(r27.u32 + 396, r28.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825eee1c
	if (!cr6.lt) goto loc_825EEE1C;
loc_825EEDC4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eee1c
	if (cr6.eq) goto loc_825EEE1C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825eee0c
	if (!cr0.lt) goto loc_825EEE0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEE0C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eedc4
	if (cr6.gt) goto loc_825EEDC4;
loc_825EEE1C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825eee58
	if (!cr0.lt) goto loc_825EEE58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEE58:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r30,15464(r27)
	PPC_STORE_U32(r27.u32 + 15464, r30.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825EEE68:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825EEE74"))) PPC_WEAK_FUNC(sub_825EEE74);
PPC_FUNC_IMPL(__imp__sub_825EEE74) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EEE78"))) PPC_WEAK_FUNC(sub_825EEE78);
PPC_FUNC_IMPL(__imp__sub_825EEE78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-496(r1)
	ea = -496 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r22,0
	r22.s64 = 0;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// li r5,88
	ctx.r5.s64 = 88;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,132
	ctx.r3.s64 = ctx.r1.s64 + 132;
	// stw r22,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r22.u32);
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// mr r23,r8
	r23.u64 = ctx.r8.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r5,126
	ctx.r5.s64 = 126;
	// sth r22,224(r1)
	PPC_STORE_U16(ctx.r1.u32 + 224, r22.u16);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,226
	ctx.r3.s64 = ctx.r1.s64 + 226;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r11,24
	r11.s64 = 24;
	// li r21,1
	r21.s64 = 1;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// stw r22,21212(r25)
	PPC_STORE_U32(r25.u32 + 21212, r22.u32);
	// mr r16,r22
	r16.u64 = r22.u64;
	// stw r22,21216(r25)
	PPC_STORE_U32(r25.u32 + 21216, r22.u32);
	// mr r19,r22
	r19.u64 = r22.u64;
	// mr r20,r21
	r20.u64 = r21.u64;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// li r11,25
	r11.s64 = 25;
	// stw r21,3900(r25)
	PPC_STORE_U32(r25.u32 + 3900, r21.u32);
	// mr r18,r21
	r18.u64 = r21.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r17,r21
	r17.u64 = r21.u64;
	// li r30,2
	r30.s64 = 2;
	// mr r29,r22
	r29.u64 = r22.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// li r11,30
	r11.s64 = 30;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// li r11,50
	r11.s64 = 50;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// li r11,60
	r11.s64 = 60;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// li r11,48
	r11.s64 = 48;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// li r11,72
	r11.s64 = 72;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825eef94
	if (!cr6.lt) goto loc_825EEF94;
loc_825EEF3C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eef94
	if (cr6.eq) goto loc_825EEF94;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825eef84
	if (!cr0.lt) goto loc_825EEF84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEF84:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eef3c
	if (cr6.gt) goto loc_825EEF3C;
loc_825EEF94:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825eefd0
	if (!cr0.lt) goto loc_825EEFD0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EEFD0:
	// cmpwi cr6,r30,3
	cr6.compare<int32_t>(r30.s32, 3, xer);
	// bne cr6,0x825f0c44
	if (!cr6.eq) goto loc_825F0C44;
	// li r11,3
	r11.s64 = 3;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r29,r22
	r29.u64 = r22.u64;
	// stw r11,3908(r25)
	PPC_STORE_U32(r25.u32 + 3908, r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825ef050
	if (!cr6.lt) goto loc_825EF050;
loc_825EEFF8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef050
	if (cr6.eq) goto loc_825EF050;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef040
	if (!cr0.lt) goto loc_825EF040;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF040:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eeff8
	if (cr6.gt) goto loc_825EEFF8;
loc_825EF050:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef08c
	if (!cr0.lt) goto loc_825EF08C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF08C:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// stw r28,21228(r25)
	PPC_STORE_U32(r25.u32 + 21228, r28.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825ef104
	if (!cr6.lt) goto loc_825EF104;
loc_825EF0AC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef104
	if (cr6.eq) goto loc_825EF104;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef0f4
	if (!cr0.lt) goto loc_825EF0F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF0F4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef0ac
	if (cr6.gt) goto loc_825EF0AC;
loc_825EF104:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef140
	if (!cr0.lt) goto loc_825EF140;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF140:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// stw r28,21200(r25)
	PPC_STORE_U32(r25.u32 + 21200, r28.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825ef1b8
	if (!cr6.lt) goto loc_825EF1B8;
loc_825EF160:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef1b8
	if (cr6.eq) goto loc_825EF1B8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef1a8
	if (!cr0.lt) goto loc_825EF1A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF1A8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef160
	if (cr6.gt) goto loc_825EF160;
loc_825EF1B8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef1f4
	if (!cr0.lt) goto loc_825EF1F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF1F4:
	// clrldi r11,r30,32
	r11.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// mr r29,r22
	r29.u64 = r22.u64;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// stfd f0,20856(r25)
	PPC_STORE_U64(r25.u32 + 20856, f0.u64);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825ef27c
	if (!cr6.lt) goto loc_825EF27C;
loc_825EF224:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef27c
	if (cr6.eq) goto loc_825EF27C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef26c
	if (!cr0.lt) goto loc_825EF26C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF26C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef224
	if (cr6.gt) goto loc_825EF224;
loc_825EF27C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef2b8
	if (!cr0.lt) goto loc_825EF2B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF2B8:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// stw r28,3660(r25)
	PPC_STORE_U32(r25.u32 + 3660, r28.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ef330
	if (!cr6.lt) goto loc_825EF330;
loc_825EF2D8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef330
	if (cr6.eq) goto loc_825EF330;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef320
	if (!cr0.lt) goto loc_825EF320;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF320:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef2d8
	if (cr6.gt) goto loc_825EF2D8;
loc_825EF330:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef36c
	if (!cr0.lt) goto loc_825EF36C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF36C:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,12
	r30.s64 = 12;
	// stw r28,20868(r25)
	PPC_STORE_U32(r25.u32 + 20868, r28.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bge cr6,0x825ef3e4
	if (!cr6.lt) goto loc_825EF3E4;
loc_825EF38C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef3e4
	if (cr6.eq) goto loc_825EF3E4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef3d4
	if (!cr0.lt) goto loc_825EF3D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF3D4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef38c
	if (cr6.gt) goto loc_825EF38C;
loc_825EF3E4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef420
	if (!cr0.lt) goto loc_825EF420;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF420:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,12
	r30.s64 = 12;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r29,r22
	r29.u64 = r22.u64;
	// stw r11,21352(r25)
	PPC_STORE_U32(r25.u32 + 21352, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bge cr6,0x825ef4a0
	if (!cr6.lt) goto loc_825EF4A0;
loc_825EF448:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef4a0
	if (cr6.eq) goto loc_825EF4A0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef490
	if (!cr0.lt) goto loc_825EF490;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF490:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef448
	if (cr6.gt) goto loc_825EF448;
loc_825EF4A0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef4dc
	if (!cr0.lt) goto loc_825EF4DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF4DC:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,21356(r25)
	PPC_STORE_U32(r25.u32 + 21356, r11.u32);
	// beq cr6,0x825ef508
	if (cr6.eq) goto loc_825EF508;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x825ef508
	if (cr6.eq) goto loc_825EF508;
	// lwz r11,21352(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21352);
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// lwz r11,21356(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21356);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_825EF508:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ef57c
	if (!cr6.lt) goto loc_825EF57C;
loc_825EF524:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef57c
	if (cr6.eq) goto loc_825EF57C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef56c
	if (!cr0.lt) goto loc_825EF56C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF56C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef524
	if (cr6.gt) goto loc_825EF524;
loc_825EF57C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef5b8
	if (!cr0.lt) goto loc_825EF5B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF5B8:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// stw r28,20832(r25)
	PPC_STORE_U32(r25.u32 + 20832, r28.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ef630
	if (!cr6.lt) goto loc_825EF630;
loc_825EF5D8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef630
	if (cr6.eq) goto loc_825EF630;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef620
	if (!cr0.lt) goto loc_825EF620;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF620:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef5d8
	if (cr6.gt) goto loc_825EF5D8;
loc_825EF630:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef66c
	if (!cr0.lt) goto loc_825EF66C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF66C:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// stw r28,21160(r25)
	PPC_STORE_U32(r25.u32 + 21160, r28.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ef6e4
	if (!cr6.lt) goto loc_825EF6E4;
loc_825EF68C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef6e4
	if (cr6.eq) goto loc_825EF6E4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef6d4
	if (!cr0.lt) goto loc_825EF6D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF6D4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef68c
	if (cr6.gt) goto loc_825EF68C;
loc_825EF6E4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef720
	if (!cr0.lt) goto loc_825EF720;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF720:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// stw r28,20848(r25)
	PPC_STORE_U32(r25.u32 + 20848, r28.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ef798
	if (!cr6.lt) goto loc_825EF798;
loc_825EF740:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef798
	if (cr6.eq) goto loc_825EF798;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef788
	if (!cr0.lt) goto loc_825EF788;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF788:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef740
	if (cr6.gt) goto loc_825EF740;
loc_825EF798:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef7d4
	if (!cr0.lt) goto loc_825EF7D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF7D4:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// stw r29,3444(r25)
	PPC_STORE_U32(r25.u32 + 3444, r29.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ef830
	if (!cr6.lt) goto loc_825EF830;
loc_825EF7F0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef830
	if (cr6.eq) goto loc_825EF830;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825ef820
	if (!cr0.lt) goto loc_825EF820;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF820:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef7f0
	if (cr6.gt) goto loc_825EF7F0;
loc_825EF830:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825ef858
	if (!cr0.lt) goto loc_825EF858;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF858:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ef8cc
	if (!cr6.lt) goto loc_825EF8CC;
loc_825EF874:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef8cc
	if (cr6.eq) goto loc_825EF8CC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef8bc
	if (!cr0.lt) goto loc_825EF8BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF8BC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef874
	if (cr6.gt) goto loc_825EF874;
loc_825EF8CC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef908
	if (!cr0.lt) goto loc_825EF908;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF908:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// stw r28,21548(r25)
	PPC_STORE_U32(r25.u32 + 21548, r28.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ef980
	if (!cr6.lt) goto loc_825EF980;
loc_825EF928:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ef980
	if (cr6.eq) goto loc_825EF980;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ef970
	if (!cr0.lt) goto loc_825EF970;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF970:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef928
	if (cr6.gt) goto loc_825EF928;
loc_825EF980:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ef9bc
	if (!cr0.lt) goto loc_825EF9BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EF9BC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825f04bc
	if (cr6.eq) goto loc_825F04BC;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,14
	r30.s64 = 14;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,14
	cr6.compare<uint32_t>(r11.u32, 14, xer);
	// bge cr6,0x825efa38
	if (!cr6.lt) goto loc_825EFA38;
loc_825EF9E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825efa38
	if (cr6.eq) goto loc_825EFA38;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825efa28
	if (!cr0.lt) goto loc_825EFA28;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFA28:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ef9e0
	if (cr6.gt) goto loc_825EF9E0;
loc_825EFA38:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825efa74
	if (!cr0.lt) goto loc_825EFA74;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFA74:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// addi r28,r30,1
	r28.s64 = r30.s64 + 1;
	// li r30,14
	r30.s64 = 14;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,14
	cr6.compare<uint32_t>(r11.u32, 14, xer);
	// bge cr6,0x825efaec
	if (!cr6.lt) goto loc_825EFAEC;
loc_825EFA94:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825efaec
	if (cr6.eq) goto loc_825EFAEC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825efadc
	if (!cr0.lt) goto loc_825EFADC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFADC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825efa94
	if (cr6.gt) goto loc_825EFA94;
loc_825EFAEC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825efb28
	if (!cr0.lt) goto loc_825EFB28;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFB28:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825efb44
	if (cr6.eq) goto loc_825EFB44;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x825efb44
	if (cr6.eq) goto loc_825EFB44;
	// stw r28,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r28.u32);
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
loc_825EFB44:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r11.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// stw r21,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r21.u32);
	// stw r28,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r28.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825efbc4
	if (!cr6.lt) goto loc_825EFBC4;
loc_825EFB6C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825efbc4
	if (cr6.eq) goto loc_825EFBC4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825efbb4
	if (!cr0.lt) goto loc_825EFBB4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFBB4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825efb6c
	if (cr6.gt) goto loc_825EFB6C;
loc_825EFBC4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825efc00
	if (!cr0.lt) goto loc_825EFC00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFC00:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825efe44
	if (cr6.eq) goto loc_825EFE44;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x825efc7c
	if (!cr6.lt) goto loc_825EFC7C;
loc_825EFC24:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825efc7c
	if (cr6.eq) goto loc_825EFC7C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825efc6c
	if (!cr0.lt) goto loc_825EFC6C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFC6C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825efc24
	if (cr6.gt) goto loc_825EFC24;
loc_825EFC7C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r27,r11,r29
	r27.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825efcb8
	if (!cr0.lt) goto loc_825EFCB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFCB8:
	// cmpwi cr6,r27,15
	cr6.compare<int32_t>(r27.s32, 15, xer);
	// bne cr6,0x825efe2c
	if (!cr6.eq) goto loc_825EFE2C;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825efd34
	if (!cr6.lt) goto loc_825EFD34;
loc_825EFCDC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825efd34
	if (cr6.eq) goto loc_825EFD34;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825efd24
	if (!cr0.lt) goto loc_825EFD24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFD24:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825efcdc
	if (cr6.gt) goto loc_825EFCDC;
loc_825EFD34:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825efd70
	if (!cr0.lt) goto loc_825EFD70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFD70:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// addi r28,r30,1
	r28.s64 = r30.s64 + 1;
	// li r30,8
	r30.s64 = 8;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825efde8
	if (!cr6.lt) goto loc_825EFDE8;
loc_825EFD90:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825efde8
	if (cr6.eq) goto loc_825EFDE8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825efdd8
	if (!cr0.lt) goto loc_825EFDD8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFDD8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825efd90
	if (cr6.gt) goto loc_825EFD90;
loc_825EFDE8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825efe24
	if (!cr0.lt) goto loc_825EFE24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFE24:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// b 0x825efe34
	goto loc_825EFE34;
loc_825EFE2C:
	// mr r11,r22
	r11.u64 = r22.u64;
	// mr r28,r22
	r28.u64 = r22.u64;
loc_825EFE34:
	// stw r21,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r21.u32);
	// stw r27,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r27.u32);
	// stw r28,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r28.u32);
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
loc_825EFE44:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825efeb8
	if (!cr6.lt) goto loc_825EFEB8;
loc_825EFE60:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825efeb8
	if (cr6.eq) goto loc_825EFEB8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825efea8
	if (!cr0.lt) goto loc_825EFEA8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFEA8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825efe60
	if (cr6.gt) goto loc_825EFE60;
loc_825EFEB8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825efef4
	if (!cr0.lt) goto loc_825EFEF4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFEF4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825f01e4
	if (cr6.eq) goto loc_825F01E4;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// stw r21,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, r21.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825eff74
	if (!cr6.lt) goto loc_825EFF74;
loc_825EFF1C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825eff74
	if (cr6.eq) goto loc_825EFF74;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825eff64
	if (!cr0.lt) goto loc_825EFF64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFF64:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825eff1c
	if (cr6.gt) goto loc_825EFF1C;
loc_825EFF74:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825effb0
	if (!cr0.lt) goto loc_825EFFB0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825EFFB0:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825f0080
	if (cr6.eq) goto loc_825F0080;
	// li r30,16
	r30.s64 = 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x825f002c
	if (!cr6.lt) goto loc_825F002C;
loc_825EFFD4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f002c
	if (cr6.eq) goto loc_825F002C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f001c
	if (!cr0.lt) goto loc_825F001C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F001C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825effd4
	if (cr6.gt) goto loc_825EFFD4;
loc_825F002C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0068
	if (!cr0.lt) goto loc_825F0068;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0068:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// stw r21,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, r21.u32);
	// mr r16,r21
	r16.u64 = r21.u64;
	// mr r17,r11
	r17.u64 = r11.u64;
	// stw r11,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r11.u32);
	// b 0x825f01e4
	goto loc_825F01E4;
loc_825F0080:
	// li r30,8
	r30.s64 = 8;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f00e4
	if (!cr6.lt) goto loc_825F00E4;
loc_825F008C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f00e4
	if (cr6.eq) goto loc_825F00E4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f00d4
	if (!cr0.lt) goto loc_825F00D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F00D4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f008c
	if (cr6.gt) goto loc_825F008C;
loc_825F00E4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0120
	if (!cr0.lt) goto loc_825F0120;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0120:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r18,r28
	r18.u64 = r28.u64;
	// li r30,4
	r30.s64 = 4;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x825f0198
	if (!cr6.lt) goto loc_825F0198;
loc_825F0140:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0198
	if (cr6.eq) goto loc_825F0198;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0188
	if (!cr0.lt) goto loc_825F0188;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0188:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f0140
	if (cr6.gt) goto loc_825F0140;
loc_825F0198:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f01d4
	if (!cr0.lt) goto loc_825F01D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F01D4:
	// mr r20,r30
	r20.u64 = r30.u64;
	// stw r28,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r28.u32);
	// mr r19,r21
	r19.u64 = r21.u64;
	// stw r30,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r30.u32);
loc_825F01E4:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f0258
	if (!cr6.lt) goto loc_825F0258;
loc_825F0200:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0258
	if (cr6.eq) goto loc_825F0258;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0248
	if (!cr0.lt) goto loc_825F0248;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0248:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f0200
	if (cr6.gt) goto loc_825F0200;
loc_825F0258:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0294
	if (!cr0.lt) goto loc_825F0294;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0294:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825f04bc
	if (cr6.eq) goto loc_825F04BC;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f0310
	if (!cr6.lt) goto loc_825F0310;
loc_825F02B8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0310
	if (cr6.eq) goto loc_825F0310;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0300
	if (!cr0.lt) goto loc_825F0300;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0300:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f02b8
	if (cr6.gt) goto loc_825F02B8;
loc_825F0310:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r27,r11,r29
	r27.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f034c
	if (!cr0.lt) goto loc_825F034C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F034C:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f03c0
	if (!cr6.lt) goto loc_825F03C0;
loc_825F0368:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f03c0
	if (cr6.eq) goto loc_825F03C0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f03b0
	if (!cr0.lt) goto loc_825F03B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F03B0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f0368
	if (cr6.gt) goto loc_825F0368;
loc_825F03C0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f03fc
	if (!cr0.lt) goto loc_825F03FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F03FC:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f0470
	if (!cr6.lt) goto loc_825F0470;
loc_825F0418:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0470
	if (cr6.eq) goto loc_825F0470;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0460
	if (!cr0.lt) goto loc_825F0460;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0460:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f0418
	if (cr6.gt) goto loc_825F0418;
loc_825F0470:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f04ac
	if (!cr0.lt) goto loc_825F04AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F04AC:
	// stw r21,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r21.u32);
	// stw r27,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r27.u32);
	// stw r28,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r28.u32);
	// stw r30,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, r30.u32);
loc_825F04BC:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f0530
	if (!cr6.lt) goto loc_825F0530;
loc_825F04D8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0530
	if (cr6.eq) goto loc_825F0530;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0520
	if (!cr0.lt) goto loc_825F0520;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0520:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f04d8
	if (cr6.gt) goto loc_825F04D8;
loc_825F0530:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f056c
	if (!cr0.lt) goto loc_825F056C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F056C:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,21204(r25)
	PPC_STORE_U32(r25.u32 + 21204, r30.u32);
	// beq cr6,0x825f0950
	if (cr6.eq) goto loc_825F0950;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825f05ec
	if (!cr6.lt) goto loc_825F05EC;
loc_825F0594:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f05ec
	if (cr6.eq) goto loc_825F05EC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f05dc
	if (!cr0.lt) goto loc_825F05DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F05DC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f0594
	if (cr6.gt) goto loc_825F0594;
loc_825F05EC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0628
	if (!cr0.lt) goto loc_825F0628;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0628:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// stw r28,21208(r25)
	PPC_STORE_U32(r25.u32 + 21208, r28.u32);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x825f06a0
	if (!cr6.lt) goto loc_825F06A0;
loc_825F0648:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f06a0
	if (cr6.eq) goto loc_825F06A0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0690
	if (!cr0.lt) goto loc_825F0690;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0690:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f0648
	if (cr6.gt) goto loc_825F0648;
loc_825F06A0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f06dc
	if (!cr0.lt) goto loc_825F06DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F06DC:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// addi r28,r30,6
	r28.s64 = r30.s64 + 6;
	// li r30,4
	r30.s64 = 4;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x825f0754
	if (!cr6.lt) goto loc_825F0754;
loc_825F06FC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0754
	if (cr6.eq) goto loc_825F0754;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0744
	if (!cr0.lt) goto loc_825F0744;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0744:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f06fc
	if (cr6.gt) goto loc_825F06FC;
loc_825F0754:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0790
	if (!cr0.lt) goto loc_825F0790;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0790:
	// lwz r11,21208(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21208);
	// addi r10,r30,4
	ctx.r10.s64 = r30.s64 + 4;
	// stw r28,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r28.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r10.u32);
	// stw r11,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r11.u32);
	// ble cr6,0x825f07b8
	if (!cr6.gt) goto loc_825F07B8;
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// stw r10,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r10.u32);
	// b 0x825f07bc
	goto loc_825F07BC;
loc_825F07B8:
	// stw r22,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r22.u32);
loc_825F07BC:
	// mr r26,r22
	r26.u64 = r22.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825f0950
	if (!cr6.gt) goto loc_825F0950;
	// addi r27,r1,226
	r27.s64 = ctx.r1.s64 + 226;
loc_825F07CC:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,16
	r30.s64 = 16;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x825f0844
	if (!cr6.lt) goto loc_825F0844;
loc_825F07E8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0844
	if (cr6.eq) goto loc_825F0844;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0834
	if (!cr0.lt) goto loc_825F0834;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0834:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f07e8
	if (cr6.gt) goto loc_825F07E8;
loc_825F0844:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0880
	if (!cr0.lt) goto loc_825F0880;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0880:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,16
	r30.s64 = 16;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x825f08f8
	if (!cr6.lt) goto loc_825F08F8;
loc_825F089C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f08f8
	if (cr6.eq) goto loc_825F08F8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f08e8
	if (!cr0.lt) goto loc_825F08E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F08E8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f089c
	if (cr6.gt) goto loc_825F089C;
loc_825F08F8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0934
	if (!cr0.lt) goto loc_825F0934;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0934:
	// lwz r11,21208(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21208);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// sth r28,-2(r27)
	PPC_STORE_U16(r27.u32 + -2, r28.u16);
	// sth r30,0(r27)
	PPC_STORE_U16(r27.u32 + 0, r30.u16);
	// cmpw cr6,r26,r11
	cr6.compare<int32_t>(r26.s32, r11.s32, xer);
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// blt cr6,0x825f07cc
	if (cr6.lt) goto loc_825F07CC;
loc_825F0950:
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f0c44
	if (!cr6.eq) goto loc_825F0C44;
	// lwz r10,21480(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 21480);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x825f0a70
	if (!cr6.eq) goto loc_825F0A70;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f13,20856(r25)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(r25.u32 + 20856);
	// lfd f0,-31368(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bne cr6,0x825f0a70
	if (!cr6.eq) goto loc_825F0A70;
	// lwz r11,3660(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 3660);
	// cmpwi cr6,r11,31
	cr6.compare<int32_t>(r11.s32, 31, xer);
	// bne cr6,0x825f0a70
	if (!cr6.eq) goto loc_825F0A70;
	// lwz r11,21160(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,21356(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21356);
	// beq cr6,0x825f09c4
	if (cr6.eq) goto loc_825F09C4;
	// cmpwi cr6,r11,486
	cr6.compare<int32_t>(r11.s32, 486, xer);
	// ble cr6,0x825f09b0
	if (!cr6.gt) goto loc_825F09B0;
	// cmpwi cr6,r11,576
	cr6.compare<int32_t>(r11.s32, 576, xer);
	// li r11,25
	r11.s64 = 25;
	// ble cr6,0x825f09b4
	if (!cr6.gt) goto loc_825F09B4;
loc_825F09B0:
	// li r11,30
	r11.s64 = 30;
loc_825F09B4:
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// b 0x825f09e8
	goto loc_825F09E8;
loc_825F09C4:
	// cmpwi cr6,r11,576
	cr6.compare<int32_t>(r11.s32, 576, xer);
	// ble cr6,0x825f09d8
	if (!cr6.gt) goto loc_825F09D8;
	// cmpwi cr6,r11,720
	cr6.compare<int32_t>(r11.s32, 720, xer);
	// li r11,60
	r11.s64 = 60;
	// ble cr6,0x825f09dc
	if (!cr6.gt) goto loc_825F09DC;
loc_825F09D8:
	// li r11,30
	r11.s64 = 30;
loc_825F09DC:
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_825F09E8:
	// fcfid f0,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(f0.s64);
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// stfd f0,20856(r25)
	PPC_STORE_U64(r25.u32 + 20856, f0.u64);
	// beq cr6,0x825f0a20
	if (cr6.eq) goto loc_825F0A20;
	// addi r11,r17,17
	r11.s64 = r17.s64 + 17;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,80(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// lfs f0,-11704(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -11704);
	f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfd f0,20856(r25)
	PPC_STORE_U64(r25.u32 + 20856, f0.u64);
loc_825F0A20:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// beq cr6,0x825f0b34
	if (cr6.eq) goto loc_825F0B34;
	// addi r11,r18,-1
	r11.s64 = r18.s64 + -1;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bgt cr6,0x825f0b34
	if (cr6.gt) goto loc_825F0B34;
	// addi r11,r20,-1
	r11.s64 = r20.s64 + -1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bgt cr6,0x825f0b34
	if (cr6.gt) goto loc_825F0B34;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// rlwinm r11,r18,2,0,29
	r11.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r20,999
	ctx.r9.s64 = r20.s64 + 999;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x825f0b10
	goto loc_825F0B10;
loc_825F0A70:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfd f12,20856(r25)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(r25.u32 + 20856);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// lfd f0,28584(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 28584);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// lfd f13,264(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 264);
	// fmadd f0,f12,f0,f13
	f0.f64 = ctx.f12.f64 * f0.f64 + ctx.f13.f64;
	// stfd f0,20856(r25)
	PPC_STORE_U64(r25.u32 + 20856, f0.u64);
	// bne cr6,0x825f0b34
	if (!cr6.eq) goto loc_825F0B34;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// beq cr6,0x825f0ac4
	if (cr6.eq) goto loc_825F0AC4;
	// addi r11,r17,17
	r11.s64 = r17.s64 + 17;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// lfs f0,-11704(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -11704);
	f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfd f0,20856(r25)
	PPC_STORE_U64(r25.u32 + 20856, f0.u64);
loc_825F0AC4:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// beq cr6,0x825f0b34
	if (cr6.eq) goto loc_825F0B34;
	// addi r11,r18,-1
	r11.s64 = r18.s64 + -1;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bgt cr6,0x825f0b34
	if (cr6.gt) goto loc_825F0B34;
	// addi r11,r20,-1
	r11.s64 = r20.s64 + -1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bgt cr6,0x825f0b34
	if (cr6.gt) goto loc_825F0B34;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// rlwinm r11,r18,2,0,29
	r11.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r20,999
	ctx.r9.s64 = r20.s64 + 999;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_825F0B10:
	// fcfid f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(ctx.f13.s64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f0
	ctx.f12.f64 = double(float(f0.f64));
	// lfs f0,17324(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 17324);
	f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fdivs f0,f0,f12
	f0.f64 = double(float(f0.f64 / ctx.f12.f64));
	// stfd f0,20856(r25)
	PPC_STORE_U64(r25.u32 + 20856, f0.u64);
loc_825F0B34:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f13,20856(r25)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(r25.u32 + 20856);
	// lwz r10,3660(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 3660);
	// addi r8,r25,3656
	ctx.r8.s64 = r25.s64 + 3656;
	// lwz r9,14772(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 14772);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r10,r10,6,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 6) & 0xFFFFFFC0;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,-28592(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -28592);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fadd f0,f13,f0
	f0.f64 = ctx.f13.f64 + f0.f64;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// addi r11,r11,28552
	r11.s64 = r11.s64 + 28552;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r10,3660(r25)
	PPC_STORE_U32(r25.u32 + 3660, ctx.r10.u32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// lwzx r11,r9,r11
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// stw r11,14776(r25)
	PPC_STORE_U32(r25.u32 + 14776, r11.u32);
	// bl 0x82603a38
	sub_82603A38(ctx, base);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x825e62e8
	sub_825E62E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f0c48
	if (!cr6.eq) goto loc_825F0C48;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x825e6398
	sub_825E6398(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825f0bc4
	if (cr6.eq) goto loc_825F0BC4;
	// cmpwi cr6,r3,12
	cr6.compare<int32_t>(ctx.r3.s32, 12, xer);
	// beq cr6,0x825f0bbc
	if (cr6.eq) goto loc_825F0BBC;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,496
	ctx.r1.s64 = ctx.r1.s64 + 496;
	// b 0x8239bd18
	return;
loc_825F0BBC:
	// stw r21,21336(r25)
	PPC_STORE_U32(r25.u32 + 21336, r21.u32);
	// b 0x825f0bc8
	goto loc_825F0BC8;
loc_825F0BC4:
	// stw r22,21336(r25)
	PPC_STORE_U32(r25.u32 + 21336, r22.u32);
loc_825F0BC8:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r22,444(r25)
	PPC_STORE_U32(r25.u32 + 444, r22.u32);
	// stw r22,3888(r25)
	PPC_STORE_U32(r25.u32 + 3888, r22.u32);
	// stw r22,3948(r25)
	PPC_STORE_U32(r25.u32 + 3948, r22.u32);
	// stw r22,3884(r25)
	PPC_STORE_U32(r25.u32 + 3884, r22.u32);
	// stw r22,14820(r25)
	PPC_STORE_U32(r25.u32 + 14820, r22.u32);
	// stw r21,1788(r25)
	PPC_STORE_U32(r25.u32 + 1788, r21.u32);
	// stw r22,396(r25)
	PPC_STORE_U32(r25.u32 + 396, r22.u32);
	// stw r22,3924(r25)
	PPC_STORE_U32(r25.u32 + 3924, r22.u32);
	// stw r22,15300(r25)
	PPC_STORE_U32(r25.u32 + 15300, r22.u32);
	// bl 0x82603be0
	sub_82603BE0(ctx, base);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82603d60
	sub_82603D60(ctx, base);
	// lwz r10,1964(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1964);
	// lwz r9,3180(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 3180);
	// lis r11,-32155
	r11.s64 = -2107310080;
	// addi r11,r11,-11552
	r11.s64 = r11.s64 + -11552;
	// stw r9,52(r10)
	PPC_STORE_U32(ctx.r10.u32 + 52, ctx.r9.u32);
	// lwz r10,3924(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 3924);
	// lwz r9,3936(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 3936);
	// stw r11,3156(r25)
	PPC_STORE_U32(r25.u32 + 3156, r11.u32);
	// stw r21,3372(r25)
	PPC_STORE_U32(r25.u32 + 3372, r21.u32);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stw r22,3368(r25)
	PPC_STORE_U32(r25.u32 + 3368, r22.u32);
	// stw r22,3364(r25)
	PPC_STORE_U32(r25.u32 + 3364, r22.u32);
	// bne cr6,0x825f0c44
	if (!cr6.eq) goto loc_825F0C44;
	// lwz r11,15300(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 15300);
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r10,3940(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 3940);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// beq cr6,0x825f0c48
	if (cr6.eq) goto loc_825F0C48;
loc_825F0C44:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825F0C48:
	// addi r1,r1,496
	ctx.r1.s64 = ctx.r1.s64 + 496;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_825F0C50"))) PPC_WEAK_FUNC(sub_825F0C50);
PPC_FUNC_IMPL(__imp__sub_825F0C50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// li r23,0
	r23.s64 = 0;
	// mr r22,r4
	r22.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// lwz r11,15472(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 15472);
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// mr r29,r23
	r29.u64 = r23.u64;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825f13c0
	if (!cr6.eq) goto loc_825F13C0;
	// li r30,8
	r30.s64 = 8;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f0cf8
	if (!cr6.lt) goto loc_825F0CF8;
loc_825F0CA0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0cf8
	if (cr6.eq) goto loc_825F0CF8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0ce8
	if (!cr0.lt) goto loc_825F0CE8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0CE8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f0ca0
	if (cr6.gt) goto loc_825F0CA0;
loc_825F0CF8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0d34
	if (!cr0.lt) goto loc_825F0D34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0D34:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825f27f0
	if (!cr6.eq) goto loc_825F27F0;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f0db0
	if (!cr6.lt) goto loc_825F0DB0;
loc_825F0D58:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0db0
	if (cr6.eq) goto loc_825F0DB0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0da0
	if (!cr0.lt) goto loc_825F0DA0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0DA0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f0d58
	if (cr6.gt) goto loc_825F0D58;
loc_825F0DB0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0dec
	if (!cr0.lt) goto loc_825F0DEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0DEC:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825f27f0
	if (!cr6.eq) goto loc_825F27F0;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f0e68
	if (!cr6.lt) goto loc_825F0E68;
loc_825F0E10:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0e68
	if (cr6.eq) goto loc_825F0E68;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0e58
	if (!cr0.lt) goto loc_825F0E58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0E58:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f0e10
	if (cr6.gt) goto loc_825F0E10;
loc_825F0E68:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0ea4
	if (!cr0.lt) goto loc_825F0EA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0EA4:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825f27f0
	if (!cr6.eq) goto loc_825F27F0;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f0f20
	if (!cr6.lt) goto loc_825F0F20;
loc_825F0EC8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f0f20
	if (cr6.eq) goto loc_825F0F20;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f0f10
	if (!cr0.lt) goto loc_825F0F10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0F10:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f0ec8
	if (cr6.gt) goto loc_825F0EC8;
loc_825F0F20:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f0f5c
	if (!cr0.lt) goto loc_825F0F5C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F0F5C:
	// cmpwi cr6,r30,15
	cr6.compare<int32_t>(r30.s32, 15, xer);
	// bne cr6,0x825f27f0
	if (!cr6.eq) goto loc_825F27F0;
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eee78
	sub_825EEE78(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f27f4
	if (!cr6.eq) goto loc_825F27F4;
	// lwz r10,21356(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 21356);
	// li r27,1
	r27.s64 = 1;
	// lwz r11,21352(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 21352);
	// lwz r9,21176(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 21176);
	// mullw r8,r11,r10
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// bgt cr6,0x825f0fbc
	if (cr6.gt) goto loc_825F0FBC;
	// lwz r9,21192(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 21192);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bgt cr6,0x825f0fbc
	if (cr6.gt) goto loc_825F0FBC;
	// lwz r11,21196(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 21196);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x825f1034
	if (!cr6.gt) goto loc_825F1034;
loc_825F0FBC:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f0fd8
	if (!cr6.eq) goto loc_825F0FD8;
	// li r3,14
	ctx.r3.s64 = 14;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_825F0FD8:
	// lwz r11,21356(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 21356);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lwz r10,21352(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 21352);
	// stw r27,3676(r24)
	PPC_STORE_U32(r24.u32 + 3676, r27.u32);
	// mullw r9,r10,r11
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,21196(r24)
	PPC_STORE_U32(r24.u32 + 21196, r11.u32);
	// stw r10,21192(r24)
	PPC_STORE_U32(r24.u32 + 21192, ctx.r10.u32);
	// stw r9,21176(r24)
	PPC_STORE_U32(r24.u32 + 21176, ctx.r9.u32);
	// bl 0x82605538
	sub_82605538(ctx, base);
	// lwz r5,21356(r24)
	ctx.r5.u64 = PPC_LOAD_U32(r24.u32 + 21356);
	// lwz r4,21352(r24)
	ctx.r4.u64 = PPC_LOAD_U32(r24.u32 + 21352);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82606168
	sub_82606168(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f27f4
	if (!cr6.eq) goto loc_825F27F4;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lwz r5,21356(r24)
	ctx.r5.u64 = PPC_LOAD_U32(r24.u32 + 21356);
	// lwz r4,21352(r24)
	ctx.r4.u64 = PPC_LOAD_U32(r24.u32 + 21352);
	// bl 0x82606a70
	sub_82606A70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f27f4
	if (!cr6.eq) goto loc_825F27F4;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82602170
	sub_82602170(ctx, base);
loc_825F1034:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f10c0
	if (cr6.eq) goto loc_825F10C0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r30,r27
	r30.u64 = r27.u64;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1098
	if (!cr6.lt) goto loc_825F1098;
loc_825F1058:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1098
	if (cr6.eq) goto loc_825F1098;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825f1088
	if (!cr0.lt) goto loc_825F1088;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1088:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1058
	if (cr6.gt) goto loc_825F1058;
loc_825F1098:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825f10c0
	if (!cr0.lt) goto loc_825F10C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F10C0:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f1144
	if (!cr6.lt) goto loc_825F1144;
loc_825F10EC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1144
	if (cr6.eq) goto loc_825F1144;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1134
	if (!cr0.lt) goto loc_825F1134;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1134:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f10ec
	if (cr6.gt) goto loc_825F10EC;
loc_825F1144:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1180
	if (!cr0.lt) goto loc_825F1180;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1180:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825f27f0
	if (!cr6.eq) goto loc_825F27F0;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f11fc
	if (!cr6.lt) goto loc_825F11FC;
loc_825F11A4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f11fc
	if (cr6.eq) goto loc_825F11FC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f11ec
	if (!cr0.lt) goto loc_825F11EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F11EC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f11a4
	if (cr6.gt) goto loc_825F11A4;
loc_825F11FC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1238
	if (!cr0.lt) goto loc_825F1238;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1238:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825f27f0
	if (!cr6.eq) goto loc_825F27F0;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f12b4
	if (!cr6.lt) goto loc_825F12B4;
loc_825F125C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f12b4
	if (cr6.eq) goto loc_825F12B4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f12a4
	if (!cr0.lt) goto loc_825F12A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F12A4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f125c
	if (cr6.gt) goto loc_825F125C;
loc_825F12B4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f12f0
	if (!cr0.lt) goto loc_825F12F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F12F0:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825f27f0
	if (!cr6.eq) goto loc_825F27F0;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f136c
	if (!cr6.lt) goto loc_825F136C;
loc_825F1314:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f136c
	if (cr6.eq) goto loc_825F136C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f135c
	if (!cr0.lt) goto loc_825F135C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F135C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1314
	if (cr6.gt) goto loc_825F1314;
loc_825F136C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f13a8
	if (!cr0.lt) goto loc_825F13A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F13A8:
	// cmpwi cr6,r30,14
	cr6.compare<int32_t>(r30.s32, 14, xer);
	// bne cr6,0x825f27f0
	if (!cr6.eq) goto loc_825F27F0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825e96c8
	sub_825E96C8(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_825F13C0:
	// li r30,2
	r30.s64 = 2;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f1424
	if (!cr6.lt) goto loc_825F1424;
loc_825F13CC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1424
	if (cr6.eq) goto loc_825F1424;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1414
	if (!cr0.lt) goto loc_825F1414;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1414:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f13cc
	if (cr6.gt) goto loc_825F13CC;
loc_825F1424:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1460
	if (!cr0.lt) goto loc_825F1460;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1460:
	// li r27,1
	r27.s64 = 1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825f1474
	if (!cr6.eq) goto loc_825F1474;
	// stw r23,3908(r24)
	PPC_STORE_U32(r24.u32 + 3908, r23.u32);
	// b 0x825f1494
	goto loc_825F1494;
loc_825F1474:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825f1484
	if (!cr6.eq) goto loc_825F1484;
	// stw r27,3908(r24)
	PPC_STORE_U32(r24.u32 + 3908, r27.u32);
	// b 0x825f1494
	goto loc_825F1494;
loc_825F1484:
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// bne cr6,0x825f1494
	if (!cr6.eq) goto loc_825F1494;
	// li r11,2
	r11.s64 = 2;
	// stw r11,3908(r24)
	PPC_STORE_U32(r24.u32 + 3908, r11.u32);
loc_825F1494:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1508
	if (!cr6.lt) goto loc_825F1508;
loc_825F14B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1508
	if (cr6.eq) goto loc_825F1508;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f14f8
	if (!cr0.lt) goto loc_825F14F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F14F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f14b0
	if (cr6.gt) goto loc_825F14B0;
loc_825F1508:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1544
	if (!cr0.lt) goto loc_825F1544;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1544:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,3924(r24)
	PPC_STORE_U32(r24.u32 + 3924, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f15bc
	if (!cr6.lt) goto loc_825F15BC;
loc_825F1564:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f15bc
	if (cr6.eq) goto loc_825F15BC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f15ac
	if (!cr0.lt) goto loc_825F15AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F15AC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1564
	if (cr6.gt) goto loc_825F1564;
loc_825F15BC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f15f8
	if (!cr0.lt) goto loc_825F15F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F15F8:
	// lwz r11,3924(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3924);
	// lwz r10,3936(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 3936);
	// stw r30,15300(r24)
	PPC_STORE_U32(r24.u32 + 15300, r30.u32);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x825f27f0
	if (!cr6.eq) goto loc_825F27F0;
	// lwz r11,3940(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3940);
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// bne cr6,0x825f27f0
	if (!cr6.eq) goto loc_825F27F0;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825f168c
	if (!cr6.lt) goto loc_825F168C;
loc_825F1634:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f168c
	if (cr6.eq) goto loc_825F168C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f167c
	if (!cr0.lt) goto loc_825F167C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F167C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1634
	if (cr6.gt) goto loc_825F1634;
loc_825F168C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f16c8
	if (!cr0.lt) goto loc_825F16C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F16C8:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// stw r28,3656(r24)
	PPC_STORE_U32(r24.u32 + 3656, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825f1740
	if (!cr6.lt) goto loc_825F1740;
loc_825F16E8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1740
	if (cr6.eq) goto loc_825F1740;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1730
	if (!cr0.lt) goto loc_825F1730;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1730:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f16e8
	if (cr6.gt) goto loc_825F16E8;
loc_825F1740:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f177c
	if (!cr0.lt) goto loc_825F177C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F177C:
	// rlwinm r11,r30,6,0,25
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 6) & 0xFFFFFFC0;
	// lwz r10,3656(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 3656);
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// addi r9,r11,32
	ctx.r9.s64 = r11.s64 + 32;
	// stw r27,3900(r24)
	PPC_STORE_U32(r24.u32 + 3900, r27.u32);
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r23
	r29.u64 = r23.u64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stw r9,3660(r24)
	PPC_STORE_U32(r24.u32 + 3660, ctx.r9.u32);
	// stw r11,3656(r24)
	PPC_STORE_U32(r24.u32 + 3656, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1810
	if (!cr6.lt) goto loc_825F1810;
loc_825F17B8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1810
	if (cr6.eq) goto loc_825F1810;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1800
	if (!cr0.lt) goto loc_825F1800;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1800:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f17b8
	if (cr6.gt) goto loc_825F17B8;
loc_825F1810:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f184c
	if (!cr0.lt) goto loc_825F184C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F184C:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,3892(r24)
	PPC_STORE_U32(r24.u32 + 3892, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f18c4
	if (!cr6.lt) goto loc_825F18C4;
loc_825F186C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f18c4
	if (cr6.eq) goto loc_825F18C4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f18b4
	if (!cr0.lt) goto loc_825F18B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F18B4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f186c
	if (cr6.gt) goto loc_825F186C;
loc_825F18C4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1900
	if (!cr0.lt) goto loc_825F1900;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1900:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,3884(r24)
	PPC_STORE_U32(r24.u32 + 3884, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1978
	if (!cr6.lt) goto loc_825F1978;
loc_825F1920:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1978
	if (cr6.eq) goto loc_825F1978;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1968
	if (!cr0.lt) goto loc_825F1968;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1968:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1920
	if (cr6.gt) goto loc_825F1920;
loc_825F1978:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f19b4
	if (!cr0.lt) goto loc_825F19B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F19B4:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,14820(r24)
	PPC_STORE_U32(r24.u32 + 14820, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1a2c
	if (!cr6.lt) goto loc_825F1A2C;
loc_825F19D4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1a2c
	if (cr6.eq) goto loc_825F1A2C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1a1c
	if (!cr0.lt) goto loc_825F1A1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1A1C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f19d4
	if (cr6.gt) goto loc_825F19D4;
loc_825F1A2C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1a68
	if (!cr0.lt) goto loc_825F1A68;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1A68:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,1788(r24)
	PPC_STORE_U32(r24.u32 + 1788, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1ae0
	if (!cr6.lt) goto loc_825F1AE0;
loc_825F1A88:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1ae0
	if (cr6.eq) goto loc_825F1AE0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1ad0
	if (!cr0.lt) goto loc_825F1AD0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1AD0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1a88
	if (cr6.gt) goto loc_825F1A88;
loc_825F1AE0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1b1c
	if (!cr0.lt) goto loc_825F1B1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1B1C:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,1792(r24)
	PPC_STORE_U32(r24.u32 + 1792, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1b94
	if (!cr6.lt) goto loc_825F1B94;
loc_825F1B3C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1b94
	if (cr6.eq) goto loc_825F1B94;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1b84
	if (!cr0.lt) goto loc_825F1B84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1B84:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1b3c
	if (cr6.gt) goto loc_825F1B3C;
loc_825F1B94:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1bd0
	if (!cr0.lt) goto loc_825F1BD0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1BD0:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// stw r28,20864(r24)
	PPC_STORE_U32(r24.u32 + 20864, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f1c48
	if (!cr6.lt) goto loc_825F1C48;
loc_825F1BF0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1c48
	if (cr6.eq) goto loc_825F1C48;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1c38
	if (!cr0.lt) goto loc_825F1C38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1C38:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1bf0
	if (cr6.gt) goto loc_825F1BF0;
loc_825F1C48:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1c84
	if (!cr0.lt) goto loc_825F1C84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1C84:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,3980(r24)
	PPC_STORE_U32(r24.u32 + 3980, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1cfc
	if (!cr6.lt) goto loc_825F1CFC;
loc_825F1CA4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1cfc
	if (cr6.eq) goto loc_825F1CFC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1cec
	if (!cr0.lt) goto loc_825F1CEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1CEC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1ca4
	if (cr6.gt) goto loc_825F1CA4;
loc_825F1CFC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1d38
	if (!cr0.lt) goto loc_825F1D38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1D38:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,436(r24)
	PPC_STORE_U32(r24.u32 + 436, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1db0
	if (!cr6.lt) goto loc_825F1DB0;
loc_825F1D58:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1db0
	if (cr6.eq) goto loc_825F1DB0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1da0
	if (!cr0.lt) goto loc_825F1DA0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1DA0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1d58
	if (cr6.gt) goto loc_825F1D58;
loc_825F1DB0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1dec
	if (!cr0.lt) goto loc_825F1DEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1DEC:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,396(r24)
	PPC_STORE_U32(r24.u32 + 396, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1e64
	if (!cr6.lt) goto loc_825F1E64;
loc_825F1E0C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1e64
	if (cr6.eq) goto loc_825F1E64;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1e54
	if (!cr0.lt) goto loc_825F1E54;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1E54:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1e0c
	if (cr6.gt) goto loc_825F1E0C;
loc_825F1E64:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1ea0
	if (!cr0.lt) goto loc_825F1EA0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1EA0:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,2972(r24)
	PPC_STORE_U32(r24.u32 + 2972, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1f18
	if (!cr6.lt) goto loc_825F1F18;
loc_825F1EC0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1f18
	if (cr6.eq) goto loc_825F1F18;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1f08
	if (!cr0.lt) goto loc_825F1F08;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1F08:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1ec0
	if (cr6.gt) goto loc_825F1EC0;
loc_825F1F18:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f1f54
	if (!cr0.lt) goto loc_825F1F54;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1F54:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,3932(r24)
	PPC_STORE_U32(r24.u32 + 3932, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f1fcc
	if (!cr6.lt) goto loc_825F1FCC;
loc_825F1F74:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f1fcc
	if (cr6.eq) goto loc_825F1FCC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f1fbc
	if (!cr0.lt) goto loc_825F1FBC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F1FBC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f1f74
	if (cr6.gt) goto loc_825F1F74;
loc_825F1FCC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2008
	if (!cr0.lt) goto loc_825F2008;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2008:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// stw r28,14792(r24)
	PPC_STORE_U32(r24.u32 + 14792, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825f2080
	if (!cr6.lt) goto loc_825F2080;
loc_825F2028:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2080
	if (cr6.eq) goto loc_825F2080;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2070
	if (!cr0.lt) goto loc_825F2070;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2070:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2028
	if (cr6.gt) goto loc_825F2028;
loc_825F2080:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f20bc
	if (!cr0.lt) goto loc_825F20BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F20BC:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r28,14772(r24)
	PPC_STORE_U32(r24.u32 + 14772, r28.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f2134
	if (!cr6.lt) goto loc_825F2134;
loc_825F20DC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2134
	if (cr6.eq) goto loc_825F2134;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2124
	if (!cr0.lt) goto loc_825F2124;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2124:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f20dc
	if (cr6.gt) goto loc_825F20DC;
loc_825F2134:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2170
	if (!cr0.lt) goto loc_825F2170;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2170:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,3436(r24)
	PPC_STORE_U32(r24.u32 + 3436, r30.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// mr r30,r27
	r30.u64 = r27.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825f223c
	if (cr6.eq) goto loc_825F223C;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f21f0
	if (!cr6.lt) goto loc_825F21F0;
loc_825F2198:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f21f0
	if (cr6.eq) goto loc_825F21F0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f21e0
	if (!cr0.lt) goto loc_825F21E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F21E0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2198
	if (cr6.gt) goto loc_825F2198;
loc_825F21F0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f222c
	if (!cr0.lt) goto loc_825F222C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F222C:
	// addi r11,r30,2
	r11.s64 = r30.s64 + 2;
	// stw r30,3428(r24)
	PPC_STORE_U32(r24.u32 + 3428, r30.u32);
	// stw r11,21524(r24)
	PPC_STORE_U32(r24.u32 + 21524, r11.u32);
	// b 0x825f22e0
	goto loc_825F22E0;
loc_825F223C:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f229c
	if (!cr6.lt) goto loc_825F229C;
loc_825F2244:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f229c
	if (cr6.eq) goto loc_825F229C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f228c
	if (!cr0.lt) goto loc_825F228C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F228C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2244
	if (cr6.gt) goto loc_825F2244;
loc_825F229C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f22d8
	if (!cr0.lt) goto loc_825F22D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F22D8:
	// stw r30,3440(r24)
	PPC_STORE_U32(r24.u32 + 3440, r30.u32);
	// stw r30,21524(r24)
	PPC_STORE_U32(r24.u32 + 21524, r30.u32);
loc_825F22E0:
	// lwz r11,3436(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3436);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f22fc
	if (!cr6.eq) goto loc_825F22FC;
	// lwz r11,3440(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3440);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r23
	r11.u64 = r23.u64;
	// beq cr6,0x825f2300
	if (cr6.eq) goto loc_825F2300;
loc_825F22FC:
	// mr r11,r27
	r11.u64 = r27.u64;
loc_825F2300:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r11,3432(r24)
	PPC_STORE_U32(r24.u32 + 3432, r11.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f2378
	if (!cr6.lt) goto loc_825F2378;
loc_825F2320:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2378
	if (cr6.eq) goto loc_825F2378;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2368
	if (!cr0.lt) goto loc_825F2368;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2368:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2320
	if (cr6.gt) goto loc_825F2320;
loc_825F2378:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f23b4
	if (!cr0.lt) goto loc_825F23B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F23B4:
	// lwz r11,15300(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 15300);
	// stw r30,3444(r24)
	PPC_STORE_U32(r24.u32 + 3444, r30.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2480
	if (!cr6.eq) goto loc_825F2480;
	// lwz r11,3908(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3908);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2454
	if (!cr6.eq) goto loc_825F2454;
	// lwz r11,3884(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3884);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,1788(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1788);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,1792(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1792);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,3932(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3932);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,20864(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20864);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,3892(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3892);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,3924(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,14820(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 14820);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,3980(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,14772(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,14792(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 14792);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// b 0x825f2470
	goto loc_825F2470;
loc_825F2454:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825f2480
	if (!cr6.eq) goto loc_825F2480;
	// lwz r11,3884(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3884);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f2474
	if (!cr6.eq) goto loc_825F2474;
	// lwz r11,1788(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1788);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
loc_825F2470:
	// beq cr6,0x825f2480
	if (cr6.eq) goto loc_825F2480;
loc_825F2474:
	// li r3,6
	ctx.r3.s64 = 6;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_825F2480:
	// lwz r10,14772(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 14772);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r9,3924(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 3924);
	// addi r11,r11,28552
	r11.s64 = r11.s64 + 28552;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,14776(r24)
	PPC_STORE_U32(r24.u32 + 14776, r11.u32);
	// beq cr6,0x825f24ac
	if (cr6.eq) goto loc_825F24AC;
	// stw r23,3948(r24)
	PPC_STORE_U32(r24.u32 + 3948, r23.u32);
	// stw r23,3884(r24)
	PPC_STORE_U32(r24.u32 + 3884, r23.u32);
loc_825F24AC:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82603a38
	sub_82603A38(ctx, base);
	// lwz r11,1788(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1788);
	// stw r23,444(r24)
	PPC_STORE_U32(r24.u32 + 444, r23.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r23,3888(r24)
	PPC_STORE_U32(r24.u32 + 3888, r23.u32);
	// beq cr6,0x825f2500
	if (cr6.eq) goto loc_825F2500;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82603ac8
	sub_82603AC8(ctx, base);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82603be0
	sub_82603BE0(ctx, base);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82603d60
	sub_82603D60(ctx, base);
	// lis r11,-32155
	r11.s64 = -2107310080;
	// lwz r10,1964(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 1964);
	// lwz r9,3180(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 3180);
	// addi r11,r11,-11552
	r11.s64 = r11.s64 + -11552;
	// stw r9,52(r10)
	PPC_STORE_U32(ctx.r10.u32 + 52, ctx.r9.u32);
	// stw r11,3156(r24)
	PPC_STORE_U32(r24.u32 + 3156, r11.u32);
loc_825F2500:
	// lwz r11,15300(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 15300);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f25d0
	if (cr6.eq) goto loc_825F25D0;
	// lwz r11,3884(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3884);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,396(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,3924(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,3892(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3892);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,20864(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20864);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,14820(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 14820);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,1792(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1792);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,3980(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,436(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 436);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,3932(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3932);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,14792(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 14792);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,3436(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3436);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,3428(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3428);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// lwz r11,3440(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 3440);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f25c4
	if (!cr6.eq) goto loc_825F25C4;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eea20
	sub_825EEA20(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_825F25C4:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_825F25D0:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r27,3364(r24)
	PPC_STORE_U32(r24.u32 + 3364, r27.u32);
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f2648
	if (!cr6.lt) goto loc_825F2648;
loc_825F25F0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2648
	if (cr6.eq) goto loc_825F2648;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2638
	if (!cr0.lt) goto loc_825F2638;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2638:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f25f0
	if (cr6.gt) goto loc_825F25F0;
loc_825F2648:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2684
	if (!cr0.lt) goto loc_825F2684;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2684:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x825f26a8
	if (!cr6.eq) goto loc_825F26A8;
	// lwz r11,84(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f26a8
	if (!cr6.eq) goto loc_825F26A8;
	// stw r27,3372(r24)
	PPC_STORE_U32(r24.u32 + 3372, r27.u32);
	// stw r23,3368(r24)
	PPC_STORE_U32(r24.u32 + 3368, r23.u32);
	// stw r23,3364(r24)
	PPC_STORE_U32(r24.u32 + 3364, r23.u32);
loc_825F26A8:
	// cmplwi cr6,r22,5
	cr6.compare<uint32_t>(r22.u32, 5, xer);
	// bne cr6,0x825f27e4
	if (!cr6.eq) goto loc_825F27E4;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// mr r29,r23
	r29.u64 = r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f2724
	if (!cr6.lt) goto loc_825F2724;
loc_825F26CC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2724
	if (cr6.eq) goto loc_825F2724;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2714
	if (!cr0.lt) goto loc_825F2714;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2714:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f26cc
	if (cr6.gt) goto loc_825F26CC;
loc_825F2724:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2760
	if (!cr0.lt) goto loc_825F2760;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2760:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r29,20868(r24)
	PPC_STORE_U32(r24.u32 + 20868, r29.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f27bc
	if (!cr6.lt) goto loc_825F27BC;
loc_825F277C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f27bc
	if (cr6.eq) goto loc_825F27BC;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825f27ac
	if (!cr0.lt) goto loc_825F27AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F27AC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f277c
	if (cr6.gt) goto loc_825F277C;
loc_825F27BC:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825f27e4
	if (!cr0.lt) goto loc_825F27E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F27E4:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_825F27F0:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825F27F4:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825F27FC"))) PPC_WEAK_FUNC(sub_825F27FC);
PPC_FUNC_IMPL(__imp__sub_825F27FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F2800"))) PPC_WEAK_FUNC(sub_825F2800);
PPC_FUNC_IMPL(__imp__sub_825F2800) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// li r24,0
	r24.s64 = 0;
	// li r30,1
	r30.s64 = 1;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f2890
	if (!cr6.lt) goto loc_825F2890;
loc_825F2838:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2890
	if (cr6.eq) goto loc_825F2890;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2880
	if (!cr0.lt) goto loc_825F2880;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2880:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2838
	if (cr6.gt) goto loc_825F2838;
loc_825F2890:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f28cc
	if (!cr0.lt) goto loc_825F28CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F28CC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825f2da0
	if (cr6.eq) goto loc_825F2DA0;
	// lwz r11,21160(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f290c
	if (cr6.eq) goto loc_825F290C;
	// lwz r11,21548(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21548);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f290c
	if (!cr6.eq) goto loc_825F290C;
	// lwz r11,20832(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20832);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f2904
	if (cr6.eq) goto loc_825F2904;
	// lwz r11,20840(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20840);
	// addi r24,r11,2
	r24.s64 = r11.s64 + 2;
	// b 0x825f2928
	goto loc_825F2928;
loc_825F2904:
	// li r24,2
	r24.s64 = 2;
	// b 0x825f2928
	goto loc_825F2928;
loc_825F290C:
	// lwz r11,20832(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20832);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f2924
	if (cr6.eq) goto loc_825F2924;
	// lwz r11,21164(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21164);
	// addi r24,r11,1
	r24.s64 = r11.s64 + 1;
	// b 0x825f2928
	goto loc_825F2928;
loc_825F2924:
	// li r24,1
	r24.s64 = 1;
loc_825F2928:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x825f2da0
	if (!cr6.gt) goto loc_825F2DA0;
	// addi r26,r28,12
	r26.s64 = r28.s64 + 12;
	// mr r25,r24
	r25.u64 = r24.u64;
loc_825F2938:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,16
	r30.s64 = 16;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x825f29b0
	if (!cr6.lt) goto loc_825F29B0;
loc_825F2954:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f29b0
	if (cr6.eq) goto loc_825F29B0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f29a0
	if (!cr0.lt) goto loc_825F29A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F29A0:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2954
	if (cr6.gt) goto loc_825F2954;
loc_825F29B0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f29ec
	if (!cr0.lt) goto loc_825F29EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F29EC:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f2a64
	if (!cr6.lt) goto loc_825F2A64;
loc_825F2A08:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2a64
	if (cr6.eq) goto loc_825F2A64;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2a54
	if (!cr0.lt) goto loc_825F2A54;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2A54:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2a08
	if (cr6.gt) goto loc_825F2A08;
loc_825F2A64:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2aa0
	if (!cr0.lt) goto loc_825F2AA0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2AA0:
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// li r30,16
	r30.s64 = 16;
	// or r11,r11,r29
	r11.u64 = r11.u64 | r29.u64;
	// li r29,0
	r29.s64 = 0;
	// stw r11,-4(r26)
	PPC_STORE_U32(r26.u32 + -4, r11.u32);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x825f2b24
	if (!cr6.lt) goto loc_825F2B24;
loc_825F2AC8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2b24
	if (cr6.eq) goto loc_825F2B24;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2b14
	if (!cr0.lt) goto loc_825F2B14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2B14:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2ac8
	if (cr6.gt) goto loc_825F2AC8;
loc_825F2B24:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2b60
	if (!cr0.lt) goto loc_825F2B60;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2B60:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f2bd8
	if (!cr6.lt) goto loc_825F2BD8;
loc_825F2B7C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2bd8
	if (cr6.eq) goto loc_825F2BD8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2bc8
	if (!cr0.lt) goto loc_825F2BC8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2BC8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2b7c
	if (cr6.gt) goto loc_825F2B7C;
loc_825F2BD8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2c14
	if (!cr0.lt) goto loc_825F2C14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2C14:
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// li r30,14
	r30.s64 = 14;
	// or r11,r11,r29
	r11.u64 = r11.u64 | r29.u64;
	// li r29,0
	r29.s64 = 0;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,14
	cr6.compare<uint32_t>(r11.u32, 14, xer);
	// bge cr6,0x825f2c98
	if (!cr6.lt) goto loc_825F2C98;
loc_825F2C3C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2c98
	if (cr6.eq) goto loc_825F2C98;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2c88
	if (!cr0.lt) goto loc_825F2C88;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2C88:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2c3c
	if (cr6.gt) goto loc_825F2C3C;
loc_825F2C98:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2cd4
	if (!cr0.lt) goto loc_825F2CD4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2CD4:
	// stw r28,-12(r26)
	PPC_STORE_U32(r26.u32 + -12, r28.u32);
	// li r30,14
	r30.s64 = 14;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,14
	cr6.compare<uint32_t>(r11.u32, 14, xer);
	// bge cr6,0x825f2d50
	if (!cr6.lt) goto loc_825F2D50;
loc_825F2CF4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2d50
	if (cr6.eq) goto loc_825F2D50;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f2d40
	if (!cr0.lt) goto loc_825F2D40;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2D40:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f2cf4
	if (cr6.gt) goto loc_825F2CF4;
loc_825F2D50:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2d8c
	if (!cr0.lt) goto loc_825F2D8C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2D8C:
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// stw r30,-8(r26)
	PPC_STORE_U32(r26.u32 + -8, r30.u32);
	// addi r26,r26,16
	r26.s64 = r26.s64 + 16;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x825f2938
	if (!cr6.eq) goto loc_825F2938;
loc_825F2DA0:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r24,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r24.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_825F2DB0"))) PPC_WEAK_FUNC(sub_825F2DB0);
PPC_FUNC_IMPL(__imp__sub_825F2DB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r30,1
	r30.s64 = 1;
	// li r28,0
	r28.s64 = 0;
	// mr r29,r30
	r29.u64 = r30.u64;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f2e38
	if (!cr6.lt) goto loc_825F2E38;
loc_825F2DE0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2e38
	if (cr6.eq) goto loc_825F2E38;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f2e28
	if (!cr0.lt) goto loc_825F2E28;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2E28:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f2de0
	if (cr6.gt) goto loc_825F2DE0;
loc_825F2E38:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2e74
	if (!cr0.lt) goto loc_825F2E74;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2E74:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825f2e88
	if (!cr6.eq) goto loc_825F2E88;
	// stw r30,284(r27)
	PPC_STORE_U32(r27.u32 + 284, r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_825F2E88:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// li r28,0
	r28.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f2efc
	if (!cr6.lt) goto loc_825F2EFC;
loc_825F2EA4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2efc
	if (cr6.eq) goto loc_825F2EFC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f2eec
	if (!cr0.lt) goto loc_825F2EEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2EEC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f2ea4
	if (cr6.gt) goto loc_825F2EA4;
loc_825F2EFC:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f2f38
	if (!cr0.lt) goto loc_825F2F38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2F38:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825f2f50
	if (!cr6.eq) goto loc_825F2F50;
	// li r11,2
	r11.s64 = 2;
	// stw r11,284(r27)
	PPC_STORE_U32(r27.u32 + 284, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_825F2F50:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// li r28,0
	r28.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f2fc4
	if (!cr6.lt) goto loc_825F2FC4;
loc_825F2F6C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f2fc4
	if (cr6.eq) goto loc_825F2FC4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f2fb4
	if (!cr0.lt) goto loc_825F2FB4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F2FB4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f2f6c
	if (cr6.gt) goto loc_825F2F6C;
loc_825F2FC4:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f3000
	if (!cr0.lt) goto loc_825F3000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F3000:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825f301c
	if (!cr6.eq) goto loc_825F301C;
	// li r11,0
	r11.s64 = 0;
	// stw r30,21584(r27)
	PPC_STORE_U32(r27.u32 + 21584, r30.u32);
	// stw r11,284(r27)
	PPC_STORE_U32(r27.u32 + 284, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_825F301C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f308c
	if (!cr6.lt) goto loc_825F308C;
loc_825F3034:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f308c
	if (cr6.eq) goto loc_825F308C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f307c
	if (!cr0.lt) goto loc_825F307C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F307C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f3034
	if (cr6.gt) goto loc_825F3034;
loc_825F308C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f30c8
	if (!cr0.lt) goto loc_825F30C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F30C8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// li r11,4
	r11.s64 = 4;
	// beq cr6,0x825f30d8
	if (cr6.eq) goto loc_825F30D8;
	// li r11,5
	r11.s64 = 5;
loc_825F30D8:
	// stw r11,284(r27)
	PPC_STORE_U32(r27.u32 + 284, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825F30E4"))) PPC_WEAK_FUNC(sub_825F30E4);
PPC_FUNC_IMPL(__imp__sub_825F30E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F30E8"))) PPC_WEAK_FUNC(sub_825F30E8);
PPC_FUNC_IMPL(__imp__sub_825F30E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,1828(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1828);
	// lwz r10,1836(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1836);
	// lwz r9,1840(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1840);
	// lwz r8,1864(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1864);
	// lwz r7,1788(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1788);
	// stw r11,1832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1832, r11.u32);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r10,1852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1852, ctx.r10.u32);
	// stw r9,1856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1856, ctx.r9.u32);
	// stw r8,1860(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1860, ctx.r8.u32);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r11,1824(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1824);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,1844(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1844);
	// lwz r8,1848(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1848);
	// lwz r7,1868(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1868);
	// stw r11,1832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1832, r11.u32);
	// stw r9,1796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1796, ctx.r9.u32);
	// stw r10,1852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1852, ctx.r10.u32);
	// stw r8,1856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1856, ctx.r8.u32);
	// stw r7,1860(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1860, ctx.r7.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F3140"))) PPC_WEAK_FUNC(sub_825F3140);
PPC_FUNC_IMPL(__imp__sub_825F3140) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// li r25,2
	r25.s64 = 2;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r11,432(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 432);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825f3220
	if (!cr6.eq) goto loc_825F3220;
	// li r27,1
	r27.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// mr r30,r27
	r30.u64 = r27.u64;
	// bge cr6,0x825f31dc
	if (!cr6.lt) goto loc_825F31DC;
loc_825F3184:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f31dc
	if (cr6.eq) goto loc_825F31DC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f31cc
	if (!cr0.lt) goto loc_825F31CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F31CC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f3184
	if (cr6.gt) goto loc_825F3184;
loc_825F31DC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f3218
	if (!cr0.lt) goto loc_825F3218;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F3218:
	// stw r30,448(r28)
	PPC_STORE_U32(r28.u32 + 448, r30.u32);
	// b 0x825f3304
	goto loc_825F3304;
loc_825F3220:
	// mr r30,r25
	r30.u64 = r25.u64;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f3284
	if (!cr6.lt) goto loc_825F3284;
loc_825F322C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f3284
	if (cr6.eq) goto loc_825F3284;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f3274
	if (!cr0.lt) goto loc_825F3274;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F3274:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f322c
	if (cr6.gt) goto loc_825F322C;
loc_825F3284:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f32c0
	if (!cr0.lt) goto loc_825F32C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F32C0:
	// li r27,1
	r27.s64 = 1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r27,448(r28)
	PPC_STORE_U32(r28.u32 + 448, r27.u32);
	// bne cr6,0x825f32d8
	if (!cr6.eq) goto loc_825F32D8;
	// stw r26,448(r28)
	PPC_STORE_U32(r28.u32 + 448, r26.u32);
	// b 0x825f3304
	goto loc_825F3304;
loc_825F32D8:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825f32e8
	if (!cr6.eq) goto loc_825F32E8;
	// stw r26,3944(r28)
	PPC_STORE_U32(r28.u32 + 3944, r26.u32);
	// b 0x825f32fc
	goto loc_825F32FC;
loc_825F32E8:
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// bne cr6,0x825f32f8
	if (!cr6.eq) goto loc_825F32F8;
	// stw r27,3944(r28)
	PPC_STORE_U32(r28.u32 + 3944, r27.u32);
	// b 0x825f32fc
	goto loc_825F32FC;
loc_825F32F8:
	// stw r25,3944(r28)
	PPC_STORE_U32(r28.u32 + 3944, r25.u32);
loc_825F32FC:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825ee128
	sub_825EE128(ctx, base);
loc_825F3304:
	// lwz r11,440(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 440);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f377c
	if (cr6.eq) goto loc_825F377C;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// lwz r11,248(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 248);
	// mr r29,r26
	r29.u64 = r26.u64;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bgt cr6,0x825f3494
	if (cr6.gt) goto loc_825F3494;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f3390
	if (!cr6.lt) goto loc_825F3390;
loc_825F3338:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f3390
	if (cr6.eq) goto loc_825F3390;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f3380
	if (!cr0.lt) goto loc_825F3380;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F3380:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f3338
	if (cr6.gt) goto loc_825F3338;
loc_825F3390:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f33cc
	if (!cr0.lt) goto loc_825F33CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F33CC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825f3774
	if (cr6.eq) goto loc_825F3774;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f3448
	if (!cr6.lt) goto loc_825F3448;
loc_825F33F0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f3448
	if (cr6.eq) goto loc_825F3448;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f3438
	if (!cr0.lt) goto loc_825F3438;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F3438:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f33f0
	if (cr6.gt) goto loc_825F33F0;
loc_825F3448:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f3484
	if (!cr0.lt) goto loc_825F3484;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F3484:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825f3544
	if (!cr6.eq) goto loc_825F3544;
	// addi r11,r28,2168
	r11.s64 = r28.s64 + 2168;
	// b 0x825f3778
	goto loc_825F3778;
loc_825F3494:
	// cmpwi cr6,r11,20
	cr6.compare<int32_t>(r11.s32, 20, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bgt cr6,0x825f360c
	if (cr6.gt) goto loc_825F360C;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f3500
	if (!cr6.lt) goto loc_825F3500;
loc_825F34A8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f3500
	if (cr6.eq) goto loc_825F3500;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f34f0
	if (!cr0.lt) goto loc_825F34F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F34F0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f34a8
	if (cr6.gt) goto loc_825F34A8;
loc_825F3500:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f353c
	if (!cr0.lt) goto loc_825F353C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F353C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825f354c
	if (!cr6.eq) goto loc_825F354C;
loc_825F3544:
	// addi r11,r28,2156
	r11.s64 = r28.s64 + 2156;
	// b 0x825f3778
	goto loc_825F3778;
loc_825F354C:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f35c0
	if (!cr6.lt) goto loc_825F35C0;
loc_825F3568:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f35c0
	if (cr6.eq) goto loc_825F35C0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f35b0
	if (!cr0.lt) goto loc_825F35B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F35B0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f3568
	if (cr6.gt) goto loc_825F3568;
loc_825F35C0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f35fc
	if (!cr0.lt) goto loc_825F35FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F35FC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825f3774
	if (cr6.eq) goto loc_825F3774;
	// addi r11,r28,2168
	r11.s64 = r28.s64 + 2168;
	// b 0x825f3778
	goto loc_825F3778;
loc_825F360C:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f366c
	if (!cr6.lt) goto loc_825F366C;
loc_825F3614:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f366c
	if (cr6.eq) goto loc_825F366C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f365c
	if (!cr0.lt) goto loc_825F365C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F365C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f3614
	if (cr6.gt) goto loc_825F3614;
loc_825F366C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f36a8
	if (!cr0.lt) goto loc_825F36A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F36A8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825f36b8
	if (!cr6.eq) goto loc_825F36B8;
	// addi r11,r28,2168
	r11.s64 = r28.s64 + 2168;
	// b 0x825f3778
	goto loc_825F3778;
loc_825F36B8:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f372c
	if (!cr6.lt) goto loc_825F372C;
loc_825F36D4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f372c
	if (cr6.eq) goto loc_825F372C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f371c
	if (!cr0.lt) goto loc_825F371C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F371C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f36d4
	if (cr6.gt) goto loc_825F36D4;
loc_825F372C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f3768
	if (!cr0.lt) goto loc_825F3768;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F3768:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r28,2156
	r11.s64 = r28.s64 + 2156;
	// beq cr6,0x825f3778
	if (cr6.eq) goto loc_825F3778;
loc_825F3774:
	// addi r11,r28,2144
	r11.s64 = r28.s64 + 2144;
loc_825F3778:
	// stw r11,2140(r28)
	PPC_STORE_U32(r28.u32 + 2140, r11.u32);
loc_825F377C:
	// lwz r11,3888(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3888);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f383c
	if (cr6.eq) goto loc_825F383C;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f37fc
	if (!cr6.lt) goto loc_825F37FC;
loc_825F37A4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f37fc
	if (cr6.eq) goto loc_825F37FC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f37ec
	if (!cr0.lt) goto loc_825F37EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F37EC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f37a4
	if (cr6.gt) goto loc_825F37A4;
loc_825F37FC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f3838
	if (!cr0.lt) goto loc_825F3838;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F3838:
	// stw r30,452(r28)
	PPC_STORE_U32(r28.u32 + 452, r30.u32);
loc_825F383C:
	// lwz r11,436(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 436);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f3910
	if (cr6.eq) goto loc_825F3910;
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f3874
	if (!cr0.lt) goto loc_825F3874;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F3874:
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// bne cr6,0x825f390c
	if (!cr6.eq) goto loc_825F390C;
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// stw r26,328(r28)
	PPC_STORE_U32(r28.u32 + 328, r26.u32);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f38ac
	if (!cr0.lt) goto loc_825F38AC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F38AC:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825f38c0
	if (!cr6.eq) goto loc_825F38C0;
	// stw r26,336(r28)
	PPC_STORE_U32(r28.u32 + 336, r26.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825F38C0:
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f38ec
	if (!cr0.lt) goto loc_825F38EC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F38EC:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825f3900
	if (!cr6.eq) goto loc_825F3900;
	// stw r27,336(r28)
	PPC_STORE_U32(r28.u32 + 336, r27.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825F3900:
	// stw r25,336(r28)
	PPC_STORE_U32(r28.u32 + 336, r25.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825F390C:
	// stw r27,328(r28)
	PPC_STORE_U32(r28.u32 + 328, r27.u32);
loc_825F3910:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825F3918"))) PPC_WEAK_FUNC(sub_825F3918);
PPC_FUNC_IMPL(__imp__sub_825F3918) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,188(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// lwz r10,180(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// lwz r9,3660(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3660);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// cmpwi cr6,r9,300
	cr6.compare<int32_t>(ctx.r9.s32, 300, xer);
	// bgt cr6,0x825f3968
	if (cr6.gt) goto loc_825F3968;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// ori r10,r10,11264
	ctx.r10.u64 = ctx.r10.u64 | 11264;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825f3968
	if (cr6.gt) goto loc_825F3968;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r10,r10,42240
	ctx.r10.u64 = ctx.r10.u64 | 42240;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825f395c
	if (!cr6.gt) goto loc_825F395C;
	// li r11,4
	r11.s64 = 4;
	// stw r11,15532(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15532, r11.u32);
	// blr 
	return;
loc_825F395C:
	// li r11,1
	r11.s64 = 1;
	// stw r11,15532(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15532, r11.u32);
	// blr 
	return;
loc_825F3968:
	// li r11,0
	r11.s64 = 0;
	// stw r11,15520(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15520, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F3974"))) PPC_WEAK_FUNC(sub_825F3974);
PPC_FUNC_IMPL(__imp__sub_825F3974) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F3978"))) PPC_WEAK_FUNC(sub_825F3978);
PPC_FUNC_IMPL(__imp__sub_825F3978) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,3704(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3704);
	// lwz r10,3700(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3700);
	// stw r11,3700(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3700, r11.u32);
	// stw r10,3704(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3704, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r9,3744(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3744, ctx.r9.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r9,3748(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3748, ctx.r9.u32);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,3752(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3752, r11.u32);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r11,3776(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3776, r11.u32);
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r11,3780(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3780, r11.u32);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r11,3784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3784, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F39C0"))) PPC_WEAK_FUNC(sub_825F39C0);
PPC_FUNC_IMPL(__imp__sub_825F39C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x825f39e4
	if (cr6.eq) goto loc_825F39E4;
	// addi r11,r4,-112
	r11.s64 = ctx.r4.s64 + -112;
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,4760
	ctx.r10.s64 = ctx.r10.s64 + 4760;
	// addi r9,r9,4816
	ctx.r9.s64 = ctx.r9.s64 + 4816;
	// b 0x825f39f8
	goto loc_825F39F8;
loc_825F39E4:
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,4704
	ctx.r10.s64 = ctx.r10.s64 + 4704;
	// addi r9,r9,4732
	ctx.r9.s64 = ctx.r9.s64 + 4732;
loc_825F39F8:
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stw r10,3392(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3392, ctx.r10.u32);
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,3388(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3388, r11.u32);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r11,r11,28552
	r11.s64 = r11.s64 + 28552;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// stw r11,14776(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14776, r11.u32);
	// lwz r11,14772(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// li r11,1
	r11.s64 = 1;
	// stw r11,14772(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14772, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F3A3C"))) PPC_WEAK_FUNC(sub_825F3A3C);
PPC_FUNC_IMPL(__imp__sub_825F3A3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F3A40"))) PPC_WEAK_FUNC(sub_825F3A40);
PPC_FUNC_IMPL(__imp__sub_825F3A40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,3668(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 3668);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825f3a58
	if (!cr6.eq) goto loc_825F3A58;
	// li r3,3
	ctx.r3.s64 = 3;
	// blr 
	return;
loc_825F3A58:
	// li r10,1
	ctx.r10.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,284(r11)
	PPC_STORE_U32(r11.u32 + 284, ctx.r10.u32);
	// stw r10,3384(r11)
	PPC_STORE_U32(r11.u32 + 3384, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F3A6C"))) PPC_WEAK_FUNC(sub_825F3A6C);
PPC_FUNC_IMPL(__imp__sub_825F3A6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F3A70"))) PPC_WEAK_FUNC(sub_825F3A70);
PPC_FUNC_IMPL(__imp__sub_825F3A70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r10,3412(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3412);
	// lwz r11,14892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14892);
	// lwz r29,14896(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 14896);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r6,14884(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 14884);
	// lwz r30,14888(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 14888);
	// bne cr6,0x825f3be8
	if (!cr6.eq) goto loc_825F3BE8;
	// lwz r10,15564(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825f3be8
	if (!cr6.eq) goto loc_825F3BE8;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r9,3708(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// stw r10,3412(r31)
	PPC_STORE_U32(r31.u32 + 3412, ctx.r10.u32);
	// lwz r10,3696(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3696);
	// lwz r10,608(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 608);
	// stw r10,608(r9)
	PPC_STORE_U32(ctx.r9.u32 + 608, ctx.r10.u32);
	// lwz r10,3696(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3696);
	// lwz r9,3708(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// lwz r10,596(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 596);
	// stw r10,596(r9)
	PPC_STORE_U32(ctx.r9.u32 + 596, ctx.r10.u32);
	// lwz r10,3696(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3696);
	// lwz r9,3708(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// lwz r10,600(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 600);
	// stw r10,600(r9)
	PPC_STORE_U32(ctx.r9.u32 + 600, ctx.r10.u32);
	// lwz r10,3696(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3696);
	// lwz r9,3708(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// lwz r10,604(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 604);
	// stw r10,604(r9)
	PPC_STORE_U32(ctx.r9.u32 + 604, ctx.r10.u32);
	// lwz r10,15900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15900);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825f3b60
	if (cr6.eq) goto loc_825F3B60;
	// lwz r10,19712(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19712);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825f3b60
	if (!cr6.eq) goto loc_825F3B60;
	// lwz r10,3708(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// li r8,2
	ctx.r8.s64 = 2;
	// lwz r9,584(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 584);
	// mulli r11,r9,68
	r11.s64 = ctx.r9.s64 * 68;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,584(r10)
	PPC_STORE_U32(ctx.r10.u32 + 584, ctx.r9.u32);
	// stw r8,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r8.u32);
	// lwz r10,3696(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3696);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r10,14884(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14884);
	// stw r10,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r10.u32);
	// lwz r10,14888(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14888);
	// stw r10,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r10.u32);
	// lwz r10,14872(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14872);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r10,14900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14900);
	// stw r10,60(r11)
	PPC_STORE_U32(r11.u32 + 60, ctx.r10.u32);
	// lwz r10,14904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14904);
	// stw r10,64(r11)
	PPC_STORE_U32(r11.u32 + 64, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_825F3B60:
	// lwz r10,21184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21184);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x825f3bb4
	if (!cr6.eq) goto loc_825F3BB4;
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825f3bb4
	if (!cr6.gt) goto loc_825F3BB4;
	// ld r10,3576(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 3576);
	// cmpdi cr6,r10,1
	cr6.compare<int64_t>(ctx.r10.s64, 1, xer);
	// ble cr6,0x825f3bb4
	if (!cr6.gt) goto loc_825F3BB4;
	// lwz r11,19696(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19696);
	// lwz r10,19700(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19700);
	// lwz r6,21484(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 21484);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,21488(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 21488);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,21492(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 21492);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// lwz r9,21496(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21496);
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// add r30,r8,r10
	r30.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r29,r9,r10
	r29.u64 = ctx.r9.u64 + ctx.r10.u64;
loc_825F3BB4:
	// mullw r5,r11,r6
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r6.s32);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// lwz r3,3788(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3788);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mullw r30,r29,r30
	r30.s64 = int64_t(r29.s32) * int64_t(r30.s32);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r3,3792(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3792);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r3,3796(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3796);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_825F3BE8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825F3BF0"))) PPC_WEAK_FUNC(sub_825F3BF0);
PPC_FUNC_IMPL(__imp__sub_825F3BF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// lwz r10,15300(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15300);
	// li r9,-3
	ctx.r9.s64 = -3;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,284(r3)
	PPC_STORE_U32(ctx.r3.u32 + 284, r11.u32);
	// stw r9,3376(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3376, ctx.r9.u32);
	// stw r11,3380(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3380, r11.u32);
	// stw r11,3396(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3396, r11.u32);
	// beq cr6,0x825f3c28
	if (cr6.eq) goto loc_825F3C28;
	// stw r11,14788(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14788, r11.u32);
	// stw r11,3384(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3384, r11.u32);
	// stw r11,3400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3400, r11.u32);
	// stw r11,21432(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21432, r11.u32);
	// blr 
	return;
loc_825F3C28:
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,3384(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3384, r11.u32);
	// stw r11,3400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3400, r11.u32);
	// stw r11,21432(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21432, r11.u32);
	// stw r10,14788(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14788, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F3C40"))) PPC_WEAK_FUNC(sub_825F3C40);
PPC_FUNC_IMPL(__imp__sub_825F3C40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,15472(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15472);
	// li r10,2
	ctx.r10.s64 = 2;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x825f3c58
	if (!cr6.eq) goto loc_825F3C58;
	// lwz r11,3924(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3924);
	// b 0x825f3c70
	goto loc_825F3C70;
loc_825F3C58:
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825f3c7c
	if (!cr6.eq) goto loc_825F3C7C;
	// lwz r11,19976(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f3c78
	if (!cr6.eq) goto loc_825F3C78;
	// lwz r11,19980(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19980);
loc_825F3C70:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f3c7c
	if (cr6.eq) goto loc_825F3C7C;
loc_825F3C78:
	// li r10,3
	ctx.r10.s64 = 3;
loc_825F3C7C:
	// li r11,0
	r11.s64 = 0;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F3C8C"))) PPC_WEAK_FUNC(sub_825F3C8C);
PPC_FUNC_IMPL(__imp__sub_825F3C8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F3C90"))) PPC_WEAK_FUNC(sub_825F3C90);
PPC_FUNC_IMPL(__imp__sub_825F3C90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-448(r1)
	ea = -448 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// li r29,0
	r29.s64 = 0;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// addi r30,r1,180
	r30.s64 = ctx.r1.s64 + 180;
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r28,r1,200
	r28.s64 = ctx.r1.s64 + 200;
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r11.u32);
	// li r16,1
	r16.s64 = 1;
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r4,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 1;
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r17,r29
	r17.u64 = r29.u64;
	// mullw r4,r4,r11
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// lwz r23,3720(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r25,220(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r22,144(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// lwz r21,228(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// lwz r26,3728(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r24,3724(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// stw r29,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r29.u32);
	// lwz r27,268(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// srawi r5,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r9.s32 >> 1;
	// stw r21,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r21.u32);
	// add r4,r4,r23
	ctx.r4.u64 = ctx.r4.u64 + r23.u64;
	// lwz r9,232(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// mullw r5,r5,r11
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// lwz r8,1892(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1892);
	// lwz r7,1896(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1896);
	// lwz r6,136(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// stw r8,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r8.u32);
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// add r4,r4,r25
	ctx.r4.u64 = ctx.r4.u64 + r25.u64;
	// add r25,r10,r5
	r25.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r5,r10,r5
	ctx.r5.u64 = ctx.r10.u64 + ctx.r5.u64;
	// mullw r11,r11,r22
	r11.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// stw r4,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r4.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r29.u32);
	// stw r4,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r4.u32);
	// stw r9,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r9.u32);
	// stw r29,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, r29.u32);
	// add r5,r5,r26
	ctx.r5.u64 = ctx.r5.u64 + r26.u64;
	// rlwinm r26,r11,2,0,29
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r25,r24
	ctx.r10.u64 = r25.u64 + r24.u64;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// addi r3,r1,136
	ctx.r3.s64 = ctx.r1.s64 + 136;
	// addi r4,r1,132
	ctx.r4.s64 = ctx.r1.s64 + 132;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// mr r18,r16
	r18.u64 = r16.u64;
	// stw r10,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r10.u32);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
	// addi r10,r1,124
	ctx.r10.s64 = ctx.r1.s64 + 124;
	// stw r3,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r3.u32);
	// mr r14,r16
	r14.u64 = r16.u64;
	// stw r29,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r29.u32);
	// stw r9,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r9.u32);
	// addi r9,r1,220
	ctx.r9.s64 = ctx.r1.s64 + 220;
	// stw r4,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r4.u32);
	// addi r19,r6,1
	r19.s64 = ctx.r6.s64 + 1;
	// stw r5,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r5.u32);
	// stw r29,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r29.u32);
	// stw r29,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r29.u32);
	// stw r11,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r11.u32);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// li r11,20
	r11.s64 = 20;
	// stw r10,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r10.u32);
	// addi r10,r1,276
	ctx.r10.s64 = ctx.r1.s64 + 276;
	// stw r29,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r29.u32);
	// stw r11,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r11.u32);
	// rlwinm r11,r6,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r29,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r29.u32);
	// addi r9,r1,120
	ctx.r9.s64 = ctx.r1.s64 + 120;
	// stw r9,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r9.u32);
	// lwz r9,19976(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// stw r8,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r8.u32);
	// li r8,192
	ctx.r8.s64 = 192;
	// stw r29,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r29.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r11.u32);
	// stw r7,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r7.u32);
	// stw r29,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, r29.u32);
	// stw r8,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r8.u32);
	// addi r8,r1,116
	ctx.r8.s64 = ctx.r1.s64 + 116;
	// stw r11,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, r11.u32);
	// stw r29,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, r29.u32);
	// stw r29,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r29.u32);
	// stw r29,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r29.u32);
	// stw r8,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r8.u32);
	// li r8,144
	ctx.r8.s64 = 144;
	// stw r8,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, ctx.r8.u32);
	// stw r29,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r29.u32);
	// stw r29,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r29.u32);
	// beq cr6,0x825f3e48
	if (cr6.eq) goto loc_825F3E48;
	// lwz r11,19980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f3e48
	if (cr6.eq) goto loc_825F3E48;
	// lwz r11,21000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21000);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825f3e48
	if (!cr6.eq) goto loc_825F3E48;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r10,21268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x825f3e4c
	goto loc_825F3E4C;
loc_825F3E48:
	// lwz r11,21268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21268);
loc_825F3E4C:
	// stw r11,21264(r31)
	PPC_STORE_U32(r31.u32 + 21264, r11.u32);
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f3ec8
	if (cr6.eq) goto loc_825F3EC8;
	// rlwinm r10,r22,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r29
	r11.u64 = r29.u64;
	// li r9,16384
	ctx.r9.s64 = 16384;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825f3e98
	if (!cr6.gt) goto loc_825F3E98;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_825F3E78:
	// lwz r8,1772(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sthx r9,r10,r8
	PPC_STORE_U16(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lwz r8,144(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// blt cr6,0x825f3e78
	if (cr6.lt) goto loc_825F3E78;
loc_825F3E98:
	// lwz r11,144(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825f3ec8
	if (!cr6.gt) goto loc_825F3EC8;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_825F3EAC:
	// lwz r8,1780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// sthx r9,r11,r8
	PPC_STORE_U16(r11.u32 + ctx.r8.u32, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lwz r8,144(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x825f3eac
	if (cr6.lt) goto loc_825F3EAC;
loc_825F3EC8:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x825f3ef0
	if (cr6.eq) goto loc_825F3EF0;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bge cr6,0x825f3ef0
	if (!cr6.lt) goto loc_825F3EF0;
	// lwz r11,2912(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2912);
	// lwz r10,2924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2924);
	// stw r11,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, r11.u32);
	// stw r10,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, ctx.r10.u32);
	// b 0x825f3f6c
	goto loc_825F3F6C;
loc_825F3EF0:
	// lwz r11,2928(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2928);
	// lwz r8,2940(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 2940);
	// addi r9,r11,726
	ctx.r9.s64 = r11.s64 + 726;
	// lwz r11,2944(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2944);
	// addi r8,r8,729
	ctx.r8.s64 = ctx.r8.s64 + 729;
	// lwz r10,2088(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2088);
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,2948(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 2948);
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,729
	ctx.r9.s64 = ctx.r9.s64 + 729;
	// lwzx r11,r7,r31
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + r31.u32);
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r10,263
	ctx.r10.s64 = ctx.r10.s64 + 263;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// stw r11,2888(r31)
	PPC_STORE_U32(r31.u32 + 2888, r11.u32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r11,2884(r31)
	PPC_STORE_U32(r31.u32 + 2884, r11.u32);
	// stw r11,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, r11.u32);
	// lwzx r11,r8,r31
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// stw r11,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, r11.u32);
	// lwzx r11,r6,r31
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	// stw r11,2896(r31)
	PPC_STORE_U32(r31.u32 + 2896, r11.u32);
	// lwzx r11,r7,r31
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + r31.u32);
	// stw r11,2900(r31)
	PPC_STORE_U32(r31.u32 + 2900, r11.u32);
	// lwz r11,2100(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2100);
	// stw r11,2092(r31)
	PPC_STORE_U32(r31.u32 + 2092, r11.u32);
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(r31.u32 + 2096, r11.u32);
loc_825F3F6C:
	// addi r15,r31,248
	r15.s64 = r31.s64 + 248;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,0(r15)
	ctx.r4.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// bl 0x825ebc08
	sub_825EBC08(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// ble cr6,0x825f4630
	if (!cr6.gt) goto loc_825F4630;
loc_825F3F90:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// clrlwi r10,r3,31
	ctx.r10.u64 = ctx.r3.u32 & 0x1;
	// lwz r24,128(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r23,136(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r22,132(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// subf r11,r3,r11
	r11.s64 = r11.s64 - ctx.r3.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r20,r11,27,31,31
	r20.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bne cr6,0x825f3fcc
	if (!cr6.eq) goto loc_825F3FCC;
	// lwz r11,1892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1892);
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// lwz r11,1896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1896);
	// b 0x825f400c
	goto loc_825F400C;
loc_825F3FCC:
	// lwz r11,14820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14820);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f4010
	if (cr6.eq) goto loc_825F4010;
	// lwz r11,14832(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14832);
	// lwz r8,1892(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1892);
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// lwz r10,1896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1896);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r7,6,0,25
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 6) & 0xFFFFFFC0;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r9.u32);
loc_825F400C:
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
loc_825F4010:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bge cr6,0x825f402c
	if (!cr6.lt) goto loc_825F402C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f402c
	if (cr6.eq) goto loc_825F402C;
	// lwz r4,15468(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15468);
	// bl 0x8263a9e0
	sub_8263A9E0(ctx, base);
loc_825F402C:
	// cntlzw r11,r3
	r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r21,r11,1
	r21.u64 = r11.u64 ^ 1;
	// lwz r11,21236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f4218
	if (cr6.eq) goto loc_825F4218;
	// lwz r11,21000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21000);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825f4068
	if (!cr6.eq) goto loc_825F4068;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825f4068
	if (!cr6.eq) goto loc_825F4068;
	// lwz r11,21272(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r11.u32);
loc_825F4068:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,21264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f41fc
	if (cr6.eq) goto loc_825F41FC;
	// lwz r11,21272(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r11.u32);
	// lwz r11,28(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f4118
	if (cr6.eq) goto loc_825F4118;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r28,r16
	r28.u64 = r16.u64;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f40f0
	if (!cr6.lt) goto loc_825F40F0;
loc_825F40B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f40f0
	if (cr6.eq) goto loc_825F40F0;
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r28,r11,r28
	r28.s64 = r28.s64 - r11.s64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// bge 0x825f40e0
	if (!cr0.lt) goto loc_825F40E0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F40E0:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bgt cr6,0x825f40b0
	if (cr6.gt) goto loc_825F40B0;
loc_825F40F0:
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r10,r28,32
	ctx.r10.u64 = r28.u64 & 0xFFFFFFFF;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// subf. r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r10.u64);
	// bge 0x825f4118
	if (!cr0.lt) goto loc_825F4118;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F4118:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r28,19976(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r27,19984(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r25,284(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// lwz r26,19980(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// bl 0x82639b10
	sub_82639B10(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r16,1944(r31)
	PPC_STORE_U32(r31.u32 + 1944, r16.u32);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825f41c0
	if (cr6.eq) goto loc_825F41C0;
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// stw r25,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r25.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r28,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r28.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r26,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r26.u32);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stw r27,19984(r31)
	PPC_STORE_U32(r31.u32 + 19984, r27.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r18
	ctx.r5.u64 = r18.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826289e8
	sub_826289E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f46cc
	if (!cr6.eq) goto loc_825F46CC;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// beq cr6,0x825f41a0
	if (cr6.eq) goto loc_825F41A0;
	// cmpwi cr6,r17,4
	cr6.compare<int32_t>(r17.s32, 4, xer);
	// bne cr6,0x825f41a4
	if (!cr6.eq) goto loc_825F41A4;
loc_825F41A0:
	// mr r17,r30
	r17.u64 = r30.u64;
loc_825F41A4:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// beq cr6,0x825f4630
	if (cr6.eq) goto loc_825F4630;
	// lwz r11,21272(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// mr r18,r29
	r18.u64 = r29.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r11.u32);
	// b 0x825f4618
	goto loc_825F4618;
loc_825F41C0:
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// bne cr6,0x825f4414
	if (!cr6.eq) goto loc_825F4414;
	// lwz r11,19980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// bne cr6,0x825f4414
	if (!cr6.eq) goto loc_825F4414;
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// bne cr6,0x825f4414
	if (!cr6.eq) goto loc_825F4414;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f41f8
	if (cr6.eq) goto loc_825F41F8;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825f4414
	if (!cr6.eq) goto loc_825F4414;
loc_825F41F8:
	// mr r18,r16
	r18.u64 = r16.u64;
loc_825F41FC:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,21264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f4218
	if (cr6.eq) goto loc_825F4218;
	// mr r14,r16
	r14.u64 = r16.u64;
loc_825F4218:
	// lwz r11,3932(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3932);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f4244
	if (cr6.eq) goto loc_825F4244;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825f4244
	if (cr6.eq) goto loc_825F4244;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// bl 0x82639ef0
	sub_82639EF0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f4744
	if (!cr6.eq) goto loc_825F4744;
loc_825F4244:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r30,r29
	r30.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,128
	r11.s64 = 128;
	// stw r11,2964(r31)
	PPC_STORE_U32(r31.u32 + 2964, r11.u32);
	// stw r11,2960(r31)
	PPC_STORE_U32(r31.u32 + 2960, r11.u32);
	// stw r11,2956(r31)
	PPC_STORE_U32(r31.u32 + 2956, r11.u32);
	// ble cr6,0x825f4560
	if (!cr6.gt) goto loc_825F4560;
loc_825F4264:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f4298
	if (!cr6.eq) goto loc_825F4298;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825fa740
	sub_825FA740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825f4298
	if (cr6.eq) goto loc_825F4298;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825fa7d0
	sub_825FA7D0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f4744
	if (!cr6.eq) goto loc_825F4744;
	// mr r19,r29
	r19.u64 = r29.u64;
loc_825F4298:
	// lwz r11,3072(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3072);
	// addi r19,r19,1
	r19.s64 = r19.s64 + 1;
	// lwz r6,112(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,124(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x825f4518
	if (!cr6.eq) goto loc_825F4518;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825f439c
	if (cr6.eq) goto loc_825F439C;
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,0,20,20
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x800;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825f439c
	if (!cr6.eq) goto loc_825F439C;
	// rlwinm r9,r11,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r8,1780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r9,2968(r31)
	PPC_STORE_U32(r31.u32 + 2968, ctx.r9.u32);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r29,r11,r8
	PPC_STORE_U16(r11.u32 + ctx.r8.u32, r29.u16);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sth r29,2(r11)
	PPC_STORE_U16(r11.u32 + 2, r29.u16);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,1772(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// sthx r29,r11,r9
	PPC_STORE_U16(r11.u32 + ctx.r9.u32, r29.u16);
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sth r29,2(r11)
	PPC_STORE_U16(r11.u32 + 2, r29.u16);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// sthx r29,r11,r10
	PPC_STORE_U16(r11.u32 + ctx.r10.u32, r29.u16);
loc_825F439C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825f43b0
	if (cr6.eq) goto loc_825F43B0;
	// cmplwi cr6,r19,1
	cr6.compare<uint32_t>(r19.u32, 1, xer);
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
	// bgt cr6,0x825f43b4
	if (cr6.gt) goto loc_825F43B4;
loc_825F43B0:
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_825F43B4:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x825f43cc
	if (cr6.eq) goto loc_825F43CC;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// cmplw cr6,r19,r11
	cr6.compare<uint32_t>(r19.u32, r11.u32, xer);
	// bgt cr6,0x825f43d0
	if (cr6.gt) goto loc_825F43D0;
loc_825F43CC:
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
loc_825F43D0:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825f43f4
	if (cr6.eq) goto loc_825F43F4;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x825f43f4
	if (cr6.eq) goto loc_825F43F4;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r19,r11
	cr6.compare<uint32_t>(r19.u32, r11.u32, xer);
	// bgt cr6,0x825f43f8
	if (cr6.gt) goto loc_825F43F8;
loc_825F43F4:
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
loc_825F43F8:
	// lwz r11,20056(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20056);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f4478
	if (cr6.eq) goto loc_825F4478;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// rlwinm r11,r11,16,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// b 0x825f447c
	goto loc_825F447C;
loc_825F4414:
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// stw r25,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r25.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r28,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r28.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r26,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r26.u32);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stw r27,19984(r31)
	PPC_STORE_U32(r31.u32 + 19984, r27.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r18
	ctx.r5.u64 = r18.u64;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826289e8
	sub_826289E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f46d8
	if (!cr6.eq) goto loc_825F46D8;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// bne cr6,0x825f445c
	if (!cr6.eq) goto loc_825F445C;
	// li r17,4
	r17.s64 = 4;
loc_825F445C:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// beq cr6,0x825f4630
	if (cr6.eq) goto loc_825F4630;
	// lwz r11,21272(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// mr r18,r29
	r18.u64 = r29.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r11.u32);
	// b 0x825f4618
	goto loc_825F4618;
loc_825F4478:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_825F447C:
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// lwz r28,3068(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3068);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// lwz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r8,120(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r4,124(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// mtctr r28
	ctr.u64 = r28.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f44cc
	if (cr6.eq) goto loc_825F44CC;
	// li r11,7
	r11.s64 = 7;
	// stw r11,2968(r31)
	PPC_STORE_U32(r31.u32 + 2968, r11.u32);
loc_825F44CC:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x825f4520
	if (!cr6.eq) goto loc_825F4520;
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r24,r24,16
	r24.s64 = r24.s64 + 16;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// addi r23,r23,8
	r23.s64 = r23.s64 + 8;
	// addi r22,r22,8
	r22.s64 = r22.s64 + 8;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r10,r10,192
	ctx.r10.s64 = ctx.r10.s64 + 192;
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r10,r10,144
	ctx.r10.s64 = ctx.r10.s64 + 144;
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// blt cr6,0x825f4264
	if (cr6.lt) goto loc_825F4264;
	// b 0x825f4560
	goto loc_825F4560;
loc_825F4518:
	// li r5,-1
	ctx.r5.s64 = -1;
	// b 0x825f4524
	goto loc_825F4524;
loc_825F4520:
	// li r5,-2
	ctx.r5.s64 = -2;
loc_825F4524:
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826289e8
	sub_826289E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f46e4
	if (!cr6.eq) goto loc_825F46E4;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// beq cr6,0x825f455c
	if (cr6.eq) goto loc_825F455C;
	// cmpwi cr6,r17,4
	cr6.compare<int32_t>(r17.s32, 4, xer);
	// bne cr6,0x825f4560
	if (!cr6.eq) goto loc_825F4560;
loc_825F455C:
	// mr r17,r28
	r17.u64 = r28.u64;
loc_825F4560:
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f4590
	if (cr6.eq) goto loc_825F4590;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r7,132(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// mr r8,r14
	ctx.r8.u64 = r14.u64;
	// lwz r6,136(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,128(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// bl 0x82653338
	sub_82653338(ctx, base);
loc_825F4590:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r14,r29
	r14.u64 = r29.u64;
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// bge cr6,0x825f45c4
	if (!cr6.lt) goto loc_825F45C4;
	// addi r11,r4,1
	r11.s64 = ctx.r4.s64 + 1;
	// lwz r10,21264(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f45c4
	if (cr6.eq) goto loc_825F45C4;
	// mr r20,r16
	r20.u64 = r16.u64;
loc_825F45C4:
	// lwz r10,228(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// lwz r9,128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// add r5,r10,r9
	ctx.r5.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// add r6,r11,r10
	ctx.r6.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// add r7,r11,r10
	ctx.r7.u64 = r11.u64 + ctx.r10.u64;
	// stw r5,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r5.u32);
	// stw r6,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r6.u32);
	// stw r7,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r7.u32);
	// beq cr6,0x825f4618
	if (cr6.eq) goto loc_825F4618;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f4618
	if (cr6.eq) goto loc_825F4618;
	// li r9,1
	ctx.r9.s64 = 1;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82653338
	sub_82653338(ctx, base);
loc_825F4618:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r3,r10,1
	ctx.r3.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// blt cr6,0x825f3f90
	if (cr6.lt) goto loc_825F3F90;
loc_825F4630:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// lwz r11,3892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3892);
	// blt cr6,0x825f46f0
	if (cr6.lt) goto loc_825F46F0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f473c
	if (cr6.eq) goto loc_825F473C;
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// li r8,0
	ctx.r8.s64 = 0;
	// srawi r5,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r10.s32 >> 1;
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r28,3720(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// li r7,0
	ctx.r7.s64 = 0;
	// srawi r26,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r26.s64 = ctx.r10.s32 >> 1;
	// lwz r4,220(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// mullw r5,r5,r11
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// lwz r6,224(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r25,0(r15)
	r25.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// lwz r24,140(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r27,3728(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r30,3724(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r25.u32);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// stw r16,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r16.u32);
	// mullw r11,r26,r11
	r11.s64 = int64_t(r26.s32) * int64_t(r11.s32);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// add r28,r6,r5
	r28.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// add r11,r6,r5
	r11.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r6,r28,r27
	ctx.r6.u64 = r28.u64 + r27.u64;
	// add r5,r11,r30
	ctx.r5.u64 = r11.u64 + r30.u64;
	// bl 0x8261b680
	sub_8261B680(ctx, base);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// stw r29,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r29.u32);
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x8239bd10
	return;
loc_825F46CC:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x8239bd10
	return;
loc_825F46D8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x8239bd10
	return;
loc_825F46E4:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x8239bd10
	return;
loc_825F46F0:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f473c
	if (cr6.eq) goto loc_825F473C;
	// lwz r28,140(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// lwz r30,220(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// stw r16,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r16.u32);
	// bl 0x826172a8
	sub_826172A8(ctx, base);
loc_825F473C:
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// stw r29,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r29.u32);
loc_825F4744:
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825F474C"))) PPC_WEAK_FUNC(sub_825F474C);
PPC_FUNC_IMPL(__imp__sub_825F474C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F4750"))) PPC_WEAK_FUNC(sub_825F4750);
PPC_FUNC_IMPL(__imp__sub_825F4750) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r30,3720(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r5,3732(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// add r26,r9,r30
	r26.u64 = ctx.r9.u64 + r30.u64;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// lwz r6,3736(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r4,3728(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r7,3740(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// stw r9,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r9.u32);
	// add r9,r6,r11
	ctx.r9.u64 = ctx.r6.u64 + r11.u64;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// lwz r8,15472(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// lwz r28,268(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// addi r15,r10,1
	r15.s64 = ctx.r10.s64 + 1;
	// lwz r27,264(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 264);
	// cmpwi cr6,r8,3
	cr6.compare<int32_t>(ctx.r8.s32, 3, xer);
	// lwz r21,1892(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 1892);
	// lwz r20,1896(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 1896);
	// stw r26,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r26.u32);
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// stw r4,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r4.u32);
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// stw r10,340(r31)
	PPC_STORE_U32(r31.u32 + 340, ctx.r10.u32);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// beq cr6,0x825f47f4
	if (cr6.eq) goto loc_825F47F4;
	// cmpwi cr6,r8,4
	cr6.compare<int32_t>(ctx.r8.s32, 4, xer);
	// bge cr6,0x825f47f4
	if (!cr6.lt) goto loc_825F47F4;
	// lwz r11,2912(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2912);
	// lwz r10,2924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2924);
	// stw r11,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, r11.u32);
	// stw r10,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, ctx.r10.u32);
	// b 0x825f4888
	goto loc_825F4888;
loc_825F47F4:
	// lwz r10,2928(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2928);
	// lwz r9,2088(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 2088);
	// addi r8,r10,726
	ctx.r8.s64 = ctx.r10.s64 + 726;
	// lwz r11,2036(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2036);
	// addi r7,r10,729
	ctx.r7.s64 = ctx.r10.s64 + 729;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r9,r9,263
	ctx.r9.s64 = ctx.r9.s64 + 263;
	// addi r5,r11,503
	ctx.r5.s64 = r11.s64 + 503;
	// rlwinm r6,r9,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r11,253
	r11.s64 = r11.s64 + 253;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// stw r11,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, r11.u32);
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r11,2884(r31)
	PPC_STORE_U32(r31.u32 + 2884, r11.u32);
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r11,2888(r31)
	PPC_STORE_U32(r31.u32 + 2888, r11.u32);
	// lwzx r11,r7,r31
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + r31.u32);
	// stw r11,2900(r31)
	PPC_STORE_U32(r31.u32 + 2900, r11.u32);
	// stw r11,2896(r31)
	PPC_STORE_U32(r31.u32 + 2896, r11.u32);
	// stw r11,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, r11.u32);
	// lwz r11,2100(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 2100);
	// stw r11,2092(r31)
	PPC_STORE_U32(r31.u32 + 2092, r11.u32);
	// lwzx r11,r6,r31
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(r31.u32 + 2096, r11.u32);
	// lwzx r11,r5,r31
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + r31.u32);
	// stw r11,1984(r31)
	PPC_STORE_U32(r31.u32 + 1984, r11.u32);
	// lwz r11,2020(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2020);
	// stw r11,1976(r31)
	PPC_STORE_U32(r31.u32 + 1976, r11.u32);
	// lwzx r11,r4,r31
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + r31.u32);
	// stw r11,1980(r31)
	PPC_STORE_U32(r31.u32 + 1980, r11.u32);
loc_825F4888:
	// addi r11,r31,248
	r11.s64 = r31.s64 + 248;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x825ebc08
	sub_825EBC08(ctx, base);
	// lwz r10,312(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 312);
	// li r22,0
	r22.s64 = 0;
	// lwz r11,316(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 316);
	// mr r30,r22
	r30.u64 = r22.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r18,r22
	r18.u64 = r22.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// neg r10,r11
	ctx.r10.s64 = -r11.s64;
	// stw r30,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r30.u32);
	// stw r11,304(r31)
	PPC_STORE_U32(r31.u32 + 304, r11.u32);
	// stw r10,308(r31)
	PPC_STORE_U32(r31.u32 + 308, ctx.r10.u32);
	// ble cr6,0x825f4fa0
	if (!cr6.gt) goto loc_825F4FA0;
	// mr r17,r22
	r17.u64 = r22.u64;
loc_825F48D0:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// lwz r4,15468(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15468);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r14,r11,1
	r14.u64 = r11.u64 ^ 1;
	// stw r14,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r14.u32);
	// bl 0x8263a9e0
	sub_8263A9E0(ctx, base);
	// cntlzw r9,r3
	ctx.r9.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// lwz r11,340(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 340);
	// clrlwi r10,r30,31
	ctx.r10.u64 = r30.u32 & 0x1;
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// xori r16,r9,1
	r16.u64 = ctx.r9.u64 ^ 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,340(r31)
	PPC_STORE_U32(r31.u32 + 340, r11.u32);
	// stw r16,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r16.u32);
	// bne cr6,0x825f4920
	if (!cr6.eq) goto loc_825F4920;
	// lwz r21,1892(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 1892);
	// lwz r20,1896(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 1896);
	// lwz r27,264(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 264);
loc_825F4920:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f493c
	if (!cr6.eq) goto loc_825F493C;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825f493c
	if (cr6.eq) goto loc_825F493C;
	// li r16,1
	r16.s64 = 1;
	// stw r16,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r16.u32);
loc_825F493C:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r23,r22
	r23.u64 = r22.u64;
	// mr r24,r22
	r24.u64 = r22.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825f4f38
	if (!cr6.gt) goto loc_825F4F38;
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mr r19,r22
	r19.u64 = r22.u64;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r30,r28,13
	r30.s64 = r28.s64 + 13;
	// lwz r29,116(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// stw r10,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r10.u32);
	// stw r11,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r11.u32);
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// subf r25,r29,r11
	r25.s64 = r11.s64 - r29.s64;
loc_825F497C:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f49b0
	if (!cr6.eq) goto loc_825F49B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825fa740
	sub_825FA740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825f49b0
	if (cr6.eq) goto loc_825F49B0;
	// addi r4,r31,248
	ctx.r4.s64 = r31.s64 + 248;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825fa7d0
	sub_825FA7D0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f4ff8
	if (!cr6.eq) goto loc_825F4FF8;
	// mr r15,r22
	r15.u64 = r22.u64;
loc_825F49B0:
	// addi r15,r15,1
	r15.s64 = r15.s64 + 1;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r15,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r15.u32);
	// bl 0x8263a938
	sub_8263A938(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,3076(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3076);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,324(r31)
	PPC_STORE_U32(r31.u32 + 324, r11.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f4ff8
	if (!cr6.eq) goto loc_825F4FF8;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// rlwinm r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825f4b60
	if (!cr6.eq) goto loc_825F4B60;
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x825f4c18
	if (!cr6.eq) goto loc_825F4C18;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bge cr6,0x825f4acc
	if (!cr6.lt) goto loc_825F4ACC;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x825f4acc
	if (cr6.eq) goto loc_825F4ACC;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825f4a98
	if (cr6.eq) goto loc_825F4A98;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825f4a98
	if (cr6.eq) goto loc_825F4A98;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f4b14
	if (!cr6.eq) goto loc_825F4B14;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// beq cr6,0x825f4a58
	if (cr6.eq) goto loc_825F4A58;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r15,r11
	cr6.compare<uint32_t>(r15.u32, r11.u32, xer);
	// bge cr6,0x825f4a5c
	if (!cr6.lt) goto loc_825F4A5C;
loc_825F4A58:
	// li r8,1
	ctx.r8.s64 = 1;
loc_825F4A5C:
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825f4a70
	if (cr6.eq) goto loc_825F4A70;
	// cmplwi cr6,r15,1
	cr6.compare<uint32_t>(r15.u32, 1, xer);
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// bne cr6,0x825f4a74
	if (!cr6.eq) goto loc_825F4A74;
loc_825F4A70:
	// li r6,1
	ctx.r6.s64 = 1;
loc_825F4A74:
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bl 0x825fe130
	sub_825FE130(ctx, base);
	// b 0x825f4b0c
	goto loc_825F4B0C;
loc_825F4A98:
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// cntlzw r10,r16
	ctx.r10.u64 = r16.u32 == 0 ? 32 : __builtin_clz(r16.u32);
	// cntlzw r9,r24
	ctx.r9.u64 = r24.u32 == 0 ? 32 : __builtin_clz(r24.u32);
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
	// rlwinm r8,r10,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r6,r9,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825fe130
	sub_825FE130(ctx, base);
	// b 0x825f4b0c
	goto loc_825F4B0C;
loc_825F4ACC:
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// cntlzw r7,r24
	ctx.r7.u64 = r24.u32 == 0 ? 32 : __builtin_clz(r24.u32);
	// lwz r5,1980(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1980);
	// cntlzw r9,r16
	ctx.r9.u64 = r16.u32 == 0 ? 32 : __builtin_clz(r16.u32);
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
	// lwz r10,1976(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1976);
	// rlwinm r6,r7,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// lwz r9,1984(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1984);
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8263a4d0
	sub_8263A4D0(ctx, base);
loc_825F4B0C:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f4ff8
	if (!cr6.eq) goto loc_825F4FF8;
loc_825F4B14:
	// lbz r11,0(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825f4b30
	if (!cr6.eq) goto loc_825F4B30;
	// lbz r11,1(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 1);
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f4b34
	if (cr6.eq) goto loc_825F4B34;
loc_825F4B30:
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
loc_825F4B34:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// rlwimi r11,r10,29,2,2
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 29) & 0x20000000) | (r11.u64 & 0xFFFFFFFFDFFFFFFF);
	// rlwinm r10,r11,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// beq cr6,0x825f4ba4
	if (cr6.eq) goto loc_825F4BA4;
	// rlwinm r10,r11,0,2,2
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825f4ba4
	if (cr6.eq) goto loc_825F4BA4;
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
loc_825F4B60:
	// lwz r11,152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// add r5,r25,r29
	ctx.r5.u64 = r25.u64 + r29.u64;
	// lwz r8,156(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// add r7,r11,r29
	ctx.r7.u64 = r11.u64 + r29.u64;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stb r22,1(r27)
	PPC_STORE_U8(r27.u32 + 1, r22.u8);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stb r22,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r22.u8);
	// add r6,r23,r11
	ctx.r6.u64 = r23.u64 + r11.u64;
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lwz r9,204(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r11,3120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3120);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x825f4edc
	goto loc_825F4EDC;
loc_825F4BA4:
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// lbz r11,0(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// lbz r10,1(r27)
	ctx.r10.u64 = PPC_LOAD_U8(r27.u32 + 1);
	// add r7,r25,r29
	ctx.r7.u64 = r25.u64 + r29.u64;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// lwz r14,3060(r31)
	r14.u64 = PPC_LOAD_U32(r31.u32 + 3060);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// lwz r9,5624(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 5624);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lbzx r8,r11,r9
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// stb r8,112(r1)
	PPC_STORE_U8(ctx.r1.u32 + 112, ctx.r8.u8);
	// add r8,r11,r19
	ctx.r8.u64 = r11.u64 + r19.u64;
	// lbzx r11,r10,r9
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// add r9,r10,r17
	ctx.r9.u64 = ctx.r10.u64 + r17.u64;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// add r11,r11,r18
	r11.u64 = r11.u64 + r18.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lbz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 112);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// mtctr r14
	ctr.u64 = r14.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f4ff8
	if (!cr6.eq) goto loc_825F4FF8;
	// lwz r14,140(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// b 0x825f4edc
	goto loc_825F4EDC;
loc_825F4C18:
	// stb r22,1(r27)
	PPC_STORE_U8(r27.u32 + 1, r22.u8);
	// stb r22,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r22.u8);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x825f4dc0
	if (cr6.lt) goto loc_825F4DC0;
	// cntlzw r11,r24
	r11.u64 = r24.u32 == 0 ? 32 : __builtin_clz(r24.u32);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r10,r11,1
	ctx.r10.u64 = r11.u64 ^ 1;
	// beq cr6,0x825f4c4c
	if (cr6.eq) goto loc_825F4C4C;
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x825f4c50
	if (!cr6.eq) goto loc_825F4C50;
loc_825F4C4C:
	// mr r11,r22
	r11.u64 = r22.u64;
loc_825F4C50:
	// lwz r9,400(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 400);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825f4ccc
	if (cr6.eq) goto loc_825F4CCC;
	// lbz r16,4(r30)
	r16.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// lbz r11,-1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + -1);
	// add r7,r25,r29
	ctx.r7.u64 = r25.u64 + r29.u64;
	// stw r14,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r14.u32);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// lbz r15,3(r30)
	r15.u64 = PPC_LOAD_U8(r30.u32 + 3);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// stb r16,-1(r30)
	PPC_STORE_U8(r30.u32 + -1, r16.u8);
	// mr r16,r11
	r16.u64 = r11.u64;
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lbz r14,2(r30)
	r14.u64 = PPC_LOAD_U8(r30.u32 + 2);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r15,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r15.u8);
	// stb r16,4(r30)
	PPC_STORE_U8(r30.u32 + 4, r16.u8);
	// mr r16,r11
	r16.u64 = r11.u64;
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// stb r14,1(r30)
	PPC_STORE_U8(r30.u32 + 1, r14.u8);
	// stb r16,3(r30)
	PPC_STORE_U8(r30.u32 + 3, r16.u8);
	// stb r11,2(r30)
	PPC_STORE_U8(r30.u32 + 2, r11.u8);
	// bl 0x8264cd78
	sub_8264CD78(ctx, base);
	// lwz r16,136(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r14,140(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r15,160(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// b 0x825f4ed4
	goto loc_825F4ED4;
loc_825F4CCC:
	// lbz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lbz r11,-1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + -1);
	// lbz r8,3(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 3);
	// lbz r7,2(r30)
	ctx.r7.u64 = PPC_LOAD_U8(r30.u32 + 2);
	// stb r9,-1(r30)
	PPC_STORE_U8(r30.u32 + -1, ctx.r9.u8);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// stb r8,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r8.u8);
	// stb r9,4(r30)
	PPC_STORE_U8(r30.u32 + 4, ctx.r9.u8);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// stb r7,1(r30)
	PPC_STORE_U8(r30.u32 + 1, ctx.r7.u8);
	// stb r9,3(r30)
	PPC_STORE_U8(r30.u32 + 3, ctx.r9.u8);
	// stb r11,2(r30)
	PPC_STORE_U8(r30.u32 + 2, r11.u8);
	// beq cr6,0x825f4d38
	if (cr6.eq) goto loc_825F4D38;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f4d3c
	if (cr6.eq) goto loc_825F4D3C;
loc_825F4D38:
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
loc_825F4D3C:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// beq cr6,0x825f4d6c
	if (cr6.eq) goto loc_825F4D6C;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// beq cr6,0x825f4d70
	if (cr6.eq) goto loc_825F4D70;
loc_825F4D6C:
	// mr r11,r22
	r11.u64 = r22.u64;
loc_825F4D70:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825f4d8c
	if (cr6.eq) goto loc_825F4D8C;
	// lwz r10,-33(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + -33);
	// rlwinm r10,r10,0,14,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// beq cr6,0x825f4d90
	if (cr6.eq) goto loc_825F4D90;
loc_825F4D8C:
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
loc_825F4D90:
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// add r7,r25,r29
	ctx.r7.u64 = r25.u64 + r29.u64;
	// stw r22,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r22.u32);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8264baa8
	sub_8264BAA8(ctx, base);
	// b 0x825f4ed4
	goto loc_825F4ED4;
loc_825F4DC0:
	// lbz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lbz r11,-1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + -1);
	// lbz r9,3(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 3);
	// lbz r8,2(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 2);
	// stb r10,-1(r30)
	PPC_STORE_U8(r30.u32 + -1, ctx.r10.u8);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// stb r9,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r9.u8);
	// stb r10,4(r30)
	PPC_STORE_U8(r30.u32 + 4, ctx.r10.u8);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// stb r8,1(r30)
	PPC_STORE_U8(r30.u32 + 1, ctx.r8.u8);
	// stb r10,3(r30)
	PPC_STORE_U8(r30.u32 + 3, ctx.r10.u8);
	// stb r11,2(r30)
	PPC_STORE_U8(r30.u32 + 2, r11.u8);
	// beq cr6,0x825f4e1c
	if (cr6.eq) goto loc_825F4E1C;
	// lwz r11,-33(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + -33);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825f4e1c
	if (!cr6.eq) goto loc_825F4E1C;
	// cmplwi cr6,r15,1
	cr6.compare<uint32_t>(r15.u32, 1, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x825f4e20
	if (cr6.gt) goto loc_825F4E20;
loc_825F4E1C:
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
loc_825F4E20:
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// beq cr6,0x825f4e58
	if (cr6.eq) goto loc_825F4E58;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r28
	ctx.r9.s64 = r28.s64 - ctx.r9.s64;
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r9,r9,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825f4e58
	if (!cr6.eq) goto loc_825F4E58;
	// cmplw cr6,r15,r11
	cr6.compare<uint32_t>(r15.u32, r11.u32, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// bgt cr6,0x825f4e5c
	if (cr6.gt) goto loc_825F4E5C;
loc_825F4E58:
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
loc_825F4E5C:
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825f4ea4
	if (cr6.eq) goto loc_825F4EA4;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// beq cr6,0x825f4ea4
	if (cr6.eq) goto loc_825F4EA4;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825f4ea4
	if (!cr6.eq) goto loc_825F4EA4;
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r15,r11
	cr6.compare<uint32_t>(r15.u32, r11.u32, xer);
	// li r11,1
	r11.s64 = 1;
	// bgt cr6,0x825f4ea8
	if (cr6.gt) goto loc_825F4EA8;
loc_825F4EA4:
	// mr r11,r22
	r11.u64 = r22.u64;
loc_825F4EA8:
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// add r7,r25,r29
	ctx.r7.u64 = r25.u64 + r29.u64;
	// stw r22,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r22.u32);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825fdc40
	sub_825FDC40(ctx, base);
loc_825F4ED4:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f4ff8
	if (!cr6.eq) goto loc_825F4FF8;
loc_825F4EDC:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x825f4efc
	if (!cr6.eq) goto loc_825F4EFC;
	// sth r22,160(r21)
	PPC_STORE_U16(r21.u32 + 160, r22.u16);
	// sth r22,128(r21)
	PPC_STORE_U16(r21.u32 + 128, r22.u16);
	// sth r22,0(r21)
	PPC_STORE_U16(r21.u32 + 0, r22.u16);
loc_825F4EFC:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r28,r28,20
	r28.s64 = r28.s64 + 20;
	// addi r30,r30,20
	r30.s64 = r30.s64 + 20;
	// addi r27,r27,2
	r27.s64 = r27.s64 + 2;
	// addi r26,r26,16
	r26.s64 = r26.s64 + 16;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// addi r21,r21,192
	r21.s64 = r21.s64 + 192;
	// addi r20,r20,144
	r20.s64 = r20.s64 + 144;
	// addi r23,r23,16
	r23.s64 = r23.s64 + 16;
	// addi r19,r19,32
	r19.s64 = r19.s64 + 32;
	// cmplw cr6,r24,r11
	cr6.compare<uint32_t>(r24.u32, r11.u32, xer);
	// blt cr6,0x825f497c
	if (cr6.lt) goto loc_825F497C;
	// lwz r30,144(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r26,148(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
loc_825F4F38:
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r18,r18,16
	r18.s64 = r18.s64 + 16;
	// lwz r10,228(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// addi r17,r17,32
	r17.s64 = r17.s64 + 32;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// lwz r9,140(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// add r26,r10,r26
	r26.u64 = ctx.r10.u64 + r26.u64;
	// stw r30,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r30.u32);
	// cmplw cr6,r30,r9
	cr6.compare<uint32_t>(r30.u32, ctx.r9.u32, xer);
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// lwz r8,120(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r26,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r26.u32);
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// stw r8,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r8.u32);
	// lwz r8,124(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// lwz r10,128(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// blt cr6,0x825f48d0
	if (cr6.lt) goto loc_825F48D0;
loc_825F4FA0:
	// lwz r11,3892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3892);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f4ff4
	if (cr6.eq) goto loc_825F4FF4;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r29,140(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r4,220(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// lwz r30,3720(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r31,1
	r31.s64 = 1;
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// bl 0x826172a8
	sub_826172A8(ctx, base);
loc_825F4FF4:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825F4FF8:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825F5000"))) PPC_WEAK_FUNC(sub_825F5000);
PPC_FUNC_IMPL(__imp__sub_825F5000) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,3664(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3664);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f50c4
	if (!cr6.eq) goto loc_825F50C4;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// blt cr6,0x825f505c
	if (cr6.lt) goto loc_825F505C;
	// bl 0x825f3918
	sub_825F3918(ctx, base);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x825f5098
	if (!cr6.eq) goto loc_825F5098;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x825f5080
	if (!cr6.eq) goto loc_825F5080;
loc_825F5044:
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825F505C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f5090
	if (cr6.eq) goto loc_825F5090;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825f5074
	if (!cr6.eq) goto loc_825F5074;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f3918
	sub_825F3918(ctx, base);
loc_825F5074:
	// cmpwi cr6,r4,22
	cr6.compare<int32_t>(ctx.r4.s32, 22, xer);
	// ble cr6,0x825f5044
	if (!cr6.gt) goto loc_825F5044;
	// addi r4,r4,-22
	ctx.r4.s64 = ctx.r4.s64 + -22;
loc_825F5080:
	// lwz r3,140(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// bl 0x8263a9f8
	sub_8263A9F8(ctx, base);
	// stw r3,15468(r31)
	PPC_STORE_U32(r31.u32 + 15468, ctx.r3.u32);
	// b 0x825f5098
	goto loc_825F5098;
loc_825F5090:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r11,15468(r31)
	PPC_STORE_U32(r31.u32 + 15468, r11.u32);
loc_825F5098:
	// lwz r11,15508(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15508);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f50b4
	if (!cr6.eq) goto loc_825F50B4;
	// lwz r11,152(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 152);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x825f50b8
	if (!cr6.eq) goto loc_825F50B8;
loc_825F50B4:
	// li r11,0
	r11.s64 = 0;
loc_825F50B8:
	// stw r11,15476(r31)
	PPC_STORE_U32(r31.u32 + 15476, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// stw r11,3664(r31)
	PPC_STORE_U32(r31.u32 + 3664, r11.u32);
loc_825F50C4:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F50DC"))) PPC_WEAK_FUNC(sub_825F50DC);
PPC_FUNC_IMPL(__imp__sub_825F50DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F50E0"))) PPC_WEAK_FUNC(sub_825F50E0);
PPC_FUNC_IMPL(__imp__sub_825F50E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,3696(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3696);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lwz r11,3688(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3688);
	// lwz r8,224(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// lwz r9,220(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// stw r10,3688(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3688, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r11,3696(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3696, r11.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r7,3720(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3720, ctx.r7.u32);
	// rotlwi r7,r7,0
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// lwz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// stw r6,3724(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3724, ctx.r6.u32);
	// lwz r6,8(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r10,3724(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3724);
	// add r5,r10,r8
	ctx.r5.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r6,3728(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3728, ctx.r6.u32);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,3728(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3728);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rotlwi r10,r6,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// stw r6,3732(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3732, ctx.r6.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r6,3736(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3736, ctx.r6.u32);
	// rotlwi r6,r6,0
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r7,3800(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3800, ctx.r7.u32);
	// stw r5,3804(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3804, ctx.r5.u32);
	// stw r8,3808(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3808, ctx.r8.u32);
	// stw r6,14764(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14764, ctx.r6.u32);
	// stw r11,3740(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3740, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r10,14760(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14760, ctx.r10.u32);
	// stw r11,14768(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14768, r11.u32);
	// add r11,r9,r10
	r11.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r11,3756(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3756, r11.u32);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r10,3708(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3708);
	// lwz r11,3704(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3704);
	// stw r10,3704(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3704, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r11,3708(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3708, r11.u32);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r9,3776(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3776, ctx.r9.u32);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r9,3780(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3780, ctx.r9.u32);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r10,3784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3784, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,3788(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3788, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,3792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3792, ctx.r10.u32);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,3796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3796, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F51C0"))) PPC_WEAK_FUNC(sub_825F51C0);
PPC_FUNC_IMPL(__imp__sub_825F51C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// li r25,1
	r25.s64 = 1;
	// mr r30,r26
	r30.u64 = r26.u64;
	// li r24,2
	r24.s64 = 2;
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// stw r26,3964(r27)
	PPC_STORE_U32(r27.u32 + 3964, r26.u32);
	// stw r26,20028(r27)
	PPC_STORE_U32(r27.u32 + 20028, r26.u32);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// stw r26,20024(r27)
	PPC_STORE_U32(r27.u32 + 20024, r26.u32);
	// bne cr6,0x825f5580
	if (!cr6.eq) goto loc_825F5580;
	// lwz r11,19980(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f5580
	if (!cr6.eq) goto loc_825F5580;
	// lwz r11,3924(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f553c
	if (cr6.eq) goto loc_825F553C;
	// stw r25,3960(r27)
	PPC_STORE_U32(r27.u32 + 3960, r25.u32);
loc_825F5214:
	// lwz r11,3964(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3964);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f5c50
	if (cr6.eq) goto loc_825F5C50;
	// lwz r11,19980(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f5ae8
	if (cr6.eq) goto loc_825F5AE8;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f5258
	if (!cr0.lt) goto loc_825F5258;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5258:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825f57e0
	if (cr6.eq) goto loc_825F57E0;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r25,20028(r27)
	PPC_STORE_U32(r27.u32 + 20028, r25.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// stw r25,20024(r27)
	PPC_STORE_U32(r27.u32 + 20024, r25.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825f52dc
	if (!cr6.lt) goto loc_825F52DC;
loc_825F5284:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f52dc
	if (cr6.eq) goto loc_825F52DC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f52cc
	if (!cr0.lt) goto loc_825F52CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F52CC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f5284
	if (cr6.gt) goto loc_825F5284;
loc_825F52DC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f5318
	if (!cr0.lt) goto loc_825F5318;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5318:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,20032(r27)
	PPC_STORE_U32(r27.u32 + 20032, r28.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825f5390
	if (!cr6.lt) goto loc_825F5390;
loc_825F5338:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f5390
	if (cr6.eq) goto loc_825F5390;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f5380
	if (!cr0.lt) goto loc_825F5380;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5380:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f5338
	if (cr6.gt) goto loc_825F5338;
loc_825F5390:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f53cc
	if (!cr0.lt) goto loc_825F53CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F53CC:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,20036(r27)
	PPC_STORE_U32(r27.u32 + 20036, r28.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825f5444
	if (!cr6.lt) goto loc_825F5444;
loc_825F53EC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f5444
	if (cr6.eq) goto loc_825F5444;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f5434
	if (!cr0.lt) goto loc_825F5434;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5434:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f53ec
	if (cr6.gt) goto loc_825F53EC;
loc_825F5444:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f5480
	if (!cr0.lt) goto loc_825F5480;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5480:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,20040(r27)
	PPC_STORE_U32(r27.u32 + 20040, r28.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825f54f8
	if (!cr6.lt) goto loc_825F54F8;
loc_825F54A0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f54f8
	if (cr6.eq) goto loc_825F54F8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f54e8
	if (!cr0.lt) goto loc_825F54E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F54E8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f54a0
	if (cr6.gt) goto loc_825F54A0;
loc_825F54F8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f597c
	if (!cr0.lt) goto loc_825F597C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
	// stw r30,20044(r27)
	PPC_STORE_U32(r27.u32 + 20044, r30.u32);
	// b 0x825f5c50
	goto loc_825F5C50;
loc_825F553C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f5568
	if (!cr0.lt) goto loc_825F5568;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5568:
	// subfic r11,r31,0
	xer.ca = r31.u32 <= 0;
	r11.s64 = 0 - r31.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// stw r11,3960(r27)
	PPC_STORE_U32(r27.u32 + 3960, r11.u32);
	// b 0x825f5214
	goto loc_825F5214;
loc_825F5580:
	// lwz r11,3924(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f55d4
	if (cr6.eq) goto loc_825F55D4;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// stw r25,3960(r27)
	PPC_STORE_U32(r27.u32 + 3960, r25.u32);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r31,r11,0
	r31.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// bge 0x825f55bc
	if (!cr0.lt) goto loc_825F55BC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F55BC:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825f57ac
	if (cr6.eq) goto loc_825F57AC;
	// stw r25,3964(r27)
	PPC_STORE_U32(r27.u32 + 3964, r25.u32);
	// b 0x825f57ac
	goto loc_825F57AC;
loc_825F55CC:
	// mr r30,r25
	r30.u64 = r25.u64;
	// stw r25,3964(r27)
	PPC_STORE_U32(r27.u32 + 3964, r25.u32);
loc_825F55D4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,248(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 248);
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// ble cr6,0x825f56d8
	if (!cr6.gt) goto loc_825F56D8;
	// bge 0x825f560c
	if (!cr0.lt) goto loc_825F560C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F560C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825f57a4
	if (!cr6.eq) goto loc_825F57A4;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f5640
	if (!cr0.lt) goto loc_825F5640;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5640:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825f57d0
	if (!cr6.eq) goto loc_825F57D0;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f5674
	if (!cr0.lt) goto loc_825F5674;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5674:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825f57d8
	if (!cr6.eq) goto loc_825F57D8;
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825f5694
	if (!cr6.eq) goto loc_825F5694;
	// lwz r11,19980(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f56d0
	if (cr6.eq) goto loc_825F56D0;
loc_825F5694:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825f56d0
	if (!cr6.eq) goto loc_825F56D0;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f56c8
	if (!cr0.lt) goto loc_825F56C8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F56C8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825f55cc
	if (!cr6.eq) goto loc_825F55CC;
loc_825F56D0:
	// stw r26,3960(r27)
	PPC_STORE_U32(r27.u32 + 3960, r26.u32);
	// b 0x825f57ac
	goto loc_825F57AC;
loc_825F56D8:
	// bge 0x825f56e0
	if (!cr0.lt) goto loc_825F56E0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F56E0:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825f57d0
	if (!cr6.eq) goto loc_825F57D0;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f5714
	if (!cr0.lt) goto loc_825F5714;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5714:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825f56d0
	if (!cr6.eq) goto loc_825F56D0;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f5748
	if (!cr0.lt) goto loc_825F5748;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5748:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825f57d8
	if (!cr6.eq) goto loc_825F57D8;
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825f5768
	if (!cr6.eq) goto loc_825F5768;
	// lwz r11,19980(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f57a4
	if (cr6.eq) goto loc_825F57A4;
loc_825F5768:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825f57a4
	if (!cr6.eq) goto loc_825F57A4;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f579c
	if (!cr0.lt) goto loc_825F579C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F579C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825f55cc
	if (!cr6.eq) goto loc_825F55CC;
loc_825F57A4:
	// li r11,3
	r11.s64 = 3;
	// stw r11,3960(r27)
	PPC_STORE_U32(r27.u32 + 3960, r11.u32);
loc_825F57AC:
	// lwz r11,3908(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3908);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f5214
	if (!cr6.eq) goto loc_825F5214;
	// lwz r11,3964(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3964);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f5214
	if (cr6.eq) goto loc_825F5214;
loc_825F57C4:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825F57D0:
	// stw r25,3960(r27)
	PPC_STORE_U32(r27.u32 + 3960, r25.u32);
	// b 0x825f57ac
	goto loc_825F57AC;
loc_825F57D8:
	// stw r24,3960(r27)
	PPC_STORE_U32(r27.u32 + 3960, r24.u32);
	// b 0x825f57ac
	goto loc_825F57AC;
loc_825F57E0:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f580c
	if (!cr0.lt) goto loc_825F580C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F580C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// mr r29,r26
	r29.u64 = r26.u64;
	// beq cr6,0x825f5984
	if (cr6.eq) goto loc_825F5984;
	// stw r25,20028(r27)
	PPC_STORE_U32(r27.u32 + 20028, r25.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825f588c
	if (!cr6.lt) goto loc_825F588C;
loc_825F5834:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f588c
	if (cr6.eq) goto loc_825F588C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f587c
	if (!cr0.lt) goto loc_825F587C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F587C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f5834
	if (cr6.gt) goto loc_825F5834;
loc_825F588C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f58c8
	if (!cr0.lt) goto loc_825F58C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F58C8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,20040(r27)
	PPC_STORE_U32(r27.u32 + 20040, r28.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825f5940
	if (!cr6.lt) goto loc_825F5940;
loc_825F58E8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f5940
	if (cr6.eq) goto loc_825F5940;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f5930
	if (!cr0.lt) goto loc_825F5930;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5930:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f58e8
	if (cr6.gt) goto loc_825F58E8;
loc_825F5940:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f597c
	if (!cr0.lt) goto loc_825F597C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F597C:
	// stw r30,20044(r27)
	PPC_STORE_U32(r27.u32 + 20044, r30.u32);
	// b 0x825f5c50
	goto loc_825F5C50;
loc_825F5984:
	// stw r25,20024(r27)
	PPC_STORE_U32(r27.u32 + 20024, r25.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825f59f0
	if (!cr6.lt) goto loc_825F59F0;
loc_825F5998:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f59f0
	if (cr6.eq) goto loc_825F59F0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f59e0
	if (!cr0.lt) goto loc_825F59E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F59E0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f5998
	if (cr6.gt) goto loc_825F5998;
loc_825F59F0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f5a2c
	if (!cr0.lt) goto loc_825F5A2C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5A2C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,20032(r27)
	PPC_STORE_U32(r27.u32 + 20032, r28.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825f5aa4
	if (!cr6.lt) goto loc_825F5AA4;
loc_825F5A4C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f5aa4
	if (cr6.eq) goto loc_825F5AA4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f5a94
	if (!cr0.lt) goto loc_825F5A94;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5A94:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f5a4c
	if (cr6.gt) goto loc_825F5A4C;
loc_825F5AA4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f5ae0
	if (!cr0.lt) goto loc_825F5AE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5AE0:
	// stw r30,20036(r27)
	PPC_STORE_U32(r27.u32 + 20036, r30.u32);
	// b 0x825f5c50
	goto loc_825F5C50;
loc_825F5AE8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825f5b5c
	if (!cr6.lt) goto loc_825F5B5C;
loc_825F5B04:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f5b5c
	if (cr6.eq) goto loc_825F5B5C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f5b4c
	if (!cr0.lt) goto loc_825F5B4C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5B4C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f5b04
	if (cr6.gt) goto loc_825F5B04;
loc_825F5B5C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f5b98
	if (!cr0.lt) goto loc_825F5B98;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5B98:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,3968(r27)
	PPC_STORE_U32(r27.u32 + 3968, r28.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825f5c10
	if (!cr6.lt) goto loc_825F5C10;
loc_825F5BB8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f5c10
	if (cr6.eq) goto loc_825F5C10;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f5c00
	if (!cr0.lt) goto loc_825F5C00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5C00:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f5bb8
	if (cr6.gt) goto loc_825F5BB8;
loc_825F5C10:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f5c4c
	if (!cr0.lt) goto loc_825F5C4C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5C4C:
	// stw r30,3972(r27)
	PPC_STORE_U32(r27.u32 + 3972, r30.u32);
loc_825F5C50:
	// lwz r11,3960(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3960);
	// lwz r31,268(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f5ce0
	if (!cr6.eq) goto loc_825F5CE0;
	// lwz r11,21580(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21580);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f5c88
	if (cr6.eq) goto loc_825F5C88;
	// bl 0x82601780
	sub_82601780(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825f5c94
	if (cr6.eq) goto loc_825F5C94;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825F5C88:
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f60a4
	if (!cr6.eq) goto loc_825F60A4;
loc_825F5C94:
	// lwz r11,348(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 348);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f5d20
	if (cr6.eq) goto loc_825F5D20;
	// lwz r11,144(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825f5d20
	if (!cr6.gt) goto loc_825F5D20;
	// mr r11,r31
	r11.u64 = r31.u64;
loc_825F5CB4:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// rlwimi r8,r9,9,23,23
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r9.u32, 9) & 0x100) | (ctx.r8.u64 & 0xFFFFFFFFFFFFFEFF);
	// rlwinm r9,r8,0,23,20
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFF9FF;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r9,144(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825f5cb4
	if (cr6.lt) goto loc_825F5CB4;
	// b 0x825f5d20
	goto loc_825F5D20;
loc_825F5CE0:
	// lwz r11,3924(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f5d20
	if (!cr6.eq) goto loc_825F5D20;
	// lwz r11,144(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825f5d20
	if (!cr6.gt) goto loc_825F5D20;
	// mr r11,r31
	r11.u64 = r31.u64;
loc_825F5D00:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r9,0,24,20
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFF8FF;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r9,144(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825f5d00
	if (cr6.lt) goto loc_825F5D00;
loc_825F5D20:
	// lwz r11,14788(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f5da0
	if (cr6.eq) goto loc_825F5DA0;
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825f5da0
	if (!cr6.eq) goto loc_825F5DA0;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f60a4
	if (!cr6.eq) goto loc_825F60A4;
	// lwz r11,14804(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 14804);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f5da0
	if (cr6.eq) goto loc_825F5DA0;
	// lwz r10,144(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// lwz r11,268(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825f5da0
	if (!cr6.gt) goto loc_825F5DA0;
loc_825F5D6C:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r10,0,0,0
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x825f5d84
	if (cr6.eq) goto loc_825F5D84;
	// rlwimi r10,r25,5,24,26
	ctx.r10.u64 = (__builtin_rotateleft32(r25.u32, 5) & 0xE0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF1F);
	// b 0x825f5d88
	goto loc_825F5D88;
loc_825F5D84:
	// rlwinm r10,r10,0,27,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFF1F;
loc_825F5D88:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r10,144(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// blt cr6,0x825f5d6c
	if (cr6.lt) goto loc_825F5D6C;
loc_825F5DA0:
	// lwz r11,21580(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21580);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f5dc8
	if (cr6.eq) goto loc_825F5DC8;
	// bl 0x82601780
	sub_82601780(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825f5dd4
	if (cr6.eq) goto loc_825F5DD4;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825F5DC8:
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f60a4
	if (!cr6.eq) goto loc_825F60A4;
loc_825F5DD4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// stw r25,448(r27)
	PPC_STORE_U32(r27.u32 + 448, r25.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f5e4c
	if (!cr6.lt) goto loc_825F5E4C;
loc_825F5DF4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f5e4c
	if (cr6.eq) goto loc_825F5E4C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f5e3c
	if (!cr0.lt) goto loc_825F5E3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5E3C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f5df4
	if (cr6.gt) goto loc_825F5DF4;
loc_825F5E4C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f5e88
	if (!cr0.lt) goto loc_825F5E88;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5E88:
	// addi r11,r30,599
	r11.s64 = r30.s64 + 599;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// stw r11,2376(r27)
	PPC_STORE_U32(r27.u32 + 2376, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f5f0c
	if (!cr6.lt) goto loc_825F5F0C;
loc_825F5EB4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f5f0c
	if (cr6.eq) goto loc_825F5F0C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f5efc
	if (!cr0.lt) goto loc_825F5EFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5EFC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f5eb4
	if (cr6.gt) goto loc_825F5EB4;
loc_825F5F0C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f5f48
	if (!cr0.lt) goto loc_825F5F48;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5F48:
	// addi r11,r30,595
	r11.s64 = r30.s64 + 595;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r11,2140(r27)
	PPC_STORE_U32(r27.u32 + 2140, r11.u32);
	// beq cr6,0x825f57c4
	if (cr6.eq) goto loc_825F57C4;
	// lwz r11,3980(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f5f78
	if (cr6.eq) goto loc_825F5F78;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825ee6e8
	sub_825EE6E8(ctx, base);
loc_825F5F78:
	// lwz r11,436(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 436);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f609c
	if (cr6.eq) goto loc_825F609C;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f5fb0
	if (!cr0.lt) goto loc_825F5FB0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F5FB0:
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// bne cr6,0x825f608c
	if (!cr6.eq) goto loc_825F608C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// stw r26,328(r27)
	PPC_STORE_U32(r27.u32 + 328, r26.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f6030
	if (!cr6.lt) goto loc_825F6030;
loc_825F5FD8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6030
	if (cr6.eq) goto loc_825F6030;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f6020
	if (!cr0.lt) goto loc_825F6020;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6020:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f5fd8
	if (cr6.gt) goto loc_825F5FD8;
loc_825F6030:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f606c
	if (!cr0.lt) goto loc_825F606C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F606C:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,5184
	r11.s64 = r11.s64 + 5184;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,336(r27)
	PPC_STORE_U32(r27.u32 + 336, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825F608C:
	// stw r25,328(r27)
	PPC_STORE_U32(r27.u32 + 328, r25.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825F609C:
	// stw r26,328(r27)
	PPC_STORE_U32(r27.u32 + 328, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
loc_825F60A4:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825F60AC"))) PPC_WEAK_FUNC(sub_825F60AC);
PPC_FUNC_IMPL(__imp__sub_825F60AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F60B0"))) PPC_WEAK_FUNC(sub_825F60B0);
PPC_FUNC_IMPL(__imp__sub_825F60B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r24,0
	r24.s64 = 0;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r25,r24
	r25.u64 = r24.u64;
	// stw r24,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r24.u32);
	// bl 0x825f2db0
	sub_825F2DB0(ctx, base);
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f6104
	if (!cr6.eq) goto loc_825F6104;
	// lwz r11,21580(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21580);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6104
	if (cr6.eq) goto loc_825F6104;
	// lwz r11,21704(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21704);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f6104
	if (!cr6.eq) goto loc_825F6104;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82653a78
	sub_82653A78(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f7ab0
	if (!cr6.eq) goto loc_825F7AB0;
loc_825F6104:
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x825f63a0
	if (!cr6.eq) goto loc_825F63A0;
	// lwz r11,20832(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20832);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6354
	if (cr6.eq) goto loc_825F6354;
	// lwz r11,21160(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f62a0
	if (cr6.eq) goto loc_825F62A0;
	// lwz r11,21548(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21548);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f62a0
	if (!cr6.eq) goto loc_825F62A0;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// mr r28,r24
	r28.u64 = r24.u64;
	// mr r29,r30
	r29.u64 = r30.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f61ac
	if (!cr6.lt) goto loc_825F61AC;
loc_825F6154:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f61ac
	if (cr6.eq) goto loc_825F61AC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f619c
	if (!cr0.lt) goto loc_825F619C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F619C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f6154
	if (cr6.gt) goto loc_825F6154;
loc_825F61AC:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f61e8
	if (!cr0.lt) goto loc_825F61E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F61E8:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r24
	r29.u64 = r24.u64;
	// stw r28,20836(r26)
	PPC_STORE_U32(r26.u32 + 20836, r28.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f625c
	if (!cr6.lt) goto loc_825F625C;
loc_825F6204:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f625c
	if (cr6.eq) goto loc_825F625C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f624c
	if (!cr0.lt) goto loc_825F624C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F624C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f6204
	if (cr6.gt) goto loc_825F6204;
loc_825F625C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6298
	if (!cr0.lt) goto loc_825F6298;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6298:
	// stw r30,20840(r26)
	PPC_STORE_U32(r26.u32 + 20840, r30.u32);
	// b 0x825f6354
	goto loc_825F6354;
loc_825F62A0:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f6314
	if (!cr6.lt) goto loc_825F6314;
loc_825F62BC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6314
	if (cr6.eq) goto loc_825F6314;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f6304
	if (!cr0.lt) goto loc_825F6304;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6304:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f62bc
	if (cr6.gt) goto loc_825F62BC;
loc_825F6314:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6350
	if (!cr0.lt) goto loc_825F6350;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6350:
	// stw r30,21164(r26)
	PPC_STORE_U32(r26.u32 + 21164, r30.u32);
loc_825F6354:
	// lwz r11,21372(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6370
	if (cr6.eq) goto loc_825F6370;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825f2800
	sub_825F2800(ctx, base);
loc_825F6370:
	// lwz r11,14772(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 14772);
	// stw r24,21528(r26)
	PPC_STORE_U32(r26.u32 + 21528, r24.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825f6388
	if (!cr6.gt) goto loc_825F6388;
	// lwz r11,20980(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20980);
	// stw r11,20972(r26)
	PPC_STORE_U32(r26.u32 + 20972, r11.u32);
loc_825F6388:
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825e6560
	sub_825E6560(ctx, base);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd38
	return;
loc_825F63A0:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f63c0
	if (cr6.eq) goto loc_825F63C0;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825f63c0
	if (cr6.eq) goto loc_825F63C0;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825f63c0
	if (cr6.eq) goto loc_825F63C0;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825f7aac
	if (!cr6.eq) goto loc_825F7AAC;
loc_825F63C0:
	// lwz r11,20848(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20848);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f644c
	if (cr6.eq) goto loc_825F644C;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825f6424
	if (!cr6.lt) goto loc_825F6424;
loc_825F63E4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6424
	if (cr6.eq) goto loc_825F6424;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825f6414
	if (!cr0.lt) goto loc_825F6414;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6414:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f63e4
	if (cr6.gt) goto loc_825F63E4;
loc_825F6424:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825f644c
	if (!cr0.lt) goto loc_825F644C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F644C:
	// lwz r11,20832(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20832);
	// li r30,1
	r30.s64 = 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6694
	if (cr6.eq) goto loc_825F6694;
	// lwz r11,21160(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f65e0
	if (cr6.eq) goto loc_825F65E0;
	// lwz r11,21548(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21548);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f65e0
	if (!cr6.eq) goto loc_825F65E0;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f64e8
	if (!cr6.lt) goto loc_825F64E8;
loc_825F6490:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f64e8
	if (cr6.eq) goto loc_825F64E8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f64d8
	if (!cr0.lt) goto loc_825F64D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F64D8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f6490
	if (cr6.gt) goto loc_825F6490;
loc_825F64E8:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r27,r11,r28
	r27.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6524
	if (!cr0.lt) goto loc_825F6524;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6524:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// stw r27,20836(r26)
	PPC_STORE_U32(r26.u32 + 20836, r27.u32);
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f659c
	if (!cr6.lt) goto loc_825F659C;
loc_825F6544:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f659c
	if (cr6.eq) goto loc_825F659C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f658c
	if (!cr0.lt) goto loc_825F658C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F658C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f6544
	if (cr6.gt) goto loc_825F6544;
loc_825F659C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f65d8
	if (!cr0.lt) goto loc_825F65D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F65D8:
	// stw r29,20840(r26)
	PPC_STORE_U32(r26.u32 + 20840, r29.u32);
	// b 0x825f6694
	goto loc_825F6694;
loc_825F65E0:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r29,2
	r29.s64 = 2;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f6654
	if (!cr6.lt) goto loc_825F6654;
loc_825F65FC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6654
	if (cr6.eq) goto loc_825F6654;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f6644
	if (!cr0.lt) goto loc_825F6644;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6644:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f65fc
	if (cr6.gt) goto loc_825F65FC;
loc_825F6654:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6690
	if (!cr0.lt) goto loc_825F6690;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6690:
	// stw r29,21164(r26)
	PPC_STORE_U32(r26.u32 + 21164, r29.u32);
loc_825F6694:
	// lwz r11,21372(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f66b0
	if (cr6.eq) goto loc_825F66B0;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825f2800
	sub_825F2800(ctx, base);
loc_825F66B0:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f6724
	if (!cr6.lt) goto loc_825F6724;
loc_825F66CC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6724
	if (cr6.eq) goto loc_825F6724;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f6714
	if (!cr0.lt) goto loc_825F6714;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6714:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f66cc
	if (cr6.gt) goto loc_825F66CC;
loc_825F6724:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6760
	if (!cr0.lt) goto loc_825F6760;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6760:
	// lwz r11,21160(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21160);
	// stw r29,3904(r26)
	PPC_STORE_U32(r26.u32 + 3904, r29.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6824
	if (cr6.eq) goto loc_825F6824;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f67e4
	if (!cr6.lt) goto loc_825F67E4;
loc_825F678C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f67e4
	if (cr6.eq) goto loc_825F67E4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f67d4
	if (!cr0.lt) goto loc_825F67D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F67D4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f678c
	if (cr6.gt) goto loc_825F678C;
loc_825F67E4:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6820
	if (!cr0.lt) goto loc_825F6820;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6820:
	// stw r29,20972(r26)
	PPC_STORE_U32(r26.u32 + 20972, r29.u32);
loc_825F6824:
	// lwz r11,3444(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3444);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f68e4
	if (cr6.eq) goto loc_825F68E4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f68a4
	if (!cr6.lt) goto loc_825F68A4;
loc_825F684C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f68a4
	if (cr6.eq) goto loc_825F68A4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f6894
	if (!cr0.lt) goto loc_825F6894;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6894:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f684c
	if (cr6.gt) goto loc_825F684C;
loc_825F68A4:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f68e0
	if (!cr0.lt) goto loc_825F68E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F68E0:
	// stw r29,3448(r26)
	PPC_STORE_U32(r26.u32 + 3448, r29.u32);
loc_825F68E4:
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825f6adc
	if (!cr6.eq) goto loc_825F6ADC;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r29,3
	r29.s64 = 3;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825f6964
	if (!cr6.lt) goto loc_825F6964;
loc_825F690C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6964
	if (cr6.eq) goto loc_825F6964;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f6954
	if (!cr0.lt) goto loc_825F6954;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6954:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f690c
	if (cr6.gt) goto loc_825F690C;
loc_825F6964:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f69a0
	if (!cr0.lt) goto loc_825F69A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F69A0:
	// cmpwi cr6,r29,7
	cr6.compare<int32_t>(r29.s32, 7, xer);
	// bne cr6,0x825f6a90
	if (!cr6.eq) goto loc_825F6A90;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r29,4
	r29.s64 = 4;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x825f6a1c
	if (!cr6.lt) goto loc_825F6A1C;
loc_825F69C4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6a1c
	if (cr6.eq) goto loc_825F6A1C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f6a0c
	if (!cr0.lt) goto loc_825F6A0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6A0C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f69c4
	if (cr6.gt) goto loc_825F69C4;
loc_825F6A1C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6a58
	if (!cr0.lt) goto loc_825F6A58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6A58:
	// cmpwi cr6,r29,14
	cr6.compare<int32_t>(r29.s32, 14, xer);
	// bge cr6,0x825f7aac
	if (!cr6.lt) goto loc_825F7AAC;
	// addi r11,r29,112
	r11.s64 = r29.s64 + 112;
	// lwz r7,14772(r26)
	ctx.r7.u64 = PPC_LOAD_U32(r26.u32 + 14772);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// addi r11,r11,-112
	r11.s64 = r11.s64 + -112;
	// addi r10,r10,4760
	ctx.r10.s64 = ctx.r10.s64 + 4760;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// addi r9,r9,4816
	ctx.r9.s64 = ctx.r9.s64 + 4816;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stw r10,3392(r26)
	PPC_STORE_U32(r26.u32 + 3392, ctx.r10.u32);
	// b 0x825f6ab4
	goto loc_825F6AB4;
loc_825F6A90:
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,4704
	ctx.r10.s64 = ctx.r10.s64 + 4704;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// addi r9,r9,4732
	ctx.r9.s64 = ctx.r9.s64 + 4732;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stw r10,3392(r26)
	PPC_STORE_U32(r26.u32 + 3392, ctx.r10.u32);
	// lwz r10,14772(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 14772);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
loc_825F6AB4:
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,28552
	ctx.r8.s64 = ctx.r8.s64 + 28552;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r11,3388(r26)
	PPC_STORE_U32(r26.u32 + 3388, r11.u32);
	// lwz r11,-4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// stw r11,14776(r26)
	PPC_STORE_U32(r26.u32 + 14776, r11.u32);
	// bne cr6,0x825f6adc
	if (!cr6.eq) goto loc_825F6ADC;
	// stw r30,14772(r26)
	PPC_STORE_U32(r26.u32 + 14772, r30.u32);
loc_825F6ADC:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r29,5
	r29.s64 = 5;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825f6b50
	if (!cr6.lt) goto loc_825F6B50;
loc_825F6AF8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6b50
	if (cr6.eq) goto loc_825F6B50;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f6b40
	if (!cr0.lt) goto loc_825F6B40;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6B40:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f6af8
	if (cr6.gt) goto loc_825F6AF8;
loc_825F6B50:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6b8c
	if (!cr0.lt) goto loc_825F6B8C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6B8C:
	// cmpwi cr6,r29,8
	cr6.compare<int32_t>(r29.s32, 8, xer);
	// stw r29,3952(r26)
	PPC_STORE_U32(r26.u32 + 3952, r29.u32);
	// bgt cr6,0x825f6c50
	if (cr6.gt) goto loc_825F6C50;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f6c0c
	if (!cr6.lt) goto loc_825F6C0C;
loc_825F6BB4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6c0c
	if (cr6.eq) goto loc_825F6C0C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f6bfc
	if (!cr0.lt) goto loc_825F6BFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6BFC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f6bb4
	if (cr6.gt) goto loc_825F6BB4;
loc_825F6C0C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6c48
	if (!cr0.lt) goto loc_825F6C48;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6C48:
	// stw r29,252(r26)
	PPC_STORE_U32(r26.u32 + 252, r29.u32);
	// b 0x825f6c54
	goto loc_825F6C54;
loc_825F6C50:
	// stw r24,252(r26)
	PPC_STORE_U32(r26.u32 + 252, r24.u32);
loc_825F6C54:
	// lwz r11,3440(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3440);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6d14
	if (cr6.eq) goto loc_825F6D14;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f6cd4
	if (!cr6.lt) goto loc_825F6CD4;
loc_825F6C7C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6cd4
	if (cr6.eq) goto loc_825F6CD4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f6cc4
	if (!cr0.lt) goto loc_825F6CC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6CC4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f6c7c
	if (cr6.gt) goto loc_825F6C7C;
loc_825F6CD4:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6d10
	if (!cr0.lt) goto loc_825F6D10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6D10:
	// stw r29,3428(r26)
	PPC_STORE_U32(r26.u32 + 3428, r29.u32);
loc_825F6D14:
	// lwz r11,3432(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3432);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f6d54
	if (!cr6.eq) goto loc_825F6D54;
	// lwz r11,3952(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3952);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bgt cr6,0x825f6d38
	if (cr6.gt) goto loc_825F6D38;
	// mr r27,r11
	r27.u64 = r11.u64;
	// stw r30,3428(r26)
	PPC_STORE_U32(r26.u32 + 3428, r30.u32);
	// b 0x825f6d58
	goto loc_825F6D58;
loc_825F6D38:
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// stw r24,3428(r26)
	PPC_STORE_U32(r26.u32 + 3428, r24.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,4872
	ctx.r10.s64 = ctx.r10.s64 + 4872;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r27,-4(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// b 0x825f6d58
	goto loc_825F6D58;
loc_825F6D54:
	// lwz r27,3952(r26)
	r27.u64 = PPC_LOAD_U32(r26.u32 + 3952);
loc_825F6D58:
	// lwz r11,2972(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2972);
	// stw r27,248(r26)
	PPC_STORE_U32(r26.u32 + 248, r27.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r24,2968(r26)
	PPC_STORE_U32(r26.u32 + 2968, r24.u32);
	// beq cr6,0x825f6da0
	if (cr6.eq) goto loc_825F6DA0;
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825f6da0
	if (cr6.eq) goto loc_825F6DA0;
	// cmpwi cr6,r27,9
	cr6.compare<int32_t>(r27.s32, 9, xer);
	// blt cr6,0x825f6d88
	if (cr6.lt) goto loc_825F6D88;
	// stw r30,2968(r26)
	PPC_STORE_U32(r26.u32 + 2968, r30.u32);
	// b 0x825f6da0
	goto loc_825F6DA0;
loc_825F6D88:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6d98
	if (cr6.eq) goto loc_825F6D98;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825f6da0
	if (!cr6.eq) goto loc_825F6DA0;
loc_825F6D98:
	// li r11,7
	r11.s64 = 7;
	// stw r11,2968(r26)
	PPC_STORE_U32(r26.u32 + 2968, r11.u32);
loc_825F6DA0:
	// lwz r11,20868(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20868);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6e60
	if (cr6.eq) goto loc_825F6E60;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r29,2
	r29.s64 = 2;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f6e20
	if (!cr6.lt) goto loc_825F6E20;
loc_825F6DC8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6e20
	if (cr6.eq) goto loc_825F6E20;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f6e10
	if (!cr0.lt) goto loc_825F6E10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6E10:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f6dc8
	if (cr6.gt) goto loc_825F6DC8;
loc_825F6E20:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6e5c
	if (!cr0.lt) goto loc_825F6E5C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6E5C:
	// stw r29,20872(r26)
	PPC_STORE_U32(r26.u32 + 20872, r29.u32);
loc_825F6E60:
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6e74
	if (cr6.eq) goto loc_825F6E74;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825f710c
	if (!cr6.eq) goto loc_825F710C;
loc_825F6E74:
	// lwz r11,21580(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21580);
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6e9c
	if (cr6.eq) goto loc_825F6E9C;
	// bl 0x82601780
	sub_82601780(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825f6ea8
	if (cr6.eq) goto loc_825F6EA8;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd38
	return;
loc_825F6E9C:
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f7ab0
	if (!cr6.eq) goto loc_825F7AB0;
loc_825F6EA8:
	// lwz r11,20004(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20004);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6ef8
	if (cr6.eq) goto loc_825F6EF8;
	// lwz r11,144(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825f6ef8
	if (!cr6.gt) goto loc_825F6EF8;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_825F6EC8:
	// lwz r11,268(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 268);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// rlwimi r7,r8,4,28,28
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 4) & 0x8) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFF7);
	// rlwinm r8,r7,0,28,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFFFFFFFEF;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r11,144(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// blt cr6,0x825f6ec8
	if (cr6.lt) goto loc_825F6EC8;
loc_825F6EF8:
	// lwz r11,2968(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2968);
	// rlwinm r11,r11,0,30,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f7108
	if (cr6.eq) goto loc_825F7108;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f6f7c
	if (!cr6.lt) goto loc_825F6F7C;
loc_825F6F24:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f6f7c
	if (cr6.eq) goto loc_825F6F7C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f6f6c
	if (!cr0.lt) goto loc_825F6F6C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6F6C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f6f24
	if (cr6.gt) goto loc_825F6F24;
loc_825F6F7C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f6fb8
	if (!cr0.lt) goto loc_825F6FB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F6FB8:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825f6fc8
	if (!cr6.eq) goto loc_825F6FC8;
	// stw r24,2968(r26)
	PPC_STORE_U32(r26.u32 + 2968, r24.u32);
	// b 0x825f7108
	goto loc_825F7108;
loc_825F6FC8:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f703c
	if (!cr6.lt) goto loc_825F703C;
loc_825F6FE4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f703c
	if (cr6.eq) goto loc_825F703C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f702c
	if (!cr0.lt) goto loc_825F702C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F702C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f6fe4
	if (cr6.gt) goto loc_825F6FE4;
loc_825F703C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f7078
	if (!cr0.lt) goto loc_825F7078;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7078:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825f7088
	if (!cr6.eq) goto loc_825F7088;
	// stw r30,2968(r26)
	PPC_STORE_U32(r26.u32 + 2968, r30.u32);
	// b 0x825f7108
	goto loc_825F7108;
loc_825F7088:
	// lwz r11,21580(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21580);
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f70b0
	if (cr6.eq) goto loc_825F70B0;
	// bl 0x82601780
	sub_82601780(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825f70bc
	if (cr6.eq) goto loc_825F70BC;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd38
	return;
loc_825F70B0:
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f7ab0
	if (!cr6.eq) goto loc_825F7AB0;
loc_825F70BC:
	// lwz r11,20940(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20940);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f7108
	if (cr6.eq) goto loc_825F7108;
	// lwz r11,144(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825f7108
	if (!cr6.gt) goto loc_825F7108;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_825F70DC:
	// lwz r11,268(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 268);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// rlwimi r7,r8,12,20,20
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 12) & 0x800) | (ctx.r7.u64 & 0xFFFFFFFFFFFFF7FF);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// lwz r11,144(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// blt cr6,0x825f70dc
	if (cr6.lt) goto loc_825F70DC;
loc_825F7108:
	// stw r24,21528(r26)
	PPC_STORE_U32(r26.u32 + 21528, r24.u32);
loc_825F710C:
	// lwz r11,2968(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f7140
	if (cr6.eq) goto loc_825F7140;
	// lwz r11,1900(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 1900);
	// sth r24,16(r11)
	PPC_STORE_U16(r11.u32 + 16, r24.u16);
	// lwz r11,1900(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 1900);
	// sth r24,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r24.u16);
	// lwz r11,1904(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 1904);
	// sth r24,16(r11)
	PPC_STORE_U16(r11.u32 + 16, r24.u16);
	// lwz r11,1904(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 1904);
	// sth r24,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r24.u16);
	// b 0x825f7164
	goto loc_825F7164;
loc_825F7140:
	// lwz r10,1900(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 1900);
	// li r11,128
	r11.s64 = 128;
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, r11.u16);
	// lwz r10,1900(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 1900);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// lwz r10,1904(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 1904);
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, r11.u16);
	// lwz r10,1904(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 1904);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
loc_825F7164:
	// lwz r10,3428(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 3428);
	// addi r11,r26,3988
	r11.s64 = r26.s64 + 3988;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825f7178
	if (!cr6.eq) goto loc_825F7178;
	// addi r11,r26,5268
	r11.s64 = r26.s64 + 5268;
loc_825F7178:
	// stw r11,6548(r26)
	PPC_STORE_U32(r26.u32 + 6548, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r26,6560
	r11.s64 = r26.s64 + 6560;
	// bne cr6,0x825f718c
	if (!cr6.eq) goto loc_825F718C;
	// addi r11,r26,10656
	r11.s64 = r26.s64 + 10656;
loc_825F718C:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// stw r11,14752(r26)
	PPC_STORE_U32(r26.u32 + 14752, r11.u32);
	// stw r27,248(r26)
	PPC_STORE_U32(r26.u32 + 248, r27.u32);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f7aac
	if (!cr6.eq) goto loc_825F7AAC;
	// addi r11,r27,-1
	r11.s64 = r27.s64 + -1;
	// stw r27,248(r26)
	PPC_STORE_U32(r26.u32 + 248, r27.u32);
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// bgt cr6,0x825f7aac
	if (cr6.gt) goto loc_825F7AAC;
	// lwz r11,3952(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3952);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bgt cr6,0x825f71d0
	if (cr6.gt) goto loc_825F71D0;
	// addi r11,r26,2840
	r11.s64 = r26.s64 + 2840;
	// addi r10,r26,2800
	ctx.r10.s64 = r26.s64 + 2800;
	// stw r11,2904(r26)
	PPC_STORE_U32(r26.u32 + 2904, r11.u32);
	// stw r10,2916(r26)
	PPC_STORE_U32(r26.u32 + 2916, ctx.r10.u32);
loc_825F71D0:
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f76ac
	if (cr6.eq) goto loc_825F76AC;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825f7448
	if (cr6.eq) goto loc_825F7448;
	// lwz r11,20864(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20864);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f7418
	if (cr6.eq) goto loc_825F7418;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f7260
	if (!cr6.lt) goto loc_825F7260;
loc_825F7208:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f7260
	if (cr6.eq) goto loc_825F7260;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f7250
	if (!cr0.lt) goto loc_825F7250;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7250:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f7208
	if (cr6.gt) goto loc_825F7208;
loc_825F7260:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r27,r11,r28
	r27.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f729c
	if (!cr0.lt) goto loc_825F729C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F729C:
	// mr r25,r27
	r25.u64 = r27.u64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x825f735c
	if (cr6.eq) goto loc_825F735C;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f731c
	if (!cr6.lt) goto loc_825F731C;
loc_825F72C4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f731c
	if (cr6.eq) goto loc_825F731C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f730c
	if (!cr0.lt) goto loc_825F730C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F730C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f72c4
	if (cr6.gt) goto loc_825F72C4;
loc_825F731C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f7358
	if (!cr0.lt) goto loc_825F7358;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7358:
	// add r25,r29,r27
	r25.u64 = r29.u64 + r27.u64;
loc_825F735C:
	// cmpwi cr6,r25,2
	cr6.compare<int32_t>(r25.s32, 2, xer);
	// bne cr6,0x825f7418
	if (!cr6.eq) goto loc_825F7418;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f73d8
	if (!cr6.lt) goto loc_825F73D8;
loc_825F7380:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f73d8
	if (cr6.eq) goto loc_825F73D8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f73c8
	if (!cr0.lt) goto loc_825F73C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F73C8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f7380
	if (cr6.gt) goto loc_825F7380;
loc_825F73D8:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f7414
	if (!cr0.lt) goto loc_825F7414;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7414:
	// addi r25,r29,2
	r25.s64 = r29.s64 + 2;
loc_825F7418:
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82603a38
	sub_82603A38(ctx, base);
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// lwz r10,404(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 404);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825f743c
	if (!cr6.eq) goto loc_825F743C;
	// stw r10,21528(r26)
	PPC_STORE_U32(r26.u32 + 21528, ctx.r10.u32);
	// b 0x825f7448
	goto loc_825F7448;
loc_825F743C:
	// lwz r9,21528(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 21528);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825f7aac
	if (cr6.lt) goto loc_825F7AAC;
loc_825F7448:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f76ac
	if (cr6.eq) goto loc_825F76AC;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825f76ac
	if (cr6.eq) goto loc_825F76AC;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825f51c0
	sub_825F51C0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f7ab0
	if (!cr6.eq) goto loc_825F7AB0;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f74dc
	if (!cr6.lt) goto loc_825F74DC;
loc_825F7484:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f74dc
	if (cr6.eq) goto loc_825F74DC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f74cc
	if (!cr0.lt) goto loc_825F74CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F74CC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f7484
	if (cr6.gt) goto loc_825F7484;
loc_825F74DC:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f7518
	if (!cr0.lt) goto loc_825F7518;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7518:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// stw r29,2928(r26)
	PPC_STORE_U32(r26.u32 + 2928, r29.u32);
	// beq cr6,0x825f75e0
	if (cr6.eq) goto loc_825F75E0;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f7598
	if (!cr6.lt) goto loc_825F7598;
loc_825F7540:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f7598
	if (cr6.eq) goto loc_825F7598;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f7588
	if (!cr0.lt) goto loc_825F7588;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7588:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f7540
	if (cr6.gt) goto loc_825F7540;
loc_825F7598:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f75d4
	if (!cr0.lt) goto loc_825F75D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F75D4:
	// lwz r11,2928(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2928);
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// stw r11,2928(r26)
	PPC_STORE_U32(r26.u32 + 2928, r11.u32);
loc_825F75E0:
	// lwz r11,2928(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2928);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// stw r11,2936(r26)
	PPC_STORE_U32(r26.u32 + 2936, r11.u32);
	// stw r11,2932(r26)
	PPC_STORE_U32(r26.u32 + 2932, r11.u32);
	// stw r11,2948(r26)
	PPC_STORE_U32(r26.u32 + 2948, r11.u32);
	// stw r11,2944(r26)
	PPC_STORE_U32(r26.u32 + 2944, r11.u32);
	// stw r11,2940(r26)
	PPC_STORE_U32(r26.u32 + 2940, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f7668
	if (!cr6.lt) goto loc_825F7668;
loc_825F7610:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f7668
	if (cr6.eq) goto loc_825F7668;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f7658
	if (!cr0.lt) goto loc_825F7658;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7658:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f7610
	if (cr6.gt) goto loc_825F7610;
loc_825F7668:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f76a4
	if (!cr0.lt) goto loc_825F76A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F76A4:
	// stw r30,2088(r26)
	PPC_STORE_U32(r26.u32 + 2088, r30.u32);
	// b 0x825f7a9c
	goto loc_825F7A9C;
loc_825F76AC:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r4,15464(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 15464);
	// bl 0x825f5000
	sub_825F5000(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f7ab0
	if (!cr6.eq) goto loc_825F7AB0;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f7aac
	if (!cr6.eq) goto loc_825F7AAC;
	// stw r24,400(r26)
	PPC_STORE_U32(r26.u32 + 400, r24.u32);
	// mr r29,r30
	r29.u64 = r30.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r28,r24
	r28.u64 = r24.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f7744
	if (!cr6.lt) goto loc_825F7744;
loc_825F76EC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f7744
	if (cr6.eq) goto loc_825F7744;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f7734
	if (!cr0.lt) goto loc_825F7734;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7734:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f76ec
	if (cr6.gt) goto loc_825F76EC;
loc_825F7744:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f7780
	if (!cr0.lt) goto loc_825F7780;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7780:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// stw r29,2928(r26)
	PPC_STORE_U32(r26.u32 + 2928, r29.u32);
	// beq cr6,0x825f7848
	if (cr6.eq) goto loc_825F7848;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f7800
	if (!cr6.lt) goto loc_825F7800;
loc_825F77A8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f7800
	if (cr6.eq) goto loc_825F7800;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f77f0
	if (!cr0.lt) goto loc_825F77F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F77F0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f77a8
	if (cr6.gt) goto loc_825F77A8;
loc_825F7800:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f783c
	if (!cr0.lt) goto loc_825F783C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F783C:
	// lwz r11,2928(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2928);
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// stw r11,2928(r26)
	PPC_STORE_U32(r26.u32 + 2928, r11.u32);
loc_825F7848:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f78bc
	if (!cr6.lt) goto loc_825F78BC;
loc_825F7864:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f78bc
	if (cr6.eq) goto loc_825F78BC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f78ac
	if (!cr0.lt) goto loc_825F78AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F78AC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f7864
	if (cr6.gt) goto loc_825F7864;
loc_825F78BC:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f78f8
	if (!cr0.lt) goto loc_825F78F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F78F8:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// stw r29,2940(r26)
	PPC_STORE_U32(r26.u32 + 2940, r29.u32);
	// beq cr6,0x825f79c0
	if (cr6.eq) goto loc_825F79C0;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f7978
	if (!cr6.lt) goto loc_825F7978;
loc_825F7920:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f7978
	if (cr6.eq) goto loc_825F7978;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x825f7968
	if (!cr0.lt) goto loc_825F7968;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7968:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x825f7920
	if (cr6.gt) goto loc_825F7920;
loc_825F7978:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f79b4
	if (!cr0.lt) goto loc_825F79B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F79B4:
	// lwz r11,2940(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2940);
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// stw r11,2940(r26)
	PPC_STORE_U32(r26.u32 + 2940, r11.u32);
loc_825F79C0:
	// lwz r11,2940(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2940);
	// mr r29,r24
	r29.u64 = r24.u64;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// stw r11,2948(r26)
	PPC_STORE_U32(r26.u32 + 2948, r11.u32);
	// stw r11,2944(r26)
	PPC_STORE_U32(r26.u32 + 2944, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f7a3c
	if (!cr6.lt) goto loc_825F7A3C;
loc_825F79E4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f7a3c
	if (cr6.eq) goto loc_825F7A3C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f7a2c
	if (!cr0.lt) goto loc_825F7A2C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7A2C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f79e4
	if (cr6.gt) goto loc_825F79E4;
loc_825F7A3C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f7a78
	if (!cr0.lt) goto loc_825F7A78;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7A78:
	// lwz r11,3980(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3980);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r30,2088(r26)
	PPC_STORE_U32(r26.u32 + 2088, r30.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f7a98
	if (cr6.eq) goto loc_825F7A98;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825ee6e8
	sub_825EE6E8(ctx, base);
	// b 0x825f7a9c
	goto loc_825F7A9C;
loc_825F7A98:
	// bl 0x826179f8
	sub_826179F8(ctx, base);
loc_825F7A9C:
	// lwz r11,84(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f6388
	if (cr6.eq) goto loc_825F6388;
loc_825F7AAC:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825F7AB0:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825F7AB8"))) PPC_WEAK_FUNC(sub_825F7AB8);
PPC_FUNC_IMPL(__imp__sub_825F7AB8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,14828(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// lwz r10,14824(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// beq cr6,0x825f7d94
	if (cr6.eq) goto loc_825F7D94;
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825f7af0
	if (cr6.eq) goto loc_825F7AF0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f7af0
	if (!cr6.eq) goto loc_825F7AF0;
	// bl 0x825f3a70
	sub_825F3A70(ctx, base);
loc_825F7AF0:
	// lwz r6,284(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x825f7b04
	if (cr6.eq) goto loc_825F7B04;
	// cmpwi cr6,r6,4
	cr6.compare<int32_t>(ctx.r6.s32, 4, xer);
	// bne cr6,0x825f7b30
	if (!cr6.eq) goto loc_825F7B30;
loc_825F7B04:
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r9,3724(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r8,3720(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r7,220(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwz r10,3728(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r9,3804(r31)
	PPC_STORE_U32(r31.u32 + 3804, ctx.r9.u32);
	// stw r8,3800(r31)
	PPC_STORE_U32(r31.u32 + 3800, ctx.r8.u32);
	// stw r11,3808(r31)
	PPC_STORE_U32(r31.u32 + 3808, r11.u32);
loc_825F7B30:
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f7b4c
	if (!cr6.eq) goto loc_825F7B4C;
	// cmpwi cr6,r6,1
	cr6.compare<int32_t>(ctx.r6.s32, 1, xer);
	// beq cr6,0x825f7b4c
	if (cr6.eq) goto loc_825F7B4C;
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// bne cr6,0x825f7d94
	if (!cr6.eq) goto loc_825F7D94;
loc_825F7B4C:
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// lwz r9,14824(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// mulli r11,r10,84
	r11.s64 = ctx.r10.s64 * 84;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// lwz r29,14900(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 14900);
	// lwz r30,14904(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 14904);
	// ble cr6,0x825f7c58
	if (!cr6.gt) goto loc_825F7C58;
	// lwz r10,152(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 152);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825f7c14
	if (!cr6.eq) goto loc_825F7C14;
	// lwz r28,14884(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 14884);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r10,14856(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14856);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r9,14832(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14832);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// lwz r5,14840(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 14840);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,3732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// lwz r28,19976(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// lwz r11,15856(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15856);
	// cntlzw r28,r28
	r28.u64 = r28.u32 == 0 ? 32 : __builtin_clz(r28.u32);
	// rlwinm r28,r28,27,31,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 27) & 0x1;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,14828(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// lwz r10,19976(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// li r9,1
	ctx.r9.s64 = 1;
	// mulli r11,r11,84
	r11.s64 = r11.s64 * 84;
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r3,3736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r28,15852(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 15852);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r27,14888(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 14888);
	// lwz r26,14860(r11)
	r26.u64 = PPC_LOAD_U32(r11.u32 + 14860);
	// lwz r6,14844(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 14844);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// lwz r10,14836(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14836);
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// mtctr r28
	ctr.u64 = r28.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_825F7C14:
	// lwz r8,220(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,3720(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lwz r8,3732(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// lwz r9,3728(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r10,3724(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// add r4,r8,r29
	ctx.r4.u64 = ctx.r8.u64 + r29.u64;
	// lwz r6,3740(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwz r5,3736(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// add r6,r30,r6
	ctx.r6.u64 = r30.u64 + ctx.r6.u64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// bl 0x82608360
	sub_82608360(ctx, base);
	// b 0x825f7c98
	goto loc_825F7C98;
loc_825F7C58:
	// lwz r8,220(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,3720(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lwz r8,3732(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// lwz r9,3728(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r10,3724(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// add r4,r8,r29
	ctx.r4.u64 = ctx.r8.u64 + r29.u64;
	// lwz r6,3740(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwz r5,3736(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// add r6,r30,r6
	ctx.r6.u64 = r30.u64 + ctx.r6.u64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// bl 0x82608e60
	sub_82608E60(ctx, base);
loc_825F7C98:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f50e0
	sub_825F50E0(ctx, base);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 184);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r9,164(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r6,220(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r5,172(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// lwz r3,3732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r30,15856(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 15856);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// mtctr r30
	ctr.u64 = r30.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// lwz r30,208(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r9,1
	ctx.r9.s64 = 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r29,196(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 196);
	// lwz r10,168(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 168);
	// li r8,1
	ctx.r8.s64 = 1;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// lwz r7,224(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r6,176(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r3,3736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r28,15852(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 15852);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// mtctr r28
	ctr.u64 = r28.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f7d94
	if (!cr6.eq) goto loc_825F7D94;
	// lwz r11,14828(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f7d94
	if (!cr6.eq) goto loc_825F7D94;
	// lwz r11,3704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,3688(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// stw r11,3688(r31)
	PPC_STORE_U32(r31.u32 + 3688, r11.u32);
	// stw r10,3704(r31)
	PPC_STORE_U32(r31.u32 + 3704, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r8,3720(r31)
	PPC_STORE_U32(r31.u32 + 3720, ctx.r8.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r8,3724(r31)
	PPC_STORE_U32(r31.u32 + 3724, ctx.r8.u32);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,3728(r31)
	PPC_STORE_U32(r31.u32 + 3728, r11.u32);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r11,3776(r31)
	PPC_STORE_U32(r31.u32 + 3776, r11.u32);
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r11,3780(r31)
	PPC_STORE_U32(r31.u32 + 3780, r11.u32);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r9,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, ctx.r9.u32);
	// stw r11,3784(r31)
	PPC_STORE_U32(r31.u32 + 3784, r11.u32);
loc_825F7D94:
	// lwz r11,14824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// stw r11,14828(r31)
	PPC_STORE_U32(r31.u32 + 14828, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825F7DA4"))) PPC_WEAK_FUNC(sub_825F7DA4);
PPC_FUNC_IMPL(__imp__sub_825F7DA4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F7DA8"))) PPC_WEAK_FUNC(sub_825F7DA8);
PPC_FUNC_IMPL(__imp__sub_825F7DA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r27,r25
	r27.u64 = r25.u64;
	// lwz r11,15472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f7dd8
	if (!cr6.eq) goto loc_825F7DD8;
	// bl 0x825ec810
	sub_825EC810(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825F7DD8:
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// stw r25,3400(r28)
	PPC_STORE_U32(r28.u32 + 3400, r25.u32);
	// li r24,1
	r24.s64 = 1;
	// li r26,2
	r26.s64 = 2;
	// bne cr6,0x825f7f74
	if (!cr6.eq) goto loc_825F7F74;
	// lwz r11,3444(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3444);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f7e28
	if (cr6.eq) goto loc_825F7E28;
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f7e24
	if (!cr0.lt) goto loc_825F7E24;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7E24:
	// stw r31,3448(r28)
	PPC_STORE_U32(r28.u32 + 3448, r31.u32);
loc_825F7E28:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f7e80
	if (!cr6.lt) goto loc_825F7E80;
loc_825F7E40:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f7e80
	if (cr6.eq) goto loc_825F7E80;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825f7e70
	if (!cr0.lt) goto loc_825F7E70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7E70:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f7e40
	if (cr6.gt) goto loc_825F7E40;
loc_825F7E80:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825f7ea8
	if (!cr0.lt) goto loc_825F7EA8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7EA8:
	// lwz r11,15472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x825f7f74
	if (!cr6.eq) goto loc_825F7F74;
	// lwz r11,14792(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 14792);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f7f74
	if (cr6.eq) goto loc_825F7F74;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f7f34
	if (!cr6.lt) goto loc_825F7F34;
loc_825F7EDC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f7f34
	if (cr6.eq) goto loc_825F7F34;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f7f24
	if (!cr0.lt) goto loc_825F7F24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7F24:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f7edc
	if (cr6.gt) goto loc_825F7EDC;
loc_825F7F34:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f7f70
	if (!cr0.lt) goto loc_825F7F70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7F70:
	// stw r30,14796(r28)
	PPC_STORE_U32(r28.u32 + 14796, r30.u32);
loc_825F7F74:
	// lwz r11,15472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x825f8734
	if (!cr6.eq) goto loc_825F8734;
	// li r11,-1
	r11.s64 = -1;
	// mr r30,r24
	r30.u64 = r24.u64;
	// stw r11,15196(r28)
	PPC_STORE_U32(r28.u32 + 15196, r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f7ffc
	if (!cr6.lt) goto loc_825F7FFC;
loc_825F7FA4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f7ffc
	if (cr6.eq) goto loc_825F7FFC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f7fec
	if (!cr0.lt) goto loc_825F7FEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F7FEC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f7fa4
	if (cr6.gt) goto loc_825F7FA4;
loc_825F7FFC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8038
	if (!cr0.lt) goto loc_825F8038;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8038:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x825f8048
	if (!cr6.eq) goto loc_825F8048;
	// stw r24,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r24.u32);
	// b 0x825f8800
	goto loc_825F8800;
loc_825F8048:
	// lwz r11,14772(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,15192(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15192);
	// bne cr6,0x825f81fc
	if (!cr6.eq) goto loc_825F81FC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f8068
	if (!cr6.eq) goto loc_825F8068;
	// stw r25,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r25.u32);
	// b 0x825f8800
	goto loc_825F8800;
loc_825F8068:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f80dc
	if (!cr6.lt) goto loc_825F80DC;
loc_825F8084:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f80dc
	if (cr6.eq) goto loc_825F80DC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f80cc
	if (!cr0.lt) goto loc_825F80CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F80CC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8084
	if (cr6.gt) goto loc_825F8084;
loc_825F80DC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8118
	if (!cr0.lt) goto loc_825F8118;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8118:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x825f8128
	if (!cr6.eq) goto loc_825F8128;
	// stw r25,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r25.u32);
	// b 0x825f8800
	goto loc_825F8800;
loc_825F8128:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f819c
	if (!cr6.lt) goto loc_825F819C;
loc_825F8144:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f819c
	if (cr6.eq) goto loc_825F819C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f818c
	if (!cr0.lt) goto loc_825F818C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F818C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8144
	if (cr6.gt) goto loc_825F8144;
loc_825F819C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f81d8
	if (!cr0.lt) goto loc_825F81D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F81D8:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x825f81f0
	if (!cr6.eq) goto loc_825F81F0;
	// lwz r3,15204(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 15204);
	// stw r25,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r25.u32);
	// bl 0x82390038
	sub_82390038(ctx, base);
	// b 0x825f8800
	goto loc_825F8800;
loc_825F81F0:
	// mr r27,r24
	r27.u64 = r24.u64;
	// stw r24,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r24.u32);
	// b 0x825f8800
	goto loc_825F8800;
loc_825F81FC:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825f82c4
	if (!cr6.eq) goto loc_825F82C4;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f8278
	if (!cr6.lt) goto loc_825F8278;
loc_825F8220:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8278
	if (cr6.eq) goto loc_825F8278;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8268
	if (!cr0.lt) goto loc_825F8268;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8268:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8220
	if (cr6.gt) goto loc_825F8220;
loc_825F8278:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f82b4
	if (!cr0.lt) goto loc_825F82B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F82B4:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x825f8368
	if (!cr6.eq) goto loc_825F8368;
	// stw r25,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r25.u32);
	// b 0x825f8500
	goto loc_825F8500;
loc_825F82C4:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f8324
	if (!cr6.lt) goto loc_825F8324;
loc_825F82CC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8324
	if (cr6.eq) goto loc_825F8324;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8314
	if (!cr0.lt) goto loc_825F8314;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8314:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f82cc
	if (cr6.gt) goto loc_825F82CC;
loc_825F8324:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8360
	if (!cr0.lt) goto loc_825F8360;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8360:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x825f8370
	if (!cr6.eq) goto loc_825F8370;
loc_825F8368:
	// stw r26,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r26.u32);
	// b 0x825f8500
	goto loc_825F8500;
loc_825F8370:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f83e4
	if (!cr6.lt) goto loc_825F83E4;
loc_825F838C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f83e4
	if (cr6.eq) goto loc_825F83E4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f83d4
	if (!cr0.lt) goto loc_825F83D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F83D4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f838c
	if (cr6.gt) goto loc_825F838C;
loc_825F83E4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8420
	if (!cr0.lt) goto loc_825F8420;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8420:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x825f8430
	if (!cr6.eq) goto loc_825F8430;
	// stw r25,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r25.u32);
	// b 0x825f8500
	goto loc_825F8500;
loc_825F8430:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f84a4
	if (!cr6.lt) goto loc_825F84A4;
loc_825F844C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f84a4
	if (cr6.eq) goto loc_825F84A4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8494
	if (!cr0.lt) goto loc_825F8494;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8494:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f844c
	if (cr6.gt) goto loc_825F844C;
loc_825F84A4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f84e0
	if (!cr0.lt) goto loc_825F84E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F84E0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x825f84f8
	if (!cr6.eq) goto loc_825F84F8;
	// lwz r3,15204(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 15204);
	// stw r25,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r25.u32);
	// bl 0x82390038
	sub_82390038(ctx, base);
	// b 0x825f8500
	goto loc_825F8500;
loc_825F84F8:
	// mr r27,r24
	r27.u64 = r24.u64;
	// stw r24,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r24.u32);
loc_825F8500:
	// lwz r11,284(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825f8800
	if (!cr6.eq) goto loc_825F8800;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825f8580
	if (!cr6.lt) goto loc_825F8580;
loc_825F8528:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8580
	if (cr6.eq) goto loc_825F8580;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8570
	if (!cr0.lt) goto loc_825F8570;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8570:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8528
	if (cr6.gt) goto loc_825F8528;
loc_825F8580:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f85bc
	if (!cr0.lt) goto loc_825F85BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F85BC:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x825f86e4
	if (!cr6.eq) goto loc_825F86E4;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x825f8638
	if (!cr6.lt) goto loc_825F8638;
loc_825F85E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8638
	if (cr6.eq) goto loc_825F8638;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8628
	if (!cr0.lt) goto loc_825F8628;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8628:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f85e0
	if (cr6.gt) goto loc_825F85E0;
loc_825F8638:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8674
	if (!cr0.lt) goto loc_825F8674;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8674:
	// cmpwi cr6,r30,14
	cr6.compare<int32_t>(r30.s32, 14, xer);
	// beq cr6,0x825f9d3c
	if (cr6.eq) goto loc_825F9D3C;
	// cmpwi cr6,r30,15
	cr6.compare<int32_t>(r30.s32, 15, xer);
	// bne cr6,0x825f868c
	if (!cr6.eq) goto loc_825F868C;
	// stw r24,3400(r28)
	PPC_STORE_U32(r28.u32 + 3400, r24.u32);
	// b 0x825f8800
	goto loc_825F8800;
loc_825F868C:
	// addi r11,r30,112
	r11.s64 = r30.s64 + 112;
	// lwz r7,14772(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 14772);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// addi r11,r11,-112
	r11.s64 = r11.s64 + -112;
	// addi r10,r10,4760
	ctx.r10.s64 = ctx.r10.s64 + 4760;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// addi r9,r9,4816
	ctx.r9.s64 = ctx.r9.s64 + 4816;
	// addi r8,r8,28552
	ctx.r8.s64 = ctx.r8.s64 + 28552;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r10,3392(r28)
	PPC_STORE_U32(r28.u32 + 3392, ctx.r10.u32);
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r11,3388(r28)
	PPC_STORE_U32(r28.u32 + 3388, r11.u32);
	// lwz r11,-4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// stw r11,14776(r28)
	PPC_STORE_U32(r28.u32 + 14776, r11.u32);
	// bne cr6,0x825f8800
	if (!cr6.eq) goto loc_825F8800;
	// stw r24,14772(r28)
	PPC_STORE_U32(r28.u32 + 14772, r24.u32);
	// b 0x825f8800
	goto loc_825F8800;
loc_825F86E4:
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// lwz r7,14772(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 14772);
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,4704
	ctx.r10.s64 = ctx.r10.s64 + 4704;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// addi r9,r9,4732
	ctx.r9.s64 = ctx.r9.s64 + 4732;
	// addi r8,r8,28552
	ctx.r8.s64 = ctx.r8.s64 + 28552;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r10,3392(r28)
	PPC_STORE_U32(r28.u32 + 3392, ctx.r10.u32);
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r11,3388(r28)
	PPC_STORE_U32(r28.u32 + 3388, r11.u32);
	// lwz r11,-4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// stw r11,14776(r28)
	PPC_STORE_U32(r28.u32 + 14776, r11.u32);
	// bne cr6,0x825f8800
	if (!cr6.eq) goto loc_825F8800;
	// stw r24,14772(r28)
	PPC_STORE_U32(r28.u32 + 14772, r24.u32);
	// b 0x825f8800
	goto loc_825F8800;
loc_825F8734:
	// addi r11,r11,-5
	r11.s64 = r11.s64 + -5;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825f8760
	if (!cr6.eq) goto loc_825F8760;
	// mr r30,r25
	r30.u64 = r25.u64;
	// b 0x825f87fc
	goto loc_825F87FC;
loc_825F8760:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825f87c0
	if (!cr6.gt) goto loc_825F87C0;
loc_825F8768:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f87c0
	if (cr6.eq) goto loc_825F87C0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f87b0
	if (!cr0.lt) goto loc_825F87B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F87B0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8768
	if (cr6.gt) goto loc_825F8768;
loc_825F87C0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f87fc
	if (!cr0.lt) goto loc_825F87FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F87FC:
	// stw r30,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r30.u32);
loc_825F8800:
	// lwz r11,3400(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3400);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f8814
	if (cr6.eq) goto loc_825F8814;
	// li r11,4
	r11.s64 = 4;
	// stw r11,284(r28)
	PPC_STORE_U32(r28.u32 + 284, r11.u32);
loc_825F8814:
	// lwz r11,284(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f8838
	if (cr6.eq) goto loc_825F8838;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825f8838
	if (cr6.eq) goto loc_825F8838;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825f8838
	if (cr6.eq) goto loc_825F8838;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825f9d3c
	if (!cr6.eq) goto loc_825F9D3C;
loc_825F8838:
	// lwz r10,15472(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x825f8854
	if (!cr6.eq) goto loc_825F8854;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f8854
	if (cr6.eq) goto loc_825F8854;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825f9d3c
	if (!cr6.eq) goto loc_825F9D3C;
loc_825F8854:
	// cmpwi cr6,r10,5
	cr6.compare<int32_t>(ctx.r10.s32, 5, xer);
	// blt cr6,0x825f88fc
	if (cr6.lt) goto loc_825F88FC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f886c
	if (cr6.eq) goto loc_825F886C;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825f88fc
	if (!cr6.eq) goto loc_825F88FC;
loc_825F886C:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,7
	r30.s64 = 7;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,7
	cr6.compare<uint32_t>(r11.u32, 7, xer);
	// bge cr6,0x825f88c4
	if (!cr6.lt) goto loc_825F88C4;
loc_825F8884:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f88c4
	if (cr6.eq) goto loc_825F88C4;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825f88b4
	if (!cr0.lt) goto loc_825F88B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F88B4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8884
	if (cr6.gt) goto loc_825F8884;
loc_825F88C4:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825f88ec
	if (!cr0.lt) goto loc_825F88EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F88EC:
	// lwz r11,84(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f9d3c
	if (!cr6.eq) goto loc_825F9D3C;
loc_825F88FC:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825f8970
	if (!cr6.lt) goto loc_825F8970;
loc_825F8918:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8970
	if (cr6.eq) goto loc_825F8970;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8960
	if (!cr0.lt) goto loc_825F8960;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8960:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8918
	if (cr6.gt) goto loc_825F8918;
loc_825F8970:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f89ac
	if (!cr0.lt) goto loc_825F89AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F89AC:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r11,r30
	r11.u64 = r30.u64;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825f9d3c
	if (!cr6.eq) goto loc_825F9D3C;
	// lwz r10,15472(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r10,6
	cr6.compare<int32_t>(ctx.r10.s32, 6, xer);
	// blt cr6,0x825f8c70
	if (cr6.lt) goto loc_825F8C70;
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// stw r30,3952(r28)
	PPC_STORE_U32(r28.u32 + 3952, r30.u32);
	// bgt cr6,0x825f8a8c
	if (cr6.gt) goto loc_825F8A8C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f8a48
	if (!cr6.lt) goto loc_825F8A48;
loc_825F89F0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8a48
	if (cr6.eq) goto loc_825F8A48;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8a38
	if (!cr0.lt) goto loc_825F8A38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8A38:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f89f0
	if (cr6.gt) goto loc_825F89F0;
loc_825F8A48:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8a84
	if (!cr0.lt) goto loc_825F8A84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8A84:
	// stw r30,252(r28)
	PPC_STORE_U32(r28.u32 + 252, r30.u32);
	// b 0x825f8a90
	goto loc_825F8A90;
loc_825F8A8C:
	// stw r25,252(r28)
	PPC_STORE_U32(r28.u32 + 252, r25.u32);
loc_825F8A90:
	// lwz r11,3440(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3440);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f8b50
	if (cr6.eq) goto loc_825F8B50;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f8b10
	if (!cr6.lt) goto loc_825F8B10;
loc_825F8AB8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8b10
	if (cr6.eq) goto loc_825F8B10;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8b00
	if (!cr0.lt) goto loc_825F8B00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8B00:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8ab8
	if (cr6.gt) goto loc_825F8AB8;
loc_825F8B10:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8b4c
	if (!cr0.lt) goto loc_825F8B4C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8B4C:
	// stw r30,3428(r28)
	PPC_STORE_U32(r28.u32 + 3428, r30.u32);
loc_825F8B50:
	// lwz r11,3432(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3432);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,3952(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3952);
	// bne cr6,0x825f8b88
	if (!cr6.eq) goto loc_825F8B88;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bgt cr6,0x825f8b70
	if (cr6.gt) goto loc_825F8B70;
	// stw r24,3428(r28)
	PPC_STORE_U32(r28.u32 + 3428, r24.u32);
	// b 0x825f8b88
	goto loc_825F8B88;
loc_825F8B70:
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// stw r25,3428(r28)
	PPC_STORE_U32(r28.u32 + 3428, r25.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,4872
	ctx.r10.s64 = ctx.r10.s64 + 4872;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
loc_825F8B88:
	// lwz r10,2972(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 2972);
	// stw r11,248(r28)
	PPC_STORE_U32(r28.u32 + 248, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r25,2968(r28)
	PPC_STORE_U32(r28.u32 + 2968, r25.u32);
	// beq cr6,0x825f8be8
	if (cr6.eq) goto loc_825F8BE8;
	// lwz r10,284(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 284);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bne cr6,0x825f8bb4
	if (!cr6.eq) goto loc_825F8BB4;
	// lwz r9,3400(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 3400);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825f8be8
	if (cr6.eq) goto loc_825F8BE8;
loc_825F8BB4:
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// blt cr6,0x825f8bc4
	if (cr6.lt) goto loc_825F8BC4;
	// stw r24,2968(r28)
	PPC_STORE_U32(r28.u32 + 2968, r24.u32);
	// b 0x825f8be8
	goto loc_825F8BE8;
loc_825F8BC4:
	// lwz r9,20056(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 20056);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825f8be8
	if (cr6.eq) goto loc_825F8BE8;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825f8be0
	if (cr6.eq) goto loc_825F8BE0;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x825f8be8
	if (!cr6.eq) goto loc_825F8BE8;
loc_825F8BE0:
	// li r10,7
	ctx.r10.s64 = 7;
	// stw r10,2968(r28)
	PPC_STORE_U32(r28.u32 + 2968, ctx.r10.u32);
loc_825F8BE8:
	// lwz r10,2968(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 2968);
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825f8c1c
	if (cr6.eq) goto loc_825F8C1C;
	// lwz r10,1900(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1900);
	// sth r25,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, r25.u16);
	// lwz r10,1900(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1900);
	// sth r25,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r25.u16);
	// lwz r10,1904(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1904);
	// sth r25,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, r25.u16);
	// lwz r10,1904(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1904);
	// sth r25,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r25.u16);
	// b 0x825f8c40
	goto loc_825F8C40;
loc_825F8C1C:
	// lwz r9,1900(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1900);
	// li r10,128
	ctx.r10.s64 = 128;
	// sth r10,16(r9)
	PPC_STORE_U16(ctx.r9.u32 + 16, ctx.r10.u16);
	// lwz r9,1900(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1900);
	// sth r10,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r10.u16);
	// lwz r9,1904(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1904);
	// sth r10,16(r9)
	PPC_STORE_U16(ctx.r9.u32 + 16, ctx.r10.u16);
	// lwz r9,1904(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1904);
	// sth r10,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r10.u16);
loc_825F8C40:
	// lwz r9,3428(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 3428);
	// addi r10,r28,3988
	ctx.r10.s64 = r28.s64 + 3988;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825f8c54
	if (!cr6.eq) goto loc_825F8C54;
	// addi r10,r28,5268
	ctx.r10.s64 = r28.s64 + 5268;
loc_825F8C54:
	// stw r10,6548(r28)
	PPC_STORE_U32(r28.u32 + 6548, ctx.r10.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r10,r28,6560
	ctx.r10.s64 = r28.s64 + 6560;
	// bne cr6,0x825f8c68
	if (!cr6.eq) goto loc_825F8C68;
	// addi r10,r28,10656
	ctx.r10.s64 = r28.s64 + 10656;
loc_825F8C68:
	// stw r10,14752(r28)
	PPC_STORE_U32(r28.u32 + 14752, ctx.r10.u32);
	// b 0x825f8c80
	goto loc_825F8C80;
loc_825F8C70:
	// addi r10,r28,5268
	ctx.r10.s64 = r28.s64 + 5268;
	// addi r9,r28,10656
	ctx.r9.s64 = r28.s64 + 10656;
	// stw r10,6548(r28)
	PPC_STORE_U32(r28.u32 + 6548, ctx.r10.u32);
	// stw r9,14752(r28)
	PPC_STORE_U32(r28.u32 + 14752, ctx.r9.u32);
loc_825F8C80:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,248(r28)
	PPC_STORE_U32(r28.u32 + 248, r11.u32);
	// ble cr6,0x825f9d3c
	if (!cr6.gt) goto loc_825F9D3C;
	// cmpwi cr6,r11,31
	cr6.compare<int32_t>(r11.s32, 31, xer);
	// bgt cr6,0x825f9d3c
	if (cr6.gt) goto loc_825F9D3C;
	// lwz r11,3952(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3952);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bgt cr6,0x825f8cbc
	if (cr6.gt) goto loc_825F8CBC;
	// lwz r11,15472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x825f8cbc
	if (!cr6.eq) goto loc_825F8CBC;
	// addi r11,r28,2840
	r11.s64 = r28.s64 + 2840;
	// addi r10,r28,2800
	ctx.r10.s64 = r28.s64 + 2800;
	// stw r11,2904(r28)
	PPC_STORE_U32(r28.u32 + 2904, r11.u32);
	// stw r10,2916(r28)
	PPC_STORE_U32(r28.u32 + 2916, ctx.r10.u32);
loc_825F8CBC:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x825f8d84
	if (cr6.eq) goto loc_825F8D84;
	// lwz r11,15192(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15192);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f8d84
	if (cr6.eq) goto loc_825F8D84;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f8d44
	if (!cr6.lt) goto loc_825F8D44;
loc_825F8CEC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8d44
	if (cr6.eq) goto loc_825F8D44;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8d34
	if (!cr0.lt) goto loc_825F8D34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8D34:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8cec
	if (cr6.gt) goto loc_825F8CEC;
loc_825F8D44:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8d80
	if (!cr0.lt) goto loc_825F8D80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8D80:
	// stw r30,15196(r28)
	PPC_STORE_U32(r28.u32 + 15196, r30.u32);
loc_825F8D84:
	// lwz r11,20864(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 20864);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f8fdc
	if (cr6.eq) goto loc_825F8FDC;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f8e04
	if (!cr6.lt) goto loc_825F8E04;
loc_825F8DAC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8e04
	if (cr6.eq) goto loc_825F8E04;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8df4
	if (!cr0.lt) goto loc_825F8DF4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8DF4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8dac
	if (cr6.gt) goto loc_825F8DAC;
loc_825F8E04:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8e40
	if (!cr0.lt) goto loc_825F8E40;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8E40:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,404(r28)
	PPC_STORE_U32(r28.u32 + 404, r30.u32);
	// beq cr6,0x825f8f08
	if (cr6.eq) goto loc_825F8F08;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f8ec0
	if (!cr6.lt) goto loc_825F8EC0;
loc_825F8E68:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8ec0
	if (cr6.eq) goto loc_825F8EC0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8eb0
	if (!cr0.lt) goto loc_825F8EB0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8EB0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8e68
	if (cr6.gt) goto loc_825F8E68;
loc_825F8EC0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8efc
	if (!cr0.lt) goto loc_825F8EFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8EFC:
	// lwz r11,404(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 404);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,404(r28)
	PPC_STORE_U32(r28.u32 + 404, r11.u32);
loc_825F8F08:
	// lwz r11,404(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 404);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825f8fd0
	if (!cr6.eq) goto loc_825F8FD0;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f8f88
	if (!cr6.lt) goto loc_825F8F88;
loc_825F8F30:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f8f88
	if (cr6.eq) goto loc_825F8F88;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f8f78
	if (!cr0.lt) goto loc_825F8F78;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8F78:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f8f30
	if (cr6.gt) goto loc_825F8F30;
loc_825F8F88:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f8fc4
	if (!cr0.lt) goto loc_825F8FC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F8FC4:
	// lwz r11,404(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 404);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,404(r28)
	PPC_STORE_U32(r28.u32 + 404, r11.u32);
loc_825F8FD0:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r4,404(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 404);
	// bl 0x82603a38
	sub_82603A38(ctx, base);
loc_825F8FDC:
	// lwz r11,284(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825f9130
	if (cr6.eq) goto loc_825F9130;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825f9130
	if (cr6.eq) goto loc_825F9130;
	// lwz r11,15472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x825f9130
	if (!cr6.eq) goto loc_825F9130;
	// lwz r11,14820(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 14820);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f9130
	if (cr6.eq) goto loc_825F9130;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r11,3924(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825f908c
	if (cr6.eq) goto loc_825F908C;
	// mr r30,r24
	r30.u64 = r24.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f90f0
	if (!cr6.lt) goto loc_825F90F0;
loc_825F9030:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f90f0
	if (cr6.eq) goto loc_825F90F0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9078
	if (!cr0.lt) goto loc_825F9078;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9078:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9030
	if (cr6.gt) goto loc_825F9030;
	// b 0x825f90f0
	goto loc_825F90F0;
loc_825F908C:
	// mr r30,r26
	r30.u64 = r26.u64;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825f90f0
	if (!cr6.lt) goto loc_825F90F0;
loc_825F9098:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f90f0
	if (cr6.eq) goto loc_825F90F0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f90e0
	if (!cr0.lt) goto loc_825F90E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F90E0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9098
	if (cr6.gt) goto loc_825F9098;
loc_825F90F0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// bge 0x825f912c
	if (!cr0.lt) goto loc_825F912C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F912C:
	// stw r30,14824(r28)
	PPC_STORE_U32(r28.u32 + 14824, r30.u32);
loc_825F9130:
	// lwz r4,14824(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 14824);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x825f914c
	if (cr6.eq) goto loc_825F914C;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
loc_825F914C:
	// lwz r11,3924(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f92c0
	if (cr6.eq) goto loc_825F92C0;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f91cc
	if (!cr6.lt) goto loc_825F91CC;
loc_825F9174:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f91cc
	if (cr6.eq) goto loc_825F91CC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f91bc
	if (!cr0.lt) goto loc_825F91BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F91BC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9174
	if (cr6.gt) goto loc_825F9174;
loc_825F91CC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f9208
	if (!cr0.lt) goto loc_825F9208;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9208:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825f9284
	if (cr6.eq) goto loc_825F9284;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82654080
	sub_82654080(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f9d40
	if (!cr6.eq) goto loc_825F9D40;
	// lwz r11,348(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 348);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f92c0
	if (cr6.eq) goto loc_825F92C0;
	// lwz r11,144(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 144);
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825f92c0
	if (!cr6.gt) goto loc_825F92C0;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_825F924C:
	// lwz r11,268(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 268);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// rlwimi r7,r8,10,22,22
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 10) & 0x200) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFDFF);
	// rlwinm r8,r7,0,24,22
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFFFFFFEFF;
	// rlwinm r8,r8,0,22,20
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFFBFF;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r11,144(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 144);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// blt cr6,0x825f924c
	if (cr6.lt) goto loc_825F924C;
	// b 0x825f92c0
	goto loc_825F92C0;
loc_825F9284:
	// lwz r11,144(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 144);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825f92c0
	if (!cr6.gt) goto loc_825F92C0;
	// mr r11,r25
	r11.u64 = r25.u64;
loc_825F9298:
	// lwz r9,268(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 268);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r8,r8,0,24,20
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFF8FF;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r9,144(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 144);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825f9298
	if (cr6.lt) goto loc_825F9298;
loc_825F92C0:
	// lwz r11,284(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f96f4
	if (cr6.eq) goto loc_825F96F4;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x825f96f4
	if (cr6.eq) goto loc_825F96F4;
	// lwz r11,15472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x825f92f8
	if (!cr6.eq) goto loc_825F92F8;
	// bl 0x825f51c0
	sub_825F51C0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825f92fc
	if (cr6.eq) goto loc_825F92FC;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825F92F8:
	// bl 0x825f3140
	sub_825F3140(ctx, base);
loc_825F92FC:
	// lwz r11,15472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// blt cr6,0x825f96d8
	if (cr6.lt) goto loc_825F96D8;
	// lwz r11,396(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f93c8
	if (cr6.eq) goto loc_825F93C8;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f9388
	if (!cr6.lt) goto loc_825F9388;
loc_825F9330:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f9388
	if (cr6.eq) goto loc_825F9388;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9378
	if (!cr0.lt) goto loc_825F9378;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9378:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9330
	if (cr6.gt) goto loc_825F9330;
loc_825F9388:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f93c4
	if (!cr0.lt) goto loc_825F93C4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F93C4:
	// stw r30,392(r28)
	PPC_STORE_U32(r28.u32 + 392, r30.u32);
loc_825F93C8:
	// lwz r11,392(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 392);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f954c
	if (!cr6.eq) goto loc_825F954C;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f9448
	if (!cr6.lt) goto loc_825F9448;
loc_825F93F0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f9448
	if (cr6.eq) goto loc_825F9448;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9438
	if (!cr0.lt) goto loc_825F9438;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9438:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f93f0
	if (cr6.gt) goto loc_825F93F0;
loc_825F9448:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f9484
	if (!cr0.lt) goto loc_825F9484;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9484:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,2928(r28)
	PPC_STORE_U32(r28.u32 + 2928, r30.u32);
	// beq cr6,0x825f954c
	if (cr6.eq) goto loc_825F954C;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f9504
	if (!cr6.lt) goto loc_825F9504;
loc_825F94AC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f9504
	if (cr6.eq) goto loc_825F9504;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f94f4
	if (!cr0.lt) goto loc_825F94F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F94F4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f94ac
	if (cr6.gt) goto loc_825F94AC;
loc_825F9504:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f9540
	if (!cr0.lt) goto loc_825F9540;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9540:
	// lwz r11,2928(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 2928);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,2928(r28)
	PPC_STORE_U32(r28.u32 + 2928, r11.u32);
loc_825F954C:
	// lwz r11,2928(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 2928);
	// mr r30,r24
	r30.u64 = r24.u64;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r29,r25
	r29.u64 = r25.u64;
	// stw r11,2936(r28)
	PPC_STORE_U32(r28.u32 + 2936, r11.u32);
	// stw r11,2932(r28)
	PPC_STORE_U32(r28.u32 + 2932, r11.u32);
	// stw r11,2948(r28)
	PPC_STORE_U32(r28.u32 + 2948, r11.u32);
	// stw r11,2944(r28)
	PPC_STORE_U32(r28.u32 + 2944, r11.u32);
	// stw r11,2940(r28)
	PPC_STORE_U32(r28.u32 + 2940, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f95d8
	if (!cr6.lt) goto loc_825F95D8;
loc_825F9580:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f95d8
	if (cr6.eq) goto loc_825F95D8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f95c8
	if (!cr0.lt) goto loc_825F95C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F95C8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9580
	if (cr6.gt) goto loc_825F9580;
loc_825F95D8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f9614
	if (!cr0.lt) goto loc_825F9614;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9614:
	// lwz r11,15472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// stw r30,2088(r28)
	PPC_STORE_U32(r28.u32 + 2088, r30.u32);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bge cr6,0x825f96d8
	if (!cr6.lt) goto loc_825F96D8;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f9698
	if (!cr6.lt) goto loc_825F9698;
loc_825F9640:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f9698
	if (cr6.eq) goto loc_825F9698;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9688
	if (!cr0.lt) goto loc_825F9688;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9688:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9640
	if (cr6.gt) goto loc_825F9640;
loc_825F9698:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f96d4
	if (!cr0.lt) goto loc_825F96D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F96D4:
	// stw r30,2036(r28)
	PPC_STORE_U32(r28.u32 + 2036, r30.u32);
loc_825F96D8:
	// lwz r11,284(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825f9d20
	if (!cr6.eq) goto loc_825F9D20;
	// lwz r11,3904(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3904);
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// stw r11,3904(r28)
	PPC_STORE_U32(r28.u32 + 3904, r11.u32);
	// b 0x825f9d20
	goto loc_825F9D20;
loc_825F96F4:
	// lwz r11,15472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bge cr6,0x825f97b4
	if (!cr6.lt) goto loc_825F97B4;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825f9774
	if (!cr6.lt) goto loc_825F9774;
loc_825F971C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f9774
	if (cr6.eq) goto loc_825F9774;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9764
	if (!cr0.lt) goto loc_825F9764;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9764:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f971c
	if (cr6.gt) goto loc_825F971C;
loc_825F9774:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f97b0
	if (!cr0.lt) goto loc_825F97B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F97B0:
	// stw r30,15464(r28)
	PPC_STORE_U32(r28.u32 + 15464, r30.u32);
loc_825F97B4:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r4,15464(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 15464);
	// bl 0x825f5000
	sub_825F5000(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f9d40
	if (!cr6.eq) goto loc_825F9D40;
	// lwz r9,15472(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r9,4
	cr6.compare<int32_t>(ctx.r9.s32, 4, xer);
	// blt cr6,0x825f9850
	if (cr6.lt) goto loc_825F9850;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825ebe10
	sub_825EBE10(ctx, base);
	// lwz r11,84(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f9d40
	if (!cr6.eq) goto loc_825F9D40;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825f9d40
	if (!cr6.eq) goto loc_825F9D40;
	// lwz r9,15472(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r9,5
	cr6.compare<int32_t>(ctx.r9.s32, 5, xer);
	// blt cr6,0x825f9808
	if (cr6.lt) goto loc_825F9808;
	// stw r25,400(r28)
	PPC_STORE_U32(r28.u32 + 400, r25.u32);
	// b 0x825f9850
	goto loc_825F9850;
loc_825F9808:
	// lwz r10,3660(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 3660);
	// mr r11,r24
	r11.u64 = r24.u64;
	// cmpwi cr6,r10,50
	cr6.compare<int32_t>(ctx.r10.s32, 50, xer);
	// bgt cr6,0x825f981c
	if (cr6.gt) goto loc_825F981C;
	// mr r11,r25
	r11.u64 = r25.u64;
loc_825F981C:
	// cmpwi cr6,r10,128
	cr6.compare<int32_t>(ctx.r10.s32, 128, xer);
	// stw r11,396(r28)
	PPC_STORE_U32(r28.u32 + 396, r11.u32);
	// bgt cr6,0x825f9848
	if (cr6.gt) goto loc_825F9848;
	// lwz r11,160(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 160);
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lwz r10,156(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 156);
	// ori r8,r8,11264
	ctx.r8.u64 = ctx.r8.u64 | 11264;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// mr r11,r24
	r11.u64 = r24.u64;
	// blt cr6,0x825f984c
	if (cr6.lt) goto loc_825F984C;
loc_825F9848:
	// mr r11,r25
	r11.u64 = r25.u64;
loc_825F984C:
	// stw r11,400(r28)
	PPC_STORE_U32(r28.u32 + 400, r11.u32);
loc_825F9850:
	// lwz r11,3948(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3948);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f9ce0
	if (!cr6.eq) goto loc_825F9CE0;
	// cmpwi cr6,r9,3
	cr6.compare<int32_t>(ctx.r9.s32, 3, xer);
	// blt cr6,0x825f9ce0
	if (cr6.lt) goto loc_825F9CE0;
	// lwz r11,396(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f9924
	if (cr6.eq) goto loc_825F9924;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f98e4
	if (!cr6.lt) goto loc_825F98E4;
loc_825F988C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f98e4
	if (cr6.eq) goto loc_825F98E4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f98d4
	if (!cr0.lt) goto loc_825F98D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F98D4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f988c
	if (cr6.gt) goto loc_825F988C;
loc_825F98E4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f9920
	if (!cr0.lt) goto loc_825F9920;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9920:
	// stw r30,392(r28)
	PPC_STORE_U32(r28.u32 + 392, r30.u32);
loc_825F9924:
	// lwz r11,392(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 392);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f9c2c
	if (!cr6.eq) goto loc_825F9C2C;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f99a4
	if (!cr6.lt) goto loc_825F99A4;
loc_825F994C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f99a4
	if (cr6.eq) goto loc_825F99A4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9994
	if (!cr0.lt) goto loc_825F9994;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9994:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f994c
	if (cr6.gt) goto loc_825F994C;
loc_825F99A4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f99e0
	if (!cr0.lt) goto loc_825F99E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F99E0:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,2928(r28)
	PPC_STORE_U32(r28.u32 + 2928, r30.u32);
	// beq cr6,0x825f9aa8
	if (cr6.eq) goto loc_825F9AA8;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f9a60
	if (!cr6.lt) goto loc_825F9A60;
loc_825F9A08:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f9a60
	if (cr6.eq) goto loc_825F9A60;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9a50
	if (!cr0.lt) goto loc_825F9A50;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9A50:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9a08
	if (cr6.gt) goto loc_825F9A08;
loc_825F9A60:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f9a9c
	if (!cr0.lt) goto loc_825F9A9C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9A9C:
	// lwz r11,2928(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 2928);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,2928(r28)
	PPC_STORE_U32(r28.u32 + 2928, r11.u32);
loc_825F9AA8:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f9b1c
	if (!cr6.lt) goto loc_825F9B1C;
loc_825F9AC4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f9b1c
	if (cr6.eq) goto loc_825F9B1C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9b0c
	if (!cr0.lt) goto loc_825F9B0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9B0C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9ac4
	if (cr6.gt) goto loc_825F9AC4;
loc_825F9B1C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f9b58
	if (!cr0.lt) goto loc_825F9B58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9B58:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,2940(r28)
	PPC_STORE_U32(r28.u32 + 2940, r30.u32);
	// beq cr6,0x825f9c20
	if (cr6.eq) goto loc_825F9C20;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f9bd8
	if (!cr6.lt) goto loc_825F9BD8;
loc_825F9B80:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f9bd8
	if (cr6.eq) goto loc_825F9BD8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9bc8
	if (!cr0.lt) goto loc_825F9BC8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9BC8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9b80
	if (cr6.gt) goto loc_825F9B80;
loc_825F9BD8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f9c14
	if (!cr0.lt) goto loc_825F9C14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9C14:
	// lwz r11,2940(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 2940);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,2940(r28)
	PPC_STORE_U32(r28.u32 + 2940, r11.u32);
loc_825F9C20:
	// lwz r11,2940(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 2940);
	// stw r11,2948(r28)
	PPC_STORE_U32(r28.u32 + 2948, r11.u32);
	// stw r11,2944(r28)
	PPC_STORE_U32(r28.u32 + 2944, r11.u32);
loc_825F9C2C:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r30,r24
	r30.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825f9ca0
	if (!cr6.lt) goto loc_825F9CA0;
loc_825F9C48:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f9ca0
	if (cr6.eq) goto loc_825F9CA0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9c90
	if (!cr0.lt) goto loc_825F9C90;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9C90:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9c48
	if (cr6.gt) goto loc_825F9C48;
loc_825F9CA0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f9cdc
	if (!cr0.lt) goto loc_825F9CDC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9CDC:
	// stw r30,2088(r28)
	PPC_STORE_U32(r28.u32 + 2088, r30.u32);
loc_825F9CE0:
	// lwz r11,20056(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 20056);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825f9cf8
	if (!cr6.eq) goto loc_825F9CF8;
	// lwz r11,3924(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f9d14
	if (cr6.eq) goto loc_825F9D14;
loc_825F9CF8:
	// lwz r11,3980(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825f9d14
	if (cr6.eq) goto loc_825F9D14;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825ee6e8
	sub_825EE6E8(ctx, base);
	// b 0x825f9d1c
	goto loc_825F9D1C;
loc_825F9D14:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x826179f8
	sub_826179F8(ctx, base);
loc_825F9D1C:
	// stw r24,3904(r28)
	PPC_STORE_U32(r28.u32 + 3904, r24.u32);
loc_825F9D20:
	// lwz r11,84(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r3,r11,0,29,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825F9D3C:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825F9D40:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825F9D48"))) PPC_WEAK_FUNC(sub_825F9D48);
PPC_FUNC_IMPL(__imp__sub_825F9D48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// subfic r11,r6,64
	xer.ca = ctx.r6.u32 <= 64;
	r11.s64 = 64 - ctx.r6.s64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r11,r5
	r30.u64 = r11.u64 + ctx.r5.u64;
	// lbz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825f9d98
	if (!cr6.eq) goto loc_825F9D98;
	// li r10,3
	ctx.r10.s64 = 3;
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
loc_825F9D98:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// li r26,0
	r26.s64 = 0;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825f9e7c
	if (!cr6.eq) goto loc_825F9E7C;
	// extsb r27,r11
	r27.s64 = r11.s8;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x825f9ee0
	if (cr6.eq) goto loc_825F9EE0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r30,r27
	r30.u64 = r27.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825f9e2c
	if (!cr6.gt) goto loc_825F9E2C;
loc_825F9DD4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825f9e2c
	if (cr6.eq) goto loc_825F9E2C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825f9e1c
	if (!cr0.lt) goto loc_825F9E1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9E1C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825f9dd4
	if (cr6.gt) goto loc_825F9DD4;
loc_825F9E2C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825f9e68
	if (!cr0.lt) goto loc_825F9E68;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9E68:
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825f9e94
	if (cr6.eq) goto loc_825F9E94;
loc_825F9E7C:
	// li r11,4
	r11.s64 = 4;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// lwz r11,1760(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1760);
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825F9E94:
	// addi r9,r27,-1
	ctx.r9.s64 = r27.s64 + -1;
	// stw r26,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r26.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// slw r9,r10,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r9.u8 & 0x3F));
	// and r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 & r11.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825f9ec4
	if (cr6.eq) goto loc_825F9EC4;
	// lwz r10,1760(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1760);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825F9EC4:
	// slw r10,r10,r27
	ctx.r10.u64 = r27.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r27.u8 & 0x3F));
	// lwz r9,1760(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1760);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825F9EE0:
	// li r26,0
	r26.s64 = 0;
	// stw r26,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r26.u32);
	// lwz r11,1760(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1760);
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825F9EF8"))) PPC_WEAK_FUNC(sub_825F9EF8);
PPC_FUNC_IMPL(__imp__sub_825F9EF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// addi r31,r11,16080
	r31.s64 = r11.s64 + 16080;
	// lwz r30,84(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// rldicl r11,r11,3,61
	r11.u64 = __builtin_rotateleft64(r11.u64, 3) & 0x7;
	// rlwinm r27,r11,1,0,30
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r27,r31
	ctx.r4.u64 = PPC_LOAD_U8(r27.u32 + r31.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// li r31,3
	r31.s64 = 3;
	// lbzx r11,r27,r11
	r11.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825f9f48
	if (!cr6.eq) goto loc_825F9F48;
	// stw r31,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r31.u32);
loc_825F9F48:
	// lwz r3,84(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// clrlwi r25,r11,24
	r25.u64 = r11.u32 & 0xFF;
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fa058
	if (!cr6.eq) goto loc_825FA058;
	// cmplwi cr6,r25,3
	cr6.compare<uint32_t>(r25.u32, 3, xer);
	// bgt cr6,0x825fa058
	if (cr6.gt) goto loc_825FA058;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825f9f8c
	if (!cr0.lt) goto loc_825F9F8C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825F9F8C:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// clrlwi r10,r30,24
	ctx.r10.u64 = r30.u32 & 0xFF;
	// rlwimi r11,r10,3,27,28
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 3) & 0x18) | (r11.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// lwz r10,84(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fa058
	if (!cr6.eq) goto loc_825FA058;
	// clrlwi r10,r11,1
	ctx.r10.u64 = r11.u32 & 0x7FFFFFFF;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// rlwinm r10,r10,0,15,13
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// addi r27,r11,15952
	r27.s64 = r11.s64 + 15952;
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
	// lwz r30,84(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// rldicl r11,r11,6,58
	r11.u64 = __builtin_rotateleft64(r11.u64, 6) & 0x3F;
	// rlwinm r26,r11,1,0,30
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r26,r27
	ctx.r4.u64 = PPC_LOAD_U8(r26.u32 + r27.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r27,1
	r11.s64 = r27.s64 + 1;
	// lbzx r11,r26,r11
	r11.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825f9ff0
	if (!cr6.eq) goto loc_825F9FF0;
	// stw r31,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r31.u32);
loc_825F9FF0:
	// lwz r10,84(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// clrlwi r29,r11,24
	r29.u64 = r11.u32 & 0xFF;
	// lwz r11,20(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fa058
	if (!cr6.eq) goto loc_825FA058;
	// li r4,5
	ctx.r4.s64 = 5;
	// srawi r5,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	ctx.r5.s64 = r25.s32 >> 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// clrlwi r5,r25,31
	ctx.r5.u64 = r25.u32 & 0x1;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r30,1
	r30.s64 = 1;
loc_825FA028:
	// sraw r11,r29,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r29.s32 < 0) & (((r29.s32 >> temp.u32) << temp.u32) != r29.s32);
	r11.s64 = r29.s32 >> temp.u32;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// clrlwi r5,r11,31
	ctx.r5.u64 = r11.u32 & 0x1;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x825fa028
	if (!cr6.lt) goto loc_825FA028;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825FA058:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825FA064"))) PPC_WEAK_FUNC(sub_825FA064);
PPC_FUNC_IMPL(__imp__sub_825FA064) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FA068"))) PPC_WEAK_FUNC(sub_825FA068);
PPC_FUNC_IMPL(__imp__sub_825FA068) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lwz r11,448(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 448);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825fa100
	if (cr6.eq) goto loc_825FA100;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fa0b4
	if (!cr0.lt) goto loc_825FA0B4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FA0B4:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwimi r11,r31,31,0,0
	r11.u64 = (__builtin_rotateleft32(r31.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// lwz r10,84(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fa328
	if (!cr6.eq) goto loc_825FA328;
	// rlwinm r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fa10c
	if (cr6.eq) goto loc_825FA10C;
	// li r11,0
	r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,12(r27)
	PPC_STORE_U32(r27.u32 + 12, r11.u32);
	// sth r11,16(r27)
	PPC_STORE_U16(r27.u32 + 16, r11.u16);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825FA100:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// clrlwi r11,r11,1
	r11.u64 = r11.u32 & 0x7FFFFFFF;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
loc_825FA10C:
	// lwz r30,84(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r31,r11,16096
	r31.s64 = r11.s64 + 16096;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// rldicl r11,r11,7,57
	r11.u64 = __builtin_rotateleft64(r11.u64, 7) & 0x7F;
	// rlwinm r29,r11,1,0,30
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r29,r31
	ctx.r4.u64 = PPC_LOAD_U8(r29.u32 + r31.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// li r31,3
	r31.s64 = 3;
	// lbzx r11,r29,r11
	r11.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fa148
	if (!cr6.eq) goto loc_825FA148;
	// stw r31,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r31.u32);
loc_825FA148:
	// lwz r10,84(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fa328
	if (!cr6.eq) goto loc_825FA328;
	// cmplwi cr6,r11,7
	cr6.compare<uint32_t>(r11.u32, 7, xer);
	// bgt cr6,0x825fa328
	if (cr6.gt) goto loc_825FA328;
	// clrlwi r25,r11,30
	r25.u64 = r11.u32 & 0x3;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x825fa20c
	if (cr6.lt) goto loc_825FA20C;
	// bne cr6,0x825fa328
	if (!cr6.eq) goto loc_825FA328;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r11,r11,0,15,13
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fa1b0
	if (!cr0.lt) goto loc_825FA1B0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FA1B0:
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r10.u32);
	// lwz r30,84(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fa328
	if (!cr6.eq) goto loc_825FA328;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r29,r11,15952
	r29.s64 = r11.s64 + 15952;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// rldicl r11,r11,6,58
	r11.u64 = __builtin_rotateleft64(r11.u64, 6) & 0x3F;
	// rlwinm r28,r11,1,0,30
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r28,r29
	ctx.r4.u64 = PPC_LOAD_U8(r28.u32 + r29.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// lbzx r11,r28,r11
	r11.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fa204
	if (!cr6.eq) goto loc_825FA204;
	// stw r31,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r31.u32);
loc_825FA204:
	// clrlwi r29,r11,24
	r29.u64 = r11.u32 & 0xFF;
	// b 0x825fa2a4
	goto loc_825FA2A4;
loc_825FA20C:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r25,3
	cr6.compare<int32_t>(r25.s32, 3, xer);
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lwz r30,84(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// addi r29,r11,15952
	r29.s64 = r11.s64 + 15952;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// rldicl r11,r11,6,58
	r11.u64 = __builtin_rotateleft64(r11.u64, 6) & 0x3F;
	// rlwinm r28,r11,1,0,30
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r28,r29
	ctx.r4.u64 = PPC_LOAD_U8(r28.u32 + r29.u32);
	// bne cr6,0x825fa260
	if (!cr6.eq) goto loc_825FA260;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// lbzx r11,r28,r11
	r11.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fa258
	if (!cr6.eq) goto loc_825FA258;
	// stw r31,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r31.u32);
loc_825FA258:
	// clrlwi r29,r11,24
	r29.u64 = r11.u32 & 0xFF;
	// b 0x825fa294
	goto loc_825FA294;
loc_825FA260:
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// lbzx r11,r28,r11
	r11.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fa278
	if (!cr6.eq) goto loc_825FA278;
	// stw r31,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r31.u32);
loc_825FA278:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// subfic r29,r11,15
	xer.ca = r11.u32 <= 15;
	r29.s64 = 15 - r11.s64;
	// bne cr6,0x825fa294
	if (!cr6.eq) goto loc_825FA294;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// beq cr6,0x825fa298
	if (cr6.eq) goto loc_825FA298;
loc_825FA294:
	// li r11,0
	r11.s64 = 0;
loc_825FA298:
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwimi r10,r11,30,1,1
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 30) & 0x40000000) | (ctx.r10.u64 & 0xFFFFFFFFBFFFFFFF);
	// stw r10,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r10.u32);
loc_825FA2A4:
	// lwz r11,84(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fa328
	if (!cr6.eq) goto loc_825FA328;
	// cmplwi cr6,r29,15
	cr6.compare<uint32_t>(r29.u32, 15, xer);
	// bgt cr6,0x825fa328
	if (cr6.gt) goto loc_825FA328;
	// li r4,5
	ctx.r4.s64 = 5;
	// srawi r5,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	ctx.r5.s64 = r25.s32 >> 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// clrlwi r5,r25,31
	ctx.r5.u64 = r25.u32 & 0x1;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r30,1
	r30.s64 = 1;
loc_825FA2E0:
	// sraw r11,r29,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r29.s32 < 0) & (((r29.s32 >> temp.u32) << temp.u32) != r29.s32);
	r11.s64 = r29.s32 >> temp.u32;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// clrlwi r5,r11,31
	ctx.r5.u64 = r11.u32 & 0x1;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x825fa2e0
	if (!cr6.lt) goto loc_825FA2E0;
	// addi r3,r27,12
	ctx.r3.s64 = r27.s64 + 12;
	// bl 0x825ebbd0
	sub_825EBBD0(ctx, base);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwimi r11,r10,19,12,13
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 19) & 0xC0000) | (r11.u64 & 0xFFFFFFFFFFF3FFFF);
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825FA328:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825FA334"))) PPC_WEAK_FUNC(sub_825FA334);
PPC_FUNC_IMPL(__imp__sub_825FA334) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FA338"))) PPC_WEAK_FUNC(sub_825FA338);
PPC_FUNC_IMPL(__imp__sub_825FA338) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// addi r31,r11,16352
	r31.s64 = r11.s64 + 16352;
	// lwz r30,84(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// rldicl r11,r11,9,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 9) & 0x1FF;
	// rlwinm r27,r11,1,0,30
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r27,r31
	ctx.r4.u64 = PPC_LOAD_U8(r27.u32 + r31.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// li r31,3
	r31.s64 = 3;
	// lbzx r11,r27,r11
	r11.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fa388
	if (!cr6.eq) goto loc_825FA388;
	// stw r31,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r31.u32);
loc_825FA388:
	// lwz r3,84(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// clrlwi r25,r11,24
	r25.u64 = r11.u32 & 0xFF;
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fa498
	if (!cr6.eq) goto loc_825FA498;
	// cmplwi cr6,r25,20
	cr6.compare<uint32_t>(r25.u32, 20, xer);
	// bgt cr6,0x825fa498
	if (cr6.gt) goto loc_825FA498;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fa3cc
	if (!cr0.lt) goto loc_825FA3CC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FA3CC:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// clrlwi r10,r30,24
	ctx.r10.u64 = r30.u32 & 0xFF;
	// rlwimi r11,r10,3,27,28
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 3) & 0x18) | (r11.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// lwz r10,84(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fa498
	if (!cr6.eq) goto loc_825FA498;
	// clrlwi r10,r11,1
	ctx.r10.u64 = r11.u32 & 0x7FFFFFFF;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// rlwinm r10,r10,0,15,13
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// addi r27,r11,15952
	r27.s64 = r11.s64 + 15952;
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
	// lwz r30,84(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// rldicl r11,r11,6,58
	r11.u64 = __builtin_rotateleft64(r11.u64, 6) & 0x3F;
	// rlwinm r26,r11,1,0,30
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r26,r27
	ctx.r4.u64 = PPC_LOAD_U8(r26.u32 + r27.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r27,1
	r11.s64 = r27.s64 + 1;
	// lbzx r11,r26,r11
	r11.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fa430
	if (!cr6.eq) goto loc_825FA430;
	// stw r31,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r31.u32);
loc_825FA430:
	// lwz r10,84(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// clrlwi r29,r11,24
	r29.u64 = r11.u32 & 0xFF;
	// lwz r11,20(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fa498
	if (!cr6.eq) goto loc_825FA498;
	// li r4,5
	ctx.r4.s64 = 5;
	// srawi r5,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	ctx.r5.s64 = r25.s32 >> 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// clrlwi r5,r25,31
	ctx.r5.u64 = r25.u32 & 0x1;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r30,1
	r30.s64 = 1;
loc_825FA468:
	// sraw r11,r29,r31
	temp.u32 = r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r29.s32 < 0) & (((r29.s32 >> temp.u32) << temp.u32) != r29.s32);
	r11.s64 = r29.s32 >> temp.u32;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// clrlwi r5,r11,31
	ctx.r5.u64 = r11.u32 & 0x1;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x825fa468
	if (!cr6.lt) goto loc_825FA468;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825FA498:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825FA4A4"))) PPC_WEAK_FUNC(sub_825FA4A4);
PPC_FUNC_IMPL(__imp__sub_825FA4A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FA4A8"))) PPC_WEAK_FUNC(sub_825FA4A8);
PPC_FUNC_IMPL(__imp__sub_825FA4A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fa4e8
	if (!cr0.lt) goto loc_825FA4E8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FA4E8:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwimi r11,r30,31,0,0
	r11.u64 = (__builtin_rotateleft32(r30.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r10,84(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fa734
	if (!cr6.eq) goto loc_825FA734;
	// rlwinm r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825fa534
	if (cr6.eq) goto loc_825FA534;
	// li r11,0
	r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// sth r11,16(r31)
	PPC_STORE_U16(r31.u32 + 16, r11.u16);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825FA534:
	// clrlwi r10,r11,1
	ctx.r10.u64 = r11.u32 & 0x7FFFFFFF;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r30,r11,17376
	r30.s64 = r11.s64 + 17376;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// lwz r29,84(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// ld r11,0(r29)
	r11.u64 = PPC_LOAD_U64(r29.u32 + 0);
	// rldicl r11,r11,9,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 9) & 0x1FF;
	// rlwinm r28,r11,1,0,30
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r28,r30
	ctx.r4.u64 = PPC_LOAD_U8(r28.u32 + r30.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// li r30,3
	r30.s64 = 3;
	// lbzx r11,r28,r11
	r11.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fa578
	if (!cr6.eq) goto loc_825FA578;
	// stw r30,20(r29)
	PPC_STORE_U32(r29.u32 + 20, r30.u32);
loc_825FA578:
	// lwz r10,84(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fa734
	if (!cr6.eq) goto loc_825FA734;
	// cmplwi cr6,r11,20
	cr6.compare<uint32_t>(r11.u32, 20, xer);
	// bgt cr6,0x825fa734
	if (cr6.gt) goto loc_825FA734;
	// clrlwi r25,r11,30
	r25.u64 = r11.u32 & 0x3;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825fa640
	if (cr6.eq) goto loc_825FA640;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x825fa734
	if (!cr6.eq) goto loc_825FA734;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r11,r11,0,15,13
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fa5e4
	if (!cr0.lt) goto loc_825FA5E4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FA5E4:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// lwz r29,84(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fa734
	if (!cr6.eq) goto loc_825FA734;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r28,r11,15952
	r28.s64 = r11.s64 + 15952;
	// ld r11,0(r29)
	r11.u64 = PPC_LOAD_U64(r29.u32 + 0);
	// rldicl r11,r11,6,58
	r11.u64 = __builtin_rotateleft64(r11.u64, 6) & 0x3F;
	// rlwinm r27,r11,1,0,30
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r27,r28
	ctx.r4.u64 = PPC_LOAD_U8(r27.u32 + r28.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
	// lbzx r11,r27,r11
	r11.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fa638
	if (!cr6.eq) goto loc_825FA638;
	// stw r30,20(r29)
	PPC_STORE_U32(r29.u32 + 20, r30.u32);
loc_825FA638:
	// clrlwi r28,r11,24
	r28.u64 = r11.u32 & 0xFF;
	// b 0x825fa6b0
	goto loc_825FA6B0;
loc_825FA640:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r28,r11,15952
	r28.s64 = r11.s64 + 15952;
	// oris r11,r10,2
	r11.u64 = ctx.r10.u64 | 131072;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r29,84(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// ld r11,0(r29)
	r11.u64 = PPC_LOAD_U64(r29.u32 + 0);
	// rldicl r11,r11,6,58
	r11.u64 = __builtin_rotateleft64(r11.u64, 6) & 0x3F;
	// rlwinm r27,r11,1,0,30
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r27,r28
	ctx.r4.u64 = PPC_LOAD_U8(r27.u32 + r28.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
	// lbzx r11,r27,r11
	r11.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fa684
	if (!cr6.eq) goto loc_825FA684;
	// stw r30,20(r29)
	PPC_STORE_U32(r29.u32 + 20, r30.u32);
loc_825FA684:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// subfic r28,r11,15
	xer.ca = r11.u32 <= 15;
	r28.s64 = 15 - r11.s64;
	// bne cr6,0x825fa6a0
	if (!cr6.eq) goto loc_825FA6A0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// beq cr6,0x825fa6a4
	if (cr6.eq) goto loc_825FA6A4;
loc_825FA6A0:
	// li r11,0
	r11.s64 = 0;
loc_825FA6A4:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwimi r10,r11,30,1,1
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 30) & 0x40000000) | (ctx.r10.u64 & 0xFFFFFFFFBFFFFFFF);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
loc_825FA6B0:
	// lwz r11,84(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fa734
	if (!cr6.eq) goto loc_825FA734;
	// cmplwi cr6,r28,15
	cr6.compare<uint32_t>(r28.u32, 15, xer);
	// bgt cr6,0x825fa734
	if (cr6.gt) goto loc_825FA734;
	// li r4,5
	ctx.r4.s64 = 5;
	// srawi r5,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	ctx.r5.s64 = r25.s32 >> 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// clrlwi r5,r25,31
	ctx.r5.u64 = r25.u32 & 0x1;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r29,1
	r29.s64 = 1;
loc_825FA6EC:
	// sraw r11,r28,r30
	temp.u32 = r30.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r28.s32 < 0) & (((r28.s32 >> temp.u32) << temp.u32) != r28.s32);
	r11.s64 = r28.s32 >> temp.u32;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r5,r11,31
	ctx.r5.u64 = r11.u32 & 0x1;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bge cr6,0x825fa6ec
	if (!cr6.lt) goto loc_825FA6EC;
	// addi r3,r31,12
	ctx.r3.s64 = r31.s64 + 12;
	// bl 0x825ebbd0
	sub_825EBBD0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwimi r11,r10,19,12,13
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 19) & 0xC0000) | (r11.u64 & 0xFFFFFFFFFFF3FFFF);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825FA734:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825FA740"))) PPC_WEAK_FUNC(sub_825FA740);
PPC_FUNC_IMPL(__imp__sub_825FA740) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,3636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3636);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fa7b8
	if (!cr6.eq) goto loc_825FA7B8;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// bl 0x825eb2a8
	sub_825EB2A8(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bne cr6,0x825fa7b8
	if (!cr6.eq) goto loc_825FA7B8;
	// lwz r11,3556(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3556);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r4,r11,16
	ctx.r4.s64 = r11.s64 + 16;
	// bl 0x825eb728
	sub_825EB728(ctx, base);
	// addi r11,r3,-1
	r11.s64 = ctx.r3.s64 + -1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825FA7B8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825FA7D0"))) PPC_WEAK_FUNC(sub_825FA7D0);
PPC_FUNC_IMPL(__imp__sub_825FA7D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// li r4,8
	ctx.r4.s64 = 8;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825fa800
	if (cr6.eq) goto loc_825FA800;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
loc_825FA800:
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r11,3556(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3556);
	// addi r30,r11,16
	r30.s64 = r11.s64 + 16;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// beq cr6,0x825fa890
	if (cr6.eq) goto loc_825FA890;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825fa868
	if (!cr6.gt) goto loc_825FA868;
loc_825FA828:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fa868
	if (cr6.eq) goto loc_825FA868;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fa858
	if (!cr0.lt) goto loc_825FA858;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FA858:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fa828
	if (cr6.gt) goto loc_825FA828;
loc_825FA868:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825fa890
	if (!cr0.lt) goto loc_825FA890;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FA890:
	// lwz r11,140(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 140);
	// li r30,0
	r30.s64 = 0;
	// lwz r10,136(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// addi r11,r10,-1
	r11.s64 = ctx.r10.s64 + -1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825fa8bc
	if (cr6.eq) goto loc_825FA8BC;
loc_825FA8AC:
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fa8ac
	if (!cr6.eq) goto loc_825FA8AC;
loc_825FA8BC:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// ble cr6,0x825fa988
	if (!cr6.gt) goto loc_825FA988;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825fa8e8
	if (!cr6.eq) goto loc_825FA8E8;
	// li r11,0
	r11.s64 = 0;
	// b 0x825fa988
	goto loc_825FA988;
loc_825FA8E8:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825fa948
	if (!cr6.gt) goto loc_825FA948;
loc_825FA8F0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fa948
	if (cr6.eq) goto loc_825FA948;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fa938
	if (!cr0.lt) goto loc_825FA938;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FA938:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fa8f0
	if (cr6.gt) goto loc_825FA8F0;
loc_825FA948:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fa984
	if (!cr0.lt) goto loc_825FA984;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FA984:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_825FA988:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// stw r11,3592(r26)
	PPC_STORE_U32(r26.u32 + 3592, r11.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825faa00
	if (!cr6.lt) goto loc_825FAA00;
loc_825FA9A8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825faa00
	if (cr6.eq) goto loc_825FAA00;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fa9f0
	if (!cr0.lt) goto loc_825FA9F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FA9F0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fa9a8
	if (cr6.gt) goto loc_825FA9A8;
loc_825FAA00:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825faa3c
	if (!cr0.lt) goto loc_825FAA3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAA3C:
	// stw r28,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r28.u32);
	// li r30,1
	r30.s64 = 1;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825faab4
	if (!cr6.lt) goto loc_825FAAB4;
loc_825FAA5C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825faab4
	if (cr6.eq) goto loc_825FAAB4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825faaa4
	if (!cr0.lt) goto loc_825FAAA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAAA4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825faa5c
	if (cr6.gt) goto loc_825FAA5C;
loc_825FAAB4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825faaf0
	if (!cr0.lt) goto loc_825FAAF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAAF0:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825faec4
	if (cr6.eq) goto loc_825FAEC4;
loc_825FAAF8:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fab6c
	if (!cr6.lt) goto loc_825FAB6C;
loc_825FAB14:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fab6c
	if (cr6.eq) goto loc_825FAB6C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fab5c
	if (!cr0.lt) goto loc_825FAB5C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAB5C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fab14
	if (cr6.gt) goto loc_825FAB14;
loc_825FAB6C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825faba8
	if (!cr0.lt) goto loc_825FABA8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FABA8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825faaf8
	if (!cr6.eq) goto loc_825FAAF8;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fac08
	if (!cr6.lt) goto loc_825FAC08;
loc_825FABC8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fac08
	if (cr6.eq) goto loc_825FAC08;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fabf8
	if (!cr0.lt) goto loc_825FABF8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FABF8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fabc8
	if (cr6.gt) goto loc_825FABC8;
loc_825FAC08:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825fac30
	if (!cr0.lt) goto loc_825FAC30;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAC30:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r30,3632(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 3632);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// beq cr6,0x825facb8
	if (cr6.eq) goto loc_825FACB8;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825fac90
	if (!cr6.gt) goto loc_825FAC90;
loc_825FAC50:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fac90
	if (cr6.eq) goto loc_825FAC90;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fac80
	if (!cr0.lt) goto loc_825FAC80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAC80:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fac50
	if (cr6.gt) goto loc_825FAC50;
loc_825FAC90:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825facb8
	if (!cr0.lt) goto loc_825FACB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FACB8:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fad10
	if (!cr6.lt) goto loc_825FAD10;
loc_825FACD0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fad10
	if (cr6.eq) goto loc_825FAD10;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fad00
	if (!cr0.lt) goto loc_825FAD00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAD00:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825facd0
	if (cr6.gt) goto loc_825FACD0;
loc_825FAD10:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825fad38
	if (!cr0.lt) goto loc_825FAD38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAD38:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825fad90
	if (!cr6.lt) goto loc_825FAD90;
loc_825FAD50:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fad90
	if (cr6.eq) goto loc_825FAD90;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fad80
	if (!cr0.lt) goto loc_825FAD80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAD80:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fad50
	if (cr6.gt) goto loc_825FAD50;
loc_825FAD90:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825fadb8
	if (!cr0.lt) goto loc_825FADB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FADB8:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825fae10
	if (!cr6.lt) goto loc_825FAE10;
loc_825FADD0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fae10
	if (cr6.eq) goto loc_825FAE10;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fae00
	if (!cr0.lt) goto loc_825FAE00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAE00:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fadd0
	if (cr6.gt) goto loc_825FADD0;
loc_825FAE10:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825fae38
	if (!cr0.lt) goto loc_825FAE38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAE38:
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825faec4
	if (!cr6.eq) goto loc_825FAEC4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825fae9c
	if (!cr6.lt) goto loc_825FAE9C;
loc_825FAE5C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fae9c
	if (cr6.eq) goto loc_825FAE9C;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fae8c
	if (!cr0.lt) goto loc_825FAE8C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAE8C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fae5c
	if (cr6.gt) goto loc_825FAE5C;
loc_825FAE9C:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825faec4
	if (!cr0.lt) goto loc_825FAEC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FAEC4:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825FAED0"))) PPC_WEAK_FUNC(sub_825FAED0);
PPC_FUNC_IMPL(__imp__sub_825FAED0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lbz r11,0(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// neg r9,r4
	ctx.r9.s64 = -ctx.r4.s64;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x825faef0
	if (!cr6.lt) goto loc_825FAEF0;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// b 0x825faf00
	goto loc_825FAF00;
loc_825FAEF0:
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// blt cr6,0x825faf04
	if (cr6.lt) goto loc_825FAF04;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
loc_825FAF00:
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
loc_825FAF04:
	// lbz r11,1(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x825faf24
	if (!cr6.lt) goto loc_825FAF24;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stb r11,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, r11.u8);
	// blr 
	return;
loc_825FAF24:
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// bltlr cr6
	if (cr6.lt) return;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stb r11,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825FAF3C"))) PPC_WEAK_FUNC(sub_825FAF3C);
PPC_FUNC_IMPL(__imp__sub_825FAF3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FAF40"))) PPC_WEAK_FUNC(sub_825FAF40);
PPC_FUNC_IMPL(__imp__sub_825FAF40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// addi r27,r11,19168
	r27.s64 = r11.s64 + 19168;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// rldicl r11,r11,13,51
	r11.u64 = __builtin_rotateleft64(r11.u64, 13) & 0x1FFF;
	// rlwinm r30,r11,1,0,30
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r30,r27
	ctx.r4.u64 = PPC_LOAD_U8(r30.u32 + r27.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r27,1
	r11.s64 = r27.s64 + 1;
	// li r25,3
	r25.s64 = 3;
	// lbzx r11,r30,r11
	r11.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825faf90
	if (!cr6.eq) goto loc_825FAF90;
	// stw r25,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r25.u32);
loc_825FAF90:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// addi r28,r11,-32
	r28.s64 = r11.s64 + -32;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x825fb064
	if (cr6.eq) goto loc_825FB064;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r11,3556(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3556);
	// addi r30,r11,-1
	r30.s64 = r11.s64 + -1;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825fb064
	if (cr6.eq) goto loc_825FB064;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825fb020
	if (!cr6.gt) goto loc_825FB020;
loc_825FAFC8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fb020
	if (cr6.eq) goto loc_825FB020;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fb010
	if (!cr0.lt) goto loc_825FB010;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB010:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fafc8
	if (cr6.gt) goto loc_825FAFC8;
loc_825FB020:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fb05c
	if (!cr0.lt) goto loc_825FB05C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB05C:
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// b 0x825fb068
	goto loc_825FB068;
loc_825FB064:
	// li r9,0
	ctx.r9.s64 = 0;
loc_825FB068:
	// lwz r10,3564(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 3564);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x825fb084
	if (!cr6.eq) goto loc_825FB084;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825fb084
	if (!cr6.eq) goto loc_825FB084;
	// li r11,0
	r11.s64 = 0;
	// b 0x825fb0c0
	goto loc_825FB0C0;
loc_825FB084:
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// mr r11,r28
	r11.u64 = r28.u64;
	// beq cr6,0x825fb0c0
	if (cr6.eq) goto loc_825FB0C0;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// li r11,1
	r11.s64 = 1;
	// bgt cr6,0x825fb0bc
	if (cr6.gt) goto loc_825FB0BC;
	// li r11,-1
	r11.s64 = -1;
loc_825FB0BC:
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
loc_825FB0C0:
	// stb r11,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r11.u8);
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// rldicl r11,r11,13,51
	r11.u64 = __builtin_rotateleft64(r11.u64, 13) & 0x1FFF;
	// rlwinm r30,r11,1,0,30
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r30,r27
	ctx.r4.u64 = PPC_LOAD_U8(r30.u32 + r27.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r27,1
	r11.s64 = r27.s64 + 1;
	// lbzx r11,r30,r11
	r11.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fb0f4
	if (!cr6.eq) goto loc_825FB0F4;
	// stw r25,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r25.u32);
loc_825FB0F4:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// addi r28,r11,-32
	r28.s64 = r11.s64 + -32;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x825fb1c8
	if (cr6.eq) goto loc_825FB1C8;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r11,3556(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3556);
	// addi r30,r11,-1
	r30.s64 = r11.s64 + -1;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825fb1c8
	if (cr6.eq) goto loc_825FB1C8;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825fb184
	if (!cr6.gt) goto loc_825FB184;
loc_825FB12C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fb184
	if (cr6.eq) goto loc_825FB184;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fb174
	if (!cr0.lt) goto loc_825FB174;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB174:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fb12c
	if (cr6.gt) goto loc_825FB12C;
loc_825FB184:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fb1c0
	if (!cr0.lt) goto loc_825FB1C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB1C0:
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// b 0x825fb1cc
	goto loc_825FB1CC;
loc_825FB1C8:
	// li r9,0
	ctx.r9.s64 = 0;
loc_825FB1CC:
	// lwz r10,3564(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 3564);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x825fb1f0
	if (!cr6.eq) goto loc_825FB1F0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825fb1f0
	if (!cr6.eq) goto loc_825FB1F0;
	// li r11,0
	r11.s64 = 0;
	// stb r11,1(r24)
	PPC_STORE_U8(r24.u32 + 1, r11.u8);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825FB1F0:
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x825fb204
	if (!cr6.eq) goto loc_825FB204;
	// stb r28,1(r24)
	PPC_STORE_U8(r24.u32 + 1, r28.u8);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_825FB204:
	// mr r11,r28
	r11.u64 = r28.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// li r11,1
	r11.s64 = 1;
	// bgt cr6,0x825fb234
	if (cr6.gt) goto loc_825FB234;
	// li r11,-1
	r11.s64 = -1;
loc_825FB234:
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stb r11,1(r24)
	PPC_STORE_U8(r24.u32 + 1, r11.u8);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825FB244"))) PPC_WEAK_FUNC(sub_825FB244);
PPC_FUNC_IMPL(__imp__sub_825FB244) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FB248"))) PPC_WEAK_FUNC(sub_825FB248);
PPC_FUNC_IMPL(__imp__sub_825FB248) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// subfic r11,r6,64
	xer.ca = ctx.r6.u32 <= 64;
	r11.s64 = 64 - ctx.r6.s64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r11,r5
	r30.u64 = r11.u64 + ctx.r5.u64;
	// lbz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fb298
	if (!cr6.eq) goto loc_825FB298;
	// li r10,3
	ctx.r10.s64 = 3;
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
loc_825FB298:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// li r26,0
	r26.s64 = 0;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fb484
	if (!cr6.eq) goto loc_825FB484;
	// extsb r28,r11
	r28.s64 = r11.s8;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x825fb510
	if (cr6.eq) goto loc_825FB510;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r28,8
	cr6.compare<int32_t>(r28.s32, 8, xer);
	// mr r30,r28
	r30.u64 = r28.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bgt cr6,0x825fb3d8
	if (cr6.gt) goto loc_825FB3D8;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825fb334
	if (!cr6.gt) goto loc_825FB334;
loc_825FB2DC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fb334
	if (cr6.eq) goto loc_825FB334;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fb324
	if (!cr0.lt) goto loc_825FB324;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB324:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fb2dc
	if (cr6.gt) goto loc_825FB2DC;
loc_825FB334:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fb370
	if (!cr0.lt) goto loc_825FB370;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB370:
	// lwz r10,84(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fb484
	if (!cr6.eq) goto loc_825FB484;
	// addi r10,r28,-1
	ctx.r10.s64 = r28.s64 + -1;
	// stw r26,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r26.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// slw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// and r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 & r11.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fb3c8
	if (!cr6.eq) goto loc_825FB3C8;
	// subfic r10,r28,8
	xer.ca = r28.u32 <= 8;
	ctx.r10.s64 = 8 - r28.s64;
	// lwz r8,1760(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// li r9,255
	ctx.r9.s64 = 255;
	// sraw r10,r9,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r10.s64 = ctx.r9.s32 >> temp.u32;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825FB3C8:
	// lwz r10,1760(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825FB3D8:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825fb438
	if (!cr6.gt) goto loc_825FB438;
loc_825FB3E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fb438
	if (cr6.eq) goto loc_825FB438;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fb428
	if (!cr0.lt) goto loc_825FB428;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB428:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fb3e0
	if (cr6.gt) goto loc_825FB3E0;
loc_825FB438:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fb474
	if (!cr0.lt) goto loc_825FB474;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB474:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825fb49c
	if (cr6.eq) goto loc_825FB49C;
loc_825FB484:
	// li r11,4
	r11.s64 = 4;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// lwz r11,1760(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825FB49C:
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// stw r26,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r26.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// and r11,r11,r30
	r11.u64 = r11.u64 & r30.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825fb4dc
	if (!cr6.eq) goto loc_825FB4DC;
	// lis r10,0
	ctx.r10.s64 = 0;
	// lwz r9,1760(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// subfic r11,r28,16
	xer.ca = r28.u32 <= 16;
	r11.s64 = 16 - r28.s64;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// sraw r11,r10,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	r11.s64 = ctx.r10.s32 >> temp.u32;
	// andc r11,r11,r30
	r11.u64 = r11.u64 & ~r30.u64;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// b 0x825fb4e4
	goto loc_825FB4E4;
loc_825FB4DC:
	// lwz r11,1760(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_825FB4E4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r11,r11,1,62
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, r11.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fb520
	if (!cr0.lt) goto loc_825FB520;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825FB510:
	// li r26,0
	r26.s64 = 0;
	// stw r26,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r26.u32);
	// lwz r11,1760(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
loc_825FB520:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825FB528"))) PPC_WEAK_FUNC(sub_825FB528);
PPC_FUNC_IMPL(__imp__sub_825FB528) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r20,r6
	r20.u64 = ctx.r6.u64;
	// mr r18,r7
	r18.u64 = ctx.r7.u64;
	// li r25,0
	r25.s64 = 0;
	// li r21,1
	r21.s64 = 1;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r17,312(r27)
	r17.u64 = PPC_LOAD_U32(r27.u32 + 312);
	// addi r23,r10,1
	r23.s64 = ctx.r10.s64 + 1;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r16,316(r27)
	r16.u64 = PPC_LOAD_U32(r27.u32 + 316);
	// lwz r24,0(r11)
	r24.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r19,28(r11)
	r19.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// lwz r22,32(r11)
	r22.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r14,24(r11)
	r14.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// lwz r15,4(r11)
	r15.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// beq cr6,0x825fbdac
	if (cr6.eq) goto loc_825FBDAC;
	// lis r11,0
	r11.s64 = 0;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
loc_825FB598:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 8);
	// lwz r29,0(r24)
	r29.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fb688
	if (cr6.lt) goto loc_825FB688;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825fb680
	if (!cr6.lt) goto loc_825FB680;
loc_825FB5E8:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825fb614
	if (cr6.lt) goto loc_825FB614;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x825fb5e8
	if (cr6.eq) goto loc_825FB5E8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fb6c4
	goto loc_825FB6C4;
loc_825FB614:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_825FB680:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fb6c4
	goto loc_825FB6C4;
loc_825FB688:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_825FB690:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r26
	r11.u64 = r30.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fb690
	if (cr6.lt) goto loc_825FB690;
loc_825FB6C4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fbdd8
	if (!cr6.eq) goto loc_825FBDD8;
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r31,r15
	cr6.compare<int32_t>(r31.s32, r15.s32, xer);
	// bgt cr6,0x825fbdd8
	if (cr6.gt) goto loc_825FBDD8;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// beq cr6,0x825fb740
	if (cr6.eq) goto loc_825FB740;
	// subfc r11,r23,r31
	xer.ca = r31.u32 >= r23.u32;
	r11.s64 = r31.s64 - r23.s64;
	// lbzx r28,r31,r22
	r28.u64 = PPC_LOAD_U8(r31.u32 + r22.u32);
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r25,r11,1
	r25.s64 = r11.s64 + 1;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fb720
	if (!cr0.lt) goto loc_825FB720;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB720:
	// lbzx r11,r31,r19
	r11.u64 = PPC_LOAD_U8(r31.u32 + r19.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825fb738
	if (cr6.eq) goto loc_825FB738;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// neg r31,r11
	r31.s64 = -r11.s64;
	// b 0x825fbd0c
	goto loc_825FBD0C;
loc_825FB738:
	// extsb r31,r11
	r31.s64 = r11.s8;
	// b 0x825fbd0c
	goto loc_825FBD0C;
loc_825FB740:
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fb764
	if (!cr0.lt) goto loc_825FB764;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB764:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825fb92c
	if (!cr6.eq) goto loc_825FB92C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fbdd8
	if (!cr6.eq) goto loc_825FBDD8;
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r24)
	r29.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fb868
	if (cr6.lt) goto loc_825FB868;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825fb860
	if (!cr6.lt) goto loc_825FB860;
loc_825FB7C8:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825fb7f4
	if (cr6.lt) goto loc_825FB7F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x825fb7c8
	if (cr6.eq) goto loc_825FB7C8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fb8a4
	goto loc_825FB8A4;
loc_825FB7F4:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_825FB860:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fb8a4
	goto loc_825FB8A4;
loc_825FB868:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_825FB870:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r26
	r11.u64 = r30.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fb870
	if (cr6.lt) goto loc_825FB870;
loc_825FB8A4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fbdd8
	if (!cr6.eq) goto loc_825FBDD8;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r11,r15
	cr6.compare<int32_t>(r11.s32, r15.s32, xer);
	// beq cr6,0x825fbdd8
	if (cr6.eq) goto loc_825FBDD8;
	// lbzx r10,r11,r19
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r19.u32);
	// cmplw cr6,r11,r23
	cr6.compare<uint32_t>(r11.u32, r23.u32, xer);
	// lbzx r28,r11,r22
	r28.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// extsb r11,r10
	r11.s64 = ctx.r10.s8;
	// blt cr6,0x825fb8e4
	if (cr6.lt) goto loc_825FB8E4;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r25,1
	r25.s64 = 1;
	// b 0x825fb8e8
	goto loc_825FB8E8;
loc_825FB8E4:
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_825FB8E8:
	// lbzx r10,r28,r10
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fb91c
	if (!cr0.lt) goto loc_825FB91C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB91C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825fbd0c
	if (cr6.eq) goto loc_825FBD0C;
	// neg r31,r31
	r31.s64 = -r31.s64;
	// b 0x825fbd0c
	goto loc_825FBD0C;
loc_825FB92C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fb958
	if (!cr0.lt) goto loc_825FB958;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FB958:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825fbb20
	if (!cr6.eq) goto loc_825FBB20;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fbdd8
	if (!cr6.eq) goto loc_825FBDD8;
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r24)
	r29.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fba5c
	if (cr6.lt) goto loc_825FBA5C;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825fba54
	if (!cr6.lt) goto loc_825FBA54;
loc_825FB9BC:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825fb9e8
	if (cr6.lt) goto loc_825FB9E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x825fb9bc
	if (cr6.eq) goto loc_825FB9BC;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fba98
	goto loc_825FBA98;
loc_825FB9E8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_825FBA54:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fba98
	goto loc_825FBA98;
loc_825FBA5C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_825FBA64:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r26
	r11.u64 = r30.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fba64
	if (cr6.lt) goto loc_825FBA64;
loc_825FBA98:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fbdd8
	if (!cr6.eq) goto loc_825FBDD8;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r11,r15
	cr6.compare<int32_t>(r11.s32, r15.s32, xer);
	// beq cr6,0x825fbdd8
	if (cr6.eq) goto loc_825FBDD8;
	// lbzx r9,r11,r19
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + r19.u32);
	// cmplw cr6,r11,r23
	cr6.compare<uint32_t>(r11.u32, r23.u32, xer);
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// extsb r31,r9
	r31.s64 = ctx.r9.s8;
	// blt cr6,0x825fbad8
	if (cr6.lt) goto loc_825FBAD8;
	// lbzx r11,r31,r14
	r11.u64 = PPC_LOAD_U8(r31.u32 + r14.u32);
	// li r25,1
	r25.s64 = 1;
	// b 0x825fbae0
	goto loc_825FBAE0;
loc_825FBAD8:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lbzx r11,r31,r11
	r11.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
loc_825FBAE0:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fbb10
	if (!cr0.lt) goto loc_825FBB10;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FBB10:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825fbd0c
	if (cr6.eq) goto loc_825FBD0C;
	// neg r31,r31
	r31.s64 = -r31.s64;
	// b 0x825fbd0c
	goto loc_825FBD0C;
loc_825FBB20:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fbb4c
	if (!cr0.lt) goto loc_825FBB4C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FBB4C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r25,r30
	r25.u64 = r30.u64;
	// li r30,6
	r30.s64 = 6;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825fbbc4
	if (!cr6.lt) goto loc_825FBBC4;
loc_825FBB6C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fbbc4
	if (cr6.eq) goto loc_825FBBC4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fbbb4
	if (!cr0.lt) goto loc_825FBBB4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FBBB4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fbb6c
	if (cr6.gt) goto loc_825FBB6C;
loc_825FBBC4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fbc00
	if (!cr0.lt) goto loc_825FBC00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FBC00:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r28,r30
	r28.u64 = r30.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r11,r11,1,62
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, r11.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fbc28
	if (!cr0.lt) goto loc_825FBC28;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FBC28:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,12
	r30.s64 = 12;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bge cr6,0x825fbc9c
	if (!cr6.lt) goto loc_825FBC9C;
loc_825FBC44:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fbc9c
	if (cr6.eq) goto loc_825FBC9C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fbc8c
	if (!cr0.lt) goto loc_825FBC8C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FBC8C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fbc44
	if (cr6.gt) goto loc_825FBC44;
loc_825FBC9C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fbcd8
	if (!cr0.lt) goto loc_825FBCD8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FBCD8:
	// mr r31,r30
	r31.u64 = r30.u64;
	// cmpwi cr6,r30,2047
	cr6.compare<int32_t>(r30.s32, 2047, xer);
	// ble cr6,0x825fbce8
	if (!cr6.gt) goto loc_825FBCE8;
	// addi r31,r30,-4096
	r31.s64 = r30.s64 + -4096;
loc_825FBCE8:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r11,r11,1,62
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, r11.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fbd0c
	if (!cr0.lt) goto loc_825FBD0C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FBD0C:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fbdd8
	if (!cr6.eq) goto loc_825FBDD8;
	// add r9,r28,r21
	ctx.r9.u64 = r28.u64 + r21.u64;
	// cmplwi cr6,r9,64
	cr6.compare<uint32_t>(ctx.r9.u32, 64, xer);
	// bge cr6,0x825fbdd8
	if (!cr6.lt) goto loc_825FBDD8;
	// lbzx r11,r9,r18
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r18.u32);
	// rlwinm r10,r11,0,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF8;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825fbd50
	if (!cr6.eq) goto loc_825FBD50;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lhzx r10,r11,r20
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r20.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// sthx r10,r11,r20
	PPC_STORE_U16(r11.u32 + r20.u32, ctx.r10.u16);
	// b 0x825fbda0
	goto loc_825FBDA0;
loc_825FBD50:
	// clrlwi r10,r11,29
	ctx.r10.u64 = r11.u32 & 0x7;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825fbd7c
	if (!cr6.eq) goto loc_825FBD7C;
	// lbzx r11,r9,r18
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r18.u32);
	// rlwinm r11,r11,29,3,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r20
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r20.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// sthx r10,r11,r20
	PPC_STORE_U16(r11.u32 + r20.u32, ctx.r10.u16);
	// b 0x825fbda0
	goto loc_825FBDA0;
loc_825FBD7C:
	// lwz r8,1760(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// mullw r10,r31,r17
	ctx.r10.s64 = int64_t(r31.s32) * int64_t(r17.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// ble cr6,0x825fbd98
	if (!cr6.gt) goto loc_825FBD98;
	// add r10,r10,r16
	ctx.r10.u64 = ctx.r10.u64 + r16.u64;
	// b 0x825fbd9c
	goto loc_825FBD9C;
loc_825FBD98:
	// subf r10,r16,r10
	ctx.r10.s64 = ctx.r10.s64 - r16.s64;
loc_825FBD9C:
	// stwx r10,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, ctx.r10.u32);
loc_825FBDA0:
	// addi r21,r9,1
	r21.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x825fb598
	if (cr6.eq) goto loc_825FB598;
loc_825FBDAC:
	// li r9,32
	ctx.r9.s64 = 32;
	// li r10,4
	ctx.r10.s64 = 4;
	// addi r8,r20,2
	ctx.r8.s64 = r20.s64 + 2;
	// li r7,0
	ctx.r7.s64 = 0;
loc_825FBDBC:
	// lhz r11,0(r8)
	r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fbde4
	if (!cr6.eq) goto loc_825FBDE4;
	// lwz r11,1760(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// stwx r7,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, ctx.r7.u32);
	// b 0x825fbe00
	goto loc_825FBE00;
loc_825FBDD8:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_825FBDE4:
	// lwz r6,1760(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// mullw r11,r11,r17
	r11.s64 = int64_t(r11.s32) * int64_t(r17.s32);
	// ble cr6,0x825fbdf8
	if (!cr6.gt) goto loc_825FBDF8;
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// b 0x825fbdfc
	goto loc_825FBDFC;
loc_825FBDF8:
	// subf r11,r16,r11
	r11.s64 = r11.s64 - r16.s64;
loc_825FBDFC:
	// stwx r11,r10,r6
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, r11.u32);
loc_825FBE00:
	// lhz r11,16(r8)
	r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + 16);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fbe1c
	if (!cr6.eq) goto loc_825FBE1C;
	// lwz r11,1760(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// stwx r7,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r7.u32);
	// b 0x825fbe38
	goto loc_825FBE38;
loc_825FBE1C:
	// lwz r6,1760(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// mullw r11,r11,r17
	r11.s64 = int64_t(r11.s32) * int64_t(r17.s32);
	// ble cr6,0x825fbe30
	if (!cr6.gt) goto loc_825FBE30;
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// b 0x825fbe34
	goto loc_825FBE34;
loc_825FBE30:
	// subf r11,r16,r11
	r11.s64 = r11.s64 - r16.s64;
loc_825FBE34:
	// stwx r11,r9,r6
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, r11.u32);
loc_825FBE38:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// blt cr6,0x825fbdbc
	if (cr6.lt) goto loc_825FBDBC;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825FBE58"))) PPC_WEAK_FUNC(sub_825FBE58);
PPC_FUNC_IMPL(__imp__sub_825FBE58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r23,0
	r23.s64 = 0;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r25,r23
	r25.u64 = r23.u64;
	// lwz r18,312(r27)
	r18.u64 = PPC_LOAD_U32(r27.u32 + 312);
	// addi r19,r10,1
	r19.s64 = ctx.r10.s64 + 1;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r17,316(r27)
	r17.u64 = PPC_LOAD_U32(r27.u32 + 316);
	// lwz r24,0(r11)
	r24.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r20,28(r11)
	r20.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// lwz r21,32(r11)
	r21.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r15,24(r11)
	r15.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// lwz r16,4(r11)
	r16.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r3,1760(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// stw r23,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r23.u32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// li r14,1
	r14.s64 = 1;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
loc_825FBED4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 8);
	// lwz r29,0(r24)
	r29.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fbfc4
	if (cr6.lt) goto loc_825FBFC4;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825fbfbc
	if (!cr6.lt) goto loc_825FBFBC;
loc_825FBF24:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825fbf50
	if (cr6.lt) goto loc_825FBF50;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x825fbf24
	if (cr6.eq) goto loc_825FBF24;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fc000
	goto loc_825FC000;
loc_825FBF50:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_825FBFBC:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fc000
	goto loc_825FC000;
loc_825FBFC4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_825FBFCC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r26
	r11.u64 = r30.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fbfcc
	if (cr6.lt) goto loc_825FBFCC;
loc_825FC000:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fc728
	if (!cr6.eq) goto loc_825FC728;
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r31,r16
	cr6.compare<int32_t>(r31.s32, r16.s32, xer);
	// beq cr6,0x825fc078
	if (cr6.eq) goto loc_825FC078;
	// cmplw cr6,r31,r19
	cr6.compare<uint32_t>(r31.u32, r19.u32, xer);
	// blt cr6,0x825fc02c
	if (cr6.lt) goto loc_825FC02C;
	// mr r25,r14
	r25.u64 = r14.u64;
loc_825FC02C:
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// lbzx r28,r31,r21
	r28.u64 = PPC_LOAD_U8(r31.u32 + r21.u32);
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fc058
	if (!cr0.lt) goto loc_825FC058;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC058:
	// lbzx r11,r31,r20
	r11.u64 = PPC_LOAD_U8(r31.u32 + r20.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825fc070
	if (cr6.eq) goto loc_825FC070;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// neg r31,r11
	r31.s64 = -r11.s64;
	// b 0x825fc648
	goto loc_825FC648;
loc_825FC070:
	// extsb r31,r11
	r31.s64 = r11.s8;
	// b 0x825fc648
	goto loc_825FC648;
loc_825FC078:
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fc0a0
	if (!cr0.lt) goto loc_825FC0A0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC0A0:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825fc268
	if (!cr6.eq) goto loc_825FC268;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fc728
	if (!cr6.eq) goto loc_825FC728;
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r24)
	r29.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fc1a4
	if (cr6.lt) goto loc_825FC1A4;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825fc19c
	if (!cr6.lt) goto loc_825FC19C;
loc_825FC104:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825fc130
	if (cr6.lt) goto loc_825FC130;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x825fc104
	if (cr6.eq) goto loc_825FC104;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fc1e0
	goto loc_825FC1E0;
loc_825FC130:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_825FC19C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fc1e0
	goto loc_825FC1E0;
loc_825FC1A4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_825FC1AC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r26
	r11.u64 = r30.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fc1ac
	if (cr6.lt) goto loc_825FC1AC;
loc_825FC1E0:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fc728
	if (!cr6.eq) goto loc_825FC728;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r11,r16
	cr6.compare<int32_t>(r11.s32, r16.s32, xer);
	// beq cr6,0x825fc728
	if (cr6.eq) goto loc_825FC728;
	// lbzx r10,r11,r20
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r20.u32);
	// cmplw cr6,r11,r19
	cr6.compare<uint32_t>(r11.u32, r19.u32, xer);
	// lbzx r28,r11,r21
	r28.u64 = PPC_LOAD_U8(r11.u32 + r21.u32);
	// extsb r11,r10
	r11.s64 = ctx.r10.s8;
	// blt cr6,0x825fc220
	if (cr6.lt) goto loc_825FC220;
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r25,r14
	r25.u64 = r14.u64;
	// b 0x825fc224
	goto loc_825FC224;
loc_825FC220:
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_825FC224:
	// lbzx r10,r28,r10
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fc258
	if (!cr0.lt) goto loc_825FC258;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC258:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825fc648
	if (cr6.eq) goto loc_825FC648;
	// neg r31,r31
	r31.s64 = -r31.s64;
	// b 0x825fc648
	goto loc_825FC648;
loc_825FC268:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fc294
	if (!cr0.lt) goto loc_825FC294;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC294:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825fc45c
	if (!cr6.eq) goto loc_825FC45C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fc728
	if (!cr6.eq) goto loc_825FC728;
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r24)
	r29.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fc398
	if (cr6.lt) goto loc_825FC398;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825fc390
	if (!cr6.lt) goto loc_825FC390;
loc_825FC2F8:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825fc324
	if (cr6.lt) goto loc_825FC324;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x825fc2f8
	if (cr6.eq) goto loc_825FC2F8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fc3d4
	goto loc_825FC3D4;
loc_825FC324:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_825FC390:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825fc3d4
	goto loc_825FC3D4;
loc_825FC398:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_825FC3A0:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r26
	r11.u64 = r30.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825fc3a0
	if (cr6.lt) goto loc_825FC3A0;
loc_825FC3D4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fc728
	if (!cr6.eq) goto loc_825FC728;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r11,r16
	cr6.compare<int32_t>(r11.s32, r16.s32, xer);
	// beq cr6,0x825fc728
	if (cr6.eq) goto loc_825FC728;
	// lbzx r9,r11,r20
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + r20.u32);
	// cmplw cr6,r11,r19
	cr6.compare<uint32_t>(r11.u32, r19.u32, xer);
	// lbzx r10,r11,r21
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r21.u32);
	// extsb r31,r9
	r31.s64 = ctx.r9.s8;
	// blt cr6,0x825fc414
	if (cr6.lt) goto loc_825FC414;
	// lbzx r11,r31,r15
	r11.u64 = PPC_LOAD_U8(r31.u32 + r15.u32);
	// mr r25,r14
	r25.u64 = r14.u64;
	// b 0x825fc41c
	goto loc_825FC41C;
loc_825FC414:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lbzx r11,r31,r11
	r11.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
loc_825FC41C:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fc44c
	if (!cr0.lt) goto loc_825FC44C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC44C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825fc648
	if (cr6.eq) goto loc_825FC648;
	// neg r31,r31
	r31.s64 = -r31.s64;
	// b 0x825fc648
	goto loc_825FC648;
loc_825FC45C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fc488
	if (!cr0.lt) goto loc_825FC488;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC488:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r25,r30
	r25.u64 = r30.u64;
	// li r30,6
	r30.s64 = 6;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x825fc500
	if (!cr6.lt) goto loc_825FC500;
loc_825FC4A8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fc500
	if (cr6.eq) goto loc_825FC500;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fc4f0
	if (!cr0.lt) goto loc_825FC4F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC4F0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fc4a8
	if (cr6.gt) goto loc_825FC4A8;
loc_825FC500:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fc53c
	if (!cr0.lt) goto loc_825FC53C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC53C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r28,r30
	r28.u64 = r30.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r11,r11,1,62
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, r11.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fc564
	if (!cr0.lt) goto loc_825FC564;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC564:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,12
	r30.s64 = 12;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bge cr6,0x825fc5d8
	if (!cr6.lt) goto loc_825FC5D8;
loc_825FC580:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fc5d8
	if (cr6.eq) goto loc_825FC5D8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fc5c8
	if (!cr0.lt) goto loc_825FC5C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC5C8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fc580
	if (cr6.gt) goto loc_825FC580;
loc_825FC5D8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fc614
	if (!cr0.lt) goto loc_825FC614;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC614:
	// mr r31,r30
	r31.u64 = r30.u64;
	// cmpwi cr6,r30,2047
	cr6.compare<int32_t>(r30.s32, 2047, xer);
	// ble cr6,0x825fc624
	if (!cr6.gt) goto loc_825FC624;
	// addi r31,r30,-4096
	r31.s64 = r30.s64 + -4096;
loc_825FC624:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r11,r11,1,62
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, r11.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825fc648
	if (!cr0.lt) goto loc_825FC648;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC648:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fc728
	if (!cr6.eq) goto loc_825FC728;
	// add r11,r28,r23
	r11.u64 = r28.u64 + r23.u64;
	// cmplwi cr6,r11,64
	cr6.compare<uint32_t>(r11.u32, 64, xer);
	// bge cr6,0x825fc728
	if (!cr6.lt) goto loc_825FC728;
	// lwz r10,1828(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 1828);
	// lbzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// clrlwi r9,r10,29
	ctx.r9.u64 = ctx.r10.u32 & 0x7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825fc690
	if (cr6.eq) goto loc_825FC690;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// slw r10,r14,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r14.u32 << (ctx.r10.u8 & 0x3F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
loc_825FC690:
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// bne cr6,0x825fc6b0
	if (!cr6.eq) goto loc_825FC6B0;
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lwz r9,1760(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// lwz r8,304(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 304);
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
	// b 0x825fc708
	goto loc_825FC708;
loc_825FC6B0:
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x825fc6d0
	if (!cr6.eq) goto loc_825FC6D0;
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lwz r9,1760(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// lwz r8,308(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 308);
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
	// b 0x825fc708
	goto loc_825FC708;
loc_825FC6D0:
	// lwz r8,1760(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x825fc6f4
	if (!cr6.gt) goto loc_825FC6F4;
	// lbzx r9,r11,r22
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// mullw r10,r31,r18
	ctx.r10.s64 = int64_t(r31.s32) * int64_t(r18.s32);
	// add r10,r10,r17
	ctx.r10.u64 = ctx.r10.u64 + r17.u64;
	// rotlwi r9,r9,2
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// stwx r10,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, ctx.r10.u32);
	// b 0x825fc708
	goto loc_825FC708;
loc_825FC6F4:
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// mullw r9,r31,r18
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r18.s32);
	// subf r9,r17,r9
	ctx.r9.s64 = ctx.r9.s64 - r17.s64;
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// stwx r9,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
loc_825FC708:
	// addi r23,r11,1
	r23.s64 = r11.s64 + 1;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x825fbed4
	if (cr6.eq) goto loc_825FBED4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,1940(r27)
	PPC_STORE_U32(r27.u32 + 1940, r11.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_825FC728:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825FC734"))) PPC_WEAK_FUNC(sub_825FC734);
PPC_FUNC_IMPL(__imp__sub_825FC734) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FC738"))) PPC_WEAK_FUNC(sub_825FC738);
PPC_FUNC_IMPL(__imp__sub_825FC738) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r30,24
	r30.s64 = 24;
	// li r29,0
	r29.s64 = 0;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bge cr6,0x825fc7bc
	if (!cr6.lt) goto loc_825FC7BC;
loc_825FC764:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fc7bc
	if (cr6.eq) goto loc_825FC7BC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fc7ac
	if (!cr0.lt) goto loc_825FC7AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC7AC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fc764
	if (cr6.gt) goto loc_825FC764;
loc_825FC7BC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fc7f8
	if (!cr0.lt) goto loc_825FC7F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC7F8:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// beq cr6,0x825fc80c
	if (cr6.eq) goto loc_825FC80C;
loc_825FC800:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_825FC80C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x825fc880
	if (!cr6.lt) goto loc_825FC880;
loc_825FC828:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fc880
	if (cr6.eq) goto loc_825FC880;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fc870
	if (!cr0.lt) goto loc_825FC870;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC870:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fc828
	if (cr6.gt) goto loc_825FC828;
loc_825FC880:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fc8bc
	if (!cr0.lt) goto loc_825FC8BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC8BC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x825fc91c
	if (!cr6.lt) goto loc_825FC91C;
loc_825FC8DC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fc91c
	if (cr6.eq) goto loc_825FC91C;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fc90c
	if (!cr0.lt) goto loc_825FC90C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC90C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fc8dc
	if (cr6.gt) goto loc_825FC8DC;
loc_825FC91C:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825fc944
	if (!cr0.lt) goto loc_825FC944;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC944:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,24
	r30.s64 = 24;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bge cr6,0x825fc9b8
	if (!cr6.lt) goto loc_825FC9B8;
loc_825FC960:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fc9b8
	if (cr6.eq) goto loc_825FC9B8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fc9a8
	if (!cr0.lt) goto loc_825FC9A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC9A8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fc960
	if (cr6.gt) goto loc_825FC960;
loc_825FC9B8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fc9f4
	if (!cr0.lt) goto loc_825FC9F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FC9F4:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x825fca70
	if (!cr6.lt) goto loc_825FCA70;
loc_825FCA18:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fca70
	if (cr6.eq) goto loc_825FCA70;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fca60
	if (!cr0.lt) goto loc_825FCA60;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCA60:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fca18
	if (cr6.gt) goto loc_825FCA18;
loc_825FCA70:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fcaac
	if (!cr0.lt) goto loc_825FCAAC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCAAC:
	// cmplwi cr6,r30,2
	cr6.compare<uint32_t>(r30.u32, 2, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x825fcb0c
	if (!cr6.lt) goto loc_825FCB0C;
loc_825FCACC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fcb0c
	if (cr6.eq) goto loc_825FCB0C;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fcafc
	if (!cr0.lt) goto loc_825FCAFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCAFC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fcacc
	if (cr6.gt) goto loc_825FCACC;
loc_825FCB0C:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825fcb34
	if (!cr0.lt) goto loc_825FCB34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCB34:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fcba8
	if (!cr6.lt) goto loc_825FCBA8;
loc_825FCB50:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fcba8
	if (cr6.eq) goto loc_825FCBA8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fcb98
	if (!cr0.lt) goto loc_825FCB98;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCB98:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fcb50
	if (cr6.gt) goto loc_825FCB50;
loc_825FCBA8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fcbe4
	if (!cr0.lt) goto loc_825FCBE4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCBE4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x825fcc44
	if (!cr6.lt) goto loc_825FCC44;
loc_825FCC04:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fcc44
	if (cr6.eq) goto loc_825FCC44;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fcc34
	if (!cr0.lt) goto loc_825FCC34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCC34:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fcc04
	if (cr6.gt) goto loc_825FCC04;
loc_825FCC44:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825fcc6c
	if (!cr0.lt) goto loc_825FCC6C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCC6C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fcce0
	if (!cr6.lt) goto loc_825FCCE0;
loc_825FCC88:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fcce0
	if (cr6.eq) goto loc_825FCCE0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fccd0
	if (!cr0.lt) goto loc_825FCCD0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCCD0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fcc88
	if (cr6.gt) goto loc_825FCC88;
loc_825FCCE0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fcd1c
	if (!cr0.lt) goto loc_825FCD1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCD1C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x825fcd98
	if (!cr6.lt) goto loc_825FCD98;
loc_825FCD40:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fcd98
	if (cr6.eq) goto loc_825FCD98;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fcd88
	if (!cr0.lt) goto loc_825FCD88;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCD88:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fcd40
	if (cr6.gt) goto loc_825FCD40;
loc_825FCD98:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fcdd4
	if (!cr0.lt) goto loc_825FCDD4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCDD4:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fce4c
	if (!cr6.lt) goto loc_825FCE4C;
loc_825FCDF4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fce4c
	if (cr6.eq) goto loc_825FCE4C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fce3c
	if (!cr0.lt) goto loc_825FCE3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCE3C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fcdf4
	if (cr6.gt) goto loc_825FCDF4;
loc_825FCE4C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fce88
	if (!cr0.lt) goto loc_825FCE88;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCE88:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x825fcee8
	if (!cr6.lt) goto loc_825FCEE8;
loc_825FCEA8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fcee8
	if (cr6.eq) goto loc_825FCEE8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x825fced8
	if (!cr0.lt) goto loc_825FCED8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCED8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fcea8
	if (cr6.gt) goto loc_825FCEA8;
loc_825FCEE8:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825fcf10
	if (!cr0.lt) goto loc_825FCF10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCF10:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fcf84
	if (!cr6.lt) goto loc_825FCF84;
loc_825FCF2C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fcf84
	if (cr6.eq) goto loc_825FCF84;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fcf74
	if (!cr0.lt) goto loc_825FCF74;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCF74:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fcf2c
	if (cr6.gt) goto loc_825FCF2C;
loc_825FCF84:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fcfc0
	if (!cr0.lt) goto loc_825FCFC0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FCFC0:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,16
	r30.s64 = 16;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x825fd03c
	if (!cr6.lt) goto loc_825FD03C;
loc_825FCFE4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd03c
	if (cr6.eq) goto loc_825FD03C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd02c
	if (!cr0.lt) goto loc_825FD02C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD02C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fcfe4
	if (cr6.gt) goto loc_825FCFE4;
loc_825FD03C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd078
	if (!cr0.lt) goto loc_825FD078;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD078:
	// mr r11,r30
	r11.u64 = r30.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825fc800
	if (cr6.eq) goto loc_825FC800;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r30,3568(r27)
	PPC_STORE_U32(r27.u32 + 3568, r30.u32);
	// stw r10,3632(r27)
	PPC_STORE_U32(r27.u32 + 3632, ctx.r10.u32);
loc_825FD090:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x825fd0b4
	if (cr6.eq) goto loc_825FD0B4;
	// lwz r10,3632(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 3632);
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,3632(r27)
	PPC_STORE_U32(r27.u32 + 3632, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmpwi cr6,r10,16
	cr6.compare<int32_t>(ctx.r10.s32, 16, xer);
	// blt cr6,0x825fd090
	if (cr6.lt) goto loc_825FD090;
loc_825FD0B4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd128
	if (!cr6.lt) goto loc_825FD128;
loc_825FD0D0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd128
	if (cr6.eq) goto loc_825FD128;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd118
	if (!cr0.lt) goto loc_825FD118;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD118:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd0d0
	if (cr6.gt) goto loc_825FD0D0;
loc_825FD128:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd164
	if (!cr0.lt) goto loc_825FD164;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD164:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd1dc
	if (!cr6.lt) goto loc_825FD1DC;
loc_825FD184:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd1dc
	if (cr6.eq) goto loc_825FD1DC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd1cc
	if (!cr0.lt) goto loc_825FD1CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD1CC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd184
	if (cr6.gt) goto loc_825FD184;
loc_825FD1DC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd218
	if (!cr0.lt) goto loc_825FD218;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD218:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd294
	if (!cr6.lt) goto loc_825FD294;
loc_825FD23C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd294
	if (cr6.eq) goto loc_825FD294;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd284
	if (!cr0.lt) goto loc_825FD284;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD284:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd23c
	if (cr6.gt) goto loc_825FD23C;
loc_825FD294:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd2d0
	if (!cr0.lt) goto loc_825FD2D0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD2D0:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,13
	r30.s64 = 13;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,13
	cr6.compare<uint32_t>(r11.u32, 13, xer);
	// bge cr6,0x825fd34c
	if (!cr6.lt) goto loc_825FD34C;
loc_825FD2F4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd34c
	if (cr6.eq) goto loc_825FD34C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd33c
	if (!cr0.lt) goto loc_825FD33C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD33C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd2f4
	if (cr6.gt) goto loc_825FD2F4;
loc_825FD34C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd388
	if (!cr0.lt) goto loc_825FD388;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD388:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,156(r27)
	PPC_STORE_U32(r27.u32 + 156, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd400
	if (!cr6.lt) goto loc_825FD400;
loc_825FD3A8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd400
	if (cr6.eq) goto loc_825FD400;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd3f0
	if (!cr0.lt) goto loc_825FD3F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD3F0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd3a8
	if (cr6.gt) goto loc_825FD3A8;
loc_825FD400:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd43c
	if (!cr0.lt) goto loc_825FD43C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD43C:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,13
	r30.s64 = 13;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,13
	cr6.compare<uint32_t>(r11.u32, 13, xer);
	// bge cr6,0x825fd4b8
	if (!cr6.lt) goto loc_825FD4B8;
loc_825FD460:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd4b8
	if (cr6.eq) goto loc_825FD4B8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd4a8
	if (!cr0.lt) goto loc_825FD4A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD4A8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd460
	if (cr6.gt) goto loc_825FD460;
loc_825FD4B8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd4f4
	if (!cr0.lt) goto loc_825FD4F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD4F4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,160(r27)
	PPC_STORE_U32(r27.u32 + 160, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd56c
	if (!cr6.lt) goto loc_825FD56C;
loc_825FD514:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd56c
	if (cr6.eq) goto loc_825FD56C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd55c
	if (!cr0.lt) goto loc_825FD55C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD55C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd514
	if (cr6.gt) goto loc_825FD514;
loc_825FD56C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd5a8
	if (!cr0.lt) goto loc_825FD5A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD5A8:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd620
	if (!cr6.lt) goto loc_825FD620;
loc_825FD5C8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd620
	if (cr6.eq) goto loc_825FD620;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd610
	if (!cr0.lt) goto loc_825FD610;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD610:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd5c8
	if (cr6.gt) goto loc_825FD5C8;
loc_825FD620:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd65c
	if (!cr0.lt) goto loc_825FD65C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD65C:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd6d8
	if (!cr6.lt) goto loc_825FD6D8;
loc_825FD680:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd6d8
	if (cr6.eq) goto loc_825FD6D8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd6c8
	if (!cr0.lt) goto loc_825FD6C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD6C8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd680
	if (cr6.gt) goto loc_825FD680;
loc_825FD6D8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd714
	if (!cr0.lt) goto loc_825FD714;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD714:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd78c
	if (!cr6.lt) goto loc_825FD78C;
loc_825FD734:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd78c
	if (cr6.eq) goto loc_825FD78C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd77c
	if (!cr0.lt) goto loc_825FD77C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD77C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd734
	if (cr6.gt) goto loc_825FD734;
loc_825FD78C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd7c8
	if (!cr0.lt) goto loc_825FD7C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD7C8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd844
	if (!cr6.lt) goto loc_825FD844;
loc_825FD7EC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd844
	if (cr6.eq) goto loc_825FD844;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd834
	if (!cr0.lt) goto loc_825FD834;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD834:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd7ec
	if (cr6.gt) goto loc_825FD7EC;
loc_825FD844:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd880
	if (!cr0.lt) goto loc_825FD880;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD880:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x825fc800
	if (!cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd8fc
	if (!cr6.lt) goto loc_825FD8FC;
loc_825FD8A4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd8fc
	if (cr6.eq) goto loc_825FD8FC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd8ec
	if (!cr0.lt) goto loc_825FD8EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD8EC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd8a4
	if (cr6.gt) goto loc_825FD8A4;
loc_825FD8FC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd938
	if (!cr0.lt) goto loc_825FD938;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD938:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// beq cr6,0x825fc800
	if (cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fd9b4
	if (!cr6.lt) goto loc_825FD9B4;
loc_825FD95C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fd9b4
	if (cr6.eq) goto loc_825FD9B4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fd9a4
	if (!cr0.lt) goto loc_825FD9A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD9A4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fd95c
	if (cr6.gt) goto loc_825FD95C;
loc_825FD9B4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fd9f0
	if (!cr0.lt) goto loc_825FD9F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FD9F0:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825fc800
	if (cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fda6c
	if (!cr6.lt) goto loc_825FDA6C;
loc_825FDA14:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fda6c
	if (cr6.eq) goto loc_825FDA6C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fda5c
	if (!cr0.lt) goto loc_825FDA5C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FDA5C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fda14
	if (cr6.gt) goto loc_825FDA14;
loc_825FDA6C:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x825fda94
	if (!cr0.lt) goto loc_825FDA94;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FDA94:
	// li r11,0
	r11.s64 = 0;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// stw r11,3636(r27)
	PPC_STORE_U32(r27.u32 + 3636, r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fdb10
	if (!cr6.lt) goto loc_825FDB10;
loc_825FDAB8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fdb10
	if (cr6.eq) goto loc_825FDB10;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fdb00
	if (!cr0.lt) goto loc_825FDB00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FDB00:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fdab8
	if (cr6.gt) goto loc_825FDAB8;
loc_825FDB10:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fdb4c
	if (!cr0.lt) goto loc_825FDB4C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FDB4C:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// beq cr6,0x825fc800
	if (cr6.eq) goto loc_825FC800;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fdbc8
	if (!cr6.lt) goto loc_825FDBC8;
loc_825FDB70:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fdbc8
	if (cr6.eq) goto loc_825FDBC8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fdbb8
	if (!cr0.lt) goto loc_825FDBB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FDBB8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fdb70
	if (cr6.gt) goto loc_825FDB70;
loc_825FDBC8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fdc04
	if (!cr0.lt) goto loc_825FDC04;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FDC04:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// beq cr6,0x825fc800
	if (cr6.eq) goto loc_825FC800;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825fdc28
	if (cr6.eq) goto loc_825FDC28;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
loc_825FDC28:
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825f5000
	sub_825F5000(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825FDC40"))) PPC_WEAK_FUNC(sub_825FDC40);
PPC_FUNC_IMPL(__imp__sub_825FDC40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r17,364(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r18,356(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// mr r21,r10
	r21.u64 = ctx.r10.u64;
	// mr r22,r4
	r22.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r19,r6
	r19.u64 = ctx.r6.u64;
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// mr r16,r7
	r16.u64 = ctx.r7.u64;
	// mr r30,r8
	r30.u64 = ctx.r8.u64;
	// stw r17,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r17.u32);
	// mr r26,r9
	r26.u64 = ctx.r9.u64;
	// stw r18,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r18.u32);
	// stw r21,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r21.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r17,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r17.u32);
	// beq cr6,0x825fdcb8
	if (cr6.eq) goto loc_825FDCB8;
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// rlwinm r11,r11,10,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x3;
	// addi r10,r11,726
	ctx.r10.s64 = r11.s64 + 726;
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// b 0x825fdcc4
	goto loc_825FDCC4;
loc_825FDCB8:
	// lwz r11,2880(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2880);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// lwz r11,2892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2892);
loc_825FDCC4:
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// li r20,0
	r20.s64 = 0;
	// addi r24,r11,29840
	r24.s64 = r11.s64 + 29840;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// mr r29,r20
	r29.u64 = r20.u64;
	// addi r27,r11,-29984
	r27.s64 = r11.s64 + -29984;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r23,2
	r23.s64 = 131072;
	// addi r25,r11,18400
	r25.s64 = r11.s64 + 18400;
loc_825FDCEC:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// blt cr6,0x825fdd14
	if (cr6.lt) goto loc_825FDD14;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,296(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// lwz r5,2092(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2092);
	// bl 0x82655558
	sub_82655558(ctx, base);
	// b 0x825fdd38
	goto loc_825FDD38;
loc_825FDD14:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825fdd2c
	if (!cr6.eq) goto loc_825FDD2C;
	// li r6,7
	ctx.r6.s64 = 7;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// bl 0x825f9d48
	sub_825F9D48(ctx, base);
	// b 0x825fdd38
	goto loc_825FDD38;
loc_825FDD2C:
	// li r6,11
	ctx.r6.s64 = 11;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// bl 0x825fb248
	sub_825FB248(ctx, base);
loc_825FDD38:
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825fe124
	if (!cr6.eq) goto loc_825FE124;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwzx r7,r11,r10
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// bl 0x826494c0
	sub_826494C0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// beq cr6,0x825fdda8
	if (cr6.eq) goto loc_825FDDA8;
	// lwz r9,1760(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// b 0x825fddf8
	goto loc_825FDDF8;
loc_825FDDA8:
	// lwz r9,296(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwzx r8,r7,r24
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + r24.u32);
	// mullw r11,r8,r11
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// add r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r11,1760(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_825FDDF8:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82649020
	sub_82649020(ctx, base);
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825fe124
	if (!cr6.eq) goto loc_825FE124;
	// cmplwi cr6,r29,1
	cr6.compare<uint32_t>(r29.u32, 1, xer);
	// li r11,8
	r11.s64 = 8;
	// bne cr6,0x825fde40
	if (!cr6.eq) goto loc_825FDE40;
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
loc_825FDE40:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// addi r30,r30,32
	r30.s64 = r30.s64 + 32;
	// addi r26,r26,24
	r26.s64 = r26.s64 + 24;
	// cmplwi cr6,r29,4
	cr6.compare<uint32_t>(r29.u32, 4, xer);
	// blt cr6,0x825fdcec
	if (cr6.lt) goto loc_825FDCEC;
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r28,r10,18656
	r28.s64 = ctx.r10.s64 + 18656;
	// lis r10,-32137
	ctx.r10.s64 = -2106130432;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// addi r27,r10,-25888
	r27.s64 = ctx.r10.s64 + -25888;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// blt cr6,0x825fde90
	if (cr6.lt) goto loc_825FDE90;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,300(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// lwz r5,2096(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2096);
	// bl 0x82655558
	sub_82655558(ctx, base);
	// b 0x825fdeb4
	goto loc_825FDEB4;
loc_825FDE90:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825fdea8
	if (!cr6.eq) goto loc_825FDEA8;
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// bl 0x825f9d48
	sub_825F9D48(ctx, base);
	// b 0x825fdeb4
	goto loc_825FDEB4;
loc_825FDEA8:
	// li r6,12
	ctx.r6.s64 = 12;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// bl 0x825fb248
	sub_825FB248(ctx, base);
loc_825FDEB4:
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825fe124
	if (!cr6.eq) goto loc_825FE124;
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// mr r7,r17
	ctx.r7.u64 = r17.u64;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826494c0
	sub_826494C0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// beq cr6,0x825fdf1c
	if (cr6.eq) goto loc_825FDF1C;
	// lwz r9,1760(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// b 0x825fdf6c
	goto loc_825FDF6C;
loc_825FDF1C:
	// lwz r9,300(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwzx r8,r7,r24
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + r24.u32);
	// mullw r11,r8,r11
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// add r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r11,1760(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_825FDF6C:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r8,4
	ctx.r8.s64 = 4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x825fdfa8
	if (cr6.eq) goto loc_825FDFA8;
	// addi r4,r1,108
	ctx.r4.s64 = ctx.r1.s64 + 108;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// bl 0x82649020
	sub_82649020(ctx, base);
	// b 0x825fdfb8
	goto loc_825FDFB8;
loc_825FDFA8:
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// bl 0x82649020
	sub_82649020(ctx, base);
loc_825FDFB8:
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825fe124
	if (!cr6.eq) goto loc_825FE124;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// addi r30,r30,32
	r30.s64 = r30.s64 + 32;
	// addi r29,r26,24
	r29.s64 = r26.s64 + 24;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// blt cr6,0x825fdff4
	if (cr6.lt) goto loc_825FDFF4;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,300(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// lwz r5,2096(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2096);
	// bl 0x82655558
	sub_82655558(ctx, base);
	// b 0x825fe018
	goto loc_825FE018;
loc_825FDFF4:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825fe00c
	if (!cr6.eq) goto loc_825FE00C;
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// bl 0x825f9d48
	sub_825F9D48(ctx, base);
	// b 0x825fe018
	goto loc_825FE018;
loc_825FE00C:
	// li r6,12
	ctx.r6.s64 = 12;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// bl 0x825fb248
	sub_825FB248(ctx, base);
loc_825FE018:
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825fe124
	if (!cr6.eq) goto loc_825FE124;
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// mr r7,r17
	ctx.r7.u64 = r17.u64;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826494c0
	sub_826494C0(ctx, base);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825fe080
	if (cr6.eq) goto loc_825FE080;
	// lwz r9,1760(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// b 0x825fe0d4
	goto loc_825FE0D4;
loc_825FE080:
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// lhz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// lwzx r11,r11,r24
	r11.u64 = PPC_LOAD_U32(r11.u32 + r24.u32);
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r11,1760(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_825FE0D4:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r8,5
	ctx.r8.s64 = 5;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x825fe114
	if (cr6.eq) goto loc_825FE114;
	// addi r4,r1,108
	ctx.r4.s64 = ctx.r1.s64 + 108;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// bl 0x82649020
	sub_82649020(ctx, base);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239bd18
	return;
loc_825FE114:
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// bl 0x82649020
	sub_82649020(ctx, base);
loc_825FE124:
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_825FE12C"))) PPC_WEAK_FUNC(sub_825FE12C);
PPC_FUNC_IMPL(__imp__sub_825FE12C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FE130"))) PPC_WEAK_FUNC(sub_825FE130);
PPC_FUNC_IMPL(__imp__sub_825FE130) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// stw r11,452(r31)
	PPC_STORE_U32(r31.u32 + 452, r11.u32);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stw r11,332(r31)
	PPC_STORE_U32(r31.u32 + 332, r11.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x8263a398
	sub_8263A398(ctx, base);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825fe2a0
	if (cr6.eq) goto loc_825FE2A0;
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r28,r11,19168
	r28.s64 = r11.s64 + 19168;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// rldicl r11,r11,13,51
	r11.u64 = __builtin_rotateleft64(r11.u64, 13) & 0x1FFF;
	// rlwinm r29,r11,1,0,30
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r29,r28
	ctx.r4.u64 = PPC_LOAD_U8(r29.u32 + r28.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
	// li r26,3
	r26.s64 = 3;
	// lbzx r11,r29,r11
	r11.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fe1b4
	if (!cr6.eq) goto loc_825FE1B4;
	// stw r26,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r26.u32);
loc_825FE1B4:
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825fe1d0
	if (cr6.eq) goto loc_825FE1D0;
loc_825FE1C4:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825FE1D0:
	// lbz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,244(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 244);
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825fe1f8
	if (!cr6.gt) goto loc_825FE1F8;
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// b 0x825fe208
	goto loc_825FE208;
loc_825FE1F8:
	// lwz r10,240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 240);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825fe208
	if (!cr6.lt) goto loc_825FE208;
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
loc_825FE208:
	// stb r11,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r11.u8);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// rldicl r11,r11,13,51
	r11.u64 = __builtin_rotateleft64(r11.u64, 13) & 0x1FFF;
	// rlwinm r29,r11,1,0,30
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r29,r28
	ctx.r4.u64 = PPC_LOAD_U8(r29.u32 + r28.u32);
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
	// lbzx r11,r29,r11
	r11.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bne cr6,0x825fe23c
	if (!cr6.eq) goto loc_825FE23C;
	// stw r26,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r26.u32);
loc_825FE23C:
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825fe1c4
	if (!cr6.eq) goto loc_825FE1C4;
	// lbz r10,81(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r9,244(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 244);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// ble cr6,0x825fe280
	if (!cr6.gt) goto loc_825FE280;
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,1(r27)
	PPC_STORE_U8(r27.u32 + 1, r11.u8);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825FE280:
	// lwz r10,240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 240);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825fe290
	if (!cr6.lt) goto loc_825FE290;
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
loc_825FE290:
	// stb r11,1(r27)
	PPC_STORE_U8(r27.u32 + 1, r11.u8);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825FE2A0:
	// addi r4,r1,82
	ctx.r4.s64 = ctx.r1.s64 + 82;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825faf40
	sub_825FAF40(ctx, base);
	// lbz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// lbz r10,81(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lbz r9,82(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 82);
	// lbz r11,83(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 83);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r4,3560(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3560);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stb r9,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r9.u8);
	// stb r11,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, r11.u8);
	// bl 0x825faed0
	sub_825FAED0(ctx, base);
	// lbz r11,80(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r10,81(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// stb r11,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r11.u8);
	// stb r10,1(r27)
	PPC_STORE_U8(r27.u32 + 1, ctx.r10.u8);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825FE2F4"))) PPC_WEAK_FUNC(sub_825FE2F4);
PPC_FUNC_IMPL(__imp__sub_825FE2F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FE2F8"))) PPC_WEAK_FUNC(sub_825FE2F8);
PPC_FUNC_IMPL(__imp__sub_825FE2F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r5,4096
	r11.s64 = ctx.r5.s64 + 4096;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// mr r29,r28
	r29.u64 = r28.u64;
	// stw r11,21324(r31)
	PPC_STORE_U32(r31.u32 + 21324, r11.u32);
	// beq cr6,0x825fe358
	if (cr6.eq) goto loc_825FE358;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825fe348
	if (!cr6.eq) goto loc_825FE348;
loc_825FE33C:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_825FE348:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,21320(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 21320);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_825FE358:
	// lwz r3,21320(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21320);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825fe36c
	if (cr6.eq) goto loc_825FE36C;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r28,21320(r31)
	PPC_STORE_U32(r31.u32 + 21320, r28.u32);
loc_825FE36C:
	// lwz r3,21316(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21316);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825fe380
	if (cr6.eq) goto loc_825FE380;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r28,21316(r31)
	PPC_STORE_U32(r31.u32 + 21316, r28.u32);
loc_825FE380:
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,21324(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21324);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21316(r31)
	PPC_STORE_U32(r31.u32 + 21316, ctx.r3.u32);
	// beq cr6,0x825fe33c
	if (cr6.eq) goto loc_825FE33C;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,21324(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21324);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21320(r31)
	PPC_STORE_U32(r31.u32 + 21320, ctx.r3.u32);
	// beq cr6,0x825fe33c
	if (cr6.eq) goto loc_825FE33C;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825fe3c4
	if (cr6.eq) goto loc_825FE3C4;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_825FE3C4:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x825fe3d4
	if (cr6.eq) goto loc_825FE3D4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_825FE3D4:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825FE3E0"))) PPC_WEAK_FUNC(sub_825FE3E0);
PPC_FUNC_IMPL(__imp__sub_825FE3E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825fe410
	if (!cr6.eq) goto loc_825FE410;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bne cr6,0x825fe4c8
	if (!cr6.eq) goto loc_825FE4C8;
loc_825FE410:
	// lwz r11,21304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21304);
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// lwz r5,21304(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 21304);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x825fe440
	if (!cr6.gt) goto loc_825FE440;
	// lwz r4,21308(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 21308);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r3,21308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21308);
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_825FE440:
	// lwz r11,21304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21304);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bne cr6,0x825fe474
	if (!cr6.eq) goto loc_825FE474;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bne cr6,0x825fe46c
	if (!cr6.eq) goto loc_825FE46C;
	// li r7,3
	ctx.r7.s64 = 3;
	// b 0x825fe484
	goto loc_825FE484;
loc_825FE46C:
	// li r7,1
	ctx.r7.s64 = 1;
	// b 0x825fe484
	goto loc_825FE484;
loc_825FE474:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// li r7,2
	ctx.r7.s64 = 2;
	// beq cr6,0x825fe484
	if (cr6.eq) goto loc_825FE484;
	// li r7,0
	ctx.r7.s64 = 0;
loc_825FE484:
	// bl 0x825e6680
	sub_825E6680(ctx, base);
	// lwz r11,21304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21304);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21304);
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// stw r27,21308(r31)
	PPC_STORE_U32(r31.u32 + 21308, r27.u32);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// stw r11,21304(r31)
	PPC_STORE_U32(r31.u32 + 21304, r11.u32);
	// bne cr6,0x825fe4c8
	if (!cr6.eq) goto loc_825FE4C8;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,21304(r31)
	PPC_STORE_U32(r31.u32 + 21304, r11.u32);
	// stw r11,21308(r31)
	PPC_STORE_U32(r31.u32 + 21308, r11.u32);
loc_825FE4C8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825FE4D4"))) PPC_WEAK_FUNC(sub_825FE4D4);
PPC_FUNC_IMPL(__imp__sub_825FE4D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FE4D8"))) PPC_WEAK_FUNC(sub_825FE4D8);
PPC_FUNC_IMPL(__imp__sub_825FE4D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r15,r8
	r15.u64 = ctx.r8.u64;
	// stw r4,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r4.u32);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r5,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r5.u32);
	// li r24,0
	r24.s64 = 0;
	// li r22,1
	r22.s64 = 1;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// mr r14,r6
	r14.u64 = ctx.r6.u64;
	// lwz r26,21320(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 21320);
	// mr r21,r24
	r21.u64 = r24.u64;
	// lwz r23,21316(r30)
	r23.u64 = PPC_LOAD_U32(r30.u32 + 21316);
	// mr r25,r24
	r25.u64 = r24.u64;
	// stw r24,21344(r30)
	PPC_STORE_U32(r30.u32 + 21344, r24.u32);
	// mr r17,r24
	r17.u64 = r24.u64;
	// stw r28,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r28.u32);
	// mr r19,r24
	r19.u64 = r24.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// mr r31,r24
	r31.u64 = r24.u64;
	// lwz r11,80(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// mr r16,r24
	r16.u64 = r24.u64;
	// stw r22,32(r11)
	PPC_STORE_U32(r11.u32 + 32, r22.u32);
	// lwz r11,3676(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3676);
	// stw r24,21348(r30)
	PPC_STORE_U32(r30.u32 + 21348, r24.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fe56c
	if (!cr6.eq) goto loc_825FE56C;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r7,21344(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 21344);
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// stw r24,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r24.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825e5318
	sub_825E5318(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825fee8c
	if (!cr6.eq) goto loc_825FEE8C;
loc_825FE56C:
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x825fee88
	if (cr6.eq) goto loc_825FEE88;
	// cmplwi cr6,r14,0
	cr6.compare<uint32_t>(r14.u32, 0, xer);
	// beq cr6,0x825fee88
	if (cr6.eq) goto loc_825FEE88;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x825fee88
	if (cr6.eq) goto loc_825FEE88;
	// lwz r29,276(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// cmplwi cr6,r29,4
	cr6.compare<uint32_t>(r29.u32, 4, xer);
	// bge cr6,0x825fe5ac
	if (!cr6.lt) goto loc_825FE5AC;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r29,276(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mr r31,r29
	r31.u64 = r29.u64;
loc_825FE5AC:
	// lwz r11,21324(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21324);
	// add r5,r31,r29
	ctx.r5.u64 = r31.u64 + r29.u64;
	// cmplw cr6,r5,r11
	cr6.compare<uint32_t>(ctx.r5.u32, r11.u32, xer);
	// ble cr6,0x825fe5e0
	if (!cr6.gt) goto loc_825FE5E0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe2f8
	sub_825FE2F8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825fee8c
	if (!cr6.eq) goto loc_825FEE8C;
	// lwz r26,21320(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 21320);
	// lwz r23,21316(r30)
	r23.u64 = PPC_LOAD_U32(r30.u32 + 21316);
	// lwz r29,276(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
loc_825FE5E0:
	// cmplwi cr6,r29,4
	cr6.compare<uint32_t>(r29.u32, 4, xer);
	// bge cr6,0x825fe67c
	if (!cr6.lt) goto loc_825FE67C;
loc_825FE5E8:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825fe6a4
	if (cr6.eq) goto loc_825FE6A4;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lwz r3,3340(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 3340);
	// addi r7,r1,276
	ctx.r7.s64 = ctx.r1.s64 + 276;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,268
	ctx.r5.s64 = ctx.r1.s64 + 268;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82491030
	sub_82491030(ctx, base);
	// lwz r11,80(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// add r5,r31,r11
	ctx.r5.u64 = r31.u64 + r11.u64;
	// lwz r11,21324(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21324);
	// cmplw cr6,r5,r11
	cr6.compare<uint32_t>(ctx.r5.u32, r11.u32, xer);
	// ble cr6,0x825fe64c
	if (!cr6.gt) goto loc_825FE64C;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe2f8
	sub_825FE2F8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825fee8c
	if (!cr6.eq) goto loc_825FEE8C;
	// lwz r26,21320(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 21320);
	// lwz r23,21316(r30)
	r23.u64 = PPC_LOAD_U32(r30.u32 + 21316);
loc_825FE64C:
	// add r3,r31,r26
	ctx.r3.u64 = r31.u64 + r26.u64;
	// lwz r5,276(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r31,r31,r11
	r31.u64 = r31.u64 + r11.u64;
	// mr r29,r31
	r29.u64 = r31.u64;
	// cmplwi cr6,r31,4
	cr6.compare<uint32_t>(r31.u32, 4, xer);
	// stw r4,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r4.u32);
	// stw r29,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, r29.u32);
	// blt cr6,0x825fe5e8
	if (cr6.lt) goto loc_825FE5E8;
loc_825FE67C:
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825fe6a4
	if (!cr6.eq) goto loc_825FE6A4;
	// lbz r11,1(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825fe6a4
	if (!cr6.eq) goto loc_825FE6A4;
	// lbz r11,2(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// mr r11,r24
	r11.u64 = r24.u64;
	// beq cr6,0x825fe6a8
	if (cr6.eq) goto loc_825FE6A8;
loc_825FE6A4:
	// mr r11,r22
	r11.u64 = r22.u64;
loc_825FE6A8:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,21292(r30)
	PPC_STORE_U32(r30.u32 + 21292, r11.u32);
	// stw r24,21340(r30)
	PPC_STORE_U32(r30.u32 + 21340, r24.u32);
	// beq cr6,0x825fe6d4
	if (cr6.eq) goto loc_825FE6D4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r29,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r29.u32);
	// stw r4,0(r14)
	PPC_STORE_U32(r14.u32 + 0, ctx.r4.u32);
	// stw r11,0(r15)
	PPC_STORE_U32(r15.u32 + 0, r11.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_825FE6D4:
	// li r20,3
	r20.s64 = 3;
	// li r18,2
	r18.s64 = 2;
	// cmplwi cr6,r29,4
	cr6.compare<uint32_t>(r29.u32, 4, xer);
	// blt cr6,0x825feda8
	if (cr6.lt) goto loc_825FEDA8;
loc_825FE6E4:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x825feda4
	if (!cr6.eq) goto loc_825FEDA4;
loc_825FE6EC:
	// addi r11,r4,4
	r11.s64 = ctx.r4.s64 + 4;
	// lbz r3,3(r4)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r4.u32 + 3);
	// cmplwi cr6,r29,8
	cr6.compare<uint32_t>(r29.u32, 8, xer);
	// mr r27,r11
	r27.u64 = r11.u64;
	// blt cr6,0x825fe7dc
	if (cr6.lt) goto loc_825FE7DC;
	// clrlwi r9,r4,31
	ctx.r9.u64 = ctx.r4.u32 & 0x1;
	// li r10,4
	ctx.r10.s64 = 4;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x825fe744
	if (cr6.eq) goto loc_825FE744;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825fe73c
	if (!cr6.eq) goto loc_825FE73C;
	// lbz r11,5(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 5);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825fe73c
	if (!cr6.eq) goto loc_825FE73C;
	// lbz r11,6(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 6);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x825fe73c
	if (!cr6.eq) goto loc_825FE73C;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// b 0x825fe7d4
	goto loc_825FE7D4;
loc_825FE73C:
	// addi r11,r4,5
	r11.s64 = ctx.r4.s64 + 5;
	// li r10,5
	ctx.r10.s64 = 5;
loc_825FE744:
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r5,r29,-1
	ctx.r5.s64 = r29.s64 + -1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// bge cr6,0x825fe7dc
	if (!cr6.lt) goto loc_825FE7DC;
	// addi r6,r10,2
	ctx.r6.s64 = ctx.r10.s64 + 2;
loc_825FE760:
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// and r31,r9,r7
	r31.u64 = ctx.r9.u64 & ctx.r7.u64;
	// clrlwi r31,r31,16
	r31.u64 = r31.u32 & 0xFFFF;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825fe7ac
	if (!cr6.eq) goto loc_825FE7AC;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825fe790
	if (!cr6.eq) goto loc_825FE790;
	// rlwinm r31,r7,0,16,23
	r31.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFF00;
	// cmplwi cr6,r31,256
	cr6.compare<uint32_t>(r31.u32, 256, xer);
	// beq cr6,0x825fe7c8
	if (cr6.eq) goto loc_825FE7C8;
loc_825FE790:
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825fe7ac
	if (!cr6.eq) goto loc_825FE7AC;
	// cmplwi cr6,r7,1
	cr6.compare<uint32_t>(ctx.r7.u32, 1, xer);
	// bne cr6,0x825fe7ac
	if (!cr6.eq) goto loc_825FE7AC;
	// cmplw cr6,r29,r6
	cr6.compare<uint32_t>(r29.u32, ctx.r6.u32, xer);
	// bgt cr6,0x825fe7d0
	if (cr6.gt) goto loc_825FE7D0;
loc_825FE7AC:
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// blt cr6,0x825fe760
	if (cr6.lt) goto loc_825FE760;
	// b 0x825fe7dc
	goto loc_825FE7DC;
loc_825FE7C8:
	// addi r28,r11,-2
	r28.s64 = r11.s64 + -2;
	// b 0x825fe7d4
	goto loc_825FE7D4;
loc_825FE7D0:
	// addi r28,r11,-1
	r28.s64 = r11.s64 + -1;
loc_825FE7D4:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x825fe7e4
	if (!cr6.eq) goto loc_825FE7E4;
loc_825FE7DC:
	// add r28,r4,r29
	r28.u64 = ctx.r4.u64 + r29.u64;
	// mr r25,r22
	r25.u64 = r22.u64;
loc_825FE7E4:
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,13
	cr6.compare<uint32_t>(r11.u32, 13, xer);
	// stw r11,21296(r30)
	PPC_STORE_U32(r30.u32 + 21296, r11.u32);
	// beq cr6,0x825fe804
	if (cr6.eq) goto loc_825FE804;
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// beq cr6,0x825fe804
	if (cr6.eq) goto loc_825FE804;
	// cmplwi cr6,r11,11
	cr6.compare<uint32_t>(r11.u32, 11, xer);
	// bne cr6,0x825fe808
	if (!cr6.eq) goto loc_825FE808;
loc_825FE804:
	// mr r17,r22
	r17.u64 = r22.u64;
loc_825FE808:
	// cmpwi cr6,r25,1
	cr6.compare<int32_t>(r25.s32, 1, xer);
	// bne cr6,0x825fe8b8
	if (!cr6.eq) goto loc_825FE8B8;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// bne cr6,0x825fe8b8
	if (!cr6.eq) goto loc_825FE8B8;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bne cr6,0x825fe8bc
	if (!cr6.eq) goto loc_825FE8BC;
	// subf r31,r4,r28
	r31.s64 = r28.s64 - ctx.r4.s64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,276
	ctx.r7.s64 = ctx.r1.s64 + 276;
	// lwz r3,3340(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 3340);
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,268
	ctx.r5.s64 = ctx.r1.s64 + 268;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82491030
	sub_82491030(ctx, base);
	// lwz r11,80(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// add r5,r31,r11
	ctx.r5.u64 = r31.u64 + r11.u64;
	// lwz r11,21324(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21324);
	// cmplw cr6,r5,r11
	cr6.compare<uint32_t>(ctx.r5.u32, r11.u32, xer);
	// ble cr6,0x825fe88c
	if (!cr6.gt) goto loc_825FE88C;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe2f8
	sub_825FE2F8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825fee8c
	if (!cr6.eq) goto loc_825FEE8C;
	// lwz r26,21320(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 21320);
	// lwz r23,21316(r30)
	r23.u64 = PPC_LOAD_U32(r30.u32 + 21316);
loc_825FE88C:
	// add r3,r31,r26
	ctx.r3.u64 = r31.u64 + r26.u64;
	// lwz r5,276(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r29,r31,r11
	r29.u64 = r31.u64 + r11.u64;
	// mr r25,r24
	r25.u64 = r24.u64;
	// stw r4,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r4.u32);
	// stw r29,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, r29.u32);
	// b 0x825fe6ec
	goto loc_825FE6EC;
loc_825FE8B8:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_825FE8BC:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825fe974
	if (cr6.eq) goto loc_825FE974;
	// cmpwi cr6,r25,1
	cr6.compare<int32_t>(r25.s32, 1, xer);
	// bne cr6,0x825fe974
	if (!cr6.eq) goto loc_825FE974;
	// subf r11,r27,r28
	r11.s64 = r28.s64 - r27.s64;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// ble cr6,0x825fe910
	if (!cr6.gt) goto loc_825FE910;
	// lbz r10,-1(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + -1);
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bne cr6,0x825fe910
	if (!cr6.eq) goto loc_825FE910;
	// lbz r10,-2(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + -2);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825fe910
	if (!cr6.eq) goto loc_825FE910;
	// lbz r10,-3(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + -3);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825fe910
	if (!cr6.eq) goto loc_825FE910;
	// stb r24,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r24.u8);
	// stb r24,21313(r30)
	PPC_STORE_U8(r30.u32 + 21313, r24.u8);
	// stb r22,21314(r30)
	PPC_STORE_U8(r30.u32 + 21314, r22.u8);
	// stw r20,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r20.u32);
	// b 0x825fe964
	goto loc_825FE964;
loc_825FE910:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825fe940
	if (!cr6.gt) goto loc_825FE940;
	// lbz r10,-1(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + -1);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825fe940
	if (!cr6.eq) goto loc_825FE940;
	// lbz r10,-2(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + -2);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825fe940
	if (!cr6.eq) goto loc_825FE940;
	// stb r24,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r24.u8);
	// stb r24,21313(r30)
	PPC_STORE_U8(r30.u32 + 21313, r24.u8);
	// stw r18,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r18.u32);
	// b 0x825fe964
	goto loc_825FE964;
loc_825FE940:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fe960
	if (cr6.eq) goto loc_825FE960;
	// lbz r10,-1(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + -1);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825fe960
	if (!cr6.eq) goto loc_825FE960;
	// stb r24,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r24.u8);
	// stw r22,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r22.u32);
	// b 0x825fe964
	goto loc_825FE964;
loc_825FE960:
	// stw r24,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r24.u32);
loc_825FE964:
	// lwz r10,21300(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// mr r19,r22
	r19.u64 = r22.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// add r28,r11,r27
	r28.u64 = r11.u64 + r27.u64;
loc_825FE974:
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,13
	cr6.compare<uint32_t>(ctx.r10.u32, 13, xer);
	// beq cr6,0x825fec14
	if (cr6.eq) goto loc_825FEC14;
	// cmplwi cr6,r10,12
	cr6.compare<uint32_t>(ctx.r10.u32, 12, xer);
	// beq cr6,0x825fec14
	if (cr6.eq) goto loc_825FEC14;
	// cmplwi cr6,r10,11
	cr6.compare<uint32_t>(ctx.r10.u32, 11, xer);
	// beq cr6,0x825fec14
	if (cr6.eq) goto loc_825FEC14;
	// addi r11,r10,-10
	r11.s64 = ctx.r10.s64 + -10;
	// subf r5,r27,r28
	ctx.r5.s64 = r28.s64 - r27.s64;
	// cmplwi cr6,r11,54
	cr6.compare<uint32_t>(r11.u32, 54, xer);
	// bgt cr6,0x825fec0c
	if (cr6.gt) goto loc_825FEC0C;
	// lis r12,-32160
	r12.s64 = -2107637760;
	// addi r12,r12,-5704
	r12.s64 = r12.s64 + -5704;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825FEC04;
	case 1:
		goto loc_825FEC0C;
	case 2:
		goto loc_825FEC0C;
	case 3:
		goto loc_825FEC0C;
	case 4:
		goto loc_825FEAD4;
	case 5:
		goto loc_825FEA94;
	case 6:
		goto loc_825FEC0C;
	case 7:
		goto loc_825FEC0C;
	case 8:
		goto loc_825FEC0C;
	case 9:
		goto loc_825FEC0C;
	case 10:
		goto loc_825FEC0C;
	case 11:
		goto loc_825FEC0C;
	case 12:
		goto loc_825FEC0C;
	case 13:
		goto loc_825FEC0C;
	case 14:
		goto loc_825FEC0C;
	case 15:
		goto loc_825FEC0C;
	case 16:
		goto loc_825FEC0C;
	case 17:
		goto loc_825FEBD4;
	case 18:
		goto loc_825FEBA4;
	case 19:
		goto loc_825FEB74;
	case 20:
		goto loc_825FEB44;
	case 21:
		goto loc_825FEB14;
	case 22:
		goto loc_825FED88;
	case 23:
		goto loc_825FED88;
	case 24:
		goto loc_825FED88;
	case 25:
		goto loc_825FED88;
	case 26:
		goto loc_825FED88;
	case 27:
		goto loc_825FED88;
	case 28:
		goto loc_825FED88;
	case 29:
		goto loc_825FED88;
	case 30:
		goto loc_825FED88;
	case 31:
		goto loc_825FED88;
	case 32:
		goto loc_825FED88;
	case 33:
		goto loc_825FED88;
	case 34:
		goto loc_825FED88;
	case 35:
		goto loc_825FED88;
	case 36:
		goto loc_825FED88;
	case 37:
		goto loc_825FED88;
	case 38:
		goto loc_825FED88;
	case 39:
		goto loc_825FED88;
	case 40:
		goto loc_825FED88;
	case 41:
		goto loc_825FED88;
	case 42:
		goto loc_825FED88;
	case 43:
		goto loc_825FED88;
	case 44:
		goto loc_825FED88;
	case 45:
		goto loc_825FED88;
	case 46:
		goto loc_825FED88;
	case 47:
		goto loc_825FED88;
	case 48:
		goto loc_825FED88;
	case 49:
		goto loc_825FED88;
	case 50:
		goto loc_825FED88;
	case 51:
		goto loc_825FED88;
	case 52:
		goto loc_825FED88;
	case 53:
		goto loc_825FED88;
	case 54:
		goto loc_825FED88;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-5116(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5116);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5420(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5420);
	// lwz r18,-5484(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5484);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5108(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5108);
	// lwz r18,-5164(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5164);
	// lwz r18,-5212(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5212);
	// lwz r18,-5260(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5260);
	// lwz r18,-5308(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5308);
	// lwz r18,-5356(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -5356);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
	// lwz r18,-4728(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -4728);
loc_825FEA94:
	// lwz r11,15472(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15472);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r3,80(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// addi r11,r11,-7
	r11.s64 = r11.s64 + -7;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bl 0x825eb668
	sub_825EB668(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82603610
	sub_82603610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825fed80
	if (cr6.eq) goto loc_825FED80;
	// stw r22,3676(r30)
	PPC_STORE_U32(r30.u32 + 3676, r22.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_825FEAD4:
	// lwz r11,15472(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15472);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r3,80(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// addi r11,r11,-7
	r11.s64 = r11.s64 + -7;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bl 0x825eb668
	sub_825EB668(ctx, base);
	// lwz r11,3676(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3676);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fee08
	if (!cr6.eq) goto loc_825FEE08;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825e96c8
	sub_825E96C8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825fee8c
	if (!cr6.eq) goto loc_825FEE8C;
	// b 0x825fed80
	goto loc_825FED80;
loc_825FEB14:
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bne cr6,0x825feb28
	if (!cr6.eq) goto loc_825FEB28;
	// cmpwi cr6,r25,1
	cr6.compare<int32_t>(r25.s32, 1, xer);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// beq cr6,0x825feb2c
	if (cr6.eq) goto loc_825FEB2C;
loc_825FEB28:
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
loc_825FEB2C:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe3e0
	sub_825FE3E0(ctx, base);
	// b 0x825fed80
	goto loc_825FED80;
loc_825FEB44:
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bne cr6,0x825feb58
	if (!cr6.eq) goto loc_825FEB58;
	// cmpwi cr6,r25,1
	cr6.compare<int32_t>(r25.s32, 1, xer);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// beq cr6,0x825feb5c
	if (cr6.eq) goto loc_825FEB5C;
loc_825FEB58:
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
loc_825FEB5C:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe3e0
	sub_825FE3E0(ctx, base);
	// b 0x825fed80
	goto loc_825FED80;
loc_825FEB74:
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bne cr6,0x825feb88
	if (!cr6.eq) goto loc_825FEB88;
	// cmpwi cr6,r25,1
	cr6.compare<int32_t>(r25.s32, 1, xer);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// beq cr6,0x825feb8c
	if (cr6.eq) goto loc_825FEB8C;
loc_825FEB88:
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
loc_825FEB8C:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe3e0
	sub_825FE3E0(ctx, base);
	// b 0x825fed80
	goto loc_825FED80;
loc_825FEBA4:
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bne cr6,0x825febb8
	if (!cr6.eq) goto loc_825FEBB8;
	// cmpwi cr6,r25,1
	cr6.compare<int32_t>(r25.s32, 1, xer);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// beq cr6,0x825febbc
	if (cr6.eq) goto loc_825FEBBC;
loc_825FEBB8:
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
loc_825FEBBC:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe3e0
	sub_825FE3E0(ctx, base);
	// b 0x825fed80
	goto loc_825FED80;
loc_825FEBD4:
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bne cr6,0x825febe8
	if (!cr6.eq) goto loc_825FEBE8;
	// cmpwi cr6,r25,1
	cr6.compare<int32_t>(r25.s32, 1, xer);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// beq cr6,0x825febec
	if (cr6.eq) goto loc_825FEBEC;
loc_825FEBE8:
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
loc_825FEBEC:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe3e0
	sub_825FE3E0(ctx, base);
	// b 0x825fed80
	goto loc_825FED80;
loc_825FEC04:
	// stw r22,3680(r30)
	PPC_STORE_U32(r30.u32 + 3680, r22.u32);
	// b 0x825fed88
	goto loc_825FED88;
loc_825FEC0C:
	// li r16,4
	r16.s64 = 4;
	// b 0x825fed88
	goto loc_825FED88;
loc_825FEC14:
	// subf r31,r27,r28
	r31.s64 = r28.s64 - r27.s64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825fec44
	if (cr6.eq) goto loc_825FEC44;
	// add r11,r31,r27
	r11.u64 = r31.u64 + r27.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
loc_825FEC28:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825fec44
	if (!cr6.eq) goto loc_825FEC44;
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825fec28
	if (!cr6.eq) goto loc_825FEC28;
loc_825FEC44:
	// lwz r11,3676(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3676);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825fed6c
	if (!cr6.eq) goto loc_825FED6C;
	// cmplwi cr6,r10,12
	cr6.compare<uint32_t>(ctx.r10.u32, 12, xer);
	// bne cr6,0x825fec6c
	if (!cr6.eq) goto loc_825FEC6C;
	// lwz r11,21244(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21244);
	// stw r31,21260(r30)
	PPC_STORE_U32(r30.u32 + 21260, r31.u32);
	// stw r22,21344(r30)
	PPC_STORE_U32(r30.u32 + 21344, r22.u32);
	// stw r21,21348(r30)
	PPC_STORE_U32(r30.u32 + 21348, r21.u32);
	// stw r11,21248(r30)
	PPC_STORE_U32(r30.u32 + 21248, r11.u32);
loc_825FEC6C:
	// cmplwi cr6,r10,13
	cr6.compare<uint32_t>(ctx.r10.u32, 13, xer);
	// bne cr6,0x825fec84
	if (!cr6.eq) goto loc_825FEC84;
	// lwz r11,21288(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21288);
	// stw r31,21256(r30)
	PPC_STORE_U32(r30.u32 + 21256, r31.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,21288(r30)
	PPC_STORE_U32(r30.u32 + 21288, r11.u32);
loc_825FEC84:
	// cmplwi cr6,r10,11
	cr6.compare<uint32_t>(ctx.r10.u32, 11, xer);
	// bne cr6,0x825fed6c
	if (!cr6.eq) goto loc_825FED6C;
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// ble cr6,0x825fed08
	if (!cr6.gt) goto loc_825FED08;
	// addi r29,r27,1
	r29.s64 = r27.s64 + 1;
	// lbz r11,0(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// lwz r7,21344(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 21344);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// li r4,1
	ctx.r4.s64 = 1;
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm r10,r10,25,7,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x1FFFFFF;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x825e5318
	sub_825E5318(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825fed6c
	if (cr6.eq) goto loc_825FED6C;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// bne cr6,0x825fee00
	if (!cr6.eq) goto loc_825FEE00;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r16,4
	r16.s64 = 4;
	// addi r31,r31,-2
	r31.s64 = r31.s64 + -2;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r27,r27,2
	r27.s64 = r27.s64 + 2;
	// stbx r11,r23,r21
	PPC_STORE_U8(r23.u32 + r21.u32, r11.u8);
	// addi r11,r21,1
	r11.s64 = r21.s64 + 1;
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r21,r11,1
	r21.s64 = r11.s64 + 1;
	// clrlwi r10,r10,25
	ctx.r10.u64 = ctx.r10.u32 & 0x7F;
	// stbx r10,r23,r11
	PPC_STORE_U8(r23.u32 + r11.u32, ctx.r10.u8);
	// b 0x825fed6c
	goto loc_825FED6C;
loc_825FED08:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825fed6c
	if (!cr6.eq) goto loc_825FED6C;
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// stw r22,21340(r30)
	PPC_STORE_U32(r30.u32 + 21340, r22.u32);
	// bne cr6,0x825fed6c
	if (!cr6.eq) goto loc_825FED6C;
	// lwz r11,21300(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825fee08
	if (cr6.gt) goto loc_825FEE08;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fed54
	if (cr6.eq) goto loc_825FED54;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// addi r10,r10,21312
	ctx.r10.s64 = ctx.r10.s64 + 21312;
loc_825FED3C:
	// lbz r9,-1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// bne cr6,0x825fed3c
	if (!cr6.eq) goto loc_825FED3C;
loc_825FED54:
	// lwz r11,21300(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// mr r31,r24
	r31.u64 = r24.u64;
	// lbz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r10,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, ctx.r10.u8);
	// stw r11,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r11.u32);
loc_825FED6C:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r3,r23,r21
	ctx.r3.u64 = r23.u64 + r21.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r21,r31,r21
	r21.u64 = r31.u64 + r21.u64;
loc_825FED80:
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// lwz r29,276(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
loc_825FED88:
	// subf r11,r28,r4
	r11.s64 = ctx.r4.s64 - r28.s64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// cmplwi cr6,r29,4
	cr6.compare<uint32_t>(r29.u32, 4, xer);
	// stw r4,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r4.u32);
	// stw r29,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, r29.u32);
	// bge cr6,0x825fe6e4
	if (!cr6.lt) goto loc_825FE6E4;
loc_825FEDA4:
	// lwz r28,292(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
loc_825FEDA8:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825fee70
	if (cr6.eq) goto loc_825FEE70;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x825fee70
	if (!cr6.eq) goto loc_825FEE70;
	// add r11,r23,r21
	r11.u64 = r23.u64 + r21.u64;
	// cmplwi cr6,r21,2
	cr6.compare<uint32_t>(r21.u32, 2, xer);
	// ble cr6,0x825fee14
	if (!cr6.gt) goto loc_825FEE14;
	// lbz r9,-1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x825fee14
	if (!cr6.eq) goto loc_825FEE14;
	// lbz r9,-2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825fee14
	if (!cr6.eq) goto loc_825FEE14;
	// lbz r9,-3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -3);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825fee14
	if (!cr6.eq) goto loc_825FEE14;
	// stb r24,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r24.u8);
	// stb r24,21313(r30)
	PPC_STORE_U8(r30.u32 + 21313, r24.u8);
	// stb r22,21314(r30)
	PPC_STORE_U8(r30.u32 + 21314, r22.u8);
	// stw r20,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r20.u32);
	// b 0x825fee68
	goto loc_825FEE68;
loc_825FEE00:
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// bne cr6,0x825fee8c
	if (!cr6.eq) goto loc_825FEE8C;
loc_825FEE08:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_825FEE14:
	// cmplwi cr6,r21,1
	cr6.compare<uint32_t>(r21.u32, 1, xer);
	// ble cr6,0x825fee44
	if (!cr6.gt) goto loc_825FEE44;
	// lbz r9,-1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825fee44
	if (!cr6.eq) goto loc_825FEE44;
	// lbz r9,-2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825fee44
	if (!cr6.eq) goto loc_825FEE44;
	// stb r24,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r24.u8);
	// stb r24,21313(r30)
	PPC_STORE_U8(r30.u32 + 21313, r24.u8);
	// stw r18,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r18.u32);
	// b 0x825fee68
	goto loc_825FEE68;
loc_825FEE44:
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x825fee64
	if (cr6.eq) goto loc_825FEE64;
	// lbz r11,-1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825fee64
	if (!cr6.eq) goto loc_825FEE64;
	// stb r24,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r24.u8);
	// stw r22,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r22.u32);
	// b 0x825fee68
	goto loc_825FEE68;
loc_825FEE64:
	// stw r24,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r24.u32);
loc_825FEE68:
	// lwz r11,21300(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// subf r21,r11,r21
	r21.s64 = r21.s64 - r11.s64;
loc_825FEE70:
	// stw r23,0(r14)
	PPC_STORE_U32(r14.u32 + 0, r23.u32);
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// stw r21,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r21.u32);
	// stw r10,0(r15)
	PPC_STORE_U32(r15.u32 + 0, ctx.r10.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_825FEE88:
	// li r3,7
	ctx.r3.s64 = 7;
loc_825FEE8C:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825FEE94"))) PPC_WEAK_FUNC(sub_825FEE94);
PPC_FUNC_IMPL(__imp__sub_825FEE94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FEE98"))) PPC_WEAK_FUNC(sub_825FEE98);
PPC_FUNC_IMPL(__imp__sub_825FEE98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// li r22,0
	r22.s64 = 0;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r31,21320(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 21320);
	// mr r14,r6
	r14.u64 = ctx.r6.u64;
	// lwz r21,21316(r30)
	r21.u64 = PPC_LOAD_U32(r30.u32 + 21316);
	// mr r15,r8
	r15.u64 = ctx.r8.u64;
	// stw r29,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r29.u32);
	// mr r23,r22
	r23.u64 = r22.u64;
	// mr r19,r22
	r19.u64 = r22.u64;
	// mr r20,r22
	r20.u64 = r22.u64;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x825ff6a8
	if (cr6.eq) goto loc_825FF6A8;
	// cmplwi cr6,r14,0
	cr6.compare<uint32_t>(r14.u32, 0, xer);
	// beq cr6,0x825ff6a8
	if (cr6.eq) goto loc_825FF6A8;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x825ff6a8
	if (cr6.eq) goto loc_825FF6A8;
	// lwz r11,21300(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// lwz r10,21324(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 21324);
	// add r5,r11,r26
	ctx.r5.u64 = r11.u64 + r26.u64;
	// cmplw cr6,r5,r10
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r10.u32, xer);
	// ble cr6,0x825fef1c
	if (!cr6.gt) goto loc_825FEF1C;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825fe2f8
	sub_825FE2F8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825ff6a8
	if (!cr6.eq) goto loc_825FF6A8;
	// lwz r31,21320(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 21320);
	// lwz r21,21316(r30)
	r21.u64 = PPC_LOAD_U32(r30.u32 + 21316);
loc_825FEF1C:
	// lwz r11,21292(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21292);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825fef3c
	if (cr6.eq) goto loc_825FEF3C;
	// stw r26,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r25,0(r14)
	PPC_STORE_U32(r14.u32 + 0, r25.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_825FEF3C:
	// lwz r10,21300(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825fef8c
	if (!cr6.gt) goto loc_825FEF8C;
	// mr r11,r22
	r11.u64 = r22.u64;
	// addi r10,r30,21312
	ctx.r10.s64 = r30.s64 + 21312;
loc_825FEF50:
	// lbzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// stbx r9,r11,r31
	PPC_STORE_U8(r11.u32 + r31.u32, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,21300(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825fef50
	if (cr6.lt) goto loc_825FEF50;
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// add r3,r11,r31
	ctx.r3.u64 = r11.u64 + r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21300(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// mr r25,r31
	r25.u64 = r31.u64;
	// stw r22,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r22.u32);
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
loc_825FEF8C:
	// lwz r11,21296(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21296);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// li r24,1
	r24.s64 = 1;
	// clrlwi r18,r11,24
	r18.u64 = r11.u32 & 0xFF;
	// li r17,3
	r17.s64 = 3;
	// li r16,2
	r16.s64 = 2;
loc_825FEFA4:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x825ff5d0
	if (!cr6.eq) goto loc_825FF5D0;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x825fefb8
	if (cr6.eq) goto loc_825FEFB8;
	// lbz r18,3(r25)
	r18.u64 = PPC_LOAD_U8(r25.u32 + 3);
loc_825FEFB8:
	// add r11,r4,r25
	r11.u64 = ctx.r4.u64 + r25.u64;
	// addi r10,r4,4
	ctx.r10.s64 = ctx.r4.s64 + 4;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplw cr6,r26,r10
	cr6.compare<uint32_t>(r26.u32, ctx.r10.u32, xer);
	// blt cr6,0x825ff0a8
	if (cr6.lt) goto loc_825FF0A8;
	// clrlwi r9,r25,31
	ctx.r9.u64 = r25.u32 & 0x1;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x825ff010
	if (cr6.eq) goto loc_825FF010;
	// lbz r10,4(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff008
	if (!cr6.eq) goto loc_825FF008;
	// lbz r10,5(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 5);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff008
	if (!cr6.eq) goto loc_825FF008;
	// lbz r10,6(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 6);
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bne cr6,0x825ff008
	if (!cr6.eq) goto loc_825FF008;
	// mr r27,r25
	r27.u64 = r25.u64;
	// b 0x825ff0a0
	goto loc_825FF0A0;
loc_825FF008:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r4,1
	ctx.r10.s64 = ctx.r4.s64 + 1;
loc_825FF010:
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r5,r26,-1
	ctx.r5.s64 = r26.s64 + -1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// bge cr6,0x825ff0a8
	if (!cr6.lt) goto loc_825FF0A8;
	// addi r6,r10,2
	ctx.r6.s64 = ctx.r10.s64 + 2;
loc_825FF02C:
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// and r3,r9,r7
	ctx.r3.u64 = ctx.r9.u64 & ctx.r7.u64;
	// clrlwi r3,r3,16
	ctx.r3.u64 = ctx.r3.u32 & 0xFFFF;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x825ff078
	if (!cr6.eq) goto loc_825FF078;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825ff05c
	if (!cr6.eq) goto loc_825FF05C;
	// rlwinm r3,r7,0,16,23
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFF00;
	// cmplwi cr6,r3,256
	cr6.compare<uint32_t>(ctx.r3.u32, 256, xer);
	// beq cr6,0x825ff094
	if (cr6.eq) goto loc_825FF094;
loc_825FF05C:
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825ff078
	if (!cr6.eq) goto loc_825FF078;
	// cmplwi cr6,r7,1
	cr6.compare<uint32_t>(ctx.r7.u32, 1, xer);
	// bne cr6,0x825ff078
	if (!cr6.eq) goto loc_825FF078;
	// cmplw cr6,r26,r6
	cr6.compare<uint32_t>(r26.u32, ctx.r6.u32, xer);
	// bgt cr6,0x825ff09c
	if (cr6.gt) goto loc_825FF09C;
loc_825FF078:
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// blt cr6,0x825ff02c
	if (cr6.lt) goto loc_825FF02C;
	// b 0x825ff0a8
	goto loc_825FF0A8;
loc_825FF094:
	// addi r27,r11,-2
	r27.s64 = r11.s64 + -2;
	// b 0x825ff0a0
	goto loc_825FF0A0;
loc_825FF09C:
	// addi r27,r11,-1
	r27.s64 = r11.s64 + -1;
loc_825FF0A0:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x825ff0b8
	if (!cr6.eq) goto loc_825FF0B8;
loc_825FF0A8:
	// clrlwi r11,r18,24
	r11.u64 = r18.u32 & 0xFF;
	// add r27,r25,r26
	r27.u64 = r25.u64 + r26.u64;
	// mr r19,r24
	r19.u64 = r24.u64;
	// stw r11,21296(r30)
	PPC_STORE_U32(r30.u32 + 21296, r11.u32);
loc_825FF0B8:
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// beq cr6,0x825ff170
	if (cr6.eq) goto loc_825FF170;
	// cmpwi cr6,r19,1
	cr6.compare<int32_t>(r19.s32, 1, xer);
	// bne cr6,0x825ff170
	if (!cr6.eq) goto loc_825FF170;
	// subf r11,r28,r27
	r11.s64 = r27.s64 - r28.s64;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// ble cr6,0x825ff10c
	if (!cr6.gt) goto loc_825FF10C;
	// lbz r10,-1(r27)
	ctx.r10.u64 = PPC_LOAD_U8(r27.u32 + -1);
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bne cr6,0x825ff10c
	if (!cr6.eq) goto loc_825FF10C;
	// lbz r10,-2(r27)
	ctx.r10.u64 = PPC_LOAD_U8(r27.u32 + -2);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff10c
	if (!cr6.eq) goto loc_825FF10C;
	// lbz r10,-3(r27)
	ctx.r10.u64 = PPC_LOAD_U8(r27.u32 + -3);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff10c
	if (!cr6.eq) goto loc_825FF10C;
	// stw r17,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r17.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r22.u8);
	// stb r22,21313(r30)
	PPC_STORE_U8(r30.u32 + 21313, r22.u8);
	// stb r24,21314(r30)
	PPC_STORE_U8(r30.u32 + 21314, r24.u8);
	// b 0x825ff160
	goto loc_825FF160;
loc_825FF10C:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825ff13c
	if (!cr6.gt) goto loc_825FF13C;
	// lbz r10,-1(r27)
	ctx.r10.u64 = PPC_LOAD_U8(r27.u32 + -1);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff13c
	if (!cr6.eq) goto loc_825FF13C;
	// lbz r10,-2(r27)
	ctx.r10.u64 = PPC_LOAD_U8(r27.u32 + -2);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff13c
	if (!cr6.eq) goto loc_825FF13C;
	// stw r16,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r16.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r22.u8);
	// stb r22,21313(r30)
	PPC_STORE_U8(r30.u32 + 21313, r22.u8);
	// b 0x825ff160
	goto loc_825FF160;
loc_825FF13C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ff15c
	if (cr6.eq) goto loc_825FF15C;
	// lbz r10,-1(r27)
	ctx.r10.u64 = PPC_LOAD_U8(r27.u32 + -1);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff15c
	if (!cr6.eq) goto loc_825FF15C;
	// stw r24,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r24.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r22.u8);
	// b 0x825ff160
	goto loc_825FF160;
loc_825FF15C:
	// stw r22,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r22.u32);
loc_825FF160:
	// lwz r10,21300(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// mr r20,r24
	r20.u64 = r24.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// add r27,r11,r28
	r27.u64 = r11.u64 + r28.u64;
loc_825FF170:
	// clrlwi r10,r18,24
	ctx.r10.u64 = r18.u32 & 0xFF;
	// cmplwi cr6,r10,13
	cr6.compare<uint32_t>(ctx.r10.u32, 13, xer);
	// beq cr6,0x825ff36c
	if (cr6.eq) goto loc_825FF36C;
	// cmplwi cr6,r10,12
	cr6.compare<uint32_t>(ctx.r10.u32, 12, xer);
	// beq cr6,0x825ff36c
	if (cr6.eq) goto loc_825FF36C;
	// cmplwi cr6,r10,11
	cr6.compare<uint32_t>(ctx.r10.u32, 11, xer);
	// beq cr6,0x825ff36c
	if (cr6.eq) goto loc_825FF36C;
	// addi r11,r10,-10
	r11.s64 = ctx.r10.s64 + -10;
	// subf r6,r28,r27
	ctx.r6.s64 = r27.s64 - r28.s64;
	// cmplwi cr6,r11,54
	cr6.compare<uint32_t>(r11.u32, 54, xer);
	// bgt cr6,0x825ff5b8
	if (cr6.gt) goto loc_825FF5B8;
	// lis r12,-32160
	r12.s64 = -2107637760;
	// addi r12,r12,-3660
	r12.s64 = r12.s64 + -3660;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825FF5B8;
	case 1:
		goto loc_825FF5B8;
	case 2:
		goto loc_825FF5B8;
	case 3:
		goto loc_825FF5B8;
	case 4:
		goto loc_825FF5B8;
	case 5:
		goto loc_825FF5B8;
	case 6:
		goto loc_825FF5B8;
	case 7:
		goto loc_825FF5B8;
	case 8:
		goto loc_825FF5B8;
	case 9:
		goto loc_825FF5B8;
	case 10:
		goto loc_825FF5B8;
	case 11:
		goto loc_825FF5B8;
	case 12:
		goto loc_825FF5B8;
	case 13:
		goto loc_825FF5B8;
	case 14:
		goto loc_825FF5B8;
	case 15:
		goto loc_825FF5B8;
	case 16:
		goto loc_825FF5B8;
	case 17:
		goto loc_825FF340;
	case 18:
		goto loc_825FF314;
	case 19:
		goto loc_825FF2E8;
	case 20:
		goto loc_825FF2BC;
	case 21:
		goto loc_825FF290;
	case 22:
		goto loc_825FF5B8;
	case 23:
		goto loc_825FF5B8;
	case 24:
		goto loc_825FF5B8;
	case 25:
		goto loc_825FF5B8;
	case 26:
		goto loc_825FF5B8;
	case 27:
		goto loc_825FF5B8;
	case 28:
		goto loc_825FF5B8;
	case 29:
		goto loc_825FF5B8;
	case 30:
		goto loc_825FF5B8;
	case 31:
		goto loc_825FF5B8;
	case 32:
		goto loc_825FF5B8;
	case 33:
		goto loc_825FF5B8;
	case 34:
		goto loc_825FF5B8;
	case 35:
		goto loc_825FF5B8;
	case 36:
		goto loc_825FF5B8;
	case 37:
		goto loc_825FF5B8;
	case 38:
		goto loc_825FF5B8;
	case 39:
		goto loc_825FF5B8;
	case 40:
		goto loc_825FF5B8;
	case 41:
		goto loc_825FF5B8;
	case 42:
		goto loc_825FF5B8;
	case 43:
		goto loc_825FF5B8;
	case 44:
		goto loc_825FF5B8;
	case 45:
		goto loc_825FF5B8;
	case 46:
		goto loc_825FF5B8;
	case 47:
		goto loc_825FF5B8;
	case 48:
		goto loc_825FF5B8;
	case 49:
		goto loc_825FF5B8;
	case 50:
		goto loc_825FF5B8;
	case 51:
		goto loc_825FF5B8;
	case 52:
		goto loc_825FF5B8;
	case 53:
		goto loc_825FF5B8;
	case 54:
		goto loc_825FF5B8;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-3264(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -3264);
	// lwz r18,-3308(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -3308);
	// lwz r18,-3352(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -3352);
	// lwz r18,-3396(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -3396);
	// lwz r18,-3440(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -3440);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
	// lwz r18,-2632(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + -2632);
loc_825FF290:
	// cmpwi cr6,r15,1
	cr6.compare<int32_t>(r15.s32, 1, xer);
	// bne cr6,0x825ff2a4
	if (!cr6.eq) goto loc_825FF2A4;
	// cmpwi cr6,r19,1
	cr6.compare<int32_t>(r19.s32, 1, xer);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// beq cr6,0x825ff2a8
	if (cr6.eq) goto loc_825FF2A8;
loc_825FF2A4:
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
loc_825FF2A8:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe3e0
	sub_825FE3E0(ctx, base);
	// b 0x825ff5b8
	goto loc_825FF5B8;
loc_825FF2BC:
	// cmpwi cr6,r15,1
	cr6.compare<int32_t>(r15.s32, 1, xer);
	// bne cr6,0x825ff2d0
	if (!cr6.eq) goto loc_825FF2D0;
	// cmpwi cr6,r19,1
	cr6.compare<int32_t>(r19.s32, 1, xer);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// beq cr6,0x825ff2d4
	if (cr6.eq) goto loc_825FF2D4;
loc_825FF2D0:
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
loc_825FF2D4:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe3e0
	sub_825FE3E0(ctx, base);
	// b 0x825ff5b8
	goto loc_825FF5B8;
loc_825FF2E8:
	// cmpwi cr6,r15,1
	cr6.compare<int32_t>(r15.s32, 1, xer);
	// bne cr6,0x825ff2fc
	if (!cr6.eq) goto loc_825FF2FC;
	// cmpwi cr6,r19,1
	cr6.compare<int32_t>(r19.s32, 1, xer);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// beq cr6,0x825ff300
	if (cr6.eq) goto loc_825FF300;
loc_825FF2FC:
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
loc_825FF300:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe3e0
	sub_825FE3E0(ctx, base);
	// b 0x825ff5b8
	goto loc_825FF5B8;
loc_825FF314:
	// cmpwi cr6,r15,1
	cr6.compare<int32_t>(r15.s32, 1, xer);
	// bne cr6,0x825ff328
	if (!cr6.eq) goto loc_825FF328;
	// cmpwi cr6,r19,1
	cr6.compare<int32_t>(r19.s32, 1, xer);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// beq cr6,0x825ff32c
	if (cr6.eq) goto loc_825FF32C;
loc_825FF328:
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
loc_825FF32C:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe3e0
	sub_825FE3E0(ctx, base);
	// b 0x825ff5b8
	goto loc_825FF5B8;
loc_825FF340:
	// cmpwi cr6,r15,1
	cr6.compare<int32_t>(r15.s32, 1, xer);
	// bne cr6,0x825ff354
	if (!cr6.eq) goto loc_825FF354;
	// cmpwi cr6,r19,1
	cr6.compare<int32_t>(r19.s32, 1, xer);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// beq cr6,0x825ff358
	if (cr6.eq) goto loc_825FF358;
loc_825FF354:
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
loc_825FF358:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825fe3e0
	sub_825FE3E0(ctx, base);
	// b 0x825ff5b8
	goto loc_825FF5B8;
loc_825FF36C:
	// subf r31,r28,r27
	r31.s64 = r27.s64 - r28.s64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825ff394
	if (cr6.eq) goto loc_825FF394;
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
loc_825FF37C:
	// lbzx r9,r11,r31
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + r31.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825ff394
	if (!cr6.eq) goto loc_825FF394;
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825ff37c
	if (!cr6.eq) goto loc_825FF37C;
loc_825FF394:
	// cmplwi cr6,r4,4
	cr6.compare<uint32_t>(ctx.r4.u32, 4, xer);
	// beq cr6,0x825ff3a8
	if (cr6.eq) goto loc_825FF3A8;
	// lwz r11,21340(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21340);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ff5a0
	if (!cr6.eq) goto loc_825FF5A0;
loc_825FF3A8:
	// cmplwi cr6,r10,12
	cr6.compare<uint32_t>(ctx.r10.u32, 12, xer);
	// bne cr6,0x825ff3c4
	if (!cr6.eq) goto loc_825FF3C4;
	// lwz r11,21244(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21244);
	// stw r31,21260(r30)
	PPC_STORE_U32(r30.u32 + 21260, r31.u32);
	// stw r24,21344(r30)
	PPC_STORE_U32(r30.u32 + 21344, r24.u32);
	// stw r23,21348(r30)
	PPC_STORE_U32(r30.u32 + 21348, r23.u32);
	// stw r11,21248(r30)
	PPC_STORE_U32(r30.u32 + 21248, r11.u32);
loc_825FF3C4:
	// cmplwi cr6,r10,13
	cr6.compare<uint32_t>(ctx.r10.u32, 13, xer);
	// bne cr6,0x825ff3dc
	if (!cr6.eq) goto loc_825FF3DC;
	// lwz r11,21288(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21288);
	// stw r31,21256(r30)
	PPC_STORE_U32(r30.u32 + 21256, r31.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,21288(r30)
	PPC_STORE_U32(r30.u32 + 21288, r11.u32);
loc_825FF3DC:
	// cmplwi cr6,r10,11
	cr6.compare<uint32_t>(ctx.r10.u32, 11, xer);
	// bne cr6,0x825ff5a0
	if (!cr6.eq) goto loc_825FF5A0;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x825ff4c4
	if (!cr6.eq) goto loc_825FF4C4;
	// lwz r11,21340(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21340);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ff5a0
	if (!cr6.eq) goto loc_825FF5A0;
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// ble cr6,0x825ff474
	if (!cr6.gt) goto loc_825FF474;
	// addi r29,r28,1
	r29.s64 = r28.s64 + 1;
	// lbz r11,0(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// lwz r7,21344(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 21344);
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// lbz r11,0(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm r11,r11,25,7,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1FFFFFF;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x825e5318
	sub_825E5318(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825ff538
	if (cr6.eq) goto loc_825FF538;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// bne cr6,0x825ff624
	if (!cr6.eq) goto loc_825FF624;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r31,r31,-2
	r31.s64 = r31.s64 + -2;
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// stbx r11,r21,r23
	PPC_STORE_U8(r21.u32 + r23.u32, r11.u8);
	// addi r11,r23,1
	r11.s64 = r23.s64 + 1;
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r23,r11,1
	r23.s64 = r11.s64 + 1;
	// clrlwi r10,r10,25
	ctx.r10.u64 = ctx.r10.u32 & 0x7F;
	// stbx r10,r21,r11
	PPC_STORE_U8(r21.u32 + r11.u32, ctx.r10.u8);
	// stw r22,21340(r30)
	PPC_STORE_U32(r30.u32 + 21340, r22.u32);
	// b 0x825ff5a0
	goto loc_825FF5A0;
loc_825FF474:
	// cmpwi cr6,r15,1
	cr6.compare<int32_t>(r15.s32, 1, xer);
	// bne cr6,0x825ff5a0
	if (!cr6.eq) goto loc_825FF5A0;
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// bne cr6,0x825ff5a0
	if (!cr6.eq) goto loc_825FF5A0;
	// lwz r11,21300(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825ff62c
	if (cr6.gt) goto loc_825FF62C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ff4b8
	if (!cr6.gt) goto loc_825FF4B8;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// addi r10,r10,21312
	ctx.r10.s64 = ctx.r10.s64 + 21312;
loc_825FF4A0:
	// lbz r9,-1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// bgt cr6,0x825ff4a0
	if (cr6.gt) goto loc_825FF4A0;
loc_825FF4B8:
	// lbz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// stw r24,21340(r30)
	PPC_STORE_U32(r30.u32 + 21340, r24.u32);
	// b 0x825ff58c
	goto loc_825FF58C;
loc_825FF4C4:
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// ble cr6,0x825ff540
	if (!cr6.gt) goto loc_825FF540;
	// addi r29,r28,1
	r29.s64 = r28.s64 + 1;
	// lbz r11,0(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// lwz r7,21344(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 21344);
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// lbz r11,0(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm r11,r11,25,7,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1FFFFFF;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x825e5318
	sub_825E5318(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825ff538
	if (cr6.eq) goto loc_825FF538;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// bne cr6,0x825ff624
	if (!cr6.eq) goto loc_825FF624;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r31,r31,-2
	r31.s64 = r31.s64 + -2;
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// stbx r11,r21,r23
	PPC_STORE_U8(r21.u32 + r23.u32, r11.u8);
	// addi r11,r23,1
	r11.s64 = r23.s64 + 1;
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r23,r11,1
	r23.s64 = r11.s64 + 1;
	// clrlwi r10,r10,25
	ctx.r10.u64 = ctx.r10.u32 & 0x7F;
	// stbx r10,r21,r11
	PPC_STORE_U8(r21.u32 + r11.u32, ctx.r10.u8);
loc_825FF538:
	// stw r22,21340(r30)
	PPC_STORE_U32(r30.u32 + 21340, r22.u32);
	// b 0x825ff5a0
	goto loc_825FF5A0;
loc_825FF540:
	// cmpwi cr6,r15,1
	cr6.compare<int32_t>(r15.s32, 1, xer);
	// bne cr6,0x825ff5a0
	if (!cr6.eq) goto loc_825FF5A0;
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// stw r24,21340(r30)
	PPC_STORE_U32(r30.u32 + 21340, r24.u32);
	// bne cr6,0x825ff5a0
	if (!cr6.eq) goto loc_825FF5A0;
	// lwz r11,21300(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825ff62c
	if (cr6.gt) goto loc_825FF62C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ff588
	if (!cr6.gt) goto loc_825FF588;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// addi r10,r10,21312
	ctx.r10.s64 = ctx.r10.s64 + 21312;
loc_825FF570:
	// lbz r9,-1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// bgt cr6,0x825ff570
	if (cr6.gt) goto loc_825FF570;
loc_825FF588:
	// lbz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + 0);
loc_825FF58C:
	// lwz r11,21300(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// mr r31,r22
	r31.u64 = r22.u64;
	// stb r10,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r11.u32);
loc_825FF5A0:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r3,r21,r23
	ctx.r3.u64 = r21.u64 + r23.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r29,292(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// add r23,r31,r23
	r23.u64 = r31.u64 + r23.u64;
loc_825FF5B8:
	// subf r11,r27,r25
	r11.s64 = r25.s64 - r27.s64;
	// li r4,4
	ctx.r4.s64 = 4;
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// mr r25,r27
	r25.u64 = r27.u64;
	// cmplwi cr6,r26,4
	cr6.compare<uint32_t>(r26.u32, 4, xer);
	// bge cr6,0x825fefa4
	if (!cr6.lt) goto loc_825FEFA4;
loc_825FF5D0:
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// beq cr6,0x825ff694
	if (cr6.eq) goto loc_825FF694;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// bne cr6,0x825ff694
	if (!cr6.eq) goto loc_825FF694;
	// add r11,r21,r23
	r11.u64 = r21.u64 + r23.u64;
	// cmplwi cr6,r23,2
	cr6.compare<uint32_t>(r23.u32, 2, xer);
	// ble cr6,0x825ff638
	if (!cr6.gt) goto loc_825FF638;
	// lbz r10,-1(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bne cr6,0x825ff638
	if (!cr6.eq) goto loc_825FF638;
	// lbz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff638
	if (!cr6.eq) goto loc_825FF638;
	// lbz r10,-3(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -3);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff638
	if (!cr6.eq) goto loc_825FF638;
	// stw r17,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r17.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r22.u8);
	// stb r22,21313(r30)
	PPC_STORE_U8(r30.u32 + 21313, r22.u8);
	// stb r24,21314(r30)
	PPC_STORE_U8(r30.u32 + 21314, r24.u8);
	// b 0x825ff68c
	goto loc_825FF68C;
loc_825FF624:
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// bne cr6,0x825ff6ac
	if (!cr6.eq) goto loc_825FF6AC;
loc_825FF62C:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_825FF638:
	// cmplwi cr6,r23,1
	cr6.compare<uint32_t>(r23.u32, 1, xer);
	// ble cr6,0x825ff668
	if (!cr6.gt) goto loc_825FF668;
	// lbz r10,-1(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff668
	if (!cr6.eq) goto loc_825FF668;
	// lbz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825ff668
	if (!cr6.eq) goto loc_825FF668;
	// stw r16,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r16.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r22.u8);
	// stb r22,21313(r30)
	PPC_STORE_U8(r30.u32 + 21313, r22.u8);
	// b 0x825ff68c
	goto loc_825FF68C;
loc_825FF668:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x825ff688
	if (cr6.eq) goto loc_825FF688;
	// lbz r11,-1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825ff688
	if (!cr6.eq) goto loc_825FF688;
	// stw r24,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r24.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(r30.u32 + 21312, r22.u8);
	// b 0x825ff68c
	goto loc_825FF68C;
loc_825FF688:
	// stw r22,21300(r30)
	PPC_STORE_U32(r30.u32 + 21300, r22.u32);
loc_825FF68C:
	// lwz r11,21300(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21300);
	// subf r23,r11,r23
	r23.s64 = r23.s64 - r11.s64;
loc_825FF694:
	// stw r21,0(r14)
	PPC_STORE_U32(r14.u32 + 0, r21.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r23,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r23.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_825FF6A8:
	// li r3,2
	ctx.r3.s64 = 2;
loc_825FF6AC:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825FF6B4"))) PPC_WEAK_FUNC(sub_825FF6B4);
PPC_FUNC_IMPL(__imp__sub_825FF6B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FF6B8"))) PPC_WEAK_FUNC(sub_825FF6B8);
PPC_FUNC_IMPL(__imp__sub_825FF6B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r30,0(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x825ff6fc
	if (cr0.eq) goto loc_825FF6FC;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
loc_825FF6FC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825FF714"))) PPC_WEAK_FUNC(sub_825FF714);
PPC_FUNC_IMPL(__imp__sub_825FF714) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825FF718"))) PPC_WEAK_FUNC(sub_825FF718);
PPC_FUNC_IMPL(__imp__sub_825FF718) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,21592(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21592);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ff78c
	if (cr6.eq) goto loc_825FF78C;
	// lwz r11,21584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21584);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ff75c
	if (!cr6.eq) goto loc_825FF75C;
	// lwz r11,21588(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21588);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ff75c
	if (!cr6.eq) goto loc_825FF75C;
	// li r11,-1
	r11.s64 = -1;
	// stw r11,15200(r31)
	PPC_STORE_U32(r31.u32 + 15200, r11.u32);
loc_825FF75C:
	// addi r4,r31,3688
	ctx.r4.s64 = r31.s64 + 3688;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r30,0(r4)
	r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// bl 0x8263c728
	sub_8263C728(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825ff7d8
	if (cr6.eq) goto loc_825FF7D8;
	// li r3,-100
	ctx.r3.s64 = -100;
	// b 0x825ff818
	goto loc_825FF818;
loc_825FF78C:
	// lwz r11,15200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15200);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// beq cr6,0x825ff7d8
	if (cr6.eq) goto loc_825FF7D8;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,15200(r31)
	PPC_STORE_U32(r31.u32 + 15200, ctx.r10.u32);
	// bne cr6,0x825ff7d8
	if (!cr6.eq) goto loc_825FF7D8;
	// addi r4,r31,3696
	ctx.r4.s64 = r31.s64 + 3696;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r30,0(r4)
	r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// bl 0x8263c728
	sub_8263C728(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825ff7d8
	if (cr6.eq) goto loc_825FF7D8;
	// li r3,-100
	ctx.r3.s64 = -100;
	// b 0x825ff818
	goto loc_825FF818;
loc_825FF7D8:
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ff7f0
	if (cr6.eq) goto loc_825FF7F0;
	// lwz r10,21584(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21584);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825ff814
	if (cr6.eq) goto loc_825FF814;
loc_825FF7F0:
	// li r9,1
	ctx.r9.s64 = 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r9,15200(r31)
	PPC_STORE_U32(r31.u32 + 15200, ctx.r9.u32);
	// stw r10,21584(r31)
	PPC_STORE_U32(r31.u32 + 21584, ctx.r10.u32);
	// bne cr6,0x825ff810
	if (!cr6.eq) goto loc_825FF810;
	// stw r9,21588(r31)
	PPC_STORE_U32(r31.u32 + 21588, ctx.r9.u32);
	// b 0x825ff814
	goto loc_825FF814;
loc_825FF810:
	// stw r10,21588(r31)
	PPC_STORE_U32(r31.u32 + 21588, ctx.r10.u32);
loc_825FF814:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825FF818:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825FF830"))) PPC_WEAK_FUNC(sub_825FF830);
PPC_FUNC_IMPL(__imp__sub_825FF830) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,21592(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21592);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ff89c
	if (cr6.eq) goto loc_825FF89C;
	// li r11,0
	r11.s64 = 0;
	// lwz r10,21588(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21588);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,21592(r31)
	PPC_STORE_U32(r31.u32 + 21592, r11.u32);
	// bne cr6,0x825ff870
	if (!cr6.eq) goto loc_825FF870;
	// li r11,-1
	r11.s64 = -1;
	// stw r11,15200(r31)
	PPC_STORE_U32(r31.u32 + 15200, r11.u32);
loc_825FF870:
	// addi r4,r31,3696
	ctx.r4.s64 = r31.s64 + 3696;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r30,0(r4)
	r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// bl 0x8263c728
	sub_8263C728(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// li r3,-100
	ctx.r3.s64 = -100;
	// bne cr6,0x825ff8a0
	if (!cr6.eq) goto loc_825FF8A0;
loc_825FF89C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825FF8A0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825FF8B8"))) PPC_WEAK_FUNC(sub_825FF8B8);
PPC_FUNC_IMPL(__imp__sub_825FF8B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// lis r11,21845
	r11.s64 = 1431633920;
	// li r19,0
	r19.s64 = 0;
	// ori r11,r11,21846
	r11.u64 = r11.u64 | 21846;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r9,140(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 140);
	// mr r20,r19
	r20.u64 = r19.u64;
	// lwz r10,136(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 136);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r23,r9,31,1,31
	r23.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r30,r10,31,1,31
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// mulhw r10,r23,r11
	ctx.r10.s64 = (int64_t(r23.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r9,r10,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf. r10,r10,r23
	ctx.r10.s64 = r23.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x825ffa04
	if (!cr0.eq) goto loc_825FFA04;
	// mulhw r10,r30,r11
	ctx.r10.s64 = (int64_t(r30.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r9,r10,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf. r10,r10,r30
	ctx.r10.s64 = r30.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x825ffa04
	if (cr0.eq) goto loc_825FFA04;
	// clrlwi r21,r30,31
	r21.u64 = r30.u32 & 0x1;
	// mr r25,r19
	r25.u64 = r19.u64;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// ble cr6,0x825ffaec
	if (!cr6.gt) goto loc_825FFAEC;
	// rlwinm r11,r30,1,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r26,r21
	r26.u64 = r21.u64;
	// add r24,r30,r11
	r24.u64 = r30.u64 + r11.u64;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r27,r11,5280
	r27.s64 = r11.s64 + 5280;
loc_825FF950:
	// mr r28,r21
	r28.u64 = r21.u64;
	// cmpw cr6,r21,r30
	cr6.compare<int32_t>(r21.s32, r30.s32, xer);
	// bge cr6,0x825ff9f0
	if (!cr6.lt) goto loc_825FF9F0;
	// mr r31,r26
	r31.u64 = r26.u64;
loc_825FF960:
	// addi r5,r27,64
	ctx.r5.s64 = r27.s64 + 64;
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 84);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x825ed308
	sub_825ED308(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825ffc78
	if (!cr6.eq) goto loc_825FFC78;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r5,r31,r29
	ctx.r5.u64 = r31.u64 + r29.u64;
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// clrlwi r8,r9,31
	ctx.r8.u64 = ctx.r9.u32 & 0x1;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// rlwinm r10,r30,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// clrlwi r7,r9,31
	ctx.r7.u64 = ctx.r9.u32 & 0x1;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// stbx r8,r31,r29
	PPC_STORE_U8(r31.u32 + r29.u32, ctx.r8.u8);
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// clrlwi r8,r9,31
	ctx.r8.u64 = ctx.r9.u32 & 0x1;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// stb r7,1(r5)
	PPC_STORE_U8(ctx.r5.u32 + 1, ctx.r7.u8);
	// add r7,r31,r11
	ctx.r7.u64 = r31.u64 + r11.u64;
	// clrlwi r6,r9,31
	ctx.r6.u64 = ctx.r9.u32 & 0x1;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// stbx r8,r31,r11
	PPC_STORE_U8(r31.u32 + r11.u32, ctx.r8.u8);
	// add r11,r31,r10
	r11.u64 = r31.u64 + ctx.r10.u64;
	// clrlwi r4,r9,31
	ctx.r4.u64 = ctx.r9.u32 & 0x1;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// stb r6,1(r7)
	PPC_STORE_U8(ctx.r7.u32 + 1, ctx.r6.u8);
	// cmpw cr6,r28,r30
	cr6.compare<int32_t>(r28.s32, r30.s32, xer);
	// stbx r4,r31,r10
	PPC_STORE_U8(r31.u32 + ctx.r10.u32, ctx.r4.u8);
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// clrlwi r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	// stb r9,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r9.u8);
	// blt cr6,0x825ff960
	if (cr6.lt) goto loc_825FF960;
loc_825FF9F0:
	// addi r25,r25,3
	r25.s64 = r25.s64 + 3;
	// add r26,r24,r26
	r26.u64 = r24.u64 + r26.u64;
	// cmpw cr6,r25,r23
	cr6.compare<int32_t>(r25.s32, r23.s32, xer);
	// blt cr6,0x825ff950
	if (cr6.lt) goto loc_825FF950;
	// b 0x825ffaec
	goto loc_825FFAEC;
loc_825FFA04:
	// mulhw r11,r30,r11
	r11.s64 = (int64_t(r30.s32) * int64_t(r11.s32)) >> 32;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// clrlwi r20,r23,31
	r20.u64 = r23.u32 & 0x1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r25,r20
	r25.u64 = r20.u64;
	// cmpw cr6,r20,r23
	cr6.compare<int32_t>(r20.s32, r23.s32, xer);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r21,r11,r30
	r21.s64 = r30.s64 - r11.s64;
	// bge cr6,0x825ffaec
	if (!cr6.lt) goto loc_825FFAEC;
	// mullw r11,r30,r20
	r11.s64 = int64_t(r30.s32) * int64_t(r20.s32);
	// add r26,r11,r21
	r26.u64 = r11.u64 + r21.u64;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// rlwinm r24,r30,1,0,30
	r24.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r27,r11,5280
	r27.s64 = r11.s64 + 5280;
loc_825FFA40:
	// mr r28,r21
	r28.u64 = r21.u64;
	// cmpw cr6,r21,r30
	cr6.compare<int32_t>(r21.s32, r30.s32, xer);
	// bge cr6,0x825ffadc
	if (!cr6.lt) goto loc_825FFADC;
	// mr r31,r26
	r31.u64 = r26.u64;
loc_825FFA50:
	// addi r5,r27,64
	ctx.r5.s64 = r27.s64 + 64;
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 84);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x825ed308
	sub_825ED308(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x825ffc78
	if (!cr6.eq) goto loc_825FFC78;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r6,r31,r29
	ctx.r6.u64 = r31.u64 + r29.u64;
	// add r5,r31,r29
	ctx.r5.u64 = r31.u64 + r29.u64;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// clrlwi r8,r10,31
	ctx.r8.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stbx r9,r31,r29
	PPC_STORE_U8(r31.u32 + r29.u32, ctx.r9.u8);
	// addi r28,r28,3
	r28.s64 = r28.s64 + 3;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stb r8,1(r6)
	PPC_STORE_U8(ctx.r6.u32 + 1, ctx.r8.u8);
	// cmpw cr6,r28,r30
	cr6.compare<int32_t>(r28.s32, r30.s32, xer);
	// clrlwi r7,r10,31
	ctx.r7.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stb r9,2(r5)
	PPC_STORE_U8(ctx.r5.u32 + 2, ctx.r9.u8);
	// add r9,r31,r11
	ctx.r9.u64 = r31.u64 + r11.u64;
	// clrlwi r4,r10,31
	ctx.r4.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stbx r7,r31,r11
	PPC_STORE_U8(r31.u32 + r11.u32, ctx.r7.u8);
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// addi r31,r31,3
	r31.s64 = r31.s64 + 3;
	// stb r4,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, ctx.r4.u8);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// stb r10,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r10.u8);
	// blt cr6,0x825ffa50
	if (cr6.lt) goto loc_825FFA50;
loc_825FFADC:
	// addi r25,r25,2
	r25.s64 = r25.s64 + 2;
	// add r26,r26,r24
	r26.u64 = r26.u64 + r24.u64;
	// cmpw cr6,r25,r23
	cr6.compare<int32_t>(r25.s32, r23.s32, xer);
	// blt cr6,0x825ffa40
	if (cr6.lt) goto loc_825FFA40;
loc_825FFAEC:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x825ffbb8
	if (!cr6.gt) goto loc_825FFBB8;
	// mr r26,r29
	r26.u64 = r29.u64;
	// mr r25,r21
	r25.u64 = r21.u64;
loc_825FFAFC:
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825ffb28
	if (!cr0.lt) goto loc_825FFB28;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FFB28:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825ffb84
	if (cr6.eq) goto loc_825FFB84;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// ble cr6,0x825ffba8
	if (!cr6.gt) goto loc_825FFBA8;
	// mr r28,r26
	r28.u64 = r26.u64;
	// mr r31,r23
	r31.u64 = r23.u64;
loc_825FFB40:
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r27,r8,0
	r27.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825ffb6c
	if (!cr0.lt) goto loc_825FFB6C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FFB6C:
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// stb r27,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r27.u8);
	// add r28,r28,r30
	r28.u64 = r28.u64 + r30.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825ffb40
	if (!cr6.eq) goto loc_825FFB40;
	// b 0x825ffba8
	goto loc_825FFBA8;
loc_825FFB84:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// ble cr6,0x825ffba8
	if (!cr6.gt) goto loc_825FFBA8;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// mr r11,r23
	r11.u64 = r23.u64;
loc_825FFB94:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r19,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r19.u8);
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825ffb94
	if (!cr6.eq) goto loc_825FFB94;
loc_825FFBA8:
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x825ffafc
	if (!cr6.eq) goto loc_825FFAFC;
loc_825FFBB8:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x825ffc74
	if (cr6.eq) goto loc_825FFC74;
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825ffbec
	if (!cr0.lt) goto loc_825FFBEC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FFBEC:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825ffc48
	if (cr6.eq) goto loc_825FFC48;
	// mr r31,r21
	r31.u64 = r21.u64;
	// cmpw cr6,r21,r30
	cr6.compare<int32_t>(r21.s32, r30.s32, xer);
	// bge cr6,0x825ffc74
	if (!cr6.lt) goto loc_825FFC74;
loc_825FFC00:
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r28,r8,0
	r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x825ffc2c
	if (!cr0.lt) goto loc_825FFC2C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FFC2C:
	// stbx r28,r31,r29
	PPC_STORE_U8(r31.u32 + r29.u32, r28.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmpw cr6,r31,r30
	cr6.compare<int32_t>(r31.s32, r30.s32, xer);
	// blt cr6,0x825ffc00
	if (cr6.lt) goto loc_825FFC00;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd24
	return;
loc_825FFC48:
	// cmpw cr6,r21,r30
	cr6.compare<int32_t>(r21.s32, r30.s32, xer);
	// bge cr6,0x825ffc74
	if (!cr6.lt) goto loc_825FFC74;
	// subf r10,r21,r30
	ctx.r10.s64 = r30.s64 - r21.s64;
	// add r11,r21,r29
	r11.u64 = r21.u64 + r29.u64;
	// mr r9,r19
	ctx.r9.u64 = r19.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825ffc74
	if (cr6.eq) goto loc_825FFC74;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_825FFC68:
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x825ffc68
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_825FFC68;
loc_825FFC74:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825FFC78:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_825FFC80"))) PPC_WEAK_FUNC(sub_825FFC80);
PPC_FUNC_IMPL(__imp__sub_825FFC80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r10,r11,32
	ctx.r10.u64 = r11.u64 & 0xFFFFFFFF;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r29.u32);
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825ffd88
	if (cr6.lt) goto loc_825FFD88;
	// clrlwi r10,r30,28
	ctx.r10.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x825ffd80
	if (!cr6.lt) goto loc_825FFD80;
loc_825FFCE8:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825ffd14
	if (cr6.lt) goto loc_825FFD14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x825ffce8
	if (cr6.eq) goto loc_825FFCE8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825ffdcc
	goto loc_825FFDCC;
loc_825FFD14:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_825FFD80:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x825ffdcc
	goto loc_825FFDCC;
loc_825FFD88:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r28,r11,32768
	r28.u64 = r11.u64 | 32768;
loc_825FFD98:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r28
	r11.u64 = r30.u64 + r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825ffd98
	if (cr6.lt) goto loc_825FFD98;
loc_825FFDCC:
	// addi r24,r30,1
	r24.s64 = r30.s64 + 1;
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// cmpwi cr6,r24,38
	cr6.compare<int32_t>(r24.s32, 38, xer);
	// blt cr6,0x825ffde8
	if (cr6.lt) goto loc_825FFDE8;
	// addi r24,r24,-38
	r24.s64 = r24.s64 + -38;
	// ori r11,r11,8
	r11.u64 = r11.u64 | 8;
	// b 0x825ffdec
	goto loc_825FFDEC;
loc_825FFDE8:
	// rlwinm r11,r11,0,29,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF7;
loc_825FFDEC:
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// rlwinm r11,r11,0,30,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFB;
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// beq cr6,0x82600904
	if (cr6.eq) goto loc_82600904;
	// cmpwi cr6,r24,36
	cr6.compare<int32_t>(r24.s32, 36, xer);
	// beq cr6,0x82600740
	if (cr6.eq) goto loc_82600740;
	// cmpwi cr6,r24,37
	cr6.compare<int32_t>(r24.s32, 37, xer);
	// beq cr6,0x82600720
	if (cr6.eq) goto loc_82600720;
	// lwz r10,1972(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 1972);
	// lis r11,10922
	r11.s64 = 715784192;
	// li r21,1
	r21.s64 = 1;
	// ori r25,r11,43691
	r25.u64 = r11.u64 | 43691;
	// mulhw r11,r24,r25
	r11.s64 = (int64_t(r24.s32) * int64_t(r25.s32)) >> 32;
	// lwz r10,76(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r26,r11,r24
	r26.s64 = r24.s64 - r11.s64;
	// beq cr6,0x825ffe5c
	if (cr6.eq) goto loc_825FFE5C;
	// cmpwi cr6,r26,5
	cr6.compare<int32_t>(r26.s32, 5, xer);
	// bne cr6,0x825ffe5c
	if (!cr6.eq) goto loc_825FFE5C;
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// b 0x825ffe68
	goto loc_825FFE68;
loc_825FFE5C:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// ble cr6,0x826001a0
	if (!cr6.gt) goto loc_826001A0;
loc_825FFE68:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// addi r9,r26,-2
	ctx.r9.s64 = r26.s64 + -2;
	// addi r22,r11,5200
	r22.s64 = r11.s64 + 5200;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r22,272
	r11.s64 = r22.s64 + 272;
	// mr r30,r21
	r30.u64 = r21.u64;
	// li r29,0
	r29.s64 = 0;
	// lwzx r11,r9,r11
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// subf r27,r10,r11
	r27.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825ffef8
	if (!cr6.lt) goto loc_825FFEF8;
loc_825FFEA0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ffef8
	if (cr6.eq) goto loc_825FFEF8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825ffee8
	if (!cr0.lt) goto loc_825FFEE8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FFEE8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825ffea0
	if (cr6.gt) goto loc_825FFEA0;
loc_825FFEF8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825fff34
	if (!cr0.lt) goto loc_825FFF34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FFF34:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825fff44
	if (cr6.eq) goto loc_825FFF44;
	// li r28,0
	r28.s64 = 0;
	// b 0x826000c0
	goto loc_826000C0;
loc_825FFF44:
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825fffb8
	if (!cr6.lt) goto loc_825FFFB8;
loc_825FFF60:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825fffb8
	if (cr6.eq) goto loc_825FFFB8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x825fffa8
	if (!cr0.lt) goto loc_825FFFA8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FFFA8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825fff60
	if (cr6.gt) goto loc_825FFF60;
loc_825FFFB8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ffff4
	if (!cr0.lt) goto loc_825FFFF4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_825FFFF4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82600004
	if (cr6.eq) goto loc_82600004;
	// mr r28,r21
	r28.u64 = r21.u64;
	// b 0x826000c0
	goto loc_826000C0;
loc_82600004:
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82600078
	if (!cr6.lt) goto loc_82600078;
loc_82600020:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82600078
	if (cr6.eq) goto loc_82600078;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82600068
	if (!cr0.lt) goto loc_82600068;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600068:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82600020
	if (cr6.gt) goto loc_82600020;
loc_82600078:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826000b4
	if (!cr0.lt) goto loc_826000B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826000B4:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r28,r11,2
	r28.s64 = r11.s64 + 2;
loc_826000C0:
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x826000f0
	if (!cr6.eq) goto loc_826000F0;
	// slw r11,r21,r27
	r11.u64 = r27.u8 & 0x20 ? 0 : (r21.u32 << (r27.u8 & 0x3F));
	// li r10,0
	ctx.r10.s64 = 0;
	// mullw r11,r11,r28
	r11.s64 = int64_t(r11.s32) * int64_t(r28.s32);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// b 0x82600270
	goto loc_82600270;
loc_826000F0:
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// ble cr6,0x82600150
	if (!cr6.gt) goto loc_82600150;
loc_826000F8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82600150
	if (cr6.eq) goto loc_82600150;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82600140
	if (!cr0.lt) goto loc_82600140;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600140:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826000f8
	if (cr6.gt) goto loc_826000F8;
loc_82600150:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8260018c
	if (!cr0.lt) goto loc_8260018C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8260018C:
	// slw r11,r21,r27
	r11.u64 = r27.u8 & 0x20 ? 0 : (r21.u32 << (r27.u8 & 0x3F));
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// mullw r11,r11,r28
	r11.s64 = int64_t(r11.s32) * int64_t(r28.s32);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// b 0x82600270
	goto loc_82600270;
loc_826001A0:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// li r29,0
	r29.s64 = 0;
	// addi r22,r11,5200
	r22.s64 = r11.s64 + 5200;
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwzx r30,r11,r22
	r30.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826001d0
	if (!cr6.eq) goto loc_826001D0;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x82600270
	goto loc_82600270;
loc_826001D0:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x82600230
	if (!cr6.gt) goto loc_82600230;
loc_826001D8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82600230
	if (cr6.eq) goto loc_82600230;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82600220
	if (!cr0.lt) goto loc_82600220;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600220:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826001d8
	if (cr6.gt) goto loc_826001D8;
loc_82600230:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8260026c
	if (!cr0.lt) goto loc_8260026C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8260026C:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_82600270:
	// rlwinm r7,r26,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r22,24
	ctx.r8.s64 = r22.s64 + 24;
	// clrlwi r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	// srawi r9,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// lwzx r10,r7,r8
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mulhw r10,r24,r25
	ctx.r10.s64 = (int64_t(r24.s32) * int64_t(r25.s32)) >> 32;
	// xor r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 ^ r11.u64;
	// sth r9,0(r20)
	PPC_STORE_U16(r20.u32 + 0, ctx.r9.u16);
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// sth r11,0(r20)
	PPC_STORE_U16(r20.u32 + 0, r11.u16);
	// lwz r11,1972(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 1972);
	// lwz r11,76(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r11,r10,1,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r26,r10,r11
	r26.u64 = ctx.r10.u64 + r11.u64;
	// beq cr6,0x826002cc
	if (cr6.eq) goto loc_826002CC;
	// cmpwi cr6,r26,5
	cr6.compare<int32_t>(r26.s32, 5, xer);
	// bne cr6,0x826002cc
	if (!cr6.eq) goto loc_826002CC;
	// mr r11,r21
	r11.u64 = r21.u64;
	// b 0x826002d8
	goto loc_826002D8;
loc_826002CC:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// ble cr6,0x82600608
	if (!cr6.gt) goto loc_82600608;
loc_826002D8:
	// addi r10,r26,-2
	ctx.r10.s64 = r26.s64 + -2;
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// addi r9,r22,272
	ctx.r9.s64 = r22.s64 + 272;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r30,r21
	r30.u64 = r21.u64;
	// li r29,0
	r29.s64 = 0;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// subf r27,r11,r10
	r27.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82600360
	if (!cr6.lt) goto loc_82600360;
loc_82600308:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82600360
	if (cr6.eq) goto loc_82600360;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82600350
	if (!cr0.lt) goto loc_82600350;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600350:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82600308
	if (cr6.gt) goto loc_82600308;
loc_82600360:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8260039c
	if (!cr0.lt) goto loc_8260039C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8260039C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826003ac
	if (cr6.eq) goto loc_826003AC;
	// li r28,0
	r28.s64 = 0;
	// b 0x82600528
	goto loc_82600528;
loc_826003AC:
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82600420
	if (!cr6.lt) goto loc_82600420;
loc_826003C8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82600420
	if (cr6.eq) goto loc_82600420;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82600410
	if (!cr0.lt) goto loc_82600410;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600410:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826003c8
	if (cr6.gt) goto loc_826003C8;
loc_82600420:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8260045c
	if (!cr0.lt) goto loc_8260045C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8260045C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8260046c
	if (cr6.eq) goto loc_8260046C;
	// mr r28,r21
	r28.u64 = r21.u64;
	// b 0x82600528
	goto loc_82600528;
loc_8260046C:
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// mr r30,r21
	r30.u64 = r21.u64;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826004e0
	if (!cr6.lt) goto loc_826004E0;
loc_82600488:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826004e0
	if (cr6.eq) goto loc_826004E0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826004d0
	if (!cr0.lt) goto loc_826004D0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826004D0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82600488
	if (cr6.gt) goto loc_82600488;
loc_826004E0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8260051c
	if (!cr0.lt) goto loc_8260051C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8260051C:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r28,r11,2
	r28.s64 = r11.s64 + 2;
loc_82600528:
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// mr r30,r27
	r30.u64 = r27.u64;
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x82600558
	if (!cr6.eq) goto loc_82600558;
	// slw r11,r21,r27
	r11.u64 = r27.u8 & 0x20 ? 0 : (r21.u32 << (r27.u8 & 0x3F));
	// li r10,0
	ctx.r10.s64 = 0;
	// mullw r11,r11,r28
	r11.s64 = int64_t(r11.s32) * int64_t(r28.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826006d0
	goto loc_826006D0;
loc_82600558:
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// ble cr6,0x826005b8
	if (!cr6.gt) goto loc_826005B8;
loc_82600560:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826005b8
	if (cr6.eq) goto loc_826005B8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826005a8
	if (!cr0.lt) goto loc_826005A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826005A8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82600560
	if (cr6.gt) goto loc_82600560;
loc_826005B8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826005f4
	if (!cr0.lt) goto loc_826005F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826005F4:
	// slw r11,r21,r27
	r11.u64 = r27.u8 & 0x20 ? 0 : (r21.u32 << (r27.u8 & 0x3F));
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// mullw r11,r11,r28
	r11.s64 = int64_t(r11.s32) * int64_t(r28.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826006d0
	goto loc_826006D0;
loc_82600608:
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// li r29,0
	r29.s64 = 0;
	// lwzx r30,r11,r22
	r30.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x82600630
	if (!cr6.eq) goto loc_82600630;
	// li r11,0
	r11.s64 = 0;
	// b 0x826006d0
	goto loc_826006D0;
loc_82600630:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x82600690
	if (!cr6.gt) goto loc_82600690;
loc_82600638:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82600690
	if (cr6.eq) goto loc_82600690;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82600680
	if (!cr0.lt) goto loc_82600680;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600680:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82600638
	if (cr6.gt) goto loc_82600638;
loc_82600690:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826006cc
	if (!cr0.lt) goto loc_826006CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826006CC:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_826006D0:
	// addi r9,r22,24
	ctx.r9.s64 = r22.s64 + 24;
	// lwz r8,0(r20)
	ctx.r8.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// rlwinm r7,r26,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// lwzx r9,r7,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// rlwinm r9,r10,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// rlwimi r11,r8,0,28,15
	r11.u64 = (__builtin_rotateleft32(ctx.r8.u32, 0) & 0xFFFFFFFFFFFF000F) | (r11.u64 & 0xFFF0);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// rlwinm r9,r9,0,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF0;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// rlwimi r10,r11,0,28,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFFFFFFF000F) | (ctx.r10.u64 & 0xFFF0);
	// stw r10,0(r20)
	PPC_STORE_U32(r20.u32 + 0, ctx.r10.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_82600720:
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,0
	ctx.r9.s64 = 0;
	// rlwimi r11,r10,2,29,29
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 2) & 0x4) | (r11.u64 & 0xFFFFFFFFFFFFFFFB);
	// rlwimi r11,r10,2,16,27
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 2) & 0xFFF0) | (r11.u64 & 0xFFFFFFFFFFFF000F);
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// sth r9,0(r20)
	PPC_STORE_U16(r20.u32 + 0, ctx.r9.u16);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_82600740:
	// lwz r10,1972(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 1972);
	// li r29,0
	r29.s64 = 0;
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// lwz r11,408(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 408);
	// lwz r10,76(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r30,r10,r11
	r30.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82600770
	if (!cr6.eq) goto loc_82600770;
	// li r11,0
	r11.s64 = 0;
	// b 0x82600810
	goto loc_82600810;
loc_82600770:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826007d0
	if (!cr6.gt) goto loc_826007D0;
loc_82600778:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826007d0
	if (cr6.eq) goto loc_826007D0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826007c0
	if (!cr0.lt) goto loc_826007C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826007C0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82600778
	if (cr6.gt) goto loc_82600778;
loc_826007D0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8260080c
	if (!cr0.lt) goto loc_8260080C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8260080C:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_82600810:
	// sth r11,0(r20)
	PPC_STORE_U16(r20.u32 + 0, r11.u16);
	// li r29,0
	r29.s64 = 0;
	// lwz r11,1972(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 1972);
	// lwz r31,84(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 84);
	// lwz r10,412(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 412);
	// lwz r11,76(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r30,r11,r10
	r30.s64 = ctx.r10.s64 - r11.s64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82600854
	if (!cr6.eq) goto loc_82600854;
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// li r30,0
	r30.s64 = 0;
	// rlwimi r11,r30,4,16,27
	r11.u64 = (__builtin_rotateleft32(r30.u32, 4) & 0xFFF0) | (r11.u64 & 0xFFFFFFFFFFFF000F);
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_82600854:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826008b4
	if (!cr6.gt) goto loc_826008B4;
loc_8260085C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826008b4
	if (cr6.eq) goto loc_826008B4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826008a4
	if (!cr0.lt) goto loc_826008A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826008A4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8260085c
	if (cr6.gt) goto loc_8260085C;
loc_826008B4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826008f0
	if (!cr0.lt) goto loc_826008F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826008F0:
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// rlwimi r11,r30,4,16,27
	r11.u64 = (__builtin_rotateleft32(r30.u32, 4) & 0xFFF0) | (r11.u64 & 0xFFFFFFFFFFFF000F);
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_82600904:
	// rlwinm r11,r11,0,28,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFF000F;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// sth r10,0(r20)
	PPC_STORE_U16(r20.u32 + 0, ctx.r10.u16);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_8260091C"))) PPC_WEAK_FUNC(sub_8260091C);
PPC_FUNC_IMPL(__imp__sub_8260091C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82600920"))) PPC_WEAK_FUNC(sub_82600920);
PPC_FUNC_IMPL(__imp__sub_82600920) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r21,r25
	r21.u64 = r25.u64;
	// lwz r11,140(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 140);
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r10,136(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r22,268(r26)
	r22.u64 = PPC_LOAD_U32(r26.u32 + 268);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r28,21636(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 21636);
	// rlwinm r20,r11,31,1,31
	r20.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rlwinm r24,r10,31,1,31
	r24.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mullw r23,r20,r24
	r23.s64 = int64_t(r20.s32) * int64_t(r24.s32);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// rotlwi r19,r8,0
	r19.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// bge 0x82600988
	if (!cr0.lt) goto loc_82600988;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600988:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x826009fc
	if (!cr6.lt) goto loc_826009FC;
loc_826009A4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826009fc
	if (cr6.eq) goto loc_826009FC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826009ec
	if (!cr0.lt) goto loc_826009EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826009EC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826009a4
	if (cr6.gt) goto loc_826009A4;
loc_826009FC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82600a38
	if (!cr0.lt) goto loc_82600A38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600A38:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// beq cr6,0x82600d6c
	if (cr6.eq) goto loc_82600D6C;
	// cmplwi cr6,r30,2
	cr6.compare<uint32_t>(r30.u32, 2, xer);
	// beq cr6,0x82600c2c
	if (cr6.eq) goto loc_82600C2C;
	// cmplwi cr6,r30,3
	cr6.compare<uint32_t>(r30.u32, 3, xer);
	// beq cr6,0x82600c10
	if (cr6.eq) goto loc_82600C10;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600a7c
	if (!cr0.lt) goto loc_82600A7C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600A7C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82600bc8
	if (cr6.eq) goto loc_82600BC8;
	// clrlwi r31,r23,31
	r31.u64 = r23.u32 & 0x1;
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// mr r21,r29
	r21.u64 = r29.u64;
	// beq cr6,0x82600acc
	if (cr6.eq) goto loc_82600ACC;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600ac4
	if (!cr0.lt) goto loc_82600AC4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600AC4:
	// stb r30,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r30.u8);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
loc_82600ACC:
	// cmpw cr6,r31,r23
	cr6.compare<int32_t>(r31.s32, r23.s32, xer);
	// bge cr6,0x82600f30
	if (!cr6.lt) goto loc_82600F30;
	// subf r11,r31,r23
	r11.s64 = r23.s64 - r31.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_82600AE4:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600b10
	if (!cr0.lt) goto loc_82600B10;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600B10:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82600ba8
	if (cr6.eq) goto loc_82600BA8;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600b44
	if (!cr0.lt) goto loc_82600B44;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600B44:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82600b5c
	if (cr6.eq) goto loc_82600B5C;
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
	// stb r29,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r29.u8);
	// stb r29,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r29.u8);
	// b 0x82600bb4
	goto loc_82600BB4;
loc_82600B5C:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600b88
	if (!cr0.lt) goto loc_82600B88;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600B88:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
	// beq cr6,0x82600ba0
	if (cr6.eq) goto loc_82600BA0;
	// stb r25,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r25.u8);
	// stb r29,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r29.u8);
	// b 0x82600bb4
	goto loc_82600BB4;
loc_82600BA0:
	// stb r29,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r29.u8);
	// b 0x82600bb0
	goto loc_82600BB0;
loc_82600BA8:
	// stb r25,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r25.u8);
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
loc_82600BB0:
	// stb r25,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r25.u8);
loc_82600BB4:
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82600ae4
	if (!cr6.eq) goto loc_82600AE4;
	// b 0x82600f30
	goto loc_82600F30;
loc_82600BC8:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600bf4
	if (!cr0.lt) goto loc_82600BF4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600BF4:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82600f30
	if (cr6.eq) goto loc_82600F30;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// li r21,1
	r21.s64 = 1;
	// bl 0x825ff8b8
	sub_825FF8B8(ctx, base);
	// b 0x82600f30
	goto loc_82600F30;
loc_82600C10:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825ff8b8
	sub_825FF8B8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82600f30
	if (cr6.eq) goto loc_82600F30;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd24
	return;
loc_82600C2C:
	// clrlwi r30,r23,31
	r30.u64 = r23.u32 & 0x1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x82600c6c
	if (cr6.eq) goto loc_82600C6C;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600c64
	if (!cr0.lt) goto loc_82600C64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600C64:
	// stb r31,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r31.u8);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
loc_82600C6C:
	// cmpw cr6,r30,r23
	cr6.compare<int32_t>(r30.s32, r23.s32, xer);
	// bge cr6,0x82600f30
	if (!cr6.lt) goto loc_82600F30;
	// subf r11,r30,r23
	r11.s64 = r23.s64 - r30.s64;
	// li r29,1
	r29.s64 = 1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_82600C88:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600cb4
	if (!cr0.lt) goto loc_82600CB4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600CB4:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82600d4c
	if (cr6.eq) goto loc_82600D4C;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600ce8
	if (!cr0.lt) goto loc_82600CE8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600CE8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82600d00
	if (cr6.eq) goto loc_82600D00;
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
	// stb r29,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r29.u8);
	// stb r29,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r29.u8);
	// b 0x82600d58
	goto loc_82600D58;
loc_82600D00:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600d2c
	if (!cr0.lt) goto loc_82600D2C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600D2C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
	// beq cr6,0x82600d44
	if (cr6.eq) goto loc_82600D44;
	// stb r25,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r25.u8);
	// stb r29,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r29.u8);
	// b 0x82600d58
	goto loc_82600D58;
loc_82600D44:
	// stb r29,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r29.u8);
	// b 0x82600d54
	goto loc_82600D54;
loc_82600D4C:
	// stb r25,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r25.u8);
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
loc_82600D54:
	// stb r25,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r25.u8);
loc_82600D58:
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82600c88
	if (!cr6.eq) goto loc_82600C88;
	// b 0x82600f30
	goto loc_82600F30;
loc_82600D6C:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600d98
	if (!cr0.lt) goto loc_82600D98;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600D98:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82600e6c
	if (cr6.eq) goto loc_82600E6C;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82600f30
	if (!cr6.gt) goto loc_82600F30;
	// mr r27,r24
	r27.u64 = r24.u64;
loc_82600DAC:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600dd8
	if (!cr0.lt) goto loc_82600DD8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600DD8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82600e34
	if (cr6.eq) goto loc_82600E34;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x82600e58
	if (!cr6.gt) goto loc_82600E58;
	// mr r30,r28
	r30.u64 = r28.u64;
	// mr r31,r20
	r31.u64 = r20.u64;
loc_82600DF0:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600e1c
	if (!cr0.lt) goto loc_82600E1C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600E1C:
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// stb r29,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r29.u8);
	// add r30,r30,r24
	r30.u64 = r30.u64 + r24.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82600df0
	if (!cr6.eq) goto loc_82600DF0;
	// b 0x82600e58
	goto loc_82600E58;
loc_82600E34:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x82600e58
	if (!cr6.gt) goto loc_82600E58;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_82600E44:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r25,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r25.u8);
	// add r10,r10,r24
	ctx.r10.u64 = ctx.r10.u64 + r24.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82600e44
	if (!cr6.eq) goto loc_82600E44;
loc_82600E58:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82600dac
	if (!cr6.eq) goto loc_82600DAC;
	// b 0x82600f30
	goto loc_82600F30;
loc_82600E6C:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x82600f30
	if (!cr6.gt) goto loc_82600F30;
	// mr r29,r20
	r29.u64 = r20.u64;
loc_82600E78:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600ea4
	if (!cr0.lt) goto loc_82600EA4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600EA4:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82600ef8
	if (cr6.eq) goto loc_82600EF8;
	// mr r31,r25
	r31.u64 = r25.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82600f20
	if (!cr6.gt) goto loc_82600F20;
loc_82600EB8:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82600ee4
	if (!cr0.lt) goto loc_82600EE4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82600EE4:
	// stbx r30,r28,r31
	PPC_STORE_U8(r28.u32 + r31.u32, r30.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmpw cr6,r31,r24
	cr6.compare<int32_t>(r31.s32, r24.s32, xer);
	// blt cr6,0x82600eb8
	if (cr6.lt) goto loc_82600EB8;
	// b 0x82600f20
	goto loc_82600F20;
loc_82600EF8:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82600f20
	if (!cr6.gt) goto loc_82600F20;
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x82600f20
	if (cr6.eq) goto loc_82600F20;
	// mtctr r24
	ctr.u64 = r24.u64;
loc_82600F14:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x82600f14
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82600F14;
loc_82600F20:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r28,r28,r24
	r28.u64 = r28.u64 + r24.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82600e78
	if (!cr6.eq) goto loc_82600E78;
loc_82600F30:
	// lwz r10,21636(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 21636);
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x82600fd8
	if (cr6.eq) goto loc_82600FD8;
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x82601008
	if (!cr6.gt) goto loc_82601008;
loc_82600F48:
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82600fc8
	if (!cr6.gt) goto loc_82600FC8;
	// subfic r8,r24,1
	xer.ca = r24.u32 <= 1;
	ctx.r8.s64 = 1 - r24.s64;
loc_82600F58:
	// add. r11,r9,r7
	r11.u64 = ctx.r9.u64 + ctx.r7.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82600fa8
	if (cr0.eq) goto loc_82600FA8;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne cr6,0x82600f74
	if (!cr6.eq) goto loc_82600F74;
	// lbz r11,-1(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// b 0x82600fac
	goto loc_82600FAC;
loc_82600F74:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x82600f8c
	if (!cr6.eq) goto loc_82600F8C;
	// add r11,r8,r10
	r11.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lbz r11,-1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// b 0x82600fac
	goto loc_82600FAC;
loc_82600F8C:
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lbz r11,-1(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// lbz r6,-1(r6)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + -1);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// beq cr6,0x82600fac
	if (cr6.eq) goto loc_82600FAC;
loc_82600FA8:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_82600FAC:
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// xor r11,r6,r11
	r11.u64 = ctx.r6.u64 ^ r11.u64;
	// cmpw cr6,r9,r24
	cr6.compare<int32_t>(ctx.r9.s32, r24.s32, xer);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x82600f58
	if (cr6.lt) goto loc_82600F58;
loc_82600FC8:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// cmpw cr6,r7,r20
	cr6.compare<int32_t>(ctx.r7.s32, r20.s32, xer);
	// blt cr6,0x82600f48
	if (cr6.lt) goto loc_82600F48;
	// b 0x82601008
	goto loc_82601008;
loc_82600FD8:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// beq cr6,0x82601008
	if (cr6.eq) goto loc_82601008;
	// mr r11,r25
	r11.u64 = r25.u64;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// ble cr6,0x82601008
	if (!cr6.gt) goto loc_82601008;
loc_82600FEC:
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r23
	cr6.compare<int32_t>(r11.s32, r23.s32, xer);
	// lbz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// xori r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 ^ 1;
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// blt cr6,0x82600fec
	if (cr6.lt) goto loc_82600FEC;
loc_82601008:
	// lwz r10,136(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// lwz r11,140(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 140);
	// clrlwi r8,r10,31
	ctx.r8.u64 = ctx.r10.u32 & 0x1;
	// lwz r20,21636(r26)
	r20.u64 = PPC_LOAD_U32(r26.u32 + 21636);
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// subf r21,r8,r10
	r21.s64 = ctx.r10.s64 - ctx.r8.s64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// lis r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ori r24,r10,32768
	r24.u64 = ctx.r10.u64 | 32768;
	// ble cr6,0x82601440
	if (!cr6.gt) goto loc_82601440;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r23,r11,1
	r23.s64 = r11.s64 + 1;
loc_82601040:
	// mr r27,r25
	r27.u64 = r25.u64;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x82601260
	if (!cr6.gt) goto loc_82601260;
	// mr r29,r22
	r29.u64 = r22.u64;
loc_82601050:
	// lbz r11,0(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826011f4
	if (!cr6.eq) goto loc_826011F4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lbz r4,21648(r26)
	ctx.r4.u64 = PPC_LOAD_U8(r26.u32 + 21648);
	// lwz r28,21640(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 21640);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82601150
	if (cr6.lt) goto loc_82601150;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x82601148
	if (!cr6.lt) goto loc_82601148;
loc_826010B0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826010dc
	if (cr6.lt) goto loc_826010DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826010b0
	if (cr6.eq) goto loc_826010B0;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8260118c
	goto loc_8260118C;
loc_826010DC:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_82601148:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8260118c
	goto loc_8260118C;
loc_82601150:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_82601158:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r24
	r11.u64 = r30.u64 + r24.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82601158
	if (cr6.lt) goto loc_82601158;
loc_8260118C:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// srawi r10,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r10.s64 = r30.s32 >> 1;
	// srawi r9,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	ctx.r9.s64 = r30.s32 >> 2;
	// rlwimi r11,r30,31,0,0
	r11.u64 = (__builtin_rotateleft32(r30.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// srawi r8,r30,3
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7) != 0);
	ctx.r8.s64 = r30.s32 >> 3;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// rlwimi r11,r10,31,0,0
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,20(r29)
	PPC_STORE_U32(r29.u32 + 20, r11.u32);
	// lwz r11,136(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// rlwimi r10,r9,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r22
	PPC_STORE_U32(r11.u32 + r22.u32, ctx.r10.u32);
	// lwz r11,136(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// rlwimi r10,r8,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r8.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// b 0x8260124c
	goto loc_8260124C;
loc_826011F4:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,20(r29)
	PPC_STORE_U32(r29.u32 + 20, r11.u32);
	// lwz r11,136(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stwx r10,r11,r22
	PPC_STORE_U32(r11.u32 + r22.u32, ctx.r10.u32);
	// lwz r11,136(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
loc_8260124C:
	// addi r27,r27,2
	r27.s64 = r27.s64 + 2;
	// stwx r10,r11,r22
	PPC_STORE_U32(r11.u32 + r22.u32, ctx.r10.u32);
	// addi r29,r29,40
	r29.s64 = r29.s64 + 40;
	// cmpw cr6,r27,r21
	cr6.compare<int32_t>(r27.s32, r21.s32, xer);
	// blt cr6,0x82601050
	if (cr6.lt) goto loc_82601050;
loc_82601260:
	// lwz r11,136(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82601420
	if (cr6.eq) goto loc_82601420;
	// lbz r11,0(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826013e8
	if (!cr6.eq) goto loc_826013E8;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lbz r4,21648(r26)
	ctx.r4.u64 = PPC_LOAD_U8(r26.u32 + 21648);
	// lwz r29,21640(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + 21640);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82601370
	if (cr6.lt) goto loc_82601370;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x82601368
	if (!cr6.lt) goto loc_82601368;
loc_826012D0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826012fc
	if (cr6.lt) goto loc_826012FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826012d0
	if (cr6.eq) goto loc_826012D0;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826013ac
	goto loc_826013AC;
loc_826012FC:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_82601368:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826013ac
	goto loc_826013AC;
loc_82601370:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_82601378:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r24
	r11.u64 = r30.u64 + r24.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82601378
	if (cr6.lt) goto loc_82601378;
loc_826013AC:
	// rlwinm r11,r27,2,0,29
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r9,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	ctx.r9.s64 = r30.s32 >> 2;
	// add r11,r27,r11
	r11.u64 = r27.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// rlwimi r10,r30,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(r30.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r22
	PPC_STORE_U32(r11.u32 + r22.u32, ctx.r10.u32);
	// lwz r11,136(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// rlwimi r10,r9,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// b 0x8260141c
	goto loc_8260141C;
loc_826013E8:
	// rlwinm r11,r27,2,0,29
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r27,r11
	r11.u64 = r27.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stwx r10,r11,r22
	PPC_STORE_U32(r11.u32 + r22.u32, ctx.r10.u32);
	// lwz r11,136(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
loc_8260141C:
	// stwx r10,r11,r22
	PPC_STORE_U32(r11.u32 + r22.u32, ctx.r10.u32);
loc_82601420:
	// lwz r11,136(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r22,r11,r22
	r22.u64 = r11.u64 + r22.u64;
	// bne cr6,0x82601040
	if (!cr6.eq) goto loc_82601040;
loc_82601440:
	// lwz r11,140(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 140);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82601770
	if (cr6.eq) goto loc_82601770;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x826015ec
	if (!cr6.gt) goto loc_826015EC;
	// addi r11,r21,-1
	r11.s64 = r21.s64 + -1;
	// mr r28,r22
	r28.u64 = r22.u64;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r27,r11,1
	r27.s64 = r11.s64 + 1;
	// rlwinm r25,r27,1,0,30
	r25.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
loc_8260146C:
	// lbz r11,0(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826015c4
	if (!cr6.eq) goto loc_826015C4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lbz r4,21648(r26)
	ctx.r4.u64 = PPC_LOAD_U8(r26.u32 + 21648);
	// lwz r29,21640(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + 21640);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8260156c
	if (cr6.lt) goto loc_8260156C;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x82601564
	if (!cr6.lt) goto loc_82601564;
loc_826014CC:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826014f8
	if (cr6.lt) goto loc_826014F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826014cc
	if (cr6.eq) goto loc_826014CC;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826015a8
	goto loc_826015A8;
loc_826014F8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_82601564:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826015a8
	goto loc_826015A8;
loc_8260156C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_82601574:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r24
	r11.u64 = r30.u64 + r24.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82601574
	if (cr6.lt) goto loc_82601574;
loc_826015A8:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// srawi r10,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r10.s64 = r30.s32 >> 1;
	// rlwimi r11,r30,31,0,0
	r11.u64 = (__builtin_rotateleft32(r30.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// lwz r11,20(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 20);
	// rlwimi r11,r10,31,0,0
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// b 0x826015d8
	goto loc_826015D8;
loc_826015C4:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// lwz r11,20(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 20);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
loc_826015D8:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// stw r11,20(r28)
	PPC_STORE_U32(r28.u32 + 20, r11.u32);
	// addi r28,r28,40
	r28.s64 = r28.s64 + 40;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x8260146c
	if (!cr6.eq) goto loc_8260146C;
loc_826015EC:
	// lwz r11,136(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 136);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82601770
	if (cr6.eq) goto loc_82601770;
	// lbz r11,0(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82601758
	if (!cr6.eq) goto loc_82601758;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lbz r4,21648(r26)
	ctx.r4.u64 = PPC_LOAD_U8(r26.u32 + 21648);
	// lwz r29,21640(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + 21640);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826016f8
	if (cr6.lt) goto loc_826016F8;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826016f0
	if (!cr6.lt) goto loc_826016F0;
loc_82601658:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82601684
	if (cr6.lt) goto loc_82601684;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82601658
	if (cr6.eq) goto loc_82601658;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82601734
	goto loc_82601734;
loc_82601684:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826016F0:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82601734
	goto loc_82601734;
loc_826016F8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_82601700:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r24
	r11.u64 = r30.u64 + r24.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82601700
	if (cr6.lt) goto loc_82601700;
loc_82601734:
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r25,r11
	r11.u64 = r25.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// rlwimi r10,r30,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(r30.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r22
	PPC_STORE_U32(r11.u32 + r22.u32, ctx.r10.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd24
	return;
loc_82601758:
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r25,r11
	r11.u64 = r25.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stwx r10,r11,r22
	PPC_STORE_U32(r11.u32 + r22.u32, ctx.r10.u32);
loc_82601770:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_8260177C"))) PPC_WEAK_FUNC(sub_8260177C);
PPC_FUNC_IMPL(__imp__sub_8260177C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82601780"))) PPC_WEAK_FUNC(sub_82601780);
PPC_FUNC_IMPL(__imp__sub_82601780) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r19{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// lwz r11,21700(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21700);
	// lwz r29,268(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826017b0
	if (!cr6.eq) goto loc_826017B0;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_826017B0:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r25,r8,0
	r25.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826017dc
	if (!cr0.lt) goto loc_826017DC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826017DC:
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// li r28,3
	r28.s64 = 3;
	// li r27,0
	r27.s64 = 0;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82601850
	if (!cr6.lt) goto loc_82601850;
loc_826017F8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82601850
	if (cr6.eq) goto loc_82601850;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r28,r11,r28
	r28.s64 = r28.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r10,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r28.u8 & 0x3F));
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// bge 0x82601840
	if (!cr0.lt) goto loc_82601840;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82601840:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bgt cr6,0x826017f8
	if (cr6.gt) goto loc_826017F8;
loc_82601850:
	// subfic r9,r28,64
	xer.ca = r28.u32 <= 64;
	ctx.r9.s64 = 64 - r28.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r28,32
	ctx.r8.u64 = r28.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r28,r10
	ctx.r10.s64 = ctx.r10.s64 - r28.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r27
	r28.u64 = r11.u64 + r27.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x8260188c
	if (!cr0.lt) goto loc_8260188C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8260188C:
	// cmplwi cr6,r28,7
	cr6.compare<uint32_t>(r28.u32, 7, xer);
	// bgt cr6,0x82601c50
	if (cr6.gt) goto loc_82601C50;
	// lis r12,-32160
	r12.s64 = -2107637760;
	// addi r12,r12,6316
	r12.s64 = r12.s64 + 6316;
	// rlwinm r0,r28,2,0,29
	r0.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r28.u64) {
	case 0:
		goto loc_826018CC;
	case 1:
		goto loc_826018D4;
	case 2:
		goto loc_826018EC;
	case 3:
		goto loc_82601940;
	case 4:
		goto loc_8260195C;
	case 5:
		goto loc_82601978;
	case 6:
		goto loc_82601A8C;
	case 7:
		goto loc_82601BA0;
	default:
		__builtin_unreachable();
	}
	// lwz r19,6348(0)
	r19.u64 = PPC_LOAD_U32(6348);
	// lwz r19,6356(0)
	r19.u64 = PPC_LOAD_U32(6356);
	// lwz r19,6380(0)
	r19.u64 = PPC_LOAD_U32(6380);
	// lwz r19,6464(0)
	r19.u64 = PPC_LOAD_U32(6464);
	// lwz r19,6492(0)
	r19.u64 = PPC_LOAD_U32(6492);
	// lwz r19,6520(0)
	r19.u64 = PPC_LOAD_U32(6520);
	// lwz r19,6796(0)
	r19.u64 = PPC_LOAD_U32(6796);
	// lwz r19,7072(0)
	r19.u64 = PPC_LOAD_U32(7072);
loc_826018CC:
	// li r26,0
	r26.s64 = 0;
	// b 0x82601c54
	goto loc_82601C54;
loc_826018D4:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,84(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r4,144(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// li r26,1
	r26.s64 = 1;
	// bl 0x825ed188
	sub_825ED188(ctx, base);
	// b 0x82601c54
	goto loc_82601C54;
loc_826018EC:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,84(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r4,144(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// li r26,2
	r26.s64 = 2;
	// bl 0x825ed188
	sub_825ED188(ctx, base);
loc_82601900:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82601c90
	if (!cr6.gt) goto loc_82601C90;
loc_82601914:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82601c3c
	if (!cr6.gt) goto loc_82601C3C;
loc_82601924:
	// add. r10,r7,r6
	ctx.r10.u64 = ctx.r7.u64 + ctx.r6.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x82601c10
	if (cr0.eq) goto loc_82601C10;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x82601bbc
	if (!cr6.eq) goto loc_82601BBC;
	// lwz r10,-20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -20);
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// b 0x82601c14
	goto loc_82601C14;
loc_82601940:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// li r26,3
	r26.s64 = 3;
	// bl 0x825ed650
	sub_825ED650(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82601c54
	if (cr6.eq) goto loc_82601C54;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8260195C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// li r26,4
	r26.s64 = 4;
	// bl 0x825ed650
	sub_825ED650(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82601900
	if (cr6.eq) goto loc_82601900;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_82601978:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r26,5
	r26.s64 = 5;
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82601c54
	if (!cr6.gt) goto loc_82601C54;
loc_8260198C:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826019b8
	if (!cr0.lt) goto loc_826019B8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826019B8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82601a34
	if (cr6.eq) goto loc_82601A34;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82601a78
	if (!cr6.gt) goto loc_82601A78;
loc_826019D0:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r28,r8,0
	r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826019fc
	if (!cr0.lt) goto loc_826019FC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826019FC:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r11,r11,r27
	r11.s64 = int64_t(r11.s32) * int64_t(r27.s32);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r29
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// rlwimi r10,r28,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(r28.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r29
	PPC_STORE_U32(r11.u32 + r29.u32, ctx.r10.u32);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x826019d0
	if (cr6.lt) goto loc_826019D0;
	// b 0x82601a78
	goto loc_82601A78;
loc_82601A34:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82601a78
	if (!cr6.gt) goto loc_82601A78;
loc_82601A44:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r10,r10,r27
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r27.s32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r29.u32);
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stwx r9,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + r29.u32, ctx.r9.u32);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82601a44
	if (cr6.lt) goto loc_82601A44;
loc_82601A78:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x8260198c
	if (cr6.lt) goto loc_8260198C;
	// b 0x82601c54
	goto loc_82601C54;
loc_82601A8C:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r26,6
	r26.s64 = 6;
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82601c54
	if (!cr6.gt) goto loc_82601C54;
loc_82601AA0:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82601acc
	if (!cr0.lt) goto loc_82601ACC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82601ACC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82601b48
	if (cr6.eq) goto loc_82601B48;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82601b8c
	if (!cr6.gt) goto loc_82601B8C;
loc_82601AE4:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r28,r8,0
	r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82601b10
	if (!cr0.lt) goto loc_82601B10;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82601B10:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r29
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// rlwimi r10,r28,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(r28.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r29
	PPC_STORE_U32(r11.u32 + r29.u32, ctx.r10.u32);
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x82601ae4
	if (cr6.lt) goto loc_82601AE4;
	// b 0x82601b8c
	goto loc_82601B8C;
loc_82601B48:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82601b8c
	if (!cr6.gt) goto loc_82601B8C;
loc_82601B58:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r29.u32);
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stwx r9,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + r29.u32, ctx.r9.u32);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82601b58
	if (cr6.lt) goto loc_82601B58;
loc_82601B8C:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x82601aa0
	if (cr6.lt) goto loc_82601AA0;
	// b 0x82601c54
	goto loc_82601C54;
loc_82601BA0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// li r26,7
	r26.s64 = 7;
	// bl 0x82600920
	sub_82600920(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82601c54
	if (cr6.eq) goto loc_82601C54;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_82601BBC:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne cr6,0x82601be4
	if (!cr6.eq) goto loc_82601BE4;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// b 0x82601c14
	goto loc_82601C14;
loc_82601BE4:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r10,-20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -20);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r9,r9,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x82601c14
	if (cr6.eq) goto loc_82601C14;
loc_82601C10:
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_82601C14:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,31,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x80000000;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// rlwimi r10,r9,0,1,31
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0x7FFFFFFF) | (ctx.r10.u64 & 0xFFFFFFFF80000000);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// blt cr6,0x82601924
	if (cr6.lt) goto loc_82601924;
loc_82601C3C:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// blt cr6,0x82601914
	if (cr6.lt) goto loc_82601914;
	// b 0x82601c90
	goto loc_82601C90;
loc_82601C50:
	// lwz r26,80(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_82601C54:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x82601c90
	if (cr6.eq) goto loc_82601C90;
	// lwz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82601c90
	if (!cr6.gt) goto loc_82601C90;
loc_82601C6C:
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// rlwimi r9,r10,0,1,31
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x7FFFFFFF) | (ctx.r9.u64 & 0xFFFFFFFF80000000);
	// stw r9,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r9.u32);
	// addi r29,r29,20
	r29.s64 = r29.s64 + 20;
	// lwz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82601c6c
	if (cr6.lt) goto loc_82601C6C;
loc_82601C90:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x82601ca8
	if (!cr6.eq) goto loc_82601CA8;
	// stw r26,344(r31)
	PPC_STORE_U32(r31.u32 + 344, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_82601CA8:
	// cmpwi cr6,r24,5
	cr6.compare<int32_t>(r24.s32, 5, xer);
	// bne cr6,0x82601cc0
	if (!cr6.eq) goto loc_82601CC0;
	// stw r26,20940(r31)
	PPC_STORE_U32(r31.u32 + 20940, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_82601CC0:
	// cmpwi cr6,r24,4
	cr6.compare<int32_t>(r24.s32, 4, xer);
	// bne cr6,0x82601cd8
	if (!cr6.eq) goto loc_82601CD8;
	// stw r26,20004(r31)
	PPC_STORE_U32(r31.u32 + 20004, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_82601CD8:
	// cmpwi cr6,r24,3
	cr6.compare<int32_t>(r24.s32, 3, xer);
	// bne cr6,0x82601cf0
	if (!cr6.eq) goto loc_82601CF0;
	// stw r26,14804(r31)
	PPC_STORE_U32(r31.u32 + 14804, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_82601CF0:
	// cmpwi cr6,r24,2
	cr6.compare<int32_t>(r24.s32, 2, xer);
	// bne cr6,0x82601d08
	if (!cr6.eq) goto loc_82601D08;
	// stw r26,19988(r31)
	PPC_STORE_U32(r31.u32 + 19988, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_82601D08:
	// stw r26,348(r31)
	PPC_STORE_U32(r31.u32 + 348, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_82601D18"))) PPC_WEAK_FUNC(sub_82601D18);
PPC_FUNC_IMPL(__imp__sub_82601D18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// li r29,1
	r29.s64 = 1;
	// li r11,-1
	r11.s64 = -1;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,114
	ctx.r4.s64 = 114;
	// std r30,3576(r31)
	PPC_STORE_U64(r31.u32 + 3576, r30.u64);
	// li r3,14
	ctx.r3.s64 = 14;
	// std r30,3584(r31)
	PPC_STORE_U64(r31.u32 + 3584, r30.u64);
	// stw r11,3644(r31)
	PPC_STORE_U32(r31.u32 + 3644, r11.u32);
	// li r11,1000
	r11.s64 = 1000;
	// std r30,3600(r31)
	PPC_STORE_U64(r31.u32 + 3600, r30.u64);
	// std r30,3608(r31)
	PPC_STORE_U64(r31.u32 + 3608, r30.u64);
	// stw r30,3632(r31)
	PPC_STORE_U32(r31.u32 + 3632, r30.u32);
	// stw r30,452(r31)
	PPC_STORE_U32(r31.u32 + 452, r30.u32);
	// stw r30,396(r31)
	PPC_STORE_U32(r31.u32 + 396, r30.u32);
	// stw r30,392(r31)
	PPC_STORE_U32(r31.u32 + 392, r30.u32);
	// stw r30,400(r31)
	PPC_STORE_U32(r31.u32 + 400, r30.u32);
	// stw r30,3888(r31)
	PPC_STORE_U32(r31.u32 + 3888, r30.u32);
	// stw r30,3892(r31)
	PPC_STORE_U32(r31.u32 + 3892, r30.u32);
	// stw r30,3896(r31)
	PPC_STORE_U32(r31.u32 + 3896, r30.u32);
	// stw r30,436(r31)
	PPC_STORE_U32(r31.u32 + 436, r30.u32);
	// stw r30,432(r31)
	PPC_STORE_U32(r31.u32 + 432, r30.u32);
	// stw r30,440(r31)
	PPC_STORE_U32(r31.u32 + 440, r30.u32);
	// stw r30,3884(r31)
	PPC_STORE_U32(r31.u32 + 3884, r30.u32);
	// stw r30,3948(r31)
	PPC_STORE_U32(r31.u32 + 3948, r30.u32);
	// stw r30,444(r31)
	PPC_STORE_U32(r31.u32 + 444, r30.u32);
	// stw r30,3900(r31)
	PPC_STORE_U32(r31.u32 + 3900, r30.u32);
	// stw r30,15508(r31)
	PPC_STORE_U32(r31.u32 + 15508, r30.u32);
	// stw r30,15512(r31)
	PPC_STORE_U32(r31.u32 + 15512, r30.u32);
	// stw r30,3664(r31)
	PPC_STORE_U32(r31.u32 + 3664, r30.u32);
	// stw r30,3668(r31)
	PPC_STORE_U32(r31.u32 + 3668, r30.u32);
	// stw r30,3672(r31)
	PPC_STORE_U32(r31.u32 + 3672, r30.u32);
	// stw r29,3676(r31)
	PPC_STORE_U32(r31.u32 + 3676, r29.u32);
	// stw r30,3680(r31)
	PPC_STORE_U32(r31.u32 + 3680, r30.u32);
	// stw r30,264(r31)
	PPC_STORE_U32(r31.u32 + 264, r30.u32);
	// stw r30,15768(r31)
	PPC_STORE_U32(r31.u32 + 15768, r30.u32);
	// stw r30,1892(r31)
	PPC_STORE_U32(r31.u32 + 1892, r30.u32);
	// stw r30,1896(r31)
	PPC_STORE_U32(r31.u32 + 1896, r30.u32);
	// stw r30,1900(r31)
	PPC_STORE_U32(r31.u32 + 1900, r30.u32);
	// stw r30,1904(r31)
	PPC_STORE_U32(r31.u32 + 1904, r30.u32);
	// stw r30,1908(r31)
	PPC_STORE_U32(r31.u32 + 1908, r30.u32);
	// stw r30,1912(r31)
	PPC_STORE_U32(r31.u32 + 1912, r30.u32);
	// stw r30,268(r31)
	PPC_STORE_U32(r31.u32 + 268, r30.u32);
	// stw r30,272(r31)
	PPC_STORE_U32(r31.u32 + 272, r30.u32);
	// stw r30,15548(r31)
	PPC_STORE_U32(r31.u32 + 15548, r30.u32);
	// stw r30,15516(r31)
	PPC_STORE_U32(r31.u32 + 15516, r30.u32);
	// std r30,2928(r31)
	PPC_STORE_U64(r31.u32 + 2928, r30.u64);
	// stw r30,2936(r31)
	PPC_STORE_U32(r31.u32 + 2936, r30.u32);
	// stw r30,2940(r31)
	PPC_STORE_U32(r31.u32 + 2940, r30.u32);
	// stw r30,2944(r31)
	PPC_STORE_U32(r31.u32 + 2944, r30.u32);
	// stw r30,2948(r31)
	PPC_STORE_U32(r31.u32 + 2948, r30.u32);
	// stw r30,1792(r31)
	PPC_STORE_U32(r31.u32 + 1792, r30.u32);
	// stw r30,15520(r31)
	PPC_STORE_U32(r31.u32 + 15520, r30.u32);
	// stw r30,15524(r31)
	PPC_STORE_U32(r31.u32 + 15524, r30.u32);
	// stw r30,15528(r31)
	PPC_STORE_U32(r31.u32 + 15528, r30.u32);
	// stw r11,15532(r31)
	PPC_STORE_U32(r31.u32 + 15532, r11.u32);
	// stw r29,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r29.u32);
	// stw r30,3344(r31)
	PPC_STORE_U32(r31.u32 + 3344, r30.u32);
	// stw r30,3348(r31)
	PPC_STORE_U32(r31.u32 + 3348, r30.u32);
	// stw r30,21540(r31)
	PPC_STORE_U32(r31.u32 + 21540, r30.u32);
	// bl 0x82278448
	sub_82278448(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,21540(r31)
	PPC_STORE_U32(r31.u32 + 21540, ctx.r3.u32);
	// beq cr6,0x82601e38
	if (cr6.eq) goto loc_82601E38;
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x82601e38
	if (cr6.eq) goto loc_82601E38;
	// stw r29,21540(r31)
	PPC_STORE_U32(r31.u32 + 21540, r29.u32);
	// b 0x82601e3c
	goto loc_82601E3C;
loc_82601E38:
	// stw r30,21540(r31)
	PPC_STORE_U32(r31.u32 + 21540, r30.u32);
loc_82601E3C:
	// stw r30,15504(r31)
	PPC_STORE_U32(r31.u32 + 15504, r30.u32);
	// stw r30,3352(r31)
	PPC_STORE_U32(r31.u32 + 3352, r30.u32);
	// stw r30,21328(r31)
	PPC_STORE_U32(r31.u32 + 21328, r30.u32);
	// stw r30,21332(r31)
	PPC_STORE_U32(r31.u32 + 21332, r30.u32);
	// stw r30,21336(r31)
	PPC_STORE_U32(r31.u32 + 21336, r30.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82601E58"))) PPC_WEAK_FUNC(sub_82601E58);
PPC_FUNC_IMPL(__imp__sub_82601E58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r4,15
	r11.s64 = ctx.r4.s64 + 15;
	// addi r8,r5,15
	ctx.r8.s64 = ctx.r5.s64 + 15;
	// rlwinm r10,r11,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF0;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// lwz r9,3924(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// rlwinm r11,r8,0,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFF0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82601e94
	if (cr6.eq) goto loc_82601E94;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// srawi r8,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 2;
	// b 0x82601e9c
	goto loc_82601E9C;
loc_82601E94:
	// srawi r8,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 1;
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
loc_82601E9C:
	// lwz r7,15472(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r7,7
	cr6.compare<int32_t>(ctx.r7.s32, 7, xer);
	// bne cr6,0x82601eb4
	if (!cr6.eq) goto loc_82601EB4;
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
loc_82601EB4:
	// lwz r6,15300(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 15300);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// addi r8,r8,32
	ctx.r8.s64 = ctx.r8.s64 + 32;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// mullw r30,r9,r8
	r30.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// mullw r26,r11,r10
	r26.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// bne cr6,0x82601f74
	if (!cr6.eq) goto loc_82601F74;
	// lwz r9,21580(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21580);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82601eec
	if (cr6.eq) goto loc_82601EEC;
	// li r11,1
	r11.s64 = 1;
loc_82601EEC:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x82601ef8
	if (cr6.eq) goto loc_82601EF8;
	// addi r11,r29,-6
	r11.s64 = r29.s64 + -6;
loc_82601EF8:
	// addi r7,r7,-7
	ctx.r7.s64 = ctx.r7.s64 + -7;
	// lwz r9,21356(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21356);
	// addi r6,r11,6
	ctx.r6.s64 = r11.s64 + 6;
	// lwz r8,21352(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 21352);
	// cntlzw r11,r7
	r11.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r7,r11,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x8263c880
	sub_8263C880(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82602164
	if (!cr6.eq) goto loc_82602164;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x82601f50
	if (cr6.eq) goto loc_82601F50;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e6710
	sub_825E6710(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_82601F50:
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// addi r4,r31,3700
	ctx.r4.s64 = r31.s64 + 3700;
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r31,3692
	ctx.r4.s64 = r31.s64 + 3692;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// b 0x82601fac
	goto loc_82601FAC;
loc_82601F74:
	// addi r11,r7,-7
	r11.s64 = ctx.r7.s64 + -7;
	// lwz r9,21356(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21356);
	// li r6,4
	ctx.r6.s64 = 4;
	// lwz r8,21352(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 21352);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r7,r11,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x8263c880
	sub_8263C880(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82602164
	if (!cr6.eq) goto loc_82602164;
loc_82601FAC:
	// addi r25,r31,3688
	r25.s64 = r31.s64 + 3688;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// li r5,-1
	ctx.r5.s64 = -1;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// addi r29,r31,3696
	r29.s64 = r31.s64 + 3696;
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// addi r28,r31,3704
	r28.s64 = r31.s64 + 3704;
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// addi r27,r31,3708
	r27.s64 = r31.s64 + 3708;
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82602160
	if (cr6.eq) goto loc_82602160;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82602160
	if (cr6.eq) goto loc_82602160;
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82602160
	if (cr6.eq) goto loc_82602160;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82602160
	if (cr6.eq) goto loc_82602160;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// li r4,0
	ctx.r4.s64 = 0;
	// rotlwi r3,r9,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// stw r9,3720(r31)
	PPC_STORE_U32(r31.u32 + 3720, ctx.r9.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// stw r8,3724(r31)
	PPC_STORE_U32(r31.u32 + 3724, ctx.r8.u32);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r11,3724(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + r11.u64;
	// rotlwi r11,r8,0
	r11.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// stw r8,3728(r31)
	PPC_STORE_U32(r31.u32 + 3728, ctx.r8.u32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r7,3804(r31)
	PPC_STORE_U32(r31.u32 + 3804, ctx.r7.u32);
	// stw r11,3808(r31)
	PPC_STORE_U32(r31.u32 + 3808, r11.u32);
	// add r11,r3,r9
	r11.u64 = ctx.r3.u64 + ctx.r9.u64;
	// stw r11,3800(r31)
	PPC_STORE_U32(r31.u32 + 3800, r11.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// li r4,128
	ctx.r4.s64 = 128;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,128
	ctx.r4.s64 = 128;
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r7,220(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r8,3700(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3700);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stw r6,3732(r31)
	PPC_STORE_U32(r31.u32 + 3732, ctx.r6.u32);
	// rotlwi r6,r6,0
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// stw r5,3736(r31)
	PPC_STORE_U32(r31.u32 + 3736, ctx.r5.u32);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r6,3756(r31)
	PPC_STORE_U32(r31.u32 + 3756, ctx.r6.u32);
	// stw r11,3740(r31)
	PPC_STORE_U32(r31.u32 + 3740, r11.u32);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r11,3776(r31)
	PPC_STORE_U32(r31.u32 + 3776, r11.u32);
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r11,3780(r31)
	PPC_STORE_U32(r31.u32 + 3780, r11.u32);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r11,3784(r31)
	PPC_STORE_U32(r31.u32 + 3784, r11.u32);
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stw r11,3788(r31)
	PPC_STORE_U32(r31.u32 + 3788, r11.u32);
	// lwz r11,4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// stw r11,3792(r31)
	PPC_STORE_U32(r31.u32 + 3792, r11.u32);
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// stw r11,3796(r31)
	PPC_STORE_U32(r31.u32 + 3796, r11.u32);
	// beq cr6,0x82602124
	if (cr6.eq) goto loc_82602124;
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// stw r11,3744(r31)
	PPC_STORE_U32(r31.u32 + 3744, r11.u32);
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r11,3748(r31)
	PPC_STORE_U32(r31.u32 + 3748, r11.u32);
	// lwz r11,8(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// stw r11,3752(r31)
	PPC_STORE_U32(r31.u32 + 3752, r11.u32);
loc_82602124:
	// lwz r11,3692(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3692);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82602154
	if (cr6.eq) goto loc_82602154;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,3760(r31)
	PPC_STORE_U32(r31.u32 + 3760, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r9,3764(r31)
	PPC_STORE_U32(r31.u32 + 3764, ctx.r9.u32);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r10,3772(r31)
	PPC_STORE_U32(r31.u32 + 3772, ctx.r10.u32);
	// stw r11,3768(r31)
	PPC_STORE_U32(r31.u32 + 3768, r11.u32);
loc_82602154:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_82602160:
	// li r3,2
	ctx.r3.s64 = 2;
loc_82602164:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_8260216C"))) PPC_WEAK_FUNC(sub_8260216C);
PPC_FUNC_IMPL(__imp__sub_8260216C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82602170"))) PPC_WEAK_FUNC(sub_82602170);
PPC_FUNC_IMPL(__imp__sub_82602170) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r19{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// lwz r11,3956(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3956);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82602308
	if (cr6.eq) goto loc_82602308;
	// lwz r11,2976(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2976);
	// lwz r10,3356(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3356);
	// addi r11,r11,7
	r11.s64 = r11.s64 + 7;
	// lwz r6,136(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmplwi cr6,r10,2
	cr6.compare<uint32_t>(ctx.r10.u32, 2, xer);
	// rlwinm r9,r11,0,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF8;
	// rlwinm r10,r6,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r11,r6,7,0,24
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 7) & 0xFFFFFF80;
	// stw r9,2980(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2980, ctx.r9.u32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r9,2984(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2984, ctx.r9.u32);
	// add r9,r8,r10
	ctx.r9.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r8,2988(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2988, ctx.r8.u32);
	// add r8,r9,r11
	ctx.r8.u64 = ctx.r9.u64 + r11.u64;
	// add r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 + r11.u64;
	// stw r9,2992(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2992, ctx.r9.u32);
	// add r9,r7,r11
	ctx.r9.u64 = ctx.r7.u64 + r11.u64;
	// stw r8,2996(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2996, ctx.r8.u32);
	// stw r7,3000(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3000, ctx.r7.u32);
	// stw r9,3004(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3004, ctx.r9.u32);
	// bne cr6,0x8260222c
	if (!cr6.eq) goto loc_8260222C;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r9,3008(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3008, ctx.r9.u32);
	// add r9,r8,r10
	ctx.r9.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r8,3012(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3012, ctx.r8.u32);
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r9,3016(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3016, ctx.r9.u32);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// stw r8,3020(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3020, ctx.r8.u32);
	// stw r10,3024(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3024, ctx.r10.u32);
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + r11.u64;
	// stw r9,3028(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3028, ctx.r9.u32);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// stw r10,3032(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3032, ctx.r10.u32);
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r9,3036(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3036, ctx.r9.u32);
	// stw r10,3040(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3040, ctx.r10.u32);
	// stw r11,3044(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3044, r11.u32);
loc_8260222C:
	// lwz r10,15240(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15240);
	// lwz r11,15184(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15184);
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// lwz r9,3924(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3924);
	// addi r8,r11,31
	ctx.r8.s64 = r11.s64 + 31;
	// rlwinm r11,r10,0,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// rlwinm r10,r8,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFE0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,15244(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15244, r11.u32);
	// stw r10,15188(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15188, ctx.r10.u32);
	// addi r10,r11,288
	ctx.r10.s64 = r11.s64 + 288;
	// addi r11,r10,288
	r11.s64 = ctx.r10.s64 + 288;
	// stw r10,15248(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15248, ctx.r10.u32);
	// addi r10,r11,96
	ctx.r10.s64 = r11.s64 + 96;
	// stw r11,15252(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15252, r11.u32);
	// addi r11,r10,96
	r11.s64 = ctx.r10.s64 + 96;
	// stw r10,15256(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15256, ctx.r10.u32);
	// addi r10,r11,96
	ctx.r10.s64 = r11.s64 + 96;
	// stw r11,15260(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15260, r11.u32);
	// lwz r11,144(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 144);
	// stw r10,15264(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15264, ctx.r10.u32);
	// rlwinm r10,r11,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// beq cr6,0x826022a0
	if (cr6.eq) goto loc_826022A0;
	// lwz r9,460(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 460);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r9,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// b 0x826022ac
	goto loc_826022AC;
loc_826022A0:
	// lwz r8,460(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 460);
	// rlwinm r9,r11,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
loc_826022AC:
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r10,464(r3)
	PPC_STORE_U32(ctx.r3.u32 + 464, ctx.r10.u32);
	// lwz r10,140(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,15208(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15208);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lwz r8,15292(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15292);
	// lwz r5,15268(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15268);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// mullw r6,r10,r6
	ctx.r6.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// stw r9,468(r3)
	PPC_STORE_U32(ctx.r3.u32 + 468, ctx.r9.u32);
	// lwz r9,15276(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15276);
	// lwz r10,15284(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15284);
	// stw r7,15212(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15212, ctx.r7.u32);
	// rlwinm r6,r6,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r8,15296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15296, ctx.r8.u32);
	// stw r9,15280(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15280, ctx.r9.u32);
	// stw r6,15272(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15272, ctx.r6.u32);
	// stw r11,15288(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15288, r11.u32);
loc_82602308:
	// lwz r11,140(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// li r30,0
	r30.s64 = 0;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x826023cc
	if (!cr6.gt) goto loc_826023CC;
loc_82602320:
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// mr r11,r30
	r11.u64 = r30.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// ble cr6,0x826023bc
	if (!cr6.gt) goto loc_826023BC;
	// cntlzw r10,r5
	ctx.r10.u64 = ctx.r5.u32 == 0 ? 32 : __builtin_clz(ctx.r5.u32);
	// rlwinm r4,r10,28,30,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0x2;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_82602344:
	// lwz r6,136(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cntlzw r31,r11
	r31.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r7,140(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// lwz r9,268(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - r11.s64;
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// cntlzw r6,r6
	ctx.r6.u64 = ctx.r6.u32 == 0 ? 32 : __builtin_clz(ctx.r6.u32);
	// cntlzw r7,r7
	ctx.r7.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// rlwinm r6,r6,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x1;
	// rlwinm r7,r7,28,30,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 28) & 0x2;
	// rlwinm r31,r31,27,31,31
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 27) & 0x1;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// or r6,r31,r4
	ctx.r6.u64 = r31.u64 | ctx.r4.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r6,r6,28
	ctx.r6.u64 = ctx.r6.u32 & 0xF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// rlwinm r7,r7,12,0,19
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFFFF000;
	// rlwinm r6,r6,0,20,15
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFFFFFFF0FFF;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r9,136(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x82602344
	if (cr6.lt) goto loc_82602344;
loc_826023BC:
	// lwz r11,140(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// cmplw cr6,r5,r11
	cr6.compare<uint32_t>(ctx.r5.u32, r11.u32, xer);
	// blt cr6,0x82602320
	if (cr6.lt) goto loc_82602320;
loc_826023CC:
	// lwz r11,272(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// stb r30,12(r11)
	PPC_STORE_U8(r11.u32 + 12, r30.u8);
	// lwz r11,272(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// stb r30,13(r11)
	PPC_STORE_U8(r11.u32 + 13, r30.u8);
	// lwz r11,272(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// stb r30,14(r11)
	PPC_STORE_U8(r11.u32 + 14, r30.u8);
	// lwz r11,272(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// stb r30,15(r11)
	PPC_STORE_U8(r11.u32 + 15, r30.u8);
	// lwz r11,272(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// stb r30,16(r11)
	PPC_STORE_U8(r11.u32 + 16, r30.u8);
	// lwz r11,272(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// stb r30,17(r11)
	PPC_STORE_U8(r11.u32 + 17, r30.u8);
	// lwz r7,208(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// lwz r8,136(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,204(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// lwz r6,144(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 144);
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,1896(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1896);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// addi r5,r9,-8
	ctx.r5.s64 = ctx.r9.s64 + -8;
	// lwz r9,1892(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1892);
	// stw r7,15180(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15180, ctx.r7.u32);
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r5,15176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15176, ctx.r5.u32);
	// rlwinm r8,r8,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// neg r6,r8
	ctx.r6.s64 = -ctx.r8.s64;
	// stw r8,1888(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1888, ctx.r8.u32);
	// ble cr6,0x82602478
	if (!cr6.gt) goto loc_82602478;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
loc_82602450:
	// lwz r7,268(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r8,r8,20
	ctx.r8.s64 = ctx.r8.s64 + 20;
	// lwz r5,0(r7)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// rlwinm r5,r5,0,4,2
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFFEFFFFFFF;
	// stw r5,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r5.u32);
	// lwz r7,144(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 144);
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// blt cr6,0x82602450
	if (cr6.lt) goto loc_82602450;
loc_82602478:
	// mr r29,r30
	r29.u64 = r30.u64;
loc_8260247C:
	// clrlwi r10,r29,31
	ctx.r10.u64 = r29.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8260248c
	if (cr6.eq) goto loc_8260248C;
	// lwz r6,1888(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1888);
loc_8260248C:
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// mr r31,r30
	r31.u64 = r30.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// ble cr6,0x82602658
	if (!cr6.gt) goto loc_82602658;
loc_8260249C:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r10,r9,-160
	ctx.r10.s64 = ctx.r9.s64 + -160;
loc_826024A4:
	// cmplwi cr6,r4,5
	cr6.compare<uint32_t>(ctx.r4.u32, 5, xer);
	// bgt cr6,0x82602630
	if (cr6.gt) goto loc_82602630;
	// lis r12,-32160
	r12.s64 = -2107637760;
	// addi r12,r12,9412
	r12.s64 = r12.s64 + 9412;
	// rlwinm r0,r4,2,0,29
	r0.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r4.u64) {
	case 0:
		goto loc_826024DC;
	case 1:
		goto loc_8260251C;
	case 2:
		goto loc_82602560;
	case 3:
		goto loc_8260258C;
	case 4:
		goto loc_826025B4;
	case 5:
		goto loc_826025F4;
	default:
		__builtin_unreachable();
	}
	// lwz r19,9436(0)
	r19.u64 = PPC_LOAD_U32(9436);
	// lwz r19,9500(0)
	r19.u64 = PPC_LOAD_U32(9500);
	// lwz r19,9568(0)
	r19.u64 = PPC_LOAD_U32(9568);
	// lwz r19,9612(0)
	r19.u64 = PPC_LOAD_U32(9612);
	// lwz r19,9652(0)
	r19.u64 = PPC_LOAD_U32(9652);
	// lwz r19,9716(0)
	r19.u64 = PPC_LOAD_U32(9716);
loc_826024DC:
	// lwz r5,1900(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// subfic r8,r6,32
	xer.ca = ctx.r6.u32 <= 32;
	ctx.r8.s64 = 32 - ctx.r6.s64;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// addi r7,r6,48
	ctx.r7.s64 = ctx.r6.s64 + 48;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// lwz r5,1900(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// stw r8,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r8.u32);
	// stw r5,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r5.u32);
	// lwz r8,1900(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// stw r7,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r7.u32);
	// stw r8,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r8.u32);
	// b 0x82602630
	goto loc_82602630;
loc_8260251C:
	// neg r7,r6
	ctx.r7.s64 = -ctx.r6.s64;
	// addi r8,r10,128
	ctx.r8.s64 = ctx.r10.s64 + 128;
	// addi r5,r7,32
	ctx.r5.s64 = ctx.r7.s64 + 32;
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r8,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r8.u32);
	// stw r8,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r8.u32);
	// add r8,r5,r9
	ctx.r8.u64 = ctx.r5.u64 + ctx.r9.u64;
	// lwz r5,1900(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// stw r8,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r8.u32);
	// stw r5,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r5.u32);
	// lwz r8,1900(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// stw r7,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r7.u32);
	// stw r8,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r8.u32);
	// b 0x82602630
	goto loc_82602630;
loc_82602560:
	// lwz r5,1900(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// addi r8,r10,96
	ctx.r8.s64 = ctx.r10.s64 + 96;
	// stw r10,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r10.u32);
	// addi r7,r10,-64
	ctx.r7.s64 = ctx.r10.s64 + -64;
	// stw r5,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r5.u32);
	// stw r8,60(r11)
	PPC_STORE_U32(r11.u32 + 60, ctx.r8.u32);
	// stw r8,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r8.u32);
	// lwz r8,1900(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// stw r7,68(r11)
	PPC_STORE_U32(r11.u32 + 68, ctx.r7.u32);
	// stw r8,64(r11)
	PPC_STORE_U32(r11.u32 + 64, ctx.r8.u32);
	// b 0x82602630
	goto loc_82602630;
loc_8260258C:
	// addi r8,r10,128
	ctx.r8.s64 = ctx.r10.s64 + 128;
	// addi r7,r10,96
	ctx.r7.s64 = ctx.r10.s64 + 96;
	// addi r5,r10,64
	ctx.r5.s64 = ctx.r10.s64 + 64;
	// stw r8,76(r11)
	PPC_STORE_U32(r11.u32 + 76, ctx.r8.u32);
	// stw r8,72(r11)
	PPC_STORE_U32(r11.u32 + 72, ctx.r8.u32);
	// stw r7,84(r11)
	PPC_STORE_U32(r11.u32 + 84, ctx.r7.u32);
	// stw r7,80(r11)
	PPC_STORE_U32(r11.u32 + 80, ctx.r7.u32);
	// stw r5,92(r11)
	PPC_STORE_U32(r11.u32 + 92, ctx.r5.u32);
	// stw r5,88(r11)
	PPC_STORE_U32(r11.u32 + 88, ctx.r5.u32);
	// b 0x82602630
	goto loc_82602630;
loc_826025B4:
	// lwz r28,1904(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r6,96
	ctx.r5.s64 = ctx.r6.s64 + 96;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r8,100(r11)
	PPC_STORE_U32(r11.u32 + 100, ctx.r8.u32);
	// stw r28,96(r11)
	PPC_STORE_U32(r11.u32 + 96, r28.u32);
	// subf r5,r5,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r5.s64;
	// lwz r8,1904(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// stw r7,108(r11)
	PPC_STORE_U32(r11.u32 + 108, ctx.r7.u32);
	// stw r8,104(r11)
	PPC_STORE_U32(r11.u32 + 104, ctx.r8.u32);
	// lwz r8,1904(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// stw r5,116(r11)
	PPC_STORE_U32(r11.u32 + 116, ctx.r5.u32);
	// stw r8,112(r11)
	PPC_STORE_U32(r11.u32 + 112, ctx.r8.u32);
	// b 0x82602630
	goto loc_82602630;
loc_826025F4:
	// lwz r28,1904(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r6,96
	ctx.r5.s64 = ctx.r6.s64 + 96;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r8,124(r11)
	PPC_STORE_U32(r11.u32 + 124, ctx.r8.u32);
	// stw r28,120(r11)
	PPC_STORE_U32(r11.u32 + 120, r28.u32);
	// subf r5,r5,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r5.s64;
	// lwz r8,1904(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// stw r7,132(r11)
	PPC_STORE_U32(r11.u32 + 132, ctx.r7.u32);
	// stw r8,128(r11)
	PPC_STORE_U32(r11.u32 + 128, ctx.r8.u32);
	// lwz r8,1904(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// stw r5,140(r11)
	PPC_STORE_U32(r11.u32 + 140, ctx.r5.u32);
	// stw r8,136(r11)
	PPC_STORE_U32(r11.u32 + 136, ctx.r8.u32);
loc_82602630:
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// cmpwi cr6,r4,6
	cr6.compare<int32_t>(ctx.r4.s32, 6, xer);
	// blt cr6,0x826024a4
	if (cr6.lt) goto loc_826024A4;
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r11,r11,144
	r11.s64 = r11.s64 + 144;
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// blt cr6,0x8260249c
	if (cr6.lt) goto loc_8260249C;
loc_82602658:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmplwi cr6,r29,2
	cr6.compare<uint32_t>(r29.u32, 2, xer);
	// blt cr6,0x8260247c
	if (cr6.lt) goto loc_8260247C;
	// lwz r11,204(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// lwz r10,14788(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14788);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// stw r11,236(r3)
	PPC_STORE_U32(ctx.r3.u32 + 236, r11.u32);
	// beq cr6,0x82602690
	if (cr6.eq) goto loc_82602690;
	// lwz r11,3760(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3760);
	// lwz r10,220(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,3772(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3772, r11.u32);
loc_82602690:
	// lwz r9,3688(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3688);
	// lwz r10,140(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// lwz r11,136(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// lwz r8,1772(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,1780(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1780);
	// stw r9,3716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3716, ctx.r9.u32);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,1776(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1776, ctx.r9.u32);
	// stw r11,1784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1784, r11.u32);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826026C8"))) PPC_WEAK_FUNC(sub_826026C8);
PPC_FUNC_IMPL(__imp__sub_826026C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-18512
	ctx.r6.s64 = r11.s64 + -18512;
	// addi r4,r31,1988
	ctx.r4.s64 = r31.s64 + 1988;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-22920
	ctx.r6.s64 = r11.s64 + -22920;
	// addi r4,r31,2000
	ctx.r4.s64 = r31.s64 + 2000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,7
	ctx.r7.s64 = 7;
	// addi r6,r11,-12024
	ctx.r6.s64 = r11.s64 + -12024;
	// addi r4,r31,2116
	ctx.r4.s64 = r31.s64 + 2116;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-12544
	ctx.r6.s64 = r11.s64 + -12544;
	// addi r4,r31,2128
	ctx.r4.s64 = r31.s64 + 2128;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-14104
	ctx.r6.s64 = r11.s64 + -14104;
	// addi r4,r31,2144
	ctx.r4.s64 = r31.s64 + 2144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-13584
	ctx.r6.s64 = r11.s64 + -13584;
	// addi r4,r31,2156
	ctx.r4.s64 = r31.s64 + 2156;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-13064
	ctx.r6.s64 = r11.s64 + -13064;
	// addi r4,r31,2168
	ctx.r4.s64 = r31.s64 + 2168;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r26,r31,2280
	r26.s64 = r31.s64 + 2280;
	// addi r6,r11,-9808
	ctx.r6.s64 = r11.s64 + -9808;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,2292
	r27.s64 = r31.s64 + 2292;
	// addi r6,r11,-9544
	ctx.r6.s64 = r11.s64 + -9544;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,2304
	r29.s64 = r31.s64 + 2304;
	// addi r6,r11,-9280
	ctx.r6.s64 = r11.s64 + -9280;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,2316
	r30.s64 = r31.s64 + 2316;
	// addi r6,r11,-9016
	ctx.r6.s64 = r11.s64 + -9016;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r28,r31,2328
	r28.s64 = r31.s64 + 2328;
	// stw r26,2380(r31)
	PPC_STORE_U32(r31.u32 + 2380, r26.u32);
	// addi r6,r11,-8752
	ctx.r6.s64 = r11.s64 + -8752;
	// stw r27,2384(r31)
	PPC_STORE_U32(r31.u32 + 2384, r27.u32);
	// li r7,138
	ctx.r7.s64 = 138;
	// stw r29,2388(r31)
	PPC_STORE_U32(r31.u32 + 2388, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r30,2392(r31)
	PPC_STORE_U32(r31.u32 + 2392, r30.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,2340
	r27.s64 = r31.s64 + 2340;
	// addi r6,r11,-8456
	ctx.r6.s64 = r11.s64 + -8456;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,2352
	r29.s64 = r31.s64 + 2352;
	// addi r6,r11,-8160
	ctx.r6.s64 = r11.s64 + -8160;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,2364
	r30.s64 = r31.s64 + 2364;
	// addi r6,r11,-7864
	ctx.r6.s64 = r11.s64 + -7864;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lwz r11,21596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21596);
	// stw r28,2396(r31)
	PPC_STORE_U32(r31.u32 + 2396, r28.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r27,2400(r31)
	PPC_STORE_U32(r31.u32 + 2400, r27.u32);
	// stw r29,2404(r31)
	PPC_STORE_U32(r31.u32 + 2404, r29.u32);
	// stw r30,2408(r31)
	PPC_STORE_U32(r31.u32 + 2408, r30.u32);
	// beq cr6,0x826029ec
	if (cr6.eq) goto loc_826029EC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,21652
	r27.s64 = r31.s64 + 21652;
	// addi r6,r11,-7568
	ctx.r6.s64 = r11.s64 + -7568;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r28,r31,21664
	r28.s64 = r31.s64 + 21664;
	// addi r6,r11,-7264
	ctx.r6.s64 = r11.s64 + -7264;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,21676
	r29.s64 = r31.s64 + 21676;
	// addi r6,r11,-6960
	ctx.r6.s64 = r11.s64 + -6960;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,21688
	r30.s64 = r31.s64 + 21688;
	// addi r6,r11,-6656
	ctx.r6.s64 = r11.s64 + -6656;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// stw r27,2396(r31)
	PPC_STORE_U32(r31.u32 + 2396, r27.u32);
	// stw r28,2400(r31)
	PPC_STORE_U32(r31.u32 + 2400, r28.u32);
	// stw r29,2404(r31)
	PPC_STORE_U32(r31.u32 + 2404, r29.u32);
	// stw r30,2408(r31)
	PPC_STORE_U32(r31.u32 + 2408, r30.u32);
loc_826029EC:
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,-6352
	ctx.r6.s64 = r11.s64 + -6352;
	// addi r4,r31,21640
	ctx.r4.s64 = r31.s64 + 21640;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-6160
	ctx.r6.s64 = r11.s64 + -6160;
	// addi r4,r31,2440
	ctx.r4.s64 = r31.s64 + 2440;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-6224
	ctx.r6.s64 = r11.s64 + -6224;
	// addi r4,r31,2452
	ctx.r4.s64 = r31.s64 + 2452;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-6288
	ctx.r6.s64 = r11.s64 + -6288;
	// addi r4,r31,2464
	ctx.r4.s64 = r31.s64 + 2464;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,-6096
	ctx.r6.s64 = r11.s64 + -6096;
	// addi r4,r31,2480
	ctx.r4.s64 = r31.s64 + 2480;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,-6024
	ctx.r6.s64 = r11.s64 + -6024;
	// addi r4,r31,2492
	ctx.r4.s64 = r31.s64 + 2492;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,-5952
	ctx.r6.s64 = r11.s64 + -5952;
	// addi r4,r31,2504
	ctx.r4.s64 = r31.s64 + 2504;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-5884
	ctx.r6.s64 = r11.s64 + -5884;
	// addi r4,r31,2520
	ctx.r4.s64 = r31.s64 + 2520;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-5848
	ctx.r6.s64 = r11.s64 + -5848;
	// addi r4,r31,2532
	ctx.r4.s64 = r31.s64 + 2532;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-5812
	ctx.r6.s64 = r11.s64 + -5812;
	// addi r4,r31,2544
	ctx.r4.s64 = r31.s64 + 2544;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x826033f0
	if (!cr6.eq) goto loc_826033F0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r26,r31,20084
	r26.s64 = r31.s64 + 20084;
	// addi r6,r11,-4912
	ctx.r6.s64 = r11.s64 + -4912;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,20096
	r27.s64 = r31.s64 + 20096;
	// addi r6,r11,-4848
	ctx.r6.s64 = r11.s64 + -4848;
	// li r7,7
	ctx.r7.s64 = 7;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,20108
	r29.s64 = r31.s64 + 20108;
	// addi r6,r11,-4784
	ctx.r6.s64 = r11.s64 + -4784;
	// li r7,7
	ctx.r7.s64 = 7;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,20120
	r30.s64 = r31.s64 + 20120;
	// addi r6,r11,-4720
	ctx.r6.s64 = r11.s64 + -4720;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r28,r31,20148
	r28.s64 = r31.s64 + 20148;
	// stw r26,20068(r31)
	PPC_STORE_U32(r31.u32 + 20068, r26.u32);
	// addi r6,r11,-4656
	ctx.r6.s64 = r11.s64 + -4656;
	// stw r27,20072(r31)
	PPC_STORE_U32(r31.u32 + 20072, r27.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r29,20076(r31)
	PPC_STORE_U32(r31.u32 + 20076, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r30,20080(r31)
	PPC_STORE_U32(r31.u32 + 20080, r30.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,20160
	r27.s64 = r31.s64 + 20160;
	// addi r6,r11,-4616
	ctx.r6.s64 = r11.s64 + -4616;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,20172
	r29.s64 = r31.s64 + 20172;
	// addi r6,r11,-4576
	ctx.r6.s64 = r11.s64 + -4576;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,20184
	r30.s64 = r31.s64 + 20184;
	// addi r6,r11,-4536
	ctx.r6.s64 = r11.s64 + -4536;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r24,r31,20304
	r24.s64 = r31.s64 + 20304;
	// stw r28,20132(r31)
	PPC_STORE_U32(r31.u32 + 20132, r28.u32);
	// addi r6,r11,-4496
	ctx.r6.s64 = r11.s64 + -4496;
	// stw r27,20136(r31)
	PPC_STORE_U32(r31.u32 + 20136, r27.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r29,20140(r31)
	PPC_STORE_U32(r31.u32 + 20140, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r30,20144(r31)
	PPC_STORE_U32(r31.u32 + 20144, r30.u32);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r23,r31,20316
	r23.s64 = r31.s64 + 20316;
	// addi r6,r11,-4240
	ctx.r6.s64 = r11.s64 + -4240;
	// li r7,7
	ctx.r7.s64 = 7;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r25,r31,20328
	r25.s64 = r31.s64 + 20328;
	// addi r6,r11,-3984
	ctx.r6.s64 = r11.s64 + -3984;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r26,r31,20340
	r26.s64 = r31.s64 + 20340;
	// addi r6,r11,-3728
	ctx.r6.s64 = r11.s64 + -3728;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,20352
	r27.s64 = r31.s64 + 20352;
	// addi r6,r11,-3472
	ctx.r6.s64 = r11.s64 + -3472;
	// li r7,7
	ctx.r7.s64 = 7;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r28,r31,20364
	r28.s64 = r31.s64 + 20364;
	// addi r6,r11,-3216
	ctx.r6.s64 = r11.s64 + -3216;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,20376
	r29.s64 = r31.s64 + 20376;
	// addi r6,r11,-2960
	ctx.r6.s64 = r11.s64 + -2960;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,20388
	r30.s64 = r31.s64 + 20388;
	// addi r6,r11,-2704
	ctx.r6.s64 = r11.s64 + -2704;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r24,21016(r31)
	PPC_STORE_U32(r31.u32 + 21016, r24.u32);
	// addi r6,r11,-2448
	ctx.r6.s64 = r11.s64 + -2448;
	// stw r23,21020(r31)
	PPC_STORE_U32(r31.u32 + 21020, r23.u32);
	// addi r4,r31,20400
	ctx.r4.s64 = r31.s64 + 20400;
	// stw r25,21024(r31)
	PPC_STORE_U32(r31.u32 + 21024, r25.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r26,21028(r31)
	PPC_STORE_U32(r31.u32 + 21028, r26.u32);
	// stw r27,21032(r31)
	PPC_STORE_U32(r31.u32 + 21032, r27.u32);
	// stw r28,21036(r31)
	PPC_STORE_U32(r31.u32 + 21036, r28.u32);
	// stw r29,21040(r31)
	PPC_STORE_U32(r31.u32 + 21040, r29.u32);
	// stw r30,21044(r31)
	PPC_STORE_U32(r31.u32 + 21044, r30.u32);
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,-1936
	ctx.r6.s64 = r11.s64 + -1936;
	// addi r4,r31,20412
	ctx.r4.s64 = r31.s64 + 20412;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,-1424
	ctx.r6.s64 = r11.s64 + -1424;
	// addi r4,r31,20424
	ctx.r4.s64 = r31.s64 + 20424;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,-912
	ctx.r6.s64 = r11.s64 + -912;
	// addi r4,r31,20436
	ctx.r4.s64 = r31.s64 + 20436;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,7
	ctx.r7.s64 = 7;
	// addi r6,r11,-400
	ctx.r6.s64 = r11.s64 + -400;
	// addi r4,r31,20448
	ctx.r4.s64 = r31.s64 + 20448;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,112
	ctx.r6.s64 = r11.s64 + 112;
	// addi r4,r31,20460
	ctx.r4.s64 = r31.s64 + 20460;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,624
	ctx.r6.s64 = r11.s64 + 624;
	// addi r4,r31,20472
	ctx.r4.s64 = r31.s64 + 20472;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,1136
	ctx.r6.s64 = r11.s64 + 1136;
	// addi r4,r31,20484
	ctx.r4.s64 = r31.s64 + 20484;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,1648
	ctx.r6.s64 = r11.s64 + 1648;
	// addi r4,r31,20496
	ctx.r4.s64 = r31.s64 + 20496;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,1944
	ctx.r6.s64 = r11.s64 + 1944;
	// addi r4,r31,20508
	ctx.r4.s64 = r31.s64 + 20508;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,2240
	ctx.r6.s64 = r11.s64 + 2240;
	// addi r4,r31,20520
	ctx.r4.s64 = r31.s64 + 20520;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,2536
	ctx.r6.s64 = r11.s64 + 2536;
	// addi r4,r31,20532
	ctx.r4.s64 = r31.s64 + 20532;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r22,r31,20544
	r22.s64 = r31.s64 + 20544;
	// addi r6,r11,-5708
	ctx.r6.s64 = r11.s64 + -5708;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r23,r31,20556
	r23.s64 = r31.s64 + 20556;
	// addi r6,r11,-5612
	ctx.r6.s64 = r11.s64 + -5612;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r24,r31,20568
	r24.s64 = r31.s64 + 20568;
	// addi r6,r11,-5516
	ctx.r6.s64 = r11.s64 + -5516;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r26,r31,20580
	r26.s64 = r31.s64 + 20580;
	// addi r6,r11,-5420
	ctx.r6.s64 = r11.s64 + -5420;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,20592
	r27.s64 = r31.s64 + 20592;
	// addi r6,r11,-5392
	ctx.r6.s64 = r11.s64 + -5392;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r28,r31,20604
	r28.s64 = r31.s64 + 20604;
	// addi r6,r11,-5364
	ctx.r6.s64 = r11.s64 + -5364;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,20616
	r29.s64 = r31.s64 + 20616;
	// addi r6,r11,-5336
	ctx.r6.s64 = r11.s64 + -5336;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,20628
	r30.s64 = r31.s64 + 20628;
	// addi r6,r11,-5308
	ctx.r6.s64 = r11.s64 + -5308;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r25,r31,20640
	r25.s64 = r31.s64 + 20640;
	// stw r22,20232(r31)
	PPC_STORE_U32(r31.u32 + 20232, r22.u32);
	// addi r6,r11,-5280
	ctx.r6.s64 = r11.s64 + -5280;
	// stw r23,20236(r31)
	PPC_STORE_U32(r31.u32 + 20236, r23.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r24,20240(r31)
	PPC_STORE_U32(r31.u32 + 20240, r24.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r26,20244(r31)
	PPC_STORE_U32(r31.u32 + 20244, r26.u32);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// stw r27,20248(r31)
	PPC_STORE_U32(r31.u32 + 20248, r27.u32);
	// stw r28,20252(r31)
	PPC_STORE_U32(r31.u32 + 20252, r28.u32);
	// stw r29,20256(r31)
	PPC_STORE_U32(r31.u32 + 20256, r29.u32);
	// stw r30,20260(r31)
	PPC_STORE_U32(r31.u32 + 20260, r30.u32);
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r22,r31,20652
	r22.s64 = r31.s64 + 20652;
	// addi r6,r11,-5244
	ctx.r6.s64 = r11.s64 + -5244;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r23,r31,20664
	r23.s64 = r31.s64 + 20664;
	// addi r6,r11,-5208
	ctx.r6.s64 = r11.s64 + -5208;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r24,r31,20676
	r24.s64 = r31.s64 + 20676;
	// addi r6,r11,-5172
	ctx.r6.s64 = r11.s64 + -5172;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r26,r31,20688
	r26.s64 = r31.s64 + 20688;
	// addi r6,r11,-5136
	ctx.r6.s64 = r11.s64 + -5136;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,20700
	r27.s64 = r31.s64 + 20700;
	// addi r6,r11,-5100
	ctx.r6.s64 = r11.s64 + -5100;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r28,r31,20712
	r28.s64 = r31.s64 + 20712;
	// addi r6,r11,-5064
	ctx.r6.s64 = r11.s64 + -5064;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,20724
	r29.s64 = r31.s64 + 20724;
	// addi r6,r11,-5028
	ctx.r6.s64 = r11.s64 + -5028;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,20736
	r30.s64 = r31.s64 + 20736;
	// stw r25,20200(r31)
	PPC_STORE_U32(r31.u32 + 20200, r25.u32);
	// addi r6,r11,-5776
	ctx.r6.s64 = r11.s64 + -5776;
	// stw r22,20204(r31)
	PPC_STORE_U32(r31.u32 + 20204, r22.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r23,20208(r31)
	PPC_STORE_U32(r31.u32 + 20208, r23.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r24,20212(r31)
	PPC_STORE_U32(r31.u32 + 20212, r24.u32);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r26,20216(r31)
	PPC_STORE_U32(r31.u32 + 20216, r26.u32);
	// stw r27,20220(r31)
	PPC_STORE_U32(r31.u32 + 20220, r27.u32);
	// stw r28,20224(r31)
	PPC_STORE_U32(r31.u32 + 20224, r28.u32);
	// stw r29,20228(r31)
	PPC_STORE_U32(r31.u32 + 20228, r29.u32);
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r26,r31,20748
	r26.s64 = r31.s64 + 20748;
	// addi r6,r11,-5680
	ctx.r6.s64 = r11.s64 + -5680;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r28,r31,20760
	r28.s64 = r31.s64 + 20760;
	// addi r6,r11,-5584
	ctx.r6.s64 = r11.s64 + -5584;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,20772
	r29.s64 = r31.s64 + 20772;
	// addi r6,r11,-5488
	ctx.r6.s64 = r11.s64 + -5488;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,20784
	r27.s64 = r31.s64 + 20784;
	// stw r30,20268(r31)
	PPC_STORE_U32(r31.u32 + 20268, r30.u32);
	// addi r6,r11,-4992
	ctx.r6.s64 = r11.s64 + -4992;
	// stw r26,20272(r31)
	PPC_STORE_U32(r31.u32 + 20272, r26.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r28,20276(r31)
	PPC_STORE_U32(r31.u32 + 20276, r28.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r29,20280(r31)
	PPC_STORE_U32(r31.u32 + 20280, r29.u32);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r28,r31,20796
	r28.s64 = r31.s64 + 20796;
	// addi r6,r11,-4972
	ctx.r6.s64 = r11.s64 + -4972;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,20808
	r29.s64 = r31.s64 + 20808;
	// addi r6,r11,-4952
	ctx.r6.s64 = r11.s64 + -4952;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,20820
	r30.s64 = r31.s64 + 20820;
	// addi r6,r11,-4932
	ctx.r6.s64 = r11.s64 + -4932;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// stw r27,20288(r31)
	PPC_STORE_U32(r31.u32 + 20288, r27.u32);
	// stw r28,20292(r31)
	PPC_STORE_U32(r31.u32 + 20292, r28.u32);
	// stw r29,20296(r31)
	PPC_STORE_U32(r31.u32 + 20296, r29.u32);
	// stw r30,20300(r31)
	PPC_STORE_U32(r31.u32 + 20300, r30.u32);
loc_826033F0:
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-11760
	ctx.r6.s64 = r11.s64 + -11760;
	// addi r4,r31,2040
	ctx.r4.s64 = r31.s64 + 2040;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-11272
	ctx.r6.s64 = r11.s64 + -11272;
	// addi r4,r31,2052
	ctx.r4.s64 = r31.s64 + 2052;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,-10784
	ctx.r6.s64 = r11.s64 + -10784;
	// addi r4,r31,2064
	ctx.r4.s64 = r31.s64 + 2064;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,7
	ctx.r7.s64 = 7;
	// addi r6,r11,-10296
	ctx.r6.s64 = r11.s64 + -10296;
	// addi r4,r31,2076
	ctx.r4.s64 = r31.s64 + 2076;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-27680
	ctx.r6.s64 = r11.s64 + -27680;
	// addi r4,r31,2180
	ctx.r4.s64 = r31.s64 + 2180;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-27000
	ctx.r6.s64 = r11.s64 + -27000;
	// addi r4,r31,2192
	ctx.r4.s64 = r31.s64 + 2192;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-26248
	ctx.r6.s64 = r11.s64 + -26248;
	// addi r4,r31,2204
	ctx.r4.s64 = r31.s64 + 2204;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-25648
	ctx.r6.s64 = r11.s64 + -25648;
	// addi r4,r31,2216
	ctx.r4.s64 = r31.s64 + 2216;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-25112
	ctx.r6.s64 = r11.s64 + -25112;
	// addi r4,r31,2228
	ctx.r4.s64 = r31.s64 + 2228;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-24696
	ctx.r6.s64 = r11.s64 + -24696;
	// addi r4,r31,2240
	ctx.r4.s64 = r31.s64 + 2240;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-24280
	ctx.r6.s64 = r11.s64 + -24280;
	// addi r4,r31,2428
	ctx.r4.s64 = r31.s64 + 2428;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826035a0
	if (!cr6.eq) goto loc_826035A0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-23576
	ctx.r6.s64 = r11.s64 + -23576;
	// addi r4,r31,2252
	ctx.r4.s64 = r31.s64 + 2252;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826035ac
	if (cr6.eq) goto loc_826035AC;
loc_826035A0:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_826035AC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_826035B8"))) PPC_WEAK_FUNC(sub_826035B8);
PPC_FUNC_IMPL(__imp__sub_826035B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,-1
	ctx.r9.s64 = -1;
	// stw r11,20956(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20956, r11.u32);
	// stw r11,3428(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3428, r11.u32);
	// stw r11,3440(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3440, r11.u32);
	// stw r11,3436(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3436, r11.u32);
	// stw r11,20840(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20840, r11.u32);
	// stw r11,21164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21164, r11.u32);
	// stw r10,20972(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20972, ctx.r10.u32);
	// stw r11,3448(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3448, r11.u32);
	// stw r9,20872(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20872, ctx.r9.u32);
	// stw r11,404(r3)
	PPC_STORE_U32(ctx.r3.u32 + 404, r11.u32);
	// stw r11,20960(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20960, r11.u32);
	// stw r11,20968(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20968, r11.u32);
	// stw r11,20964(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20964, r11.u32);
	// stw r11,3964(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3964, r11.u32);
	// stw r11,21080(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21080, r11.u32);
	// stw r11,336(r3)
	PPC_STORE_U32(ctx.r3.u32 + 336, r11.u32);
	// stw r11,328(r3)
	PPC_STORE_U32(ctx.r3.u32 + 328, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260360C"))) PPC_WEAK_FUNC(sub_8260360C);
PPC_FUNC_IMPL(__imp__sub_8260360C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82603610"))) PPC_WEAK_FUNC(sub_82603610);
PPC_FUNC_IMPL(__imp__sub_82603610) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// li r11,1
	r11.s64 = 1;
	// li r10,-1
	ctx.r10.s64 = -1;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r30,20956(r31)
	PPC_STORE_U32(r31.u32 + 20956, r30.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r30,3428(r31)
	PPC_STORE_U32(r31.u32 + 3428, r30.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r30,3440(r31)
	PPC_STORE_U32(r31.u32 + 3440, r30.u32);
	// stw r30,3436(r31)
	PPC_STORE_U32(r31.u32 + 3436, r30.u32);
	// stw r30,20840(r31)
	PPC_STORE_U32(r31.u32 + 20840, r30.u32);
	// stw r30,21164(r31)
	PPC_STORE_U32(r31.u32 + 21164, r30.u32);
	// stw r11,20972(r31)
	PPC_STORE_U32(r31.u32 + 20972, r11.u32);
	// stw r30,3448(r31)
	PPC_STORE_U32(r31.u32 + 3448, r30.u32);
	// stw r10,20872(r31)
	PPC_STORE_U32(r31.u32 + 20872, ctx.r10.u32);
	// stw r30,404(r31)
	PPC_STORE_U32(r31.u32 + 404, r30.u32);
	// stw r30,20960(r31)
	PPC_STORE_U32(r31.u32 + 20960, r30.u32);
	// stw r30,20968(r31)
	PPC_STORE_U32(r31.u32 + 20968, r30.u32);
	// stw r30,20964(r31)
	PPC_STORE_U32(r31.u32 + 20964, r30.u32);
	// stw r30,3964(r31)
	PPC_STORE_U32(r31.u32 + 3964, r30.u32);
	// stw r30,21080(r31)
	PPC_STORE_U32(r31.u32 + 21080, r30.u32);
	// stw r30,336(r31)
	PPC_STORE_U32(r31.u32 + 336, r30.u32);
	// stw r30,328(r31)
	PPC_STORE_U32(r31.u32 + 328, r30.u32);
	// stw r30,3672(r31)
	PPC_STORE_U32(r31.u32 + 3672, r30.u32);
	// stw r30,3668(r31)
	PPC_STORE_U32(r31.u32 + 3668, r30.u32);
	// bl 0x825eee78
	sub_825EEE78(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826036e4
	if (!cr6.eq) goto loc_826036E4;
	// lwz r10,21356(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21356);
	// lwz r11,21352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21352);
	// lwz r9,21176(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21176);
	// mullw r8,r11,r10
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stw r30,3676(r31)
	PPC_STORE_U32(r31.u32 + 3676, r30.u32);
	// stw r30,21236(r31)
	PPC_STORE_U32(r31.u32 + 21236, r30.u32);
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// bgt cr6,0x826036e0
	if (cr6.gt) goto loc_826036E0;
	// lwz r9,21192(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21192);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bgt cr6,0x826036e0
	if (cr6.gt) goto loc_826036E0;
	// lwz r11,21196(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21196);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bgt cr6,0x826036e0
	if (cr6.gt) goto loc_826036E0;
	// stw r30,21180(r31)
	PPC_STORE_U32(r31.u32 + 21180, r30.u32);
	// b 0x826036e4
	goto loc_826036E4;
loc_826036E0:
	// li r3,14
	ctx.r3.s64 = 14;
loc_826036E4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826036FC"))) PPC_WEAK_FUNC(sub_826036FC);
PPC_FUNC_IMPL(__imp__sub_826036FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82603700"))) PPC_WEAK_FUNC(sub_82603700);
PPC_FUNC_IMPL(__imp__sub_82603700) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,19976(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82603778
	if (cr6.eq) goto loc_82603778;
	// lwz r11,19980(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82603764
	if (cr6.eq) goto loc_82603764;
	// addi r7,r3,20448
	ctx.r7.s64 = ctx.r3.s64 + 20448;
	// lwz r11,20996(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20996);
	// addi r6,r3,20460
	ctx.r6.s64 = ctx.r3.s64 + 20460;
	// addi r5,r3,20472
	ctx.r5.s64 = ctx.r3.s64 + 20472;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r7,2412(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2412, ctx.r7.u32);
	// stw r6,2416(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2416, ctx.r6.u32);
	// stw r5,2420(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2420, ctx.r5.u32);
	// beq cr6,0x82603750
	if (cr6.eq) goto loc_82603750;
	// addi r11,r3,20400
	r11.s64 = ctx.r3.s64 + 20400;
	// addi r10,r3,20412
	ctx.r10.s64 = ctx.r3.s64 + 20412;
	// addi r9,r3,20424
	ctx.r9.s64 = ctx.r3.s64 + 20424;
	// addi r8,r3,20436
	ctx.r8.s64 = ctx.r3.s64 + 20436;
	// b 0x826037c4
	goto loc_826037C4;
loc_82603750:
	// addi r11,r3,20496
	r11.s64 = ctx.r3.s64 + 20496;
	// addi r10,r3,20508
	ctx.r10.s64 = ctx.r3.s64 + 20508;
	// addi r9,r3,20520
	ctx.r9.s64 = ctx.r3.s64 + 20520;
	// addi r8,r3,20532
	ctx.r8.s64 = ctx.r3.s64 + 20532;
	// b 0x826037c4
	goto loc_826037C4;
loc_82603764:
	// addi r11,r3,20496
	r11.s64 = ctx.r3.s64 + 20496;
	// addi r10,r3,20508
	ctx.r10.s64 = ctx.r3.s64 + 20508;
	// addi r9,r3,20520
	ctx.r9.s64 = ctx.r3.s64 + 20520;
	// addi r8,r3,20532
	ctx.r8.s64 = ctx.r3.s64 + 20532;
	// b 0x826037ac
	goto loc_826037AC;
loc_82603778:
	// lwz r11,21596(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21596);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260379c
	if (cr6.eq) goto loc_8260379C;
	// addi r10,r3,21664
	ctx.r10.s64 = ctx.r3.s64 + 21664;
	// addi r11,r3,21652
	r11.s64 = ctx.r3.s64 + 21652;
	// addi r9,r3,21676
	ctx.r9.s64 = ctx.r3.s64 + 21676;
	// addi r8,r3,21688
	ctx.r8.s64 = ctx.r3.s64 + 21688;
	// stw r10,2400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2400, ctx.r10.u32);
	// b 0x826037d0
	goto loc_826037D0;
loc_8260379C:
	// addi r11,r3,2328
	r11.s64 = ctx.r3.s64 + 2328;
	// addi r10,r3,2340
	ctx.r10.s64 = ctx.r3.s64 + 2340;
	// addi r9,r3,2352
	ctx.r9.s64 = ctx.r3.s64 + 2352;
	// addi r8,r3,2364
	ctx.r8.s64 = ctx.r3.s64 + 2364;
loc_826037AC:
	// addi r5,r3,20472
	ctx.r5.s64 = ctx.r3.s64 + 20472;
	// addi r6,r3,20460
	ctx.r6.s64 = ctx.r3.s64 + 20460;
	// addi r7,r3,20448
	ctx.r7.s64 = ctx.r3.s64 + 20448;
	// stw r5,2420(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2420, ctx.r5.u32);
	// stw r6,2416(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2416, ctx.r6.u32);
	// stw r7,2412(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2412, ctx.r7.u32);
loc_826037C4:
	// stw r10,2400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2400, ctx.r10.u32);
	// addi r10,r3,20484
	ctx.r10.s64 = ctx.r3.s64 + 20484;
	// stw r10,2424(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2424, ctx.r10.u32);
loc_826037D0:
	// stw r11,2396(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2396, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r9,2404(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2404, ctx.r9.u32);
	// stw r8,2408(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2408, ctx.r8.u32);
	// stw r11,2376(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2376, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826037E8"))) PPC_WEAK_FUNC(sub_826037E8);
PPC_FUNC_IMPL(__imp__sub_826037E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r8,16
	ctx.r8.s64 = 16;
	// li r10,32
	ctx.r10.s64 = 32;
	// lwz r9,180(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// stw r8,19700(r31)
	PPC_STORE_U32(r31.u32 + 19700, ctx.r8.u32);
	// lwz r7,192(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// lwz r5,200(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// lwz r8,188(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// lwz r11,20056(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20056);
	// stw r10,19696(r31)
	PPC_STORE_U32(r31.u32 + 19696, ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r9,164(r31)
	PPC_STORE_U32(r31.u32 + 164, ctx.r9.u32);
	// stw r7,168(r31)
	PPC_STORE_U32(r31.u32 + 168, ctx.r7.u32);
	// stw r5,176(r31)
	PPC_STORE_U32(r31.u32 + 176, ctx.r5.u32);
	// stw r8,172(r31)
	PPC_STORE_U32(r31.u32 + 172, ctx.r8.u32);
	// beq cr6,0x82603864
	if (cr6.eq) goto loc_82603864;
	// lwz r10,156(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// lwz r11,160(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r6,r11,1
	ctx.r6.s64 = r11.s64 + 1;
	// srawi r11,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r11.s64 = ctx.r10.s32 >> 1;
	// srawi r10,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 1;
	// rlwinm r6,r11,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r10,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,168(r31)
	PPC_STORE_U32(r31.u32 + 168, r11.u32);
	// stw r10,176(r31)
	PPC_STORE_U32(r31.u32 + 176, ctx.r10.u32);
	// stw r6,164(r31)
	PPC_STORE_U32(r31.u32 + 164, ctx.r6.u32);
	// stw r4,172(r31)
	PPC_STORE_U32(r31.u32 + 172, ctx.r4.u32);
loc_82603864:
	// lwz r10,164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// lwz r11,168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 168);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// lwz r6,156(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmpw cr6,r9,r6
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, xer);
	// stw r10,184(r31)
	PPC_STORE_U32(r31.u32 + 184, ctx.r10.u32);
	// stw r11,196(r31)
	PPC_STORE_U32(r31.u32 + 196, r11.u32);
	// bne cr6,0x82603898
	if (!cr6.eq) goto loc_82603898;
	// lwz r11,160(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// li r30,1
	r30.s64 = 1;
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// beq cr6,0x8260389c
	if (cr6.eq) goto loc_8260389C;
loc_82603898:
	// li r30,0
	r30.s64 = 0;
loc_8260389C:
	// addi r11,r9,64
	r11.s64 = ctx.r9.s64 + 64;
	// lwz r29,21184(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 21184);
	// srawi r9,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// stw r30,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r30.u32);
	// addi r10,r7,32
	ctx.r10.s64 = ctx.r7.s64 + 32;
	// lwz r3,3732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// srawi r7,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 4;
	// lwz r4,3760(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3760);
	// cmpwi cr6,r29,1
	cr6.compare<int32_t>(r29.s32, 1, xer);
	// mullw r29,r7,r9
	r29.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// stw r11,204(r31)
	PPC_STORE_U32(r31.u32 + 204, r11.u32);
	// stw r9,136(r31)
	PPC_STORE_U32(r31.u32 + 136, ctx.r9.u32);
	// stw r10,208(r31)
	PPC_STORE_U32(r31.u32 + 208, ctx.r10.u32);
	// stw r7,140(r31)
	PPC_STORE_U32(r31.u32 + 140, ctx.r7.u32);
	// stw r29,144(r31)
	PPC_STORE_U32(r31.u32 + 144, r29.u32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r6,r8,64
	ctx.r6.s64 = ctx.r8.s64 + 64;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// addi r30,r10,1
	r30.s64 = ctx.r10.s64 + 1;
	// rlwinm r8,r8,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// stw r9,148(r31)
	PPC_STORE_U32(r31.u32 + 148, ctx.r9.u32);
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r30,r30,4,0,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r6,212(r31)
	PPC_STORE_U32(r31.u32 + 212, ctx.r6.u32);
	// rlwinm r29,r11,4,0,27
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// stw r8,220(r31)
	PPC_STORE_U32(r31.u32 + 220, ctx.r8.u32);
	// stw r5,216(r31)
	PPC_STORE_U32(r31.u32 + 216, ctx.r5.u32);
	// stw r9,232(r31)
	PPC_STORE_U32(r31.u32 + 232, ctx.r9.u32);
	// add r9,r3,r8
	ctx.r9.u64 = ctx.r3.u64 + ctx.r8.u64;
	// stw r30,224(r31)
	PPC_STORE_U32(r31.u32 + 224, r30.u32);
	// stw r29,228(r31)
	PPC_STORE_U32(r31.u32 + 228, r29.u32);
	// stw r4,3772(r31)
	PPC_STORE_U32(r31.u32 + 3772, ctx.r4.u32);
	// stw r9,3756(r31)
	PPC_STORE_U32(r31.u32 + 3756, ctx.r9.u32);
	// bne cr6,0x82603944
	if (!cr6.eq) goto loc_82603944;
	// lwz r9,14772(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82603944
	if (!cr6.gt) goto loc_82603944;
	// ld r9,3576(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 3576);
	// cmpdi cr6,r9,1
	cr6.compare<int64_t>(ctx.r9.s64, 1, xer);
	// bgt cr6,0x82603964
	if (cr6.gt) goto loc_82603964;
loc_82603944:
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
	// rlwinm r8,r10,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r10.u32);
	// stw r6,104(r31)
	PPC_STORE_U32(r31.u32 + 104, ctx.r6.u32);
	// stw r5,116(r31)
	PPC_STORE_U32(r31.u32 + 116, ctx.r5.u32);
	// stw r9,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r9.u32);
	// stw r8,112(r31)
	PPC_STORE_U32(r31.u32 + 112, ctx.r8.u32);
loc_82603964:
	// lwz r11,3356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bne cr6,0x8260398c
	if (!cr6.eq) goto loc_8260398C;
	// cmplwi cr6,r7,4
	cr6.compare<uint32_t>(ctx.r7.u32, 4, xer);
	// bge cr6,0x826039a4
	if (!cr6.lt) goto loc_826039A4;
	// li r11,2
	r11.s64 = 2;
	// subfc r11,r11,r7
	xer.ca = ctx.r7.u32 >= r11.u32;
	r11.s64 = ctx.r7.s64 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// b 0x826039a0
	goto loc_826039A0;
loc_8260398C:
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x826039a4
	if (!cr6.eq) goto loc_826039A4;
	// cmplwi cr6,r7,1
	cr6.compare<uint32_t>(ctx.r7.u32, 1, xer);
	// bne cr6,0x826039a4
	if (!cr6.eq) goto loc_826039A4;
	// li r11,1
	r11.s64 = 1;
loc_826039A0:
	// stw r11,3356(r31)
	PPC_STORE_U32(r31.u32 + 3356, r11.u32);
loc_826039A4:
	// li r11,3
	r11.s64 = 3;
	// li r10,10
	ctx.r10.s64 = 10;
	// li r9,64
	ctx.r9.s64 = 64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,2264(r31)
	PPC_STORE_U32(r31.u32 + 2264, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r10,2268(r31)
	PPC_STORE_U32(r31.u32 + 2268, ctx.r10.u32);
	// stw r9,2272(r31)
	PPC_STORE_U32(r31.u32 + 2272, ctx.r9.u32);
	// stw r11,2276(r31)
	PPC_STORE_U32(r31.u32 + 2276, r11.u32);
	// bl 0x82607f40
	sub_82607F40(ctx, base);
	// lwz r11,188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,21276(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21276);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r10,21276(r31)
	PPC_STORE_U32(r31.u32 + 21276, ctx.r10.u32);
	// bge cr6,0x82603a30
	if (!cr6.lt) goto loc_82603A30;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// rlwinm r30,r11,2,0,29
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,21240(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// rlwinm r29,r10,2,0,29
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,21252(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21252);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,21268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_82603A30:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82603A38"))) PPC_WEAK_FUNC(sub_82603A38);
PPC_FUNC_IMPL(__imp__sub_82603A38) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r6,12
	ctx.r6.s64 = 12;
	// stw r4,404(r3)
	PPC_STORE_U32(ctx.r3.u32 + 404, ctx.r4.u32);
	// li r10,10
	ctx.r10.s64 = 10;
	// li r9,9
	ctx.r9.s64 = 9;
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,-32
	ctx.r8.s64 = ctx.r1.s64 + -32;
	// stw r6,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, ctx.r6.u32);
	// li r6,13
	ctx.r6.s64 = 13;
	// stw r10,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r10.u32);
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// stw r10,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, ctx.r10.u32);
	// li r10,11
	ctx.r10.s64 = 11;
	// stw r9,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, ctx.r9.u32);
	// stw r9,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r9.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r6,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, ctx.r6.u32);
	// li r6,8
	ctx.r6.s64 = 8;
	// stw r10,-4(r1)
	PPC_STORE_U32(ctx.r1.u32 + -4, ctx.r10.u32);
	// lwzx r10,r11,r8
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// stw r6,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r6.u32);
	// addi r8,r10,-1
	ctx.r8.s64 = ctx.r10.s64 + -1;
	// lwzx r11,r11,r7
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// stw r10,408(r3)
	PPC_STORE_U32(ctx.r3.u32 + 408, ctx.r10.u32);
	// stw r11,412(r3)
	PPC_STORE_U32(ctx.r3.u32 + 412, r11.u32);
	// slw r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// slw r10,r9,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r7.u8 & 0x3F));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,416(r3)
	PPC_STORE_U32(ctx.r3.u32 + 416, r11.u32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r10,420(r3)
	PPC_STORE_U32(ctx.r3.u32 + 420, ctx.r10.u32);
	// stw r8,424(r3)
	PPC_STORE_U32(ctx.r3.u32 + 424, ctx.r8.u32);
	// stw r9,428(r3)
	PPC_STORE_U32(ctx.r3.u32 + 428, ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82603AC8"))) PPC_WEAK_FUNC(sub_82603AC8);
PPC_FUNC_IMPL(__imp__sub_82603AC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r6,-32157
	ctx.r6.s64 = -2107441152;
	// lwz r5,3924(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3924);
	// lis r7,-32157
	ctx.r7.s64 = -2107441152;
	// lis r8,-32157
	ctx.r8.s64 = -2107441152;
	// lis r9,-32157
	ctx.r9.s64 = -2107441152;
	// lis r10,-32157
	ctx.r10.s64 = -2107441152;
	// lis r11,-32157
	r11.s64 = -2107441152;
	// addi r6,r6,3424
	ctx.r6.s64 = ctx.r6.s64 + 3424;
	// addi r7,r7,5376
	ctx.r7.s64 = ctx.r7.s64 + 5376;
	// addi r8,r8,4432
	ctx.r8.s64 = ctx.r8.s64 + 4432;
	// addi r9,r9,6240
	ctx.r9.s64 = ctx.r9.s64 + 6240;
	// addi r10,r10,6760
	ctx.r10.s64 = ctx.r10.s64 + 6760;
	// addi r11,r11,8544
	r11.s64 = r11.s64 + 8544;
	// stw r6,3160(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3160, ctx.r6.u32);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// stw r7,3164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3164, ctx.r7.u32);
	// stw r8,3180(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3180, ctx.r8.u32);
	// stw r9,3168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3168, ctx.r9.u32);
	// stw r10,3172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3172, ctx.r10.u32);
	// stw r11,3176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3176, r11.u32);
	// beq cr6,0x82603b40
	if (cr6.eq) goto loc_82603B40;
	// lis r9,-32157
	ctx.r9.s64 = -2107441152;
	// lis r10,-32157
	ctx.r10.s64 = -2107441152;
	// lis r11,-32157
	r11.s64 = -2107441152;
	// addi r9,r9,4432
	ctx.r9.s64 = ctx.r9.s64 + 4432;
	// addi r10,r10,7344
	ctx.r10.s64 = ctx.r10.s64 + 7344;
	// addi r11,r11,7912
	r11.s64 = r11.s64 + 7912;
	// stw r9,3164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3164, ctx.r9.u32);
	// stw r10,3168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3168, ctx.r10.u32);
	// stw r11,3172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3172, r11.u32);
loc_82603B40:
	// lwz r11,1788(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82603b9c
	if (cr6.eq) goto loc_82603B9C;
	// lis r6,-32155
	ctx.r6.s64 = -2107310080;
	// lis r7,-32155
	ctx.r7.s64 = -2107310080;
	// lis r8,-32155
	ctx.r8.s64 = -2107310080;
	// lis r9,-32155
	ctx.r9.s64 = -2107310080;
	// lis r10,-32155
	ctx.r10.s64 = -2107310080;
	// lis r11,-32155
	r11.s64 = -2107310080;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r6,r6,25504
	ctx.r6.s64 = ctx.r6.s64 + 25504;
	// addi r7,r7,24536
	ctx.r7.s64 = ctx.r7.s64 + 24536;
	// addi r8,r8,26688
	ctx.r8.s64 = ctx.r8.s64 + 26688;
	// addi r9,r9,27656
	ctx.r9.s64 = ctx.r9.s64 + 27656;
	// addi r10,r10,28616
	ctx.r10.s64 = ctx.r10.s64 + 28616;
	// stw r5,1796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1796, ctx.r5.u32);
	// addi r11,r11,24536
	r11.s64 = r11.s64 + 24536;
	// stw r6,3160(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3160, ctx.r6.u32);
	// stw r7,3164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3164, ctx.r7.u32);
	// stw r8,3168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3168, ctx.r8.u32);
	// stw r9,3172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3172, ctx.r9.u32);
	// stw r10,3176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3176, ctx.r10.u32);
	// stw r11,3180(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3180, r11.u32);
loc_82603B9C:
	// lwz r11,1796(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1796);
	// li r10,8
	ctx.r10.s64 = 8;
	// li r9,3
	ctx.r9.s64 = 3;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// beq cr6,0x82603bc8
	if (cr6.eq) goto loc_82603BC8;
	// stw r11,1916(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1916, r11.u32);
	// stw r10,1920(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1920, ctx.r10.u32);
	// stw r11,1924(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1924, r11.u32);
	// stw r9,1928(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1928, ctx.r9.u32);
	// blr 
	return;
loc_82603BC8:
	// stw r10,1916(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1916, ctx.r10.u32);
	// stw r11,1920(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1920, r11.u32);
	// stw r9,1924(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1924, ctx.r9.u32);
	// stw r11,1928(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1928, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82603BDC"))) PPC_WEAK_FUNC(sub_82603BDC);
PPC_FUNC_IMPL(__imp__sub_82603BDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82603BE0"))) PPC_WEAK_FUNC(sub_82603BE0);
PPC_FUNC_IMPL(__imp__sub_82603BE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82603ac8
	sub_82603AC8(ctx, base);
	// lwz r11,1828(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1828);
	// lwz r10,1836(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1836);
	// lwz r9,1840(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1840);
	// lwz r8,1864(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1864);
	// lwz r7,1788(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1788);
	// stw r11,1832(r31)
	PPC_STORE_U32(r31.u32 + 1832, r11.u32);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r10,1852(r31)
	PPC_STORE_U32(r31.u32 + 1852, ctx.r10.u32);
	// stw r9,1856(r31)
	PPC_STORE_U32(r31.u32 + 1856, ctx.r9.u32);
	// stw r8,1860(r31)
	PPC_STORE_U32(r31.u32 + 1860, ctx.r8.u32);
	// beq cr6,0x82603c50
	if (cr6.eq) goto loc_82603C50;
	// lwz r11,1824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1824);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,1844(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1844);
	// lwz r8,1848(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1848);
	// lwz r7,1868(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1868);
	// stw r11,1832(r31)
	PPC_STORE_U32(r31.u32 + 1832, r11.u32);
	// stw r9,1796(r31)
	PPC_STORE_U32(r31.u32 + 1796, ctx.r9.u32);
	// stw r10,1852(r31)
	PPC_STORE_U32(r31.u32 + 1852, ctx.r10.u32);
	// stw r8,1856(r31)
	PPC_STORE_U32(r31.u32 + 1856, ctx.r8.u32);
	// stw r7,1860(r31)
	PPC_STORE_U32(r31.u32 + 1860, ctx.r7.u32);
loc_82603C50:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,20056(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 20056);
	// bl 0x8261c320
	sub_8261C320(ctx, base);
	// lwz r11,1796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1796);
	// li r10,8
	ctx.r10.s64 = 8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// li r9,3
	ctx.r9.s64 = 3;
	// beq cr6,0x82603c88
	if (cr6.eq) goto loc_82603C88;
	// stw r11,1916(r31)
	PPC_STORE_U32(r31.u32 + 1916, r11.u32);
	// stw r10,1920(r31)
	PPC_STORE_U32(r31.u32 + 1920, ctx.r10.u32);
	// stw r11,1924(r31)
	PPC_STORE_U32(r31.u32 + 1924, r11.u32);
	// stw r9,1928(r31)
	PPC_STORE_U32(r31.u32 + 1928, ctx.r9.u32);
	// b 0x82603c98
	goto loc_82603C98;
loc_82603C88:
	// stw r10,1916(r31)
	PPC_STORE_U32(r31.u32 + 1916, ctx.r10.u32);
	// stw r11,1920(r31)
	PPC_STORE_U32(r31.u32 + 1920, r11.u32);
	// stw r9,1924(r31)
	PPC_STORE_U32(r31.u32 + 1924, ctx.r9.u32);
	// stw r11,1928(r31)
	PPC_STORE_U32(r31.u32 + 1928, r11.u32);
loc_82603C98:
	// bl 0x82617978
	sub_82617978(ctx, base);
	// lis r5,-32154
	ctx.r5.s64 = -2107244544;
	// lis r6,-32155
	ctx.r6.s64 = -2107310080;
	// lwz r4,3956(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3956);
	// lis r7,-32154
	ctx.r7.s64 = -2107244544;
	// stw r3,260(r31)
	PPC_STORE_U32(r31.u32 + 260, ctx.r3.u32);
	// lis r8,-32154
	ctx.r8.s64 = -2107244544;
	// lis r9,-32154
	ctx.r9.s64 = -2107244544;
	// lis r10,-32155
	ctx.r10.s64 = -2107310080;
	// lis r11,-32155
	r11.s64 = -2107310080;
	// addi r5,r5,-25880
	ctx.r5.s64 = ctx.r5.s64 + -25880;
	// addi r6,r6,32128
	ctx.r6.s64 = ctx.r6.s64 + 32128;
	// addi r7,r7,-31960
	ctx.r7.s64 = ctx.r7.s64 + -31960;
	// addi r8,r8,-29664
	ctx.r8.s64 = ctx.r8.s64 + -29664;
	// addi r9,r9,-28176
	ctx.r9.s64 = ctx.r9.s64 + -28176;
	// addi r10,r10,-6344
	ctx.r10.s64 = ctx.r10.s64 + -6344;
	// stw r5,3120(r31)
	PPC_STORE_U32(r31.u32 + 3120, ctx.r5.u32);
	// addi r11,r11,-1280
	r11.s64 = r11.s64 + -1280;
	// stw r6,3088(r31)
	PPC_STORE_U32(r31.u32 + 3088, ctx.r6.u32);
	// stw r7,3092(r31)
	PPC_STORE_U32(r31.u32 + 3092, ctx.r7.u32);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// stw r8,3096(r31)
	PPC_STORE_U32(r31.u32 + 3096, ctx.r8.u32);
	// stw r9,3100(r31)
	PPC_STORE_U32(r31.u32 + 3100, ctx.r9.u32);
	// stw r10,3112(r31)
	PPC_STORE_U32(r31.u32 + 3112, ctx.r10.u32);
	// stw r11,3116(r31)
	PPC_STORE_U32(r31.u32 + 3116, r11.u32);
	// beq cr6,0x82603d14
	if (cr6.eq) goto loc_82603D14;
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// lis r11,-32159
	r11.s64 = -2107572224;
	// addi r10,r10,31256
	ctx.r10.s64 = ctx.r10.s64 + 31256;
	// addi r11,r11,31680
	r11.s64 = r11.s64 + 31680;
	// b 0x82603d24
	goto loc_82603D24;
loc_82603D14:
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// lis r11,-32159
	r11.s64 = -2107572224;
	// addi r10,r10,28568
	ctx.r10.s64 = ctx.r10.s64 + 28568;
	// addi r11,r11,28960
	r11.s64 = r11.s64 + 28960;
loc_82603D24:
	// stw r11,15868(r31)
	PPC_STORE_U32(r31.u32 + 15868, r11.u32);
	// lis r11,-32159
	r11.s64 = -2107572224;
	// stw r10,15864(r31)
	PPC_STORE_U32(r31.u32 + 15864, ctx.r10.u32);
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// addi r11,r11,-12232
	r11.s64 = r11.s64 + -12232;
	// addi r10,r10,30960
	ctx.r10.s64 = ctx.r10.s64 + 30960;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,15860(r31)
	PPC_STORE_U32(r31.u32 + 15860, r11.u32);
	// stw r10,2952(r31)
	PPC_STORE_U32(r31.u32 + 2952, ctx.r10.u32);
	// bl 0x826576f0
	sub_826576F0(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82603D60"))) PPC_WEAK_FUNC(sub_82603D60);
PPC_FUNC_IMPL(__imp__sub_82603D60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r5,15472(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// stw r30,15520(r31)
	PPC_STORE_U32(r31.u32 + 15520, r30.u32);
	// cmpwi cr6,r5,5
	cr6.compare<int32_t>(ctx.r5.s32, 5, xer);
	// beq cr6,0x82603d90
	if (cr6.eq) goto loc_82603D90;
	// cmpwi cr6,r5,6
	cr6.compare<int32_t>(ctx.r5.s32, 6, xer);
	// blt cr6,0x82603e00
	if (cr6.lt) goto loc_82603E00;
loc_82603D90:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// cmpwi cr6,r5,7
	cr6.compare<int32_t>(ctx.r5.s32, 7, xer);
	// addi r10,r11,6808
	ctx.r10.s64 = r11.s64 + 6808;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r9,r11,6844
	ctx.r9.s64 = r11.s64 + 6844;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// stw r10,1836(r31)
	PPC_STORE_U32(r31.u32 + 1836, ctx.r10.u32);
	// addi r8,r11,6880
	ctx.r8.s64 = r11.s64 + 6880;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// stw r9,1840(r31)
	PPC_STORE_U32(r31.u32 + 1840, ctx.r9.u32);
	// addi r7,r11,6916
	ctx.r7.s64 = r11.s64 + 6916;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// stw r8,1844(r31)
	PPC_STORE_U32(r31.u32 + 1844, ctx.r8.u32);
	// addi r6,r11,6952
	ctx.r6.s64 = r11.s64 + 6952;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// stw r7,1848(r31)
	PPC_STORE_U32(r31.u32 + 1848, ctx.r7.u32);
	// addi r11,r11,6972
	r11.s64 = r11.s64 + 6972;
	// stw r6,1864(r31)
	PPC_STORE_U32(r31.u32 + 1864, ctx.r6.u32);
	// stw r11,1868(r31)
	PPC_STORE_U32(r31.u32 + 1868, r11.u32);
	// bne cr6,0x82603de4
	if (!cr6.eq) goto loc_82603DE4;
	// stw r11,1860(r31)
	PPC_STORE_U32(r31.u32 + 1860, r11.u32);
loc_82603DE4:
	// cmpwi cr6,r5,5
	cr6.compare<int32_t>(ctx.r5.s32, 5, xer);
	// bne cr6,0x82603df8
	if (!cr6.eq) goto loc_82603DF8;
	// stw r30,432(r31)
	PPC_STORE_U32(r31.u32 + 432, r30.u32);
	// stw r30,440(r31)
	PPC_STORE_U32(r31.u32 + 440, r30.u32);
	// b 0x82603e00
	goto loc_82603E00;
loc_82603DF8:
	// stw r29,432(r31)
	PPC_STORE_U32(r31.u32 + 432, r29.u32);
	// stw r29,440(r31)
	PPC_STORE_U32(r31.u32 + 440, r29.u32);
loc_82603E00:
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// blt cr6,0x82603e80
	if (cr6.lt) goto loc_82603E80;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r3,-32155
	ctx.r3.s64 = -2107310080;
	// addi r11,r11,6000
	r11.s64 = r11.s64 + 6000;
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// lis r7,-32138
	ctx.r7.s64 = -2106195968;
	// lis r6,-32138
	ctx.r6.s64 = -2106195968;
	// stw r11,1800(r31)
	PPC_STORE_U32(r31.u32 + 1800, r11.u32);
	// lis r5,-32138
	ctx.r5.s64 = -2106195968;
	// lis r4,-32138
	ctx.r4.s64 = -2106195968;
	// addi r11,r3,-17752
	r11.s64 = ctx.r3.s64 + -17752;
	// addi r10,r10,5792
	ctx.r10.s64 = ctx.r10.s64 + 5792;
	// addi r9,r9,6072
	ctx.r9.s64 = ctx.r9.s64 + 6072;
	// addi r8,r8,6136
	ctx.r8.s64 = ctx.r8.s64 + 6136;
	// addi r7,r7,5664
	ctx.r7.s64 = ctx.r7.s64 + 5664;
	// addi r6,r6,5728
	ctx.r6.s64 = ctx.r6.s64 + 5728;
	// stw r11,3068(r31)
	PPC_STORE_U32(r31.u32 + 3068, r11.u32);
	// addi r5,r5,5864
	ctx.r5.s64 = ctx.r5.s64 + 5864;
	// stw r10,1812(r31)
	PPC_STORE_U32(r31.u32 + 1812, ctx.r10.u32);
	// addi r4,r4,5928
	ctx.r4.s64 = ctx.r4.s64 + 5928;
	// stw r9,1804(r31)
	PPC_STORE_U32(r31.u32 + 1804, ctx.r9.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r8,1808(r31)
	PPC_STORE_U32(r31.u32 + 1808, ctx.r8.u32);
	// stw r7,1816(r31)
	PPC_STORE_U32(r31.u32 + 1816, ctx.r7.u32);
	// stw r6,1820(r31)
	PPC_STORE_U32(r31.u32 + 1820, ctx.r6.u32);
	// stw r5,1824(r31)
	PPC_STORE_U32(r31.u32 + 1824, ctx.r5.u32);
	// stw r4,1828(r31)
	PPC_STORE_U32(r31.u32 + 1828, ctx.r4.u32);
	// bl 0x825f3918
	sub_825F3918(ctx, base);
	// b 0x82603ecc
	goto loc_82603ECC;
loc_82603E80:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// lis r7,-32160
	ctx.r7.s64 = -2107637760;
	// addi r11,r11,7056
	r11.s64 = r11.s64 + 7056;
	// addi r10,r10,6992
	ctx.r10.s64 = ctx.r10.s64 + 6992;
	// addi r9,r9,7128
	ctx.r9.s64 = ctx.r9.s64 + 7128;
	// addi r8,r8,7192
	ctx.r8.s64 = ctx.r8.s64 + 7192;
	// addi r7,r7,-9152
	ctx.r7.s64 = ctx.r7.s64 + -9152;
	// stw r11,1800(r31)
	PPC_STORE_U32(r31.u32 + 1800, r11.u32);
	// stw r10,1812(r31)
	PPC_STORE_U32(r31.u32 + 1812, ctx.r10.u32);
	// stw r9,1804(r31)
	PPC_STORE_U32(r31.u32 + 1804, ctx.r9.u32);
	// stw r8,1808(r31)
	PPC_STORE_U32(r31.u32 + 1808, ctx.r8.u32);
	// stw r7,3068(r31)
	PPC_STORE_U32(r31.u32 + 3068, ctx.r7.u32);
	// stw r8,1816(r31)
	PPC_STORE_U32(r31.u32 + 1816, ctx.r8.u32);
	// stw r9,1820(r31)
	PPC_STORE_U32(r31.u32 + 1820, ctx.r9.u32);
	// stw r10,1824(r31)
	PPC_STORE_U32(r31.u32 + 1824, ctx.r10.u32);
	// stw r11,1828(r31)
	PPC_STORE_U32(r31.u32 + 1828, r11.u32);
loc_82603ECC:
	// lwz r8,15472(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r8,7
	cr6.compare<int32_t>(ctx.r8.s32, 7, xer);
	// bne cr6,0x82603f4c
	if (!cr6.eq) goto loc_82603F4C;
	// lwz r11,1836(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1836);
	// lwz r10,1840(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1840);
	// lwz r9,1864(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1864);
	// lwz r7,1828(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1828);
	// lwz r6,1804(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1804);
	// lwz r5,1808(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1808);
	// lwz r4,1788(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 1788);
	// stw r11,1852(r31)
	PPC_STORE_U32(r31.u32 + 1852, r11.u32);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// stw r10,1856(r31)
	PPC_STORE_U32(r31.u32 + 1856, ctx.r10.u32);
	// stw r9,1860(r31)
	PPC_STORE_U32(r31.u32 + 1860, ctx.r9.u32);
	// stw r29,1796(r31)
	PPC_STORE_U32(r31.u32 + 1796, r29.u32);
	// stw r7,1832(r31)
	PPC_STORE_U32(r31.u32 + 1832, ctx.r7.u32);
	// stw r6,20048(r31)
	PPC_STORE_U32(r31.u32 + 20048, ctx.r6.u32);
	// stw r5,20052(r31)
	PPC_STORE_U32(r31.u32 + 20052, ctx.r5.u32);
	// beq cr6,0x82603f4c
	if (cr6.eq) goto loc_82603F4C;
	// lwz r11,1824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1824);
	// lwz r10,1844(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1844);
	// lwz r9,1848(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1848);
	// lwz r7,1868(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1868);
	// lwz r6,1816(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1816);
	// lwz r5,1820(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1820);
	// stw r30,1796(r31)
	PPC_STORE_U32(r31.u32 + 1796, r30.u32);
	// stw r11,1832(r31)
	PPC_STORE_U32(r31.u32 + 1832, r11.u32);
	// stw r10,1852(r31)
	PPC_STORE_U32(r31.u32 + 1852, ctx.r10.u32);
	// stw r9,1856(r31)
	PPC_STORE_U32(r31.u32 + 1856, ctx.r9.u32);
	// stw r7,1860(r31)
	PPC_STORE_U32(r31.u32 + 1860, ctx.r7.u32);
	// stw r6,20048(r31)
	PPC_STORE_U32(r31.u32 + 20048, ctx.r6.u32);
	// stw r5,20052(r31)
	PPC_STORE_U32(r31.u32 + 20052, ctx.r5.u32);
loc_82603F4C:
	// lis r9,-32161
	ctx.r9.s64 = -2107703296;
	// lis r10,-32161
	ctx.r10.s64 = -2107703296;
	// lis r11,-32155
	r11.s64 = -2107310080;
	// addi r9,r9,15504
	ctx.r9.s64 = ctx.r9.s64 + 15504;
	// addi r10,r10,18256
	ctx.r10.s64 = ctx.r10.s64 + 18256;
	// addi r11,r11,-23376
	r11.s64 = r11.s64 + -23376;
	// cmpwi cr6,r8,6
	cr6.compare<int32_t>(ctx.r8.s32, 6, xer);
	// stw r9,15772(r31)
	PPC_STORE_U32(r31.u32 + 15772, ctx.r9.u32);
	// stw r10,15776(r31)
	PPC_STORE_U32(r31.u32 + 15776, ctx.r10.u32);
	// stw r11,3060(r31)
	PPC_STORE_U32(r31.u32 + 3060, r11.u32);
	// blt cr6,0x82603fa0
	if (cr6.lt) goto loc_82603FA0;
	// lis r9,-32157
	ctx.r9.s64 = -2107441152;
	// lis r10,-32158
	ctx.r10.s64 = -2107506688;
	// lis r11,-32158
	r11.s64 = -2107506688;
	// addi r9,r9,672
	ctx.r9.s64 = ctx.r9.s64 + 672;
	// addi r10,r10,30968
	ctx.r10.s64 = ctx.r10.s64 + 30968;
	// addi r11,r11,23968
	r11.s64 = r11.s64 + 23968;
	// stw r9,15776(r31)
	PPC_STORE_U32(r31.u32 + 15776, ctx.r9.u32);
	// stw r10,3056(r31)
	PPC_STORE_U32(r31.u32 + 3056, ctx.r10.u32);
	// stw r11,3064(r31)
	PPC_STORE_U32(r31.u32 + 3064, r11.u32);
	// b 0x82603fac
	goto loc_82603FAC;
loc_82603FA0:
	// lis r11,-32158
	r11.s64 = -2107506688;
	// addi r11,r11,23920
	r11.s64 = r11.s64 + 23920;
	// stw r11,3056(r31)
	PPC_STORE_U32(r31.u32 + 3056, r11.u32);
loc_82603FAC:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x82604038
	if (cr6.eq) goto loc_82604038;
	// cmpwi cr6,r8,3
	cr6.compare<int32_t>(ctx.r8.s32, 3, xer);
	// mr r11,r30
	r11.u64 = r30.u64;
	// bge cr6,0x82603fc4
	if (!cr6.lt) goto loc_82603FC4;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_82603FC4:
	// stw r11,1932(r31)
	PPC_STORE_U32(r31.u32 + 1932, r11.u32);
	// cmpwi cr6,r8,4
	cr6.compare<int32_t>(ctx.r8.s32, 4, xer);
	// mr r11,r30
	r11.u64 = r30.u64;
	// bge cr6,0x82603fd8
	if (!cr6.lt) goto loc_82603FD8;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_82603FD8:
	// lis r10,-32157
	ctx.r10.s64 = -2107441152;
	// lwz r9,3164(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// stw r11,1936(r31)
	PPC_STORE_U32(r31.u32 + 1936, r11.u32);
	// addi r10,r10,5376
	ctx.r10.s64 = ctx.r10.s64 + 5376;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bne cr6,0x82603ffc
	if (!cr6.eq) goto loc_82603FFC;
	// lis r11,-32155
	r11.s64 = -2107310080;
	// addi r11,r11,-26520
	r11.s64 = r11.s64 + -26520;
	// b 0x82604004
	goto loc_82604004;
loc_82603FFC:
	// lis r11,-32155
	r11.s64 = -2107310080;
	// addi r11,r11,-11552
	r11.s64 = r11.s64 + -11552;
loc_82604004:
	// stw r11,3156(r31)
	PPC_STORE_U32(r31.u32 + 3156, r11.u32);
	// cmpwi cr6,r8,3
	cr6.compare<int32_t>(ctx.r8.s32, 3, xer);
	// blt cr6,0x82604024
	if (cr6.lt) goto loc_82604024;
	// lis r10,-32156
	ctx.r10.s64 = -2107375616;
	// lis r11,-32156
	r11.s64 = -2107375616;
	// addi r10,r10,-16800
	ctx.r10.s64 = ctx.r10.s64 + -16800;
	// addi r11,r11,-27584
	r11.s64 = r11.s64 + -27584;
	// b 0x82604060
	goto loc_82604060;
loc_82604024:
	// lis r10,-32160
	ctx.r10.s64 = -2107637760;
	// lis r11,-32160
	r11.s64 = -2107637760;
	// addi r10,r10,-24840
	ctx.r10.s64 = ctx.r10.s64 + -24840;
	// addi r11,r11,-24472
	r11.s64 = r11.s64 + -24472;
	// b 0x82604060
	goto loc_82604060;
loc_82604038:
	// lis r9,-32160
	ctx.r9.s64 = -2107637760;
	// lwz r8,140(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lis r10,-32160
	ctx.r10.s64 = -2107637760;
	// stw r30,1936(r31)
	PPC_STORE_U32(r31.u32 + 1936, r30.u32);
	// addi r9,r9,-16808
	ctx.r9.s64 = ctx.r9.s64 + -16808;
	// lis r11,-32160
	r11.s64 = -2107637760;
	// addi r10,r10,-23752
	ctx.r10.s64 = ctx.r10.s64 + -23752;
	// stw r8,15468(r31)
	PPC_STORE_U32(r31.u32 + 15468, ctx.r8.u32);
	// addi r11,r11,-23384
	r11.s64 = r11.s64 + -23384;
	// stw r9,3156(r31)
	PPC_STORE_U32(r31.u32 + 3156, ctx.r9.u32);
loc_82604060:
	// stw r11,3076(r31)
	PPC_STORE_U32(r31.u32 + 3076, r11.u32);
	// lis r11,-32158
	r11.s64 = -2107506688;
	// stw r10,3072(r31)
	PPC_STORE_U32(r31.u32 + 3072, ctx.r10.u32);
	// addi r11,r11,-7840
	r11.s64 = r11.s64 + -7840;
	// stw r11,20020(r31)
	PPC_STORE_U32(r31.u32 + 20020, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8260407C"))) PPC_WEAK_FUNC(sub_8260407C);
PPC_FUNC_IMPL(__imp__sub_8260407C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82604080"))) PPC_WEAK_FUNC(sub_82604080);
PPC_FUNC_IMPL(__imp__sub_82604080) {
	PPC_FUNC_PROLOGUE();
	// lis r4,9356
	ctx.r4.s64 = 613154816;
	// ori r4,r4,32769
	ctx.r4.u64 = ctx.r4.u64 | 32769;
	// b 0x82121108
	sub_82121108(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8260408C"))) PPC_WEAK_FUNC(sub_8260408C);
PPC_FUNC_IMPL(__imp__sub_8260408C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82604090"))) PPC_WEAK_FUNC(sub_82604090);
PPC_FUNC_IMPL(__imp__sub_82604090) {
	PPC_FUNC_PROLOGUE();
	// lis r4,9356
	ctx.r4.s64 = 613154816;
	// ori r4,r4,32769
	ctx.r4.u64 = ctx.r4.u64 | 32769;
	// b 0x82120e68
	sub_82120E68(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8260409C"))) PPC_WEAK_FUNC(sub_8260409C);
PPC_FUNC_IMPL(__imp__sub_8260409C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826040A0"))) PPC_WEAK_FUNC(sub_826040A0);
PPC_FUNC_IMPL(__imp__sub_826040A0) {
	PPC_FUNC_PROLOGUE();
	// lis r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r5,r5,32768
	ctx.r5.u64 = ctx.r5.u64 | 32768;
	// b 0x826a8de0
	sub_826A8DE0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_826040B0"))) PPC_WEAK_FUNC(sub_826040B0);
PPC_FUNC_IMPL(__imp__sub_826040B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// addi r29,r5,1
	r29.s64 = ctx.r5.s64 + 1;
	// lis r4,9356
	ctx.r4.s64 = 613154816;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// ori r4,r4,32769
	ctx.r4.u64 = ctx.r4.u64 | 32769;
	// rlwinm r3,r29,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r11,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r11.u32);
	// bne cr6,0x826040f8
	if (!cr6.eq) goto loc_826040F8;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_826040F8:
	// addi r10,r29,-1
	ctx.r10.s64 = r29.s64 + -1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82604134
	if (!cr6.gt) goto loc_82604134;
loc_82604104:
	// lbz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x82604104
	if (!cr6.eq) goto loc_82604104;
loc_82604134:
	// li r10,0
	ctx.r10.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82604148"))) PPC_WEAK_FUNC(sub_82604148);
PPC_FUNC_IMPL(__imp__sub_82604148) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r30,r11,31208
	r30.s64 = r11.s64 + 31208;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// addi r29,r11,31040
	r29.s64 = r11.s64 + 31040;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lwz r3,2596(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 2596);
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// stw r30,2588(r31)
	PPC_STORE_U32(r31.u32 + 2588, r30.u32);
	// addi r11,r11,7356
	r11.s64 = r11.s64 + 7356;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// addi r21,r31,2560
	r21.s64 = r31.s64 + 2560;
	// stw r29,2592(r31)
	PPC_STORE_U32(r31.u32 + 2592, r29.u32);
	// addi r10,r10,7384
	ctx.r10.s64 = ctx.r10.s64 + 7384;
	// addi r9,r9,7424
	ctx.r9.s64 = ctx.r9.s64 + 7424;
	// addi r8,r8,7448
	ctx.r8.s64 = ctx.r8.s64 + 7448;
	// stw r11,2572(r31)
	PPC_STORE_U32(r31.u32 + 2572, r11.u32);
	// addi r7,r31,2180
	ctx.r7.s64 = r31.s64 + 2180;
	// li r6,168
	ctx.r6.s64 = 168;
	// li r5,98
	ctx.r5.s64 = 98;
	// stw r10,2576(r31)
	PPC_STORE_U32(r31.u32 + 2576, ctx.r10.u32);
	// lis r11,9356
	r11.s64 = 613154816;
	// stw r9,2580(r31)
	PPC_STORE_U32(r31.u32 + 2580, ctx.r9.u32);
	// stw r8,2584(r31)
	PPC_STORE_U32(r31.u32 + 2584, ctx.r8.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r7,0(r21)
	PPC_STORE_U32(r21.u32 + 0, ctx.r7.u32);
	// ori r20,r11,32769
	r20.u64 = r11.u64 | 32769;
	// stw r6,2564(r31)
	PPC_STORE_U32(r31.u32 + 2564, ctx.r6.u32);
	// stw r5,2568(r31)
	PPC_STORE_U32(r31.u32 + 2568, ctx.r5.u32);
	// beq cr6,0x826041d8
	if (cr6.eq) goto loc_826041D8;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
loc_826041D8:
	// li r23,0
	r23.s64 = 0;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// li r5,168
	ctx.r5.s64 = 168;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r23,2596(r31)
	PPC_STORE_U32(r31.u32 + 2596, r23.u32);
	// bl 0x826040b0
	sub_826040B0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260469c
	if (!cr6.eq) goto loc_8260469C;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r3,2636(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 2636);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// addi r30,r11,31568
	r30.s64 = r11.s64 + 31568;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// addi r29,r11,31376
	r29.s64 = r11.s64 + 31376;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// stw r30,2628(r31)
	PPC_STORE_U32(r31.u32 + 2628, r30.u32);
	// addi r22,r31,2600
	r22.s64 = r31.s64 + 2600;
	// addi r11,r11,7256
	r11.s64 = r11.s64 + 7256;
	// addi r10,r10,7288
	ctx.r10.s64 = ctx.r10.s64 + 7288;
	// stw r29,2632(r31)
	PPC_STORE_U32(r31.u32 + 2632, r29.u32);
	// addi r9,r9,7328
	ctx.r9.s64 = ctx.r9.s64 + 7328;
	// addi r8,r8,7348
	ctx.r8.s64 = ctx.r8.s64 + 7348;
	// addi r7,r31,2192
	ctx.r7.s64 = r31.s64 + 2192;
	// li r6,185
	ctx.r6.s64 = 185;
	// stw r11,2612(r31)
	PPC_STORE_U32(r31.u32 + 2612, r11.u32);
	// li r5,118
	ctx.r5.s64 = 118;
	// stw r10,2616(r31)
	PPC_STORE_U32(r31.u32 + 2616, ctx.r10.u32);
	// stw r9,2620(r31)
	PPC_STORE_U32(r31.u32 + 2620, ctx.r9.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r8,2624(r31)
	PPC_STORE_U32(r31.u32 + 2624, ctx.r8.u32);
	// stw r7,0(r22)
	PPC_STORE_U32(r22.u32 + 0, ctx.r7.u32);
	// stw r6,2604(r31)
	PPC_STORE_U32(r31.u32 + 2604, ctx.r6.u32);
	// stw r5,2608(r31)
	PPC_STORE_U32(r31.u32 + 2608, ctx.r5.u32);
	// beq cr6,0x82604274
	if (cr6.eq) goto loc_82604274;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
loc_82604274:
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// stw r23,2636(r31)
	PPC_STORE_U32(r31.u32 + 2636, r23.u32);
	// li r5,185
	ctx.r5.s64 = 185;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826040b0
	sub_826040B0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260469c
	if (!cr6.eq) goto loc_8260469C;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r3,2676(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 2676);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// addi r30,r11,31912
	r30.s64 = r11.s64 + 31912;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// addi r29,r11,31760
	r29.s64 = r11.s64 + 31760;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// stw r30,2668(r31)
	PPC_STORE_U32(r31.u32 + 2668, r30.u32);
	// addi r24,r31,2640
	r24.s64 = r31.s64 + 2640;
	// addi r11,r11,7540
	r11.s64 = r11.s64 + 7540;
	// addi r10,r10,7572
	ctx.r10.s64 = ctx.r10.s64 + 7572;
	// stw r29,2672(r31)
	PPC_STORE_U32(r31.u32 + 2672, r29.u32);
	// addi r9,r9,7616
	ctx.r9.s64 = ctx.r9.s64 + 7616;
	// addi r8,r8,7632
	ctx.r8.s64 = ctx.r8.s64 + 7632;
	// addi r7,r31,2204
	ctx.r7.s64 = r31.s64 + 2204;
	// li r6,148
	ctx.r6.s64 = 148;
	// stw r11,2652(r31)
	PPC_STORE_U32(r31.u32 + 2652, r11.u32);
	// li r5,80
	ctx.r5.s64 = 80;
	// stw r10,2656(r31)
	PPC_STORE_U32(r31.u32 + 2656, ctx.r10.u32);
	// stw r9,2660(r31)
	PPC_STORE_U32(r31.u32 + 2660, ctx.r9.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r8,2664(r31)
	PPC_STORE_U32(r31.u32 + 2664, ctx.r8.u32);
	// stw r7,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r7.u32);
	// stw r6,2644(r31)
	PPC_STORE_U32(r31.u32 + 2644, ctx.r6.u32);
	// stw r5,2648(r31)
	PPC_STORE_U32(r31.u32 + 2648, ctx.r5.u32);
	// beq cr6,0x8260430c
	if (cr6.eq) goto loc_8260430C;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
loc_8260430C:
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// stw r23,2676(r31)
	PPC_STORE_U32(r31.u32 + 2676, r23.u32);
	// li r5,148
	ctx.r5.s64 = 148;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826040b0
	sub_826040B0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260469c
	if (!cr6.eq) goto loc_8260469C;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r3,2716(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 2716);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// addi r30,r11,32200
	r30.s64 = r11.s64 + 32200;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// addi r29,r11,32064
	r29.s64 = r11.s64 + 32064;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// stw r30,2708(r31)
	PPC_STORE_U32(r31.u32 + 2708, r30.u32);
	// addi r25,r31,2680
	r25.s64 = r31.s64 + 2680;
	// addi r11,r11,7460
	r11.s64 = r11.s64 + 7460;
	// addi r10,r10,7484
	ctx.r10.s64 = ctx.r10.s64 + 7484;
	// stw r29,2712(r31)
	PPC_STORE_U32(r31.u32 + 2712, r29.u32);
	// addi r9,r9,7512
	ctx.r9.s64 = ctx.r9.s64 + 7512;
	// addi r8,r8,7532
	ctx.r8.s64 = ctx.r8.s64 + 7532;
	// addi r7,r31,2216
	ctx.r7.s64 = r31.s64 + 2216;
	// li r6,132
	ctx.r6.s64 = 132;
	// stw r11,2692(r31)
	PPC_STORE_U32(r31.u32 + 2692, r11.u32);
	// li r5,84
	ctx.r5.s64 = 84;
	// stw r10,2696(r31)
	PPC_STORE_U32(r31.u32 + 2696, ctx.r10.u32);
	// stw r9,2700(r31)
	PPC_STORE_U32(r31.u32 + 2700, ctx.r9.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r8,2704(r31)
	PPC_STORE_U32(r31.u32 + 2704, ctx.r8.u32);
	// stw r7,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r7.u32);
	// stw r6,2684(r31)
	PPC_STORE_U32(r31.u32 + 2684, ctx.r6.u32);
	// stw r5,2688(r31)
	PPC_STORE_U32(r31.u32 + 2688, ctx.r5.u32);
	// beq cr6,0x826043a4
	if (cr6.eq) goto loc_826043A4;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
loc_826043A4:
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// stw r23,2716(r31)
	PPC_STORE_U32(r31.u32 + 2716, r23.u32);
	// li r5,132
	ctx.r5.s64 = 132;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826040b0
	sub_826040B0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260469c
	if (!cr6.eq) goto loc_8260469C;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r3,2756(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 2756);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// addi r30,r11,-32512
	r30.s64 = r11.s64 + -32512;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// addi r29,r11,-32408
	r29.s64 = r11.s64 + -32408;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// stw r30,2748(r31)
	PPC_STORE_U32(r31.u32 + 2748, r30.u32);
	// addi r26,r31,2720
	r26.s64 = r31.s64 + 2720;
	// addi r11,r11,7720
	r11.s64 = r11.s64 + 7720;
	// addi r10,r10,7748
	ctx.r10.s64 = ctx.r10.s64 + 7748;
	// stw r29,2752(r31)
	PPC_STORE_U32(r31.u32 + 2752, r29.u32);
	// addi r9,r9,7792
	ctx.r9.s64 = ctx.r9.s64 + 7792;
	// addi r8,r8,7808
	ctx.r8.s64 = ctx.r8.s64 + 7808;
	// addi r7,r31,2228
	ctx.r7.s64 = r31.s64 + 2228;
	// li r28,102
	r28.s64 = 102;
	// stw r11,2732(r31)
	PPC_STORE_U32(r31.u32 + 2732, r11.u32);
	// li r6,57
	ctx.r6.s64 = 57;
	// stw r10,2736(r31)
	PPC_STORE_U32(r31.u32 + 2736, ctx.r10.u32);
	// stw r9,2740(r31)
	PPC_STORE_U32(r31.u32 + 2740, ctx.r9.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r8,2744(r31)
	PPC_STORE_U32(r31.u32 + 2744, ctx.r8.u32);
	// stw r7,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r7.u32);
	// stw r28,2724(r31)
	PPC_STORE_U32(r31.u32 + 2724, r28.u32);
	// stw r6,2728(r31)
	PPC_STORE_U32(r31.u32 + 2728, ctx.r6.u32);
	// beq cr6,0x8260443c
	if (cr6.eq) goto loc_8260443C;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
loc_8260443C:
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// stw r23,2756(r31)
	PPC_STORE_U32(r31.u32 + 2756, r23.u32);
	// li r5,102
	ctx.r5.s64 = 102;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826040b0
	sub_826040B0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260469c
	if (!cr6.eq) goto loc_8260469C;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r3,2796(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 2796);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// stw r28,2764(r31)
	PPC_STORE_U32(r31.u32 + 2764, r28.u32);
	// addi r30,r11,-32304
	r30.s64 = r11.s64 + -32304;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// addi r29,r11,-32200
	r29.s64 = r11.s64 + -32200;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// stw r30,2788(r31)
	PPC_STORE_U32(r31.u32 + 2788, r30.u32);
	// addi r27,r31,2760
	r27.s64 = r31.s64 + 2760;
	// addi r11,r11,7640
	r11.s64 = r11.s64 + 7640;
	// addi r10,r10,7656
	ctx.r10.s64 = ctx.r10.s64 + 7656;
	// stw r29,2792(r31)
	PPC_STORE_U32(r31.u32 + 2792, r29.u32);
	// addi r9,r9,7680
	ctx.r9.s64 = ctx.r9.s64 + 7680;
	// addi r8,r8,7708
	ctx.r8.s64 = ctx.r8.s64 + 7708;
	// addi r7,r31,2240
	ctx.r7.s64 = r31.s64 + 2240;
	// li r6,66
	ctx.r6.s64 = 66;
	// stw r11,2772(r31)
	PPC_STORE_U32(r31.u32 + 2772, r11.u32);
	// stw r10,2776(r31)
	PPC_STORE_U32(r31.u32 + 2776, ctx.r10.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r9,2780(r31)
	PPC_STORE_U32(r31.u32 + 2780, ctx.r9.u32);
	// stw r8,2784(r31)
	PPC_STORE_U32(r31.u32 + 2784, ctx.r8.u32);
	// stw r7,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r7.u32);
	// stw r6,2768(r31)
	PPC_STORE_U32(r31.u32 + 2768, ctx.r6.u32);
	// beq cr6,0x826044d0
	if (cr6.eq) goto loc_826044D0;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
loc_826044D0:
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// stw r23,2796(r31)
	PPC_STORE_U32(r31.u32 + 2796, r23.u32);
	// li r5,102
	ctx.r5.s64 = 102;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826040b0
	sub_826040B0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260469c
	if (!cr6.eq) goto loc_8260469C;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r3,2876(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 2876);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// addi r29,r11,32512
	r29.s64 = r11.s64 + 32512;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// addi r28,r11,32336
	r28.s64 = r11.s64 + 32336;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// stw r29,2868(r31)
	PPC_STORE_U32(r31.u32 + 2868, r29.u32);
	// addi r30,r31,2840
	r30.s64 = r31.s64 + 2840;
	// addi r11,r11,7916
	r11.s64 = r11.s64 + 7916;
	// addi r10,r10,7944
	ctx.r10.s64 = ctx.r10.s64 + 7944;
	// stw r28,2872(r31)
	PPC_STORE_U32(r31.u32 + 2872, r28.u32);
	// addi r9,r9,7976
	ctx.r9.s64 = ctx.r9.s64 + 7976;
	// addi r8,r8,8012
	ctx.r8.s64 = ctx.r8.s64 + 8012;
	// addi r7,r31,2428
	ctx.r7.s64 = r31.s64 + 2428;
	// li r6,174
	ctx.r6.s64 = 174;
	// stw r11,2852(r31)
	PPC_STORE_U32(r31.u32 + 2852, r11.u32);
	// li r5,108
	ctx.r5.s64 = 108;
	// stw r10,2856(r31)
	PPC_STORE_U32(r31.u32 + 2856, ctx.r10.u32);
	// stw r9,2860(r31)
	PPC_STORE_U32(r31.u32 + 2860, ctx.r9.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r8,2864(r31)
	PPC_STORE_U32(r31.u32 + 2864, ctx.r8.u32);
	// stw r7,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r7.u32);
	// stw r6,2844(r31)
	PPC_STORE_U32(r31.u32 + 2844, ctx.r6.u32);
	// stw r5,2848(r31)
	PPC_STORE_U32(r31.u32 + 2848, ctx.r5.u32);
	// beq cr6,0x82604568
	if (cr6.eq) goto loc_82604568;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
loc_82604568:
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// stw r23,2876(r31)
	PPC_STORE_U32(r31.u32 + 2876, r23.u32);
	// li r5,174
	ctx.r5.s64 = 174;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x826040b0
	sub_826040B0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260469c
	if (!cr6.eq) goto loc_8260469C;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r3,2836(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 2836);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// addi r29,r11,-32680
	r29.s64 = r11.s64 + -32680;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// addi r28,r11,32688
	r28.s64 = r11.s64 + 32688;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r8,-32138
	ctx.r8.s64 = -2106195968;
	// stw r29,2828(r31)
	PPC_STORE_U32(r31.u32 + 2828, r29.u32);
	// addi r30,r31,2800
	r30.s64 = r31.s64 + 2800;
	// addi r11,r11,7812
	r11.s64 = r11.s64 + 7812;
	// addi r10,r10,7828
	ctx.r10.s64 = ctx.r10.s64 + 7828;
	// stw r28,2832(r31)
	PPC_STORE_U32(r31.u32 + 2832, r28.u32);
	// addi r9,r9,7848
	ctx.r9.s64 = ctx.r9.s64 + 7848;
	// addi r8,r8,7908
	ctx.r8.s64 = ctx.r8.s64 + 7908;
	// addi r7,r31,2252
	ctx.r7.s64 = r31.s64 + 2252;
	// li r6,162
	ctx.r6.s64 = 162;
	// stw r11,2812(r31)
	PPC_STORE_U32(r31.u32 + 2812, r11.u32);
	// li r5,125
	ctx.r5.s64 = 125;
	// stw r10,2816(r31)
	PPC_STORE_U32(r31.u32 + 2816, ctx.r10.u32);
	// stw r9,2820(r31)
	PPC_STORE_U32(r31.u32 + 2820, ctx.r9.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r8,2824(r31)
	PPC_STORE_U32(r31.u32 + 2824, ctx.r8.u32);
	// stw r7,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r7.u32);
	// stw r6,2804(r31)
	PPC_STORE_U32(r31.u32 + 2804, ctx.r6.u32);
	// stw r5,2808(r31)
	PPC_STORE_U32(r31.u32 + 2808, ctx.r5.u32);
	// beq cr6,0x82604600
	if (cr6.eq) goto loc_82604600;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
loc_82604600:
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// stw r23,2836(r31)
	PPC_STORE_U32(r31.u32 + 2836, r23.u32);
	// li r5,162
	ctx.r5.s64 = 162;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x826040b0
	sub_826040B0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260469c
	if (!cr6.eq) goto loc_8260469C;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// stw r24,2904(r31)
	PPC_STORE_U32(r31.u32 + 2904, r24.u32);
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// stw r21,2908(r31)
	PPC_STORE_U32(r31.u32 + 2908, r21.u32);
	// lis r9,-32243
	ctx.r9.s64 = -2113077248;
	// stw r26,2912(r31)
	PPC_STORE_U32(r31.u32 + 2912, r26.u32);
	// lis r8,-32243
	ctx.r8.s64 = -2113077248;
	// stw r25,2916(r31)
	PPC_STORE_U32(r31.u32 + 2916, r25.u32);
	// addi r3,r31,1988
	ctx.r3.s64 = r31.s64 + 1988;
	// stw r22,2920(r31)
	PPC_STORE_U32(r31.u32 + 2920, r22.u32);
	// addi r11,r11,-29888
	r11.s64 = r11.s64 + -29888;
	// stw r27,2924(r31)
	PPC_STORE_U32(r31.u32 + 2924, r27.u32);
	// addi r10,r10,-28784
	ctx.r10.s64 = ctx.r10.s64 + -28784;
	// addi r9,r9,-32096
	ctx.r9.s64 = ctx.r9.s64 + -32096;
	// addi r8,r8,-30992
	ctx.r8.s64 = ctx.r8.s64 + -30992;
	// addi r7,r31,2040
	ctx.r7.s64 = r31.s64 + 2040;
	// stw r3,2012(r31)
	PPC_STORE_U32(r31.u32 + 2012, ctx.r3.u32);
	// addi r6,r31,2052
	ctx.r6.s64 = r31.s64 + 2052;
	// stw r11,2020(r31)
	PPC_STORE_U32(r31.u32 + 2020, r11.u32);
	// addi r5,r31,2064
	ctx.r5.s64 = r31.s64 + 2064;
	// stw r10,2024(r31)
	PPC_STORE_U32(r31.u32 + 2024, ctx.r10.u32);
	// addi r4,r31,2076
	ctx.r4.s64 = r31.s64 + 2076;
	// stw r9,2028(r31)
	PPC_STORE_U32(r31.u32 + 2028, ctx.r9.u32);
	// addi r30,r31,2000
	r30.s64 = r31.s64 + 2000;
	// stw r8,2032(r31)
	PPC_STORE_U32(r31.u32 + 2032, ctx.r8.u32);
	// stw r7,2100(r31)
	PPC_STORE_U32(r31.u32 + 2100, ctx.r7.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r6,2104(r31)
	PPC_STORE_U32(r31.u32 + 2104, ctx.r6.u32);
	// stw r5,2108(r31)
	PPC_STORE_U32(r31.u32 + 2108, ctx.r5.u32);
	// stw r4,2112(r31)
	PPC_STORE_U32(r31.u32 + 2112, ctx.r4.u32);
	// stw r30,2016(r31)
	PPC_STORE_U32(r31.u32 + 2016, r30.u32);
loc_8260469C:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_826046A4"))) PPC_WEAK_FUNC(sub_826046A4);
PPC_FUNC_IMPL(__imp__sub_826046A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826046A8"))) PPC_WEAK_FUNC(sub_826046A8);
PPC_FUNC_IMPL(__imp__sub_826046A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82604720
	if (cr6.eq) goto loc_82604720;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lis r11,0
	r11.s64 = 0;
	// li r30,0
	r30.s64 = 0;
	// ori r29,r11,32768
	r29.u64 = r11.u64 | 32768;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826046e8
	if (cr6.eq) goto loc_826046E8;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x826a8de0
	sub_826A8DE0(ctx, base);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
loc_826046E8:
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604704
	if (cr6.eq) goto loc_82604704;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x826a8de0
	sub_826A8DE0(ctx, base);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
loc_82604704:
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604720
	if (cr6.eq) goto loc_82604720;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x826a8de0
	sub_826A8DE0(ctx, base);
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
loc_82604720:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82604728"))) PPC_WEAK_FUNC(sub_82604728);
PPC_FUNC_IMPL(__imp__sub_82604728) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,15372(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15372);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604748
	if (cr6.eq) goto loc_82604748;
	// bl 0x82643300
	sub_82643300(ctx, base);
loc_82604748:
	// lwz r3,15384(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15384);
	// lis r11,9356
	r11.s64 = 613154816;
	// li r30,0
	r30.s64 = 0;
	// ori r29,r11,32769
	r29.u64 = r11.u64 | 32769;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260476c
	if (cr6.eq) goto loc_8260476C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15384(r31)
	PPC_STORE_U32(r31.u32 + 15384, r30.u32);
loc_8260476C:
	// lwz r3,15392(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15392);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604784
	if (cr6.eq) goto loc_82604784;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15392(r31)
	PPC_STORE_U32(r31.u32 + 15392, r30.u32);
loc_82604784:
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260479c
	if (cr6.eq) goto loc_8260479C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r30.u32);
loc_8260479C:
	// lwz r3,1880(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1880);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826047b4
	if (cr6.eq) goto loc_826047B4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1880(r31)
	PPC_STORE_U32(r31.u32 + 1880, r30.u32);
loc_826047B4:
	// lwz r3,21564(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21564);
	// stw r30,1884(r31)
	PPC_STORE_U32(r31.u32 + 1884, r30.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826047d8
	if (cr6.eq) goto loc_826047D8;
	// lis r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r5,r5,32768
	ctx.r5.u64 = ctx.r5.u64 | 32768;
	// bl 0x826a8de0
	sub_826A8DE0(ctx, base);
	// stw r30,21564(r31)
	PPC_STORE_U32(r31.u32 + 21564, r30.u32);
loc_826047D8:
	// lwz r3,21568(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21568);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826047f0
	if (cr6.eq) goto loc_826047F0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21568(r31)
	PPC_STORE_U32(r31.u32 + 21568, r30.u32);
loc_826047F0:
	// lwz r3,21572(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21572);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604808
	if (cr6.eq) goto loc_82604808;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21572(r31)
	PPC_STORE_U32(r31.u32 + 21572, r30.u32);
loc_82604808:
	// addis r28,r31,2
	r28.s64 = r31.s64 + 131072;
	// addi r28,r28,-31848
	r28.s64 = r28.s64 + -31848;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604828
	if (cr6.eq) goto loc_82604828;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r30.u32);
loc_82604828:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x82604864
	if (cr6.lt) goto loc_82604864;
	// lwz r3,352(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260484c
	if (cr6.eq) goto loc_8260484C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,352(r31)
	PPC_STORE_U32(r31.u32 + 352, r30.u32);
loc_8260484C:
	// lwz r3,15216(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15216);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604864
	if (cr6.eq) goto loc_82604864;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15216(r31)
	PPC_STORE_U32(r31.u32 + 15216, r30.u32);
loc_82604864:
	// lwz r3,21240(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260487c
	if (cr6.eq) goto loc_8260487C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21240(r31)
	PPC_STORE_U32(r31.u32 + 21240, r30.u32);
loc_8260487C:
	// lwz r3,21268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604894
	if (cr6.eq) goto loc_82604894;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21268(r31)
	PPC_STORE_U32(r31.u32 + 21268, r30.u32);
loc_82604894:
	// lwz r3,21252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21252);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826048ac
	if (cr6.eq) goto loc_826048AC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21252(r31)
	PPC_STORE_U32(r31.u32 + 21252, r30.u32);
loc_826048AC:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x82604900
	if (!cr6.eq) goto loc_82604900;
	// lwz r3,19992(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 19992);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826048d0
	if (cr6.eq) goto loc_826048D0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,19992(r31)
	PPC_STORE_U32(r31.u32 + 19992, r30.u32);
loc_826048D0:
	// lwz r3,19996(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 19996);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826048e8
	if (cr6.eq) goto loc_826048E8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,19996(r31)
	PPC_STORE_U32(r31.u32 + 19996, r30.u32);
loc_826048E8:
	// lwz r3,20000(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20000);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604900
	if (cr6.eq) goto loc_82604900;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,20000(r31)
	PPC_STORE_U32(r31.u32 + 20000, r30.u32);
loc_82604900:
	// lwz r3,21636(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21636);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604918
	if (cr6.eq) goto loc_82604918;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21636(r31)
	PPC_STORE_U32(r31.u32 + 21636, r30.u32);
loc_82604918:
	// addi r4,r31,21652
	ctx.r4.s64 = r31.s64 + 21652;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,21664
	ctx.r4.s64 = r31.s64 + 21664;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,21676
	ctx.r4.s64 = r31.s64 + 21676;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,21688
	ctx.r4.s64 = r31.s64 + 21688;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,21640
	ctx.r4.s64 = r31.s64 + 21640;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// lwz r3,20952(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20952);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604968
	if (cr6.eq) goto loc_82604968;
	// bl 0x82635040
	sub_82635040(ctx, base);
	// stw r30,20952(r31)
	PPC_STORE_U32(r31.u32 + 20952, r30.u32);
loc_82604968:
	// lwz r3,1872(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1872);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604980
	if (cr6.eq) goto loc_82604980;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1872(r31)
	PPC_STORE_U32(r31.u32 + 1872, r30.u32);
loc_82604980:
	// lis r11,1
	r11.s64 = 65536;
	// ori r11,r11,33704
	r11.u64 = r11.u64 | 33704;
	// lwzx r11,r31,r11
	r11.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82604a8c
	if (!cr6.eq) goto loc_82604A8C;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r3,3688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826049b8
	if (cr6.eq) goto loc_826049B8;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826049d4
	if (cr6.eq) goto loc_826049D4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// b 0x826049d4
	goto loc_826049D4;
loc_826049B8:
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826049d4
	if (cr6.eq) goto loc_826049D4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3688(r31)
	PPC_STORE_U32(r31.u32 + 3688, r30.u32);
loc_826049D4:
	// lwz r3,3696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3696);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3696);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826049f4
	if (cr6.eq) goto loc_826049F4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3696(r31)
	PPC_STORE_U32(r31.u32 + 3696, r30.u32);
loc_826049F4:
	// lwz r3,3692(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3692);
	// stw r30,3696(r31)
	PPC_STORE_U32(r31.u32 + 3696, r30.u32);
	// stw r30,3688(r31)
	PPC_STORE_U32(r31.u32 + 3688, r30.u32);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3692(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3692);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604a1c
	if (cr6.eq) goto loc_82604A1C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3692(r31)
	PPC_STORE_U32(r31.u32 + 3692, r30.u32);
loc_82604A1C:
	// lwz r3,3700(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3700);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3700(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3700);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604a3c
	if (cr6.eq) goto loc_82604A3C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3700(r31)
	PPC_STORE_U32(r31.u32 + 3700, r30.u32);
loc_82604A3C:
	// lwz r3,3704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// stw r30,3692(r31)
	PPC_STORE_U32(r31.u32 + 3692, r30.u32);
	// stw r30,3700(r31)
	PPC_STORE_U32(r31.u32 + 3700, r30.u32);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604a64
	if (cr6.eq) goto loc_82604A64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3704(r31)
	PPC_STORE_U32(r31.u32 + 3704, r30.u32);
loc_82604A64:
	// lwz r3,3708(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3708(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604a84
	if (cr6.eq) goto loc_82604A84;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3708(r31)
	PPC_STORE_U32(r31.u32 + 3708, r30.u32);
loc_82604A84:
	// stw r30,3708(r31)
	PPC_STORE_U32(r31.u32 + 3708, r30.u32);
	// stw r30,3704(r31)
	PPC_STORE_U32(r31.u32 + 3704, r30.u32);
loc_82604A8C:
	// lwz r3,264(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 264);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604aa4
	if (cr6.eq) goto loc_82604AA4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,264(r31)
	PPC_STORE_U32(r31.u32 + 264, r30.u32);
loc_82604AA4:
	// lwz r3,272(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 272);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604abc
	if (cr6.eq) goto loc_82604ABC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,272(r31)
	PPC_STORE_U32(r31.u32 + 272, r30.u32);
loc_82604ABC:
	// lwz r3,1900(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1900);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604ad4
	if (cr6.eq) goto loc_82604AD4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1900(r31)
	PPC_STORE_U32(r31.u32 + 1900, r30.u32);
loc_82604AD4:
	// lwz r3,1904(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1904);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604aec
	if (cr6.eq) goto loc_82604AEC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1904(r31)
	PPC_STORE_U32(r31.u32 + 1904, r30.u32);
loc_82604AEC:
	// lwz r3,1908(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1908);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604b04
	if (cr6.eq) goto loc_82604B04;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1908(r31)
	PPC_STORE_U32(r31.u32 + 1908, r30.u32);
loc_82604B04:
	// lwz r3,1912(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1912);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604b1c
	if (cr6.eq) goto loc_82604B1C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1912(r31)
	PPC_STORE_U32(r31.u32 + 1912, r30.u32);
loc_82604B1C:
	// lwz r3,3916(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3916);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604b34
	if (cr6.eq) goto loc_82604B34;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3916(r31)
	PPC_STORE_U32(r31.u32 + 3916, r30.u32);
loc_82604B34:
	// lwz r3,21576(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21576);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604b4c
	if (cr6.eq) goto loc_82604B4C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21576(r31)
	PPC_STORE_U32(r31.u32 + 21576, r30.u32);
loc_82604B4C:
	// lwz r11,23968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23968);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82604c40
	if (cr6.eq) goto loc_82604C40;
	// lwz r11,16472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x82604bf4
	if (!cr6.eq) goto loc_82604BF4;
	// lwz r3,3048(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3048);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604b7c
	if (cr6.eq) goto loc_82604B7C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3048(r31)
	PPC_STORE_U32(r31.u32 + 3048, r30.u32);
loc_82604B7C:
	// lwz r3,15208(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15208);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604b94
	if (cr6.eq) goto loc_82604B94;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15208(r31)
	PPC_STORE_U32(r31.u32 + 15208, r30.u32);
loc_82604B94:
	// lwz r3,15268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15268);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604bac
	if (cr6.eq) goto loc_82604BAC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15268(r31)
	PPC_STORE_U32(r31.u32 + 15268, r30.u32);
loc_82604BAC:
	// lwz r3,15292(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15292);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604bc4
	if (cr6.eq) goto loc_82604BC4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15292(r31)
	PPC_STORE_U32(r31.u32 + 15292, r30.u32);
loc_82604BC4:
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604bdc
	if (cr6.eq) goto loc_82604BDC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, r30.u32);
loc_82604BDC:
	// lwz r3,1780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604c0c
	if (cr6.eq) goto loc_82604C0C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// b 0x82604c08
	goto loc_82604C08;
loc_82604BF4:
	// stw r30,3048(r31)
	PPC_STORE_U32(r31.u32 + 3048, r30.u32);
	// stw r30,15268(r31)
	PPC_STORE_U32(r31.u32 + 15268, r30.u32);
	// stw r30,15292(r31)
	PPC_STORE_U32(r31.u32 + 15292, r30.u32);
	// stw r30,15208(r31)
	PPC_STORE_U32(r31.u32 + 15208, r30.u32);
	// stw r30,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, r30.u32);
loc_82604C08:
	// stw r30,1780(r31)
	PPC_STORE_U32(r31.u32 + 1780, r30.u32);
loc_82604C0C:
	// lwz r3,15276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15276);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604c24
	if (cr6.eq) goto loc_82604C24;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15276(r31)
	PPC_STORE_U32(r31.u32 + 15276, r30.u32);
loc_82604C24:
	// lwz r3,15284(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15284);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604d00
	if (cr6.eq) goto loc_82604D00;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15284(r31)
	PPC_STORE_U32(r31.u32 + 15284, r30.u32);
	// b 0x82604d00
	goto loc_82604D00;
loc_82604C40:
	// lwz r3,3048(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3048);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604c58
	if (cr6.eq) goto loc_82604C58;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3048(r31)
	PPC_STORE_U32(r31.u32 + 3048, r30.u32);
loc_82604C58:
	// lwz r3,15208(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15208);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604c70
	if (cr6.eq) goto loc_82604C70;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15208(r31)
	PPC_STORE_U32(r31.u32 + 15208, r30.u32);
loc_82604C70:
	// lwz r3,15268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15268);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604c88
	if (cr6.eq) goto loc_82604C88;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15268(r31)
	PPC_STORE_U32(r31.u32 + 15268, r30.u32);
loc_82604C88:
	// lwz r3,15292(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15292);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604ca0
	if (cr6.eq) goto loc_82604CA0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15292(r31)
	PPC_STORE_U32(r31.u32 + 15292, r30.u32);
loc_82604CA0:
	// lwz r3,15276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15276);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604cb8
	if (cr6.eq) goto loc_82604CB8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15276(r31)
	PPC_STORE_U32(r31.u32 + 15276, r30.u32);
loc_82604CB8:
	// lwz r3,15284(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15284);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604cd0
	if (cr6.eq) goto loc_82604CD0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15284(r31)
	PPC_STORE_U32(r31.u32 + 15284, r30.u32);
loc_82604CD0:
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604ce8
	if (cr6.eq) goto loc_82604CE8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, r30.u32);
loc_82604CE8:
	// lwz r3,1780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604d00
	if (cr6.eq) goto loc_82604D00;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1780(r31)
	PPC_STORE_U32(r31.u32 + 1780, r30.u32);
loc_82604D00:
	// lwz r3,276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 276);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604d18
	if (cr6.eq) goto loc_82604D18;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,276(r31)
	PPC_STORE_U32(r31.u32 + 276, r30.u32);
loc_82604D18:
	// lwz r3,268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604d30
	if (cr6.eq) goto loc_82604D30;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,268(r31)
	PPC_STORE_U32(r31.u32 + 268, r30.u32);
loc_82604D30:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82604d54
	if (!cr6.eq) goto loc_82604D54;
	// lwz r3,3052(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3052);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604d54
	if (cr6.eq) goto loc_82604D54;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3052(r31)
	PPC_STORE_U32(r31.u32 + 3052, r30.u32);
loc_82604D54:
	// lwz r11,14788(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82604dc0
	if (cr6.eq) goto loc_82604DC0;
	// lwz r3,14808(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14808);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604d78
	if (cr6.eq) goto loc_82604D78;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,14808(r31)
	PPC_STORE_U32(r31.u32 + 14808, r30.u32);
loc_82604D78:
	// lwz r3,14812(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14812);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604d90
	if (cr6.eq) goto loc_82604D90;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,14812(r31)
	PPC_STORE_U32(r31.u32 + 14812, r30.u32);
loc_82604D90:
	// lwz r3,14816(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14816);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604da8
	if (cr6.eq) goto loc_82604DA8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,14816(r31)
	PPC_STORE_U32(r31.u32 + 14816, r30.u32);
loc_82604DA8:
	// lwz r3,3712(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3712);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604dc0
	if (cr6.eq) goto loc_82604DC0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3712(r31)
	PPC_STORE_U32(r31.u32 + 3712, r30.u32);
loc_82604DC0:
	// lwz r3,1892(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1892);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604dd8
	if (cr6.eq) goto loc_82604DD8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1892(r31)
	PPC_STORE_U32(r31.u32 + 1892, r30.u32);
loc_82604DD8:
	// lwz r3,1896(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1896);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82604df0
	if (cr6.eq) goto loc_82604DF0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1896(r31)
	PPC_STORE_U32(r31.u32 + 1896, r30.u32);
loc_82604DF0:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// stw r30,15768(r31)
	PPC_STORE_U32(r31.u32 + 15768, r30.u32);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// blt cr6,0x82604e24
	if (cr6.lt) goto loc_82604E24;
	// lwz r3,1972(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// bl 0x8265b930
	sub_8265B930(ctx, base);
	// lwz r3,1964(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1964);
	// stw r30,1972(r31)
	PPC_STORE_U32(r31.u32 + 1972, r30.u32);
	// bl 0x82659b90
	sub_82659B90(ctx, base);
	// lwz r3,1968(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1968);
	// stw r30,1964(r31)
	PPC_STORE_U32(r31.u32 + 1964, r30.u32);
	// bl 0x8265a4d8
	sub_8265A4D8(ctx, base);
	// stw r30,1968(r31)
	PPC_STORE_U32(r31.u32 + 1968, r30.u32);
loc_82604E24:
	// addi r4,r31,1988
	ctx.r4.s64 = r31.s64 + 1988;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2000
	ctx.r4.s64 = r31.s64 + 2000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2040
	ctx.r4.s64 = r31.s64 + 2040;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2052
	ctx.r4.s64 = r31.s64 + 2052;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2064
	ctx.r4.s64 = r31.s64 + 2064;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2076
	ctx.r4.s64 = r31.s64 + 2076;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2116
	ctx.r4.s64 = r31.s64 + 2116;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2128
	ctx.r4.s64 = r31.s64 + 2128;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2144
	ctx.r4.s64 = r31.s64 + 2144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2156
	ctx.r4.s64 = r31.s64 + 2156;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2168
	ctx.r4.s64 = r31.s64 + 2168;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2180
	ctx.r4.s64 = r31.s64 + 2180;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2192
	ctx.r4.s64 = r31.s64 + 2192;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2204
	ctx.r4.s64 = r31.s64 + 2204;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2216
	ctx.r4.s64 = r31.s64 + 2216;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2228
	ctx.r4.s64 = r31.s64 + 2228;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2240
	ctx.r4.s64 = r31.s64 + 2240;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2428
	ctx.r4.s64 = r31.s64 + 2428;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2252
	ctx.r4.s64 = r31.s64 + 2252;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2280
	ctx.r4.s64 = r31.s64 + 2280;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2292
	ctx.r4.s64 = r31.s64 + 2292;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2304
	ctx.r4.s64 = r31.s64 + 2304;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2316
	ctx.r4.s64 = r31.s64 + 2316;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2328
	ctx.r4.s64 = r31.s64 + 2328;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2340
	ctx.r4.s64 = r31.s64 + 2340;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2352
	ctx.r4.s64 = r31.s64 + 2352;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2364
	ctx.r4.s64 = r31.s64 + 2364;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2440
	ctx.r4.s64 = r31.s64 + 2440;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2452
	ctx.r4.s64 = r31.s64 + 2452;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2464
	ctx.r4.s64 = r31.s64 + 2464;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2480
	ctx.r4.s64 = r31.s64 + 2480;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2492
	ctx.r4.s64 = r31.s64 + 2492;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2504
	ctx.r4.s64 = r31.s64 + 2504;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2520
	ctx.r4.s64 = r31.s64 + 2520;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2532
	ctx.r4.s64 = r31.s64 + 2532;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,2544
	ctx.r4.s64 = r31.s64 + 2544;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x82605280
	if (!cr6.eq) goto loc_82605280;
	// addi r4,r31,20084
	ctx.r4.s64 = r31.s64 + 20084;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20096
	ctx.r4.s64 = r31.s64 + 20096;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20108
	ctx.r4.s64 = r31.s64 + 20108;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20120
	ctx.r4.s64 = r31.s64 + 20120;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20148
	ctx.r4.s64 = r31.s64 + 20148;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20160
	ctx.r4.s64 = r31.s64 + 20160;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20172
	ctx.r4.s64 = r31.s64 + 20172;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20184
	ctx.r4.s64 = r31.s64 + 20184;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20304
	ctx.r4.s64 = r31.s64 + 20304;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20316
	ctx.r4.s64 = r31.s64 + 20316;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20328
	ctx.r4.s64 = r31.s64 + 20328;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20340
	ctx.r4.s64 = r31.s64 + 20340;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20352
	ctx.r4.s64 = r31.s64 + 20352;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20364
	ctx.r4.s64 = r31.s64 + 20364;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20376
	ctx.r4.s64 = r31.s64 + 20376;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20388
	ctx.r4.s64 = r31.s64 + 20388;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20400
	ctx.r4.s64 = r31.s64 + 20400;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20412
	ctx.r4.s64 = r31.s64 + 20412;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20424
	ctx.r4.s64 = r31.s64 + 20424;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20436
	ctx.r4.s64 = r31.s64 + 20436;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20448
	ctx.r4.s64 = r31.s64 + 20448;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20460
	ctx.r4.s64 = r31.s64 + 20460;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20472
	ctx.r4.s64 = r31.s64 + 20472;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20484
	ctx.r4.s64 = r31.s64 + 20484;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20496
	ctx.r4.s64 = r31.s64 + 20496;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20508
	ctx.r4.s64 = r31.s64 + 20508;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20520
	ctx.r4.s64 = r31.s64 + 20520;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20532
	ctx.r4.s64 = r31.s64 + 20532;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20544
	ctx.r4.s64 = r31.s64 + 20544;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20556
	ctx.r4.s64 = r31.s64 + 20556;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20568
	ctx.r4.s64 = r31.s64 + 20568;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20580
	ctx.r4.s64 = r31.s64 + 20580;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20592
	ctx.r4.s64 = r31.s64 + 20592;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20604
	ctx.r4.s64 = r31.s64 + 20604;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20616
	ctx.r4.s64 = r31.s64 + 20616;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20628
	ctx.r4.s64 = r31.s64 + 20628;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20640
	ctx.r4.s64 = r31.s64 + 20640;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20652
	ctx.r4.s64 = r31.s64 + 20652;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20664
	ctx.r4.s64 = r31.s64 + 20664;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20676
	ctx.r4.s64 = r31.s64 + 20676;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20688
	ctx.r4.s64 = r31.s64 + 20688;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20700
	ctx.r4.s64 = r31.s64 + 20700;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20712
	ctx.r4.s64 = r31.s64 + 20712;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20724
	ctx.r4.s64 = r31.s64 + 20724;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20736
	ctx.r4.s64 = r31.s64 + 20736;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20748
	ctx.r4.s64 = r31.s64 + 20748;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20760
	ctx.r4.s64 = r31.s64 + 20760;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20772
	ctx.r4.s64 = r31.s64 + 20772;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20784
	ctx.r4.s64 = r31.s64 + 20784;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20796
	ctx.r4.s64 = r31.s64 + 20796;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20808
	ctx.r4.s64 = r31.s64 + 20808;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// addi r4,r31,20820
	ctx.r4.s64 = r31.s64 + 20820;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655bc0
	sub_82655BC0(ctx, base);
	// lwz r3,21316(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21316);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605268
	if (cr6.eq) goto loc_82605268;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21316(r31)
	PPC_STORE_U32(r31.u32 + 21316, r30.u32);
loc_82605268:
	// lwz r3,21320(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21320);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605280
	if (cr6.eq) goto loc_82605280;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21320(r31)
	PPC_STORE_U32(r31.u32 + 21320, r30.u32);
loc_82605280:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82648990
	sub_82648990(ctx, base);
	// addi r28,r31,2800
	r28.s64 = r31.s64 + 2800;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826052a4
	if (cr6.eq) goto loc_826052A4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r30.u32);
loc_826052A4:
	// addi r28,r31,2840
	r28.s64 = r31.s64 + 2840;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826052c0
	if (cr6.eq) goto loc_826052C0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r30.u32);
loc_826052C0:
	// addi r28,r31,2760
	r28.s64 = r31.s64 + 2760;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826052dc
	if (cr6.eq) goto loc_826052DC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r30.u32);
loc_826052DC:
	// addi r28,r31,2720
	r28.s64 = r31.s64 + 2720;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826052f8
	if (cr6.eq) goto loc_826052F8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r30.u32);
loc_826052F8:
	// addi r28,r31,2680
	r28.s64 = r31.s64 + 2680;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605314
	if (cr6.eq) goto loc_82605314;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r30.u32);
loc_82605314:
	// addi r28,r31,2640
	r28.s64 = r31.s64 + 2640;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605330
	if (cr6.eq) goto loc_82605330;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r30.u32);
loc_82605330:
	// addi r28,r31,2600
	r28.s64 = r31.s64 + 2600;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260534c
	if (cr6.eq) goto loc_8260534C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r30.u32);
loc_8260534C:
	// addi r28,r31,2560
	r28.s64 = r31.s64 + 2560;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605368
	if (cr6.eq) goto loc_82605368;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r30.u32);
loc_82605368:
	// lwz r3,352(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605380
	if (cr6.eq) goto loc_82605380;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,352(r31)
	PPC_STORE_U32(r31.u32 + 352, r30.u32);
loc_82605380:
	// lwz r3,356(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 356);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605398
	if (cr6.eq) goto loc_82605398;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,356(r31)
	PPC_STORE_U32(r31.u32 + 356, r30.u32);
loc_82605398:
	// lwz r3,360(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 360);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826053b0
	if (cr6.eq) goto loc_826053B0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,360(r31)
	PPC_STORE_U32(r31.u32 + 360, r30.u32);
loc_826053B0:
	// lwz r3,364(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 364);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826053c8
	if (cr6.eq) goto loc_826053C8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,364(r31)
	PPC_STORE_U32(r31.u32 + 364, r30.u32);
loc_826053C8:
	// lwz r3,368(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 368);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826053e0
	if (cr6.eq) goto loc_826053E0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,368(r31)
	PPC_STORE_U32(r31.u32 + 368, r30.u32);
loc_826053E0:
	// lwz r3,372(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 372);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826053f8
	if (cr6.eq) goto loc_826053F8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,372(r31)
	PPC_STORE_U32(r31.u32 + 372, r30.u32);
loc_826053F8:
	// lwz r3,384(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 384);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605410
	if (cr6.eq) goto loc_82605410;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r30.u32);
loc_82605410:
	// lwz r3,388(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 388);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605428
	if (cr6.eq) goto loc_82605428;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,388(r31)
	PPC_STORE_U32(r31.u32 + 388, r30.u32);
loc_82605428:
	// lwz r3,15216(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15216);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605440
	if (cr6.eq) goto loc_82605440;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15216(r31)
	PPC_STORE_U32(r31.u32 + 15216, r30.u32);
loc_82605440:
	// lwz r11,3956(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3956);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826054d0
	if (cr6.eq) goto loc_826054D0;
	// lwz r3,460(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605464
	if (cr6.eq) goto loc_82605464;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,460(r31)
	PPC_STORE_U32(r31.u32 + 460, r30.u32);
loc_82605464:
	// lwz r3,15184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15184);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260547c
	if (cr6.eq) goto loc_8260547C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15184(r31)
	PPC_STORE_U32(r31.u32 + 15184, r30.u32);
loc_8260547C:
	// lwz r3,2976(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 2976);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605494
	if (cr6.eq) goto loc_82605494;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,2976(r31)
	PPC_STORE_U32(r31.u32 + 2976, r30.u32);
loc_82605494:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826054b8
	if (!cr6.eq) goto loc_826054B8;
	// lwz r3,3052(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3052);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826054b8
	if (cr6.eq) goto loc_826054B8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3052(r31)
	PPC_STORE_U32(r31.u32 + 3052, r30.u32);
loc_826054B8:
	// lwz r3,15240(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15240);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826054d0
	if (cr6.eq) goto loc_826054D0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15240(r31)
	PPC_STORE_U32(r31.u32 + 15240, r30.u32);
loc_826054D0:
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826054f8
	if (cr6.eq) goto loc_826054F8;
	// bl 0x8263c590
	sub_8263C590(ctx, base);
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826054f8
	if (cr6.eq) goto loc_826054F8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15204(r31)
	PPC_STORE_U32(r31.u32 + 15204, r30.u32);
loc_826054F8:
	// lwz r3,23252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 23252);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605510
	if (cr6.eq) goto loc_82605510;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,23252(r31)
	PPC_STORE_U32(r31.u32 + 23252, r30.u32);
loc_82605510:
	// lwz r3,21420(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21420);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605528
	if (cr6.eq) goto loc_82605528;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21420(r31)
	PPC_STORE_U32(r31.u32 + 21420, r30.u32);
loc_82605528:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82605534"))) PPC_WEAK_FUNC(sub_82605534);
PPC_FUNC_IMPL(__imp__sub_82605534) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82605538"))) PPC_WEAK_FUNC(sub_82605538);
PPC_FUNC_IMPL(__imp__sub_82605538) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// lwz r3,21564(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21564);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260556c
	if (cr6.eq) goto loc_8260556C;
	// lis r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r5,r5,32768
	ctx.r5.u64 = ctx.r5.u64 | 32768;
	// bl 0x826a8de0
	sub_826A8DE0(ctx, base);
	// stw r30,21564(r31)
	PPC_STORE_U32(r31.u32 + 21564, r30.u32);
loc_8260556C:
	// lwz r3,21568(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21568);
	// lis r11,9356
	r11.s64 = 613154816;
	// ori r29,r11,32769
	r29.u64 = r11.u64 | 32769;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260558c
	if (cr6.eq) goto loc_8260558C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21568(r31)
	PPC_STORE_U32(r31.u32 + 21568, r30.u32);
loc_8260558C:
	// lwz r3,21572(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21572);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826055a4
	if (cr6.eq) goto loc_826055A4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21572(r31)
	PPC_STORE_U32(r31.u32 + 21572, r30.u32);
loc_826055A4:
	// lwz r3,21576(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21576);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826055bc
	if (cr6.eq) goto loc_826055BC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21576(r31)
	PPC_STORE_U32(r31.u32 + 21576, r30.u32);
loc_826055BC:
	// lis r11,1
	r11.s64 = 65536;
	// ori r11,r11,33704
	r11.u64 = r11.u64 | 33704;
	// lwzx r11,r31,r11
	r11.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826056cc
	if (!cr6.eq) goto loc_826056CC;
	// lwz r3,3688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826055f0
	if (cr6.eq) goto loc_826055F0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3688(r31)
	PPC_STORE_U32(r31.u32 + 3688, r30.u32);
loc_826055F0:
	// lwz r3,3696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3696);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3696);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605610
	if (cr6.eq) goto loc_82605610;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3696(r31)
	PPC_STORE_U32(r31.u32 + 3696, r30.u32);
loc_82605610:
	// lwz r3,3692(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3692);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3692(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3692);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605630
	if (cr6.eq) goto loc_82605630;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3692(r31)
	PPC_STORE_U32(r31.u32 + 3692, r30.u32);
loc_82605630:
	// lwz r3,3700(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3700);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3700(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3700);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605650
	if (cr6.eq) goto loc_82605650;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3700(r31)
	PPC_STORE_U32(r31.u32 + 3700, r30.u32);
loc_82605650:
	// lwz r3,3704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605670
	if (cr6.eq) goto loc_82605670;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3704(r31)
	PPC_STORE_U32(r31.u32 + 3704, r30.u32);
loc_82605670:
	// lwz r3,3708(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,3708(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3708);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605690
	if (cr6.eq) goto loc_82605690;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3708(r31)
	PPC_STORE_U32(r31.u32 + 3708, r30.u32);
loc_82605690:
	// lwz r11,21580(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21580);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826056cc
	if (cr6.eq) goto loc_826056CC;
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15204);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// bl 0x8263c6d8
	sub_8263C6D8(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x826046a8
	sub_826046A8(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826056cc
	if (cr6.eq) goto loc_826056CC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
loc_826056CC:
	// lwz r3,21240(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21240);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826056e4
	if (cr6.eq) goto loc_826056E4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21240(r31)
	PPC_STORE_U32(r31.u32 + 21240, r30.u32);
loc_826056E4:
	// lwz r3,21268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826056fc
	if (cr6.eq) goto loc_826056FC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21268(r31)
	PPC_STORE_U32(r31.u32 + 21268, r30.u32);
loc_826056FC:
	// lwz r3,21252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21252);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605714
	if (cr6.eq) goto loc_82605714;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21252(r31)
	PPC_STORE_U32(r31.u32 + 21252, r30.u32);
loc_82605714:
	// lwz r3,3916(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3916);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260572c
	if (cr6.eq) goto loc_8260572C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3916(r31)
	PPC_STORE_U32(r31.u32 + 3916, r30.u32);
loc_8260572C:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x82605780
	if (!cr6.eq) goto loc_82605780;
	// lwz r3,19992(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 19992);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605750
	if (cr6.eq) goto loc_82605750;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,19992(r31)
	PPC_STORE_U32(r31.u32 + 19992, r30.u32);
loc_82605750:
	// lwz r3,19996(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 19996);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605768
	if (cr6.eq) goto loc_82605768;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,19996(r31)
	PPC_STORE_U32(r31.u32 + 19996, r30.u32);
loc_82605768:
	// lwz r3,20000(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20000);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605780
	if (cr6.eq) goto loc_82605780;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,20000(r31)
	PPC_STORE_U32(r31.u32 + 20000, r30.u32);
loc_82605780:
	// lwz r11,3956(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3956);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826059f4
	if (cr6.eq) goto loc_826059F4;
	// lwz r3,460(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826057a4
	if (cr6.eq) goto loc_826057A4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,460(r31)
	PPC_STORE_U32(r31.u32 + 460, r30.u32);
loc_826057A4:
	// lwz r3,15184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15184);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826057bc
	if (cr6.eq) goto loc_826057BC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15184(r31)
	PPC_STORE_U32(r31.u32 + 15184, r30.u32);
loc_826057BC:
	// lwz r3,2976(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 2976);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826057d4
	if (cr6.eq) goto loc_826057D4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,2976(r31)
	PPC_STORE_U32(r31.u32 + 2976, r30.u32);
loc_826057D4:
	// lwz r11,23968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23968);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826058c8
	if (cr6.eq) goto loc_826058C8;
	// lwz r11,16472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x8260587c
	if (!cr6.eq) goto loc_8260587C;
	// lwz r3,3048(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3048);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605804
	if (cr6.eq) goto loc_82605804;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3048(r31)
	PPC_STORE_U32(r31.u32 + 3048, r30.u32);
loc_82605804:
	// lwz r3,15268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15268);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260581c
	if (cr6.eq) goto loc_8260581C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15268(r31)
	PPC_STORE_U32(r31.u32 + 15268, r30.u32);
loc_8260581C:
	// lwz r3,15292(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15292);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605834
	if (cr6.eq) goto loc_82605834;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15292(r31)
	PPC_STORE_U32(r31.u32 + 15292, r30.u32);
loc_82605834:
	// lwz r3,15208(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15208);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260584c
	if (cr6.eq) goto loc_8260584C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15208(r31)
	PPC_STORE_U32(r31.u32 + 15208, r30.u32);
loc_8260584C:
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605864
	if (cr6.eq) goto loc_82605864;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, r30.u32);
loc_82605864:
	// lwz r3,1780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605894
	if (cr6.eq) goto loc_82605894;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// b 0x82605890
	goto loc_82605890;
loc_8260587C:
	// stw r30,3048(r31)
	PPC_STORE_U32(r31.u32 + 3048, r30.u32);
	// stw r30,15268(r31)
	PPC_STORE_U32(r31.u32 + 15268, r30.u32);
	// stw r30,15292(r31)
	PPC_STORE_U32(r31.u32 + 15292, r30.u32);
	// stw r30,15208(r31)
	PPC_STORE_U32(r31.u32 + 15208, r30.u32);
	// stw r30,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, r30.u32);
loc_82605890:
	// stw r30,1780(r31)
	PPC_STORE_U32(r31.u32 + 1780, r30.u32);
loc_82605894:
	// lwz r3,15276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15276);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826058ac
	if (cr6.eq) goto loc_826058AC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15276(r31)
	PPC_STORE_U32(r31.u32 + 15276, r30.u32);
loc_826058AC:
	// lwz r3,15284(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15284);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605988
	if (cr6.eq) goto loc_82605988;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15284(r31)
	PPC_STORE_U32(r31.u32 + 15284, r30.u32);
	// b 0x82605988
	goto loc_82605988;
loc_826058C8:
	// lwz r3,15208(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15208);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826058e0
	if (cr6.eq) goto loc_826058E0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15208(r31)
	PPC_STORE_U32(r31.u32 + 15208, r30.u32);
loc_826058E0:
	// lwz r3,3048(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3048);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826058f8
	if (cr6.eq) goto loc_826058F8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3048(r31)
	PPC_STORE_U32(r31.u32 + 3048, r30.u32);
loc_826058F8:
	// lwz r3,15268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15268);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605910
	if (cr6.eq) goto loc_82605910;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15268(r31)
	PPC_STORE_U32(r31.u32 + 15268, r30.u32);
loc_82605910:
	// lwz r3,15292(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15292);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605928
	if (cr6.eq) goto loc_82605928;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15292(r31)
	PPC_STORE_U32(r31.u32 + 15292, r30.u32);
loc_82605928:
	// lwz r3,15276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15276);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605940
	if (cr6.eq) goto loc_82605940;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15276(r31)
	PPC_STORE_U32(r31.u32 + 15276, r30.u32);
loc_82605940:
	// lwz r3,15284(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15284);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605958
	if (cr6.eq) goto loc_82605958;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15284(r31)
	PPC_STORE_U32(r31.u32 + 15284, r30.u32);
loc_82605958:
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605970
	if (cr6.eq) goto loc_82605970;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, r30.u32);
loc_82605970:
	// lwz r3,1780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605988
	if (cr6.eq) goto loc_82605988;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1780(r31)
	PPC_STORE_U32(r31.u32 + 1780, r30.u32);
loc_82605988:
	// lwz r3,276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 276);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826059a0
	if (cr6.eq) goto loc_826059A0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,276(r31)
	PPC_STORE_U32(r31.u32 + 276, r30.u32);
loc_826059A0:
	// lwz r3,268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826059b8
	if (cr6.eq) goto loc_826059B8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,268(r31)
	PPC_STORE_U32(r31.u32 + 268, r30.u32);
loc_826059B8:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826059dc
	if (!cr6.eq) goto loc_826059DC;
	// lwz r3,3052(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3052);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826059dc
	if (cr6.eq) goto loc_826059DC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,3052(r31)
	PPC_STORE_U32(r31.u32 + 3052, r30.u32);
loc_826059DC:
	// lwz r3,15240(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15240);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826059f4
	if (cr6.eq) goto loc_826059F4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15240(r31)
	PPC_STORE_U32(r31.u32 + 15240, r30.u32);
loc_826059F4:
	// lwz r11,14788(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82605a48
	if (cr6.eq) goto loc_82605A48;
	// lwz r3,14808(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14808);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605a18
	if (cr6.eq) goto loc_82605A18;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,14808(r31)
	PPC_STORE_U32(r31.u32 + 14808, r30.u32);
loc_82605A18:
	// lwz r3,14812(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14812);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605a30
	if (cr6.eq) goto loc_82605A30;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,14812(r31)
	PPC_STORE_U32(r31.u32 + 14812, r30.u32);
loc_82605A30:
	// lwz r3,14816(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14816);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605a48
	if (cr6.eq) goto loc_82605A48;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,14816(r31)
	PPC_STORE_U32(r31.u32 + 14816, r30.u32);
loc_82605A48:
	// lwz r3,264(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 264);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605a60
	if (cr6.eq) goto loc_82605A60;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,264(r31)
	PPC_STORE_U32(r31.u32 + 264, r30.u32);
loc_82605A60:
	// lwz r3,272(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 272);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605a78
	if (cr6.eq) goto loc_82605A78;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,272(r31)
	PPC_STORE_U32(r31.u32 + 272, r30.u32);
loc_82605A78:
	// lwz r3,1892(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1892);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605a90
	if (cr6.eq) goto loc_82605A90;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1892(r31)
	PPC_STORE_U32(r31.u32 + 1892, r30.u32);
loc_82605A90:
	// lwz r3,1896(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1896);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605aa8
	if (cr6.eq) goto loc_82605AA8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,1896(r31)
	PPC_STORE_U32(r31.u32 + 1896, r30.u32);
loc_82605AA8:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// blt cr6,0x82605abc
	if (cr6.lt) goto loc_82605ABC;
	// lwz r3,1968(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1968);
	// bl 0x8265a4d8
	sub_8265A4D8(ctx, base);
loc_82605ABC:
	// lwz r3,15656(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15656);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605ad4
	if (cr6.eq) goto loc_82605AD4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15656(r31)
	PPC_STORE_U32(r31.u32 + 15656, r30.u32);
loc_82605AD4:
	// lwz r3,15664(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15664);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605aec
	if (cr6.eq) goto loc_82605AEC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15664(r31)
	PPC_STORE_U32(r31.u32 + 15664, r30.u32);
loc_82605AEC:
	// lwz r3,15660(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15660);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605b04
	if (cr6.eq) goto loc_82605B04;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15660(r31)
	PPC_STORE_U32(r31.u32 + 15660, r30.u32);
loc_82605B04:
	// lwz r3,15668(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15668);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605b1c
	if (cr6.eq) goto loc_82605B1C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15668(r31)
	PPC_STORE_U32(r31.u32 + 15668, r30.u32);
loc_82605B1C:
	// lwz r11,23968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23968);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82605b64
	if (cr6.eq) goto loc_82605B64;
	// lwz r11,16472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x82605b64
	if (cr6.eq) goto loc_82605B64;
	// stw r30,15672(r31)
	PPC_STORE_U32(r31.u32 + 15672, r30.u32);
	// stw r30,15680(r31)
	PPC_STORE_U32(r31.u32 + 15680, r30.u32);
	// stw r30,15688(r31)
	PPC_STORE_U32(r31.u32 + 15688, r30.u32);
	// stw r30,15696(r31)
	PPC_STORE_U32(r31.u32 + 15696, r30.u32);
	// stw r30,15704(r31)
	PPC_STORE_U32(r31.u32 + 15704, r30.u32);
	// stw r30,15712(r31)
	PPC_STORE_U32(r31.u32 + 15712, r30.u32);
	// stw r30,15720(r31)
	PPC_STORE_U32(r31.u32 + 15720, r30.u32);
	// stw r30,15728(r31)
	PPC_STORE_U32(r31.u32 + 15728, r30.u32);
	// stw r30,15736(r31)
	PPC_STORE_U32(r31.u32 + 15736, r30.u32);
	// stw r30,15744(r31)
	PPC_STORE_U32(r31.u32 + 15744, r30.u32);
	// stw r30,15752(r31)
	PPC_STORE_U32(r31.u32 + 15752, r30.u32);
	// b 0x82605c80
	goto loc_82605C80;
loc_82605B64:
	// lwz r3,15672(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15672);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605b7c
	if (cr6.eq) goto loc_82605B7C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15672(r31)
	PPC_STORE_U32(r31.u32 + 15672, r30.u32);
loc_82605B7C:
	// lwz r3,15680(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15680);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605b94
	if (cr6.eq) goto loc_82605B94;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15680(r31)
	PPC_STORE_U32(r31.u32 + 15680, r30.u32);
loc_82605B94:
	// lwz r3,15688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15688);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605bac
	if (cr6.eq) goto loc_82605BAC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15688(r31)
	PPC_STORE_U32(r31.u32 + 15688, r30.u32);
loc_82605BAC:
	// lwz r3,15696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15696);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605bc4
	if (cr6.eq) goto loc_82605BC4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15696(r31)
	PPC_STORE_U32(r31.u32 + 15696, r30.u32);
loc_82605BC4:
	// lwz r3,15704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15704);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605bdc
	if (cr6.eq) goto loc_82605BDC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15704(r31)
	PPC_STORE_U32(r31.u32 + 15704, r30.u32);
loc_82605BDC:
	// lwz r3,15712(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15712);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605bf4
	if (cr6.eq) goto loc_82605BF4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15712(r31)
	PPC_STORE_U32(r31.u32 + 15712, r30.u32);
loc_82605BF4:
	// lwz r3,15720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15720);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605c0c
	if (cr6.eq) goto loc_82605C0C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15720(r31)
	PPC_STORE_U32(r31.u32 + 15720, r30.u32);
loc_82605C0C:
	// lwz r3,15728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15728);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605c24
	if (cr6.eq) goto loc_82605C24;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15728(r31)
	PPC_STORE_U32(r31.u32 + 15728, r30.u32);
loc_82605C24:
	// lwz r3,15736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15736);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605c3c
	if (cr6.eq) goto loc_82605C3C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15736(r31)
	PPC_STORE_U32(r31.u32 + 15736, r30.u32);
loc_82605C3C:
	// lwz r3,15744(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15744);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605c54
	if (cr6.eq) goto loc_82605C54;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15744(r31)
	PPC_STORE_U32(r31.u32 + 15744, r30.u32);
loc_82605C54:
	// lwz r3,15752(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15752);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605c6c
	if (cr6.eq) goto loc_82605C6C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15752(r31)
	PPC_STORE_U32(r31.u32 + 15752, r30.u32);
loc_82605C6C:
	// lwz r3,15760(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15760);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605c84
	if (cr6.eq) goto loc_82605C84;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
loc_82605C80:
	// stw r30,15760(r31)
	PPC_STORE_U32(r31.u32 + 15760, r30.u32);
loc_82605C84:
	// lwz r11,3356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// blt cr6,0x82605db0
	if (cr6.lt) goto loc_82605DB0;
	// lwz r3,15676(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15676);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605ca8
	if (cr6.eq) goto loc_82605CA8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15676(r31)
	PPC_STORE_U32(r31.u32 + 15676, r30.u32);
loc_82605CA8:
	// lwz r3,15684(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15684);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605cc0
	if (cr6.eq) goto loc_82605CC0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15684(r31)
	PPC_STORE_U32(r31.u32 + 15684, r30.u32);
loc_82605CC0:
	// lwz r3,15692(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15692);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605cd8
	if (cr6.eq) goto loc_82605CD8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15692(r31)
	PPC_STORE_U32(r31.u32 + 15692, r30.u32);
loc_82605CD8:
	// lwz r3,15700(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15700);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605cf0
	if (cr6.eq) goto loc_82605CF0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15700(r31)
	PPC_STORE_U32(r31.u32 + 15700, r30.u32);
loc_82605CF0:
	// lwz r3,15708(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15708);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605d08
	if (cr6.eq) goto loc_82605D08;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15708(r31)
	PPC_STORE_U32(r31.u32 + 15708, r30.u32);
loc_82605D08:
	// lwz r3,15716(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15716);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605d20
	if (cr6.eq) goto loc_82605D20;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15716(r31)
	PPC_STORE_U32(r31.u32 + 15716, r30.u32);
loc_82605D20:
	// lwz r3,15724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15724);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605d38
	if (cr6.eq) goto loc_82605D38;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15724(r31)
	PPC_STORE_U32(r31.u32 + 15724, r30.u32);
loc_82605D38:
	// lwz r3,15732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15732);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605d50
	if (cr6.eq) goto loc_82605D50;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15732(r31)
	PPC_STORE_U32(r31.u32 + 15732, r30.u32);
loc_82605D50:
	// lwz r3,15740(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15740);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605d68
	if (cr6.eq) goto loc_82605D68;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15740(r31)
	PPC_STORE_U32(r31.u32 + 15740, r30.u32);
loc_82605D68:
	// lwz r3,15748(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15748);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605d80
	if (cr6.eq) goto loc_82605D80;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15748(r31)
	PPC_STORE_U32(r31.u32 + 15748, r30.u32);
loc_82605D80:
	// lwz r3,15756(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15756);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605d98
	if (cr6.eq) goto loc_82605D98;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15756(r31)
	PPC_STORE_U32(r31.u32 + 15756, r30.u32);
loc_82605D98:
	// lwz r3,15764(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15764);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605db0
	if (cr6.eq) goto loc_82605DB0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,15764(r31)
	PPC_STORE_U32(r31.u32 + 15764, r30.u32);
loc_82605DB0:
	// lwz r3,21420(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21420);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82605dc8
	if (cr6.eq) goto loc_82605DC8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r30,21420(r31)
	PPC_STORE_U32(r31.u32 + 21420, r30.u32);
loc_82605DC8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82605DD0"))) PPC_WEAK_FUNC(sub_82605DD0);
PPC_FUNC_IMPL(__imp__sub_82605DD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r4,15
	r11.s64 = ctx.r4.s64 + 15;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// rlwinm r30,r11,0,0,27
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF0;
	// addi r11,r5,15
	r11.s64 = ctx.r5.s64 + 15;
	// cmpw cr6,r4,r30
	cr6.compare<int32_t>(ctx.r4.s32, r30.s32, xer);
	// rlwinm r29,r11,0,0,27
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF0;
	// lwz r11,21500(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21500);
	// lwz r10,21504(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21504);
	// lwz r9,21508(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21508);
	// lwz r8,21512(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 21512);
	// stw r4,156(r31)
	PPC_STORE_U32(r31.u32 + 156, ctx.r4.u32);
	// stw r5,160(r31)
	PPC_STORE_U32(r31.u32 + 160, ctx.r5.u32);
	// stw r11,21484(r31)
	PPC_STORE_U32(r31.u32 + 21484, r11.u32);
	// stw r10,21488(r31)
	PPC_STORE_U32(r31.u32 + 21488, ctx.r10.u32);
	// stw r9,21492(r31)
	PPC_STORE_U32(r31.u32 + 21492, ctx.r9.u32);
	// stw r8,21496(r31)
	PPC_STORE_U32(r31.u32 + 21496, ctx.r8.u32);
	// bne cr6,0x82605e2c
	if (!cr6.eq) goto loc_82605E2C;
	// cmpw cr6,r5,r29
	cr6.compare<int32_t>(ctx.r5.s32, r29.s32, xer);
	// li r11,1
	r11.s64 = 1;
	// beq cr6,0x82605e30
	if (cr6.eq) goto loc_82605E30;
loc_82605E2C:
	// li r11,0
	r11.s64 = 0;
loc_82605E30:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r11.u32);
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82605e54
	if (!cr6.eq) goto loc_82605E54;
	// lwz r11,180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// stw r11,21380(r31)
	PPC_STORE_U32(r31.u32 + 21380, r11.u32);
	// stw r10,21384(r31)
	PPC_STORE_U32(r31.u32 + 21384, ctx.r10.u32);
loc_82605E54:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r30,180(r31)
	PPC_STORE_U32(r31.u32 + 180, r30.u32);
	// stw r29,188(r31)
	PPC_STORE_U32(r31.u32 + 188, r29.u32);
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82605e7c
	if (cr6.eq) goto loc_82605E7C;
	// lwz r11,180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// stw r11,21380(r31)
	PPC_STORE_U32(r31.u32 + 21380, r11.u32);
	// stw r10,21384(r31)
	PPC_STORE_U32(r31.u32 + 21384, ctx.r10.u32);
loc_82605E7C:
	// li r11,-63
	r11.s64 = -63;
	// lwz r10,3924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// li r9,63
	ctx.r9.s64 = 63;
	// stw r30,21500(r31)
	PPC_STORE_U32(r31.u32 + 21500, r30.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r29,21504(r31)
	PPC_STORE_U32(r31.u32 + 21504, r29.u32);
	// stw r11,240(r31)
	PPC_STORE_U32(r31.u32 + 240, r11.u32);
	// stw r9,244(r31)
	PPC_STORE_U32(r31.u32 + 244, ctx.r9.u32);
	// beq cr6,0x82605ec0
	if (cr6.eq) goto loc_82605EC0;
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r11,188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// stw r11,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r11.u32);
	// stw r10,192(r31)
	PPC_STORE_U32(r31.u32 + 192, ctx.r10.u32);
	// stw r10,21508(r31)
	PPC_STORE_U32(r31.u32 + 21508, ctx.r10.u32);
	// stw r11,21512(r31)
	PPC_STORE_U32(r31.u32 + 21512, r11.u32);
	// b 0x82605ee0
	goto loc_82605EE0;
loc_82605EC0:
	// lwz r11,180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stw r11,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r11.u32);
	// stw r11,21508(r31)
	PPC_STORE_U32(r31.u32 + 21508, r11.u32);
	// stw r10,200(r31)
	PPC_STORE_U32(r31.u32 + 200, ctx.r10.u32);
	// stw r10,21512(r31)
	PPC_STORE_U32(r31.u32 + 21512, ctx.r10.u32);
loc_82605EE0:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bge cr6,0x82605f04
	if (!cr6.lt) goto loc_82605F04;
	// lwz r11,21508(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21508);
	// lwz r10,21512(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21512);
	// stw r30,21484(r31)
	PPC_STORE_U32(r31.u32 + 21484, r30.u32);
	// stw r29,21488(r31)
	PPC_STORE_U32(r31.u32 + 21488, r29.u32);
	// stw r11,21492(r31)
	PPC_STORE_U32(r31.u32 + 21492, r11.u32);
	// stw r10,21496(r31)
	PPC_STORE_U32(r31.u32 + 21496, ctx.r10.u32);
loc_82605F04:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826037e8
	sub_826037E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82648c58
	sub_82648C58(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82605F20"))) PPC_WEAK_FUNC(sub_82605F20);
PPC_FUNC_IMPL(__imp__sub_82605F20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,9356
	r11.s64 = 613154816;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// ori r29,r11,32769
	r29.u64 = r11.u64 | 32769;
	// li r3,16
	ctx.r3.s64 = 16;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r11,15204(r31)
	PPC_STORE_U32(r31.u32 + 15204, r11.u32);
	// beq cr6,0x82606158
	if (cr6.eq) goto loc_82606158;
	// li r30,0
	r30.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r3,32
	ctx.r3.s64 = 32;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// stw r30,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r30.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r3,32
	ctx.r3.s64 = 32;
	// stw r11,1900(r31)
	PPC_STORE_U32(r31.u32 + 1900, r11.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r3,32
	ctx.r3.s64 = 32;
	// stw r11,1904(r31)
	PPC_STORE_U32(r31.u32 + 1904, r11.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r3,32
	ctx.r3.s64 = 32;
	// stw r11,1908(r31)
	PPC_STORE_U32(r31.u32 + 1908, r11.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r3,1900(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1900);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r11,1912(r31)
	PPC_STORE_U32(r31.u32 + 1912, r11.u32);
	// beq cr6,0x82606158
	if (cr6.eq) goto loc_82606158;
	// lwz r10,1904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1904);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82606158
	if (cr6.eq) goto loc_82606158;
	// lwz r10,1908(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1908);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82606158
	if (cr6.eq) goto loc_82606158;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82606158
	if (cr6.eq) goto loc_82606158;
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,1904(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1904);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,1908(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1908);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,1912(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1912);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// lwz r10,1900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1900);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8260605c
	if (!cr6.eq) goto loc_8260605C;
	// li r11,1024
	r11.s64 = 1024;
	// li r9,1
	ctx.r9.s64 = 1;
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, r11.u16);
	// lwz r10,1900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1900);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// lwz r10,1904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1904);
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, r11.u16);
	// lwz r10,1904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1904);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// stw r9,3900(r31)
	PPC_STORE_U32(r31.u32 + 3900, ctx.r9.u32);
	// b 0x82606080
	goto loc_82606080;
loc_8260605C:
	// li r11,128
	r11.s64 = 128;
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, r11.u16);
	// lwz r10,1900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1900);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// lwz r10,1904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1904);
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, r11.u16);
	// lwz r10,1904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1904);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// stw r30,3900(r31)
	PPC_STORE_U32(r31.u32 + 3900, r30.u32);
loc_82606080:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// blt cr6,0x826060b4
	if (cr6.lt) goto loc_826060B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,3340(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// bl 0x8265b810
	sub_8265B810(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1972(r31)
	PPC_STORE_U32(r31.u32 + 1972, ctx.r3.u32);
	// beq cr6,0x82606158
	if (cr6.eq) goto loc_82606158;
	// bl 0x8265a768
	sub_8265A768(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1964(r31)
	PPC_STORE_U32(r31.u32 + 1964, ctx.r3.u32);
	// beq cr6,0x82606158
	if (cr6.eq) goto loc_82606158;
loc_826060B4:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x82606108
	if (cr6.lt) goto loc_82606108;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826060e4
	if (!cr6.eq) goto loc_826060E4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,352(r31)
	PPC_STORE_U32(r31.u32 + 352, ctx.r3.u32);
	// beq cr6,0x82606158
	if (cr6.eq) goto loc_82606158;
loc_826060E4:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r3,832
	ctx.r3.s64 = 832;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15216(r31)
	PPC_STORE_U32(r31.u32 + 15216, ctx.r3.u32);
	// beq cr6,0x82606158
	if (cr6.eq) goto loc_82606158;
	// addi r11,r3,31
	r11.s64 = ctx.r3.s64 + 31;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// stw r11,15220(r31)
	PPC_STORE_U32(r31.u32 + 15220, r11.u32);
loc_82606108:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r3,1568
	ctx.r3.s64 = 1568;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1880(r31)
	PPC_STORE_U32(r31.u32 + 1880, ctx.r3.u32);
	// beq cr6,0x82606158
	if (cr6.eq) goto loc_82606158;
	// addi r11,r3,31
	r11.s64 = ctx.r3.s64 + 31;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// li r3,832
	ctx.r3.s64 = 832;
	// stw r11,1884(r31)
	PPC_STORE_U32(r31.u32 + 1884, r11.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r10,r11,60
	ctx.r10.s64 = r11.s64 + 60;
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// stw r11,1872(r31)
	PPC_STORE_U32(r31.u32 + 1872, r11.u32);
	// stw r10,1876(r31)
	PPC_STORE_U32(r31.u32 + 1876, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_82606158:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82606164"))) PPC_WEAK_FUNC(sub_82606164);
PPC_FUNC_IMPL(__imp__sub_82606164) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82606168"))) PPC_WEAK_FUNC(sub_82606168);
PPC_FUNC_IMPL(__imp__sub_82606168) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// addi r11,r28,15
	r11.s64 = r28.s64 + 15;
	// addi r10,r25,15
	ctx.r10.s64 = r25.s64 + 15;
	// rlwinm r11,r11,0,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF0;
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// srawi r24,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r24.s64 = r11.s32 >> 4;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// srawi r22,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	r22.s64 = ctx.r10.s32 >> 4;
	// mullw r26,r22,r24
	r26.s64 = int64_t(r22.s32) * int64_t(r24.s32);
	// stw r11,180(r31)
	PPC_STORE_U32(r31.u32 + 180, r11.u32);
	// stw r10,188(r31)
	PPC_STORE_U32(r31.u32 + 188, ctx.r10.u32);
	// stw r24,136(r31)
	PPC_STORE_U32(r31.u32 + 136, r24.u32);
	// stw r22,140(r31)
	PPC_STORE_U32(r31.u32 + 140, r22.u32);
	// bl 0x82648cf8
	sub_82648CF8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82606920
	if (!cr6.eq) goto loc_82606920;
	// lis r11,1
	r11.s64 = 65536;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// ori r9,r11,33704
	ctx.r9.u64 = r11.u64 | 33704;
	// li r11,32
	r11.s64 = 32;
	// stw r11,13632(r10)
	PPC_STORE_U32(ctx.r10.u32 + 13632, r11.u32);
	// li r11,16
	r11.s64 = 16;
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// stw r11,3380(r10)
	PPC_STORE_U32(ctx.r10.u32 + 3380, r11.u32);
	// lwzx r11,r31,r9
	r11.u64 = PPC_LOAD_U32(r31.u32 + ctx.r9.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82606200
	if (cr6.eq) goto loc_82606200;
	// lis r11,1
	r11.s64 = 65536;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// ori r11,r11,33696
	r11.u64 = r11.u64 | 33696;
	// ori r10,r10,33700
	ctx.r10.u64 = ctx.r10.u64 | 33700;
	// stwx r28,r31,r11
	PPC_STORE_U32(r31.u32 + r11.u32, r28.u32);
	// stwx r25,r31,r10
	PPC_STORE_U32(r31.u32 + ctx.r10.u32, r25.u32);
	// b 0x8260621c
	goto loc_8260621C;
loc_82606200:
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82601e58
	sub_82601E58(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82606920
	if (!cr6.eq) goto loc_82606920;
loc_8260621C:
	// lis r11,9356
	r11.s64 = 613154816;
	// addi r23,r22,2
	r23.s64 = r22.s64 + 2;
	// ori r30,r11,32769
	r30.u64 = r11.u64 | 32769;
	// rlwinm r29,r23,2,0,29
	r29.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r11,21240(r31)
	PPC_STORE_U32(r31.u32 + 21240, r11.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// li r27,0
	r27.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21268(r31)
	PPC_STORE_U32(r31.u32 + 21268, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,21268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,21264(r31)
	PPC_STORE_U32(r31.u32 + 21264, r11.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21252(r31)
	PPC_STORE_U32(r31.u32 + 21252, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x82606330
	if (!cr6.eq) goto loc_82606330;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r3,r24,9,0,22
	ctx.r3.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 9) & 0xFFFFFE00;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// rlwinm r29,r24,7,0,24
	r29.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 7) & 0xFFFFFF80;
	// stw r3,19992(r31)
	PPC_STORE_U32(r31.u32 + 19992, ctx.r3.u32);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,19996(r31)
	PPC_STORE_U32(r31.u32 + 19996, r11.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// stw r3,20000(r31)
	PPC_STORE_U32(r31.u32 + 20000, ctx.r3.u32);
	// lwz r3,21636(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21636);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826062fc
	if (cr6.eq) goto loc_826062FC;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82120e68
	sub_82120E68(ctx, base);
	// stw r27,21636(r31)
	PPC_STORE_U32(r31.u32 + 21636, r27.u32);
loc_826062FC:
	// addi r11,r22,1
	r11.s64 = r22.s64 + 1;
	// addi r10,r24,1
	ctx.r10.s64 = r24.s64 + 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mullw r3,r11,r10
	ctx.r3.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// stw r3,21636(r31)
	PPC_STORE_U32(r31.u32 + 21636, ctx.r3.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82606330
	if (!cr6.eq) goto loc_82606330;
	// li r3,-3
	ctx.r3.s64 = -3;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_82606330:
	// lwz r11,3956(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3956);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82606624
	if (cr6.eq) goto loc_82606624;
	// lwz r11,3356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// li r11,1536
	r11.s64 = 1536;
	// beq cr6,0x82606350
	if (cr6.eq) goto loc_82606350;
	// li r11,640
	r11.s64 = 640;
loc_82606350:
	// mullw r11,r11,r24
	r11.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,2976(r31)
	PPC_STORE_U32(r31.u32 + 2976, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// lwz r11,3924(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r27,460(r31)
	PPC_STORE_U32(r31.u32 + 460, r27.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260638c
	if (cr6.eq) goto loc_8260638C;
	// mulli r3,r26,224
	ctx.r3.s64 = r26.s64 * 224;
	// b 0x82606398
	goto loc_82606398;
loc_8260638C:
	// rlwinm r11,r26,1,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r26,r11
	r11.u64 = r26.u64 + r11.u64;
	// rlwinm r3,r11,6,0,25
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
loc_82606398:
	// bl 0x82121108
	sub_82121108(ctx, base);
	// rotlwi r11,r3,0
	r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// stw r3,460(r31)
	PPC_STORE_U32(r31.u32 + 460, ctx.r3.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// lwz r11,23968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23968);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826063c4
	if (cr6.eq) goto loc_826063C4;
	// lwz r10,16472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x826063f0
	if (!cr6.eq) goto loc_826063F0;
loc_826063C4:
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// add r11,r26,r11
	r11.u64 = r26.u64 + r11.u64;
	// rlwinm r3,r11,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,3048(r31)
	PPC_STORE_U32(r31.u32 + 3048, ctx.r3.u32);
	// bne cr6,0x826063fc
	if (!cr6.eq) goto loc_826063FC;
loc_826063E4:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_826063F0:
	// lwz r11,16472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// lwz r11,3048(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 3048);
	// stw r11,3048(r31)
	PPC_STORE_U32(r31.u32 + 3048, r11.u32);
loc_826063FC:
	// lwz r11,3356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r10,r11,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r3,r11,5,0,26
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15240(r31)
	PPC_STORE_U32(r31.u32 + 15240, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r27,15184(r31)
	PPC_STORE_U32(r31.u32 + 15184, r27.u32);
	// addi r11,r25,72
	r11.s64 = r25.s64 + 72;
	// add r10,r28,r10
	ctx.r10.u64 = r28.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r3,r10,160
	ctx.r3.s64 = ctx.r10.s64 + 160;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bge cr6,0x8260644c
	if (!cr6.lt) goto loc_8260644C;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
loc_8260644C:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15184(r31)
	PPC_STORE_U32(r31.u32 + 15184, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// addi r11,r3,31
	r11.s64 = ctx.r3.s64 + 31;
	// lwz r10,3924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// lwz r3,460(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,15188(r31)
	PPC_STORE_U32(r31.u32 + 15188, r11.u32);
	// beq cr6,0x82606488
	if (cr6.eq) goto loc_82606488;
	// mulli r5,r26,224
	ctx.r5.s64 = r26.s64 * 224;
	// b 0x82606494
	goto loc_82606494;
loc_82606488:
	// rlwinm r11,r26,1,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r26,r11
	r11.u64 = r26.u64 + r11.u64;
	// rlwinm r5,r11,6,0,25
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
loc_82606494:
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,23968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23968);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82606588
	if (cr6.eq) goto loc_82606588;
	// lwz r10,16472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x82606520
	if (!cr6.eq) goto loc_82606520;
	// rlwinm r28,r26,4,0,27
	r28.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15208(r31)
	PPC_STORE_U32(r31.u32 + 15208, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mullw r11,r23,r24
	r11.s64 = int64_t(r23.s32) * int64_t(r24.s32);
	// rlwinm r29,r11,4,0,27
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15268(r31)
	PPC_STORE_U32(r31.u32 + 15268, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15292(r31)
	PPC_STORE_U32(r31.u32 + 15292, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// rlwinm r11,r26,3,0,28
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,15296(r31)
	PPC_STORE_U32(r31.u32 + 15296, r11.u32);
	// b 0x82606548
	goto loc_82606548;
loc_82606520:
	// lwz r11,16472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// rlwinm r10,r26,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,15208(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 15208);
	// stw r9,15208(r31)
	PPC_STORE_U32(r31.u32 + 15208, ctx.r9.u32);
	// lwz r9,15268(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 15268);
	// stw r9,15268(r31)
	PPC_STORE_U32(r31.u32 + 15268, ctx.r9.u32);
	// lwz r11,15292(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 15292);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,15292(r31)
	PPC_STORE_U32(r31.u32 + 15292, r11.u32);
	// stw r10,15296(r31)
	PPC_STORE_U32(r31.u32 + 15296, ctx.r10.u32);
loc_82606548:
	// rlwinm r29,r26,4,0,27
	r29.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15276(r31)
	PPC_STORE_U32(r31.u32 + 15276, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15284(r31)
	PPC_STORE_U32(r31.u32 + 15284, ctx.r3.u32);
	// bne cr6,0x82606624
	if (!cr6.eq) goto loc_82606624;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_82606588:
	// rlwinm r29,r26,4,0,27
	r29.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15208(r31)
	PPC_STORE_U32(r31.u32 + 15208, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mullw r11,r23,r24
	r11.s64 = int64_t(r23.s32) * int64_t(r24.s32);
	// rlwinm r28,r11,4,0,27
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15268(r31)
	PPC_STORE_U32(r31.u32 + 15268, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15292(r31)
	PPC_STORE_U32(r31.u32 + 15292, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// rlwinm r11,r26,3,0,28
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,15296(r31)
	PPC_STORE_U32(r31.u32 + 15296, r11.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15276(r31)
	PPC_STORE_U32(r31.u32 + 15276, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15284(r31)
	PPC_STORE_U32(r31.u32 + 15284, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
loc_82606624:
	// add r11,r26,r24
	r11.u64 = r26.u64 + r24.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r11,3916(r31)
	PPC_STORE_U32(r31.u32 + 3916, r11.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r11,3920(r31)
	PPC_STORE_U32(r31.u32 + 3920, r11.u32);
	// rlwinm r3,r26,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21576(r31)
	PPC_STORE_U32(r31.u32 + 21576, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r3,r24,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,264(r31)
	PPC_STORE_U32(r31.u32 + 264, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// lwz r11,14788(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826066a4
	if (cr6.eq) goto loc_826066A4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r3,r24,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,14808(r31)
	PPC_STORE_U32(r31.u32 + 14808, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
loc_826066A4:
	// addi r11,r22,1
	r11.s64 = r22.s64 + 1;
	// lis r5,8320
	ctx.r5.s64 = 545259520;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// li r6,4
	ctx.r6.s64 = 4;
	// mullw r29,r11,r24
	r29.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// ori r5,r5,4096
	ctx.r5.u64 = ctx.r5.u64 | 4096;
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r27,r11,9,0,22
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0xFFFFFE00;
	// addi r4,r27,256
	ctx.r4.s64 = r27.s64 + 256;
	// bl 0x826a8d90
	sub_826A8D90(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21564(r31)
	PPC_STORE_U32(r31.u32 + 21564, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// rlwinm r10,r29,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r3,127
	r11.s64 = ctx.r3.s64 + 127;
	// add r10,r29,r10
	ctx.r10.u64 = r29.u64 + ctx.r10.u64;
	// rlwinm r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	// rlwinm r28,r10,4,0,27
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// add r10,r27,r11
	ctx.r10.u64 = r27.u64 + r11.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r10,r10,128
	ctx.r10.s64 = ctx.r10.s64 + 128;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r11,21556(r31)
	PPC_STORE_U32(r31.u32 + 21556, r11.u32);
	// rlwinm r10,r10,0,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFF80;
	// stw r10,21560(r31)
	PPC_STORE_U32(r31.u32 + 21560, ctx.r10.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21568(r31)
	PPC_STORE_U32(r31.u32 + 21568, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r28,4
	ctx.r3.s64 = r28.s64 + 4;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21572(r31)
	PPC_STORE_U32(r31.u32 + 21572, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// rlwinm r3,r11,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,268(r31)
	PPC_STORE_U32(r31.u32 + 268, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// lwz r11,14788(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82606794
	if (cr6.eq) goto loc_82606794;
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// add r11,r26,r11
	r11.u64 = r26.u64 + r11.u64;
	// rlwinm r29,r11,2,0,29
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,276(r31)
	PPC_STORE_U32(r31.u32 + 276, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_82606794:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// li r3,20
	ctx.r3.s64 = 20;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,272(r31)
	PPC_STORE_U32(r31.u32 + 272, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// rlwinm r11,r24,1,0,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// add r10,r24,r11
	ctx.r10.u64 = r24.u64 + r11.u64;
	// rlwinm r11,r24,1,0,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r24,r11
	r11.u64 = r24.u64 + r11.u64;
	// rlwinm r3,r11,8,0,23
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// stw r10,1888(r31)
	PPC_STORE_U32(r31.u32 + 1888, ctx.r10.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1892(r31)
	PPC_STORE_U32(r31.u32 + 1892, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// rlwinm r11,r24,3,0,28
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// add r11,r24,r11
	r11.u64 = r24.u64 + r11.u64;
	// rlwinm r3,r11,5,0,26
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1896(r31)
	PPC_STORE_U32(r31.u32 + 1896, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// blt cr6,0x82606820
	if (cr6.lt) goto loc_82606820;
	// li r4,2
	ctx.r4.s64 = 2;
	// rlwinm r3,r24,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x8265a440
	sub_8265A440(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1968(r31)
	PPC_STORE_U32(r31.u32 + 1968, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
loc_82606820:
	// lwz r11,23968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23968);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826068a4
	if (cr6.eq) goto loc_826068A4;
	// lwz r10,16472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x82606880
	if (!cr6.eq) goto loc_82606880;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r3,r26,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// rlwinm r11,r26,3,0,28
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, ctx.r3.u32);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,1776(r31)
	PPC_STORE_U32(r31.u32 + 1776, r11.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r3,r26,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1780(r31)
	PPC_STORE_U32(r31.u32 + 1780, ctx.r3.u32);
	// bne cr6,0x826068e4
	if (!cr6.eq) goto loc_826068E4;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_82606880:
	// lwz r11,16472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// rlwinm r9,r26,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,1772(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 1772);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r10,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, ctx.r10.u32);
	// stw r9,1776(r31)
	PPC_STORE_U32(r31.u32 + 1776, ctx.r9.u32);
	// lwz r11,1780(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1780);
	// stw r11,1780(r31)
	PPC_STORE_U32(r31.u32 + 1780, r11.u32);
	// b 0x826068e4
	goto loc_826068E4;
loc_826068A4:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r3,r26,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// rlwinm r11,r26,3,0,28
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// rotlwi r10,r3,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// stw r3,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, ctx.r3.u32);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,1776(r31)
	PPC_STORE_U32(r31.u32 + 1776, r11.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r3,r26,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1780(r31)
	PPC_STORE_U32(r31.u32 + 1780, ctx.r3.u32);
	// beq cr6,0x826063e4
	if (cr6.eq) goto loc_826063E4;
loc_826068E4:
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,3716(r31)
	PPC_STORE_U32(r31.u32 + 3716, r11.u32);
	// bne cr6,0x8260691c
	if (!cr6.eq) goto loc_8260691C;
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// add r11,r26,r11
	r11.u64 = r26.u64 + r11.u64;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,3052(r31)
	PPC_STORE_U32(r31.u32 + 3052, ctx.r3.u32);
	// li r3,2
	ctx.r3.s64 = 2;
	// beq cr6,0x82606920
	if (cr6.eq) goto loc_82606920;
loc_8260691C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82606920:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_82606928"))) PPC_WEAK_FUNC(sub_82606928);
PPC_FUNC_IMPL(__imp__sub_82606928) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// li r5,636
	ctx.r5.s64 = 636;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// addi r11,r28,16
	r11.s64 = r28.s64 + 16;
	// lis r10,8320
	ctx.r10.s64 = 545259520;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// ori r26,r10,4096
	r26.u64 = ctx.r10.u64 | 4096;
	// add r29,r11,r25
	r29.u64 = r11.u64 + r25.u64;
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x826a8d90
	sub_826A8D90(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// bne cr6,0x82606994
	if (!cr6.eq) goto loc_82606994;
loc_82606988:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_82606994:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// add r11,r27,r28
	r11.u64 = r27.u64 + r28.u64;
	// add r10,r30,r28
	ctx.r10.u64 = r30.u64 + r28.u64;
	// addi r29,r11,32
	r29.s64 = r11.s64 + 32;
	// addi r11,r10,31
	r11.s64 = ctx.r10.s64 + 31;
	// li r6,4
	ctx.r6.s64 = 4;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// bl 0x826a8d90
	sub_826A8D90(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// beq cr6,0x82606988
	if (cr6.eq) goto loc_82606988;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// srawi r27,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r27.s64 = r28.s32 >> 1;
	// add r10,r27,r30
	ctx.r10.u64 = r27.u64 + r30.u64;
	// clrlwi r11,r10,27
	r11.u64 = ctx.r10.u32 & 0x1F;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82606a04
	if (cr6.eq) goto loc_82606A04;
	// subfic r11,r11,32
	xer.ca = r11.u32 <= 32;
	r11.s64 = 32 - r11.s64;
loc_82606A04:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bl 0x826a8d90
	sub_826A8D90(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
	// beq cr6,0x82606988
	if (cr6.eq) goto loc_82606988;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// add r10,r27,r30
	ctx.r10.u64 = r27.u64 + r30.u64;
	// clrlwi r11,r10,27
	r11.u64 = ctx.r10.u32 & 0x1F;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82606a50
	if (cr6.eq) goto loc_82606A50;
	// subfic r11,r11,32
	xer.ca = r11.u32 <= 32;
	r11.s64 = 32 - r11.s64;
loc_82606A50:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r28,592(r31)
	PPC_STORE_U32(r31.u32 + 592, r28.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r25,588(r31)
	PPC_STORE_U32(r31.u32 + 588, r25.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_82606A6C"))) PPC_WEAK_FUNC(sub_82606A6C);
PPC_FUNC_IMPL(__imp__sub_82606A6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82606A70"))) PPC_WEAK_FUNC(sub_82606A70);
PPC_FUNC_IMPL(__imp__sub_82606A70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r11,0
	r11.s64 = 0;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lwz r9,3392(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3392);
	// stw r10,14788(r31)
	PPC_STORE_U32(r31.u32 + 14788, ctx.r10.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r11.u32);
	// beq cr6,0x82606ad4
	if (cr6.eq) goto loc_82606AD4;
	// lwz r10,21360(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21360);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82606abc
	if (cr6.eq) goto loc_82606ABC;
	// lwz r10,21364(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21364);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82606ad4
	if (cr6.eq) goto loc_82606AD4;
loc_82606ABC:
	// li r10,-3
	ctx.r10.s64 = -3;
	// stw r11,3380(r31)
	PPC_STORE_U32(r31.u32 + 3380, r11.u32);
	// stw r11,3396(r31)
	PPC_STORE_U32(r31.u32 + 3396, r11.u32);
	// stw r11,3384(r31)
	PPC_STORE_U32(r31.u32 + 3384, r11.u32);
	// stw r11,3400(r31)
	PPC_STORE_U32(r31.u32 + 3400, r11.u32);
	// stw r10,3376(r31)
	PPC_STORE_U32(r31.u32 + 3376, ctx.r10.u32);
loc_82606AD4:
	// stw r11,3360(r31)
	PPC_STORE_U32(r31.u32 + 3360, r11.u32);
	// lwz r11,1872(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1872);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82606b04
	if (!cr6.eq) goto loc_82606B04;
	// lis r4,9356
	ctx.r4.s64 = 613154816;
	// li r3,832
	ctx.r3.s64 = 832;
	// ori r4,r4,32769
	ctx.r4.u64 = ctx.r4.u64 | 32769;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// addi r11,r3,60
	r11.s64 = ctx.r3.s64 + 60;
	// stw r3,1872(r31)
	PPC_STORE_U32(r31.u32 + 1872, ctx.r3.u32);
	// rlwinm r11,r11,0,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF0;
	// stw r11,1876(r31)
	PPC_STORE_U32(r31.u32 + 1876, r11.u32);
loc_82606B04:
	// lwz r11,21184(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21184);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82606b30
	if (cr6.eq) goto loc_82606B30;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82605dd0
	sub_82605DD0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82606b34
	if (!cr6.eq) goto loc_82606B34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82602170
	sub_82602170(ctx, base);
loc_82606B30:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82606B34:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82606B3C"))) PPC_WEAK_FUNC(sub_82606B3C);
PPC_FUNC_IMPL(__imp__sub_82606B3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82606B40"))) PPC_WEAK_FUNC(sub_82606B40);
PPC_FUNC_IMPL(__imp__sub_82606B40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// li r29,1
	r29.s64 = 1;
	// li r11,1000
	r11.s64 = 1000;
	// addi r27,r31,1988
	r27.s64 = r31.s64 + 1988;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// stw r5,3656(r31)
	PPC_STORE_U32(r31.u32 + 3656, ctx.r5.u32);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r30,20864(r31)
	PPC_STORE_U32(r31.u32 + 20864, r30.u32);
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// stw r30,404(r31)
	PPC_STORE_U32(r31.u32 + 404, r30.u32);
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// stw r30,21236(r31)
	PPC_STORE_U32(r31.u32 + 21236, r30.u32);
	// mr r18,r8
	r18.u64 = ctx.r8.u64;
	// stw r30,21240(r31)
	PPC_STORE_U32(r31.u32 + 21240, r30.u32);
	// stw r30,21252(r31)
	PPC_STORE_U32(r31.u32 + 21252, r30.u32);
	// stw r30,21256(r31)
	PPC_STORE_U32(r31.u32 + 21256, r30.u32);
	// stw r30,21260(r31)
	PPC_STORE_U32(r31.u32 + 21260, r30.u32);
	// stw r30,21244(r31)
	PPC_STORE_U32(r31.u32 + 21244, r30.u32);
	// stw r29,15468(r31)
	PPC_STORE_U32(r31.u32 + 15468, r29.u32);
	// stw r29,448(r31)
	PPC_STORE_U32(r31.u32 + 448, r29.u32);
	// stw r29,1944(r31)
	PPC_STORE_U32(r31.u32 + 1944, r29.u32);
	// stw r11,15532(r31)
	PPC_STORE_U32(r31.u32 + 15532, r11.u32);
	// stw r30,15548(r31)
	PPC_STORE_U32(r31.u32 + 15548, r30.u32);
	// stw r29,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r29.u32);
	// stw r30,3916(r31)
	PPC_STORE_U32(r31.u32 + 3916, r30.u32);
	// stw r30,3920(r31)
	PPC_STORE_U32(r31.u32 + 3920, r30.u32);
	// stw r30,3932(r31)
	PPC_STORE_U32(r31.u32 + 3932, r30.u32);
	// stw r30,2976(r31)
	PPC_STORE_U32(r31.u32 + 2976, r30.u32);
	// stw r30,2972(r31)
	PPC_STORE_U32(r31.u32 + 2972, r30.u32);
	// stw r30,2968(r31)
	PPC_STORE_U32(r31.u32 + 2968, r30.u32);
	// stw r30,1792(r31)
	PPC_STORE_U32(r31.u32 + 1792, r30.u32);
	// stw r30,14796(r31)
	PPC_STORE_U32(r31.u32 + 14796, r30.u32);
	// stw r30,14800(r31)
	PPC_STORE_U32(r31.u32 + 14800, r30.u32);
	// stw r30,14792(r31)
	PPC_STORE_U32(r31.u32 + 14792, r30.u32);
	// stw r29,3368(r31)
	PPC_STORE_U32(r31.u32 + 3368, r29.u32);
	// stw r30,3372(r31)
	PPC_STORE_U32(r31.u32 + 3372, r30.u32);
	// stw r30,3364(r31)
	PPC_STORE_U32(r31.u32 + 3364, r30.u32);
	// stw r30,21232(r31)
	PPC_STORE_U32(r31.u32 + 21232, r30.u32);
	// stw r30,21292(r31)
	PPC_STORE_U32(r31.u32 + 21292, r30.u32);
	// stw r30,356(r31)
	PPC_STORE_U32(r31.u32 + 356, r30.u32);
	// stw r30,360(r31)
	PPC_STORE_U32(r31.u32 + 360, r30.u32);
	// stw r30,364(r31)
	PPC_STORE_U32(r31.u32 + 364, r30.u32);
	// stw r30,368(r31)
	PPC_STORE_U32(r31.u32 + 368, r30.u32);
	// stw r30,372(r31)
	PPC_STORE_U32(r31.u32 + 372, r30.u32);
	// stw r30,352(r31)
	PPC_STORE_U32(r31.u32 + 352, r30.u32);
	// stw r30,14772(r31)
	PPC_STORE_U32(r31.u32 + 14772, r30.u32);
	// stw r30,21296(r31)
	PPC_STORE_U32(r31.u32 + 21296, r30.u32);
	// stw r30,21300(r31)
	PPC_STORE_U32(r31.u32 + 21300, r30.u32);
	// stw r30,21308(r31)
	PPC_STORE_U32(r31.u32 + 21308, r30.u32);
	// stw r30,21304(r31)
	PPC_STORE_U32(r31.u32 + 21304, r30.u32);
	// stw r30,14808(r31)
	PPC_STORE_U32(r31.u32 + 14808, r30.u32);
	// stw r30,14812(r31)
	PPC_STORE_U32(r31.u32 + 14812, r30.u32);
	// stw r30,14816(r31)
	PPC_STORE_U32(r31.u32 + 14816, r30.u32);
	// stw r30,276(r31)
	PPC_STORE_U32(r31.u32 + 276, r30.u32);
	// stw r30,3712(r31)
	PPC_STORE_U32(r31.u32 + 3712, r30.u32);
	// stw r30,3700(r31)
	PPC_STORE_U32(r31.u32 + 3700, r30.u32);
	// stw r30,3412(r31)
	PPC_STORE_U32(r31.u32 + 3412, r30.u32);
	// stw r30,3052(r31)
	PPC_STORE_U32(r31.u32 + 3052, r30.u32);
	// stw r30,3048(r31)
	PPC_STORE_U32(r31.u32 + 3048, r30.u32);
	// stw r30,15240(r31)
	PPC_STORE_U32(r31.u32 + 15240, r30.u32);
	// stw r30,15244(r31)
	PPC_STORE_U32(r31.u32 + 15244, r30.u32);
	// stw r30,15248(r31)
	PPC_STORE_U32(r31.u32 + 15248, r30.u32);
	// stw r30,15252(r31)
	PPC_STORE_U32(r31.u32 + 15252, r30.u32);
	// stw r30,15256(r31)
	PPC_STORE_U32(r31.u32 + 15256, r30.u32);
	// stw r30,15260(r31)
	PPC_STORE_U32(r31.u32 + 15260, r30.u32);
	// stw r30,15264(r31)
	PPC_STORE_U32(r31.u32 + 15264, r30.u32);
	// stw r30,15552(r31)
	PPC_STORE_U32(r31.u32 + 15552, r30.u32);
	// stw r30,3420(r31)
	PPC_STORE_U32(r31.u32 + 3420, r30.u32);
	// stw r30,15268(r31)
	PPC_STORE_U32(r31.u32 + 15268, r30.u32);
	// stw r30,15292(r31)
	PPC_STORE_U32(r31.u32 + 15292, r30.u32);
	// stw r30,15276(r31)
	PPC_STORE_U32(r31.u32 + 15276, r30.u32);
	// stw r30,15284(r31)
	PPC_STORE_U32(r31.u32 + 15284, r30.u32);
	// stw r30,3436(r31)
	PPC_STORE_U32(r31.u32 + 3436, r30.u32);
	// stw r30,3440(r31)
	PPC_STORE_U32(r31.u32 + 3440, r30.u32);
	// stw r30,3428(r31)
	PPC_STORE_U32(r31.u32 + 3428, r30.u32);
	// stw r30,252(r31)
	PPC_STORE_U32(r31.u32 + 252, r30.u32);
	// stw r30,3444(r31)
	PPC_STORE_U32(r31.u32 + 3444, r30.u32);
	// stw r30,3448(r31)
	PPC_STORE_U32(r31.u32 + 3448, r30.u32);
	// stw r30,3716(r31)
	PPC_STORE_U32(r31.u32 + 3716, r30.u32);
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r26,r31,2000
	r26.s64 = r31.s64 + 2000;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r17,r31,2040
	r17.s64 = r31.s64 + 2040;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r16,r31,2052
	r16.s64 = r31.s64 + 2052;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r15,r31,2064
	r15.s64 = r31.s64 + 2064;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r14,r31,2076
	r14.s64 = r31.s64 + 2076;
	// mr r3,r14
	ctx.r3.u64 = r14.u64;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r23,r31,2116
	r23.s64 = r31.s64 + 2116;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r22,r31,2128
	r22.s64 = r31.s64 + 2128;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r21,r31,2144
	r21.s64 = r31.s64 + 2144;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r20,r31,2156
	r20.s64 = r31.s64 + 2156;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r19,r31,2168
	r19.s64 = r31.s64 + 2168;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2180
	ctx.r3.s64 = r31.s64 + 2180;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2192
	ctx.r3.s64 = r31.s64 + 2192;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2204
	ctx.r3.s64 = r31.s64 + 2204;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2216
	ctx.r3.s64 = r31.s64 + 2216;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2228
	ctx.r3.s64 = r31.s64 + 2228;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2240
	ctx.r3.s64 = r31.s64 + 2240;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2428
	ctx.r3.s64 = r31.s64 + 2428;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2252
	ctx.r3.s64 = r31.s64 + 2252;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2280
	ctx.r3.s64 = r31.s64 + 2280;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2292
	ctx.r3.s64 = r31.s64 + 2292;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2304
	ctx.r3.s64 = r31.s64 + 2304;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2316
	ctx.r3.s64 = r31.s64 + 2316;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2328
	ctx.r3.s64 = r31.s64 + 2328;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2340
	ctx.r3.s64 = r31.s64 + 2340;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2352
	ctx.r3.s64 = r31.s64 + 2352;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,2364
	ctx.r3.s64 = r31.s64 + 2364;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,21652
	ctx.r3.s64 = r31.s64 + 21652;
	// stw r30,2140(r31)
	PPC_STORE_U32(r31.u32 + 2140, r30.u32);
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,21664
	ctx.r3.s64 = r31.s64 + 21664;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,21676
	ctx.r3.s64 = r31.s64 + 21676;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,21688
	ctx.r3.s64 = r31.s64 + 21688;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// addi r3,r31,21640
	ctx.r3.s64 = r31.s64 + 21640;
	// bl 0x82390038
	sub_82390038(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82601d18
	sub_82601D18(ctx, base);
	// stw r30,23252(r31)
	PPC_STORE_U32(r31.u32 + 23252, r30.u32);
	// stw r30,23248(r31)
	PPC_STORE_U32(r31.u32 + 23248, r30.u32);
	// stw r30,1788(r31)
	PPC_STORE_U32(r31.u32 + 1788, r30.u32);
	// stw r30,14820(r31)
	PPC_STORE_U32(r31.u32 + 14820, r30.u32);
	// stw r30,280(r31)
	PPC_STORE_U32(r31.u32 + 280, r30.u32);
	// stw r30,3980(r31)
	PPC_STORE_U32(r31.u32 + 3980, r30.u32);
	// lis r11,1
	r11.s64 = 65536;
	// stw r30,472(r31)
	PPC_STORE_U32(r31.u32 + 472, r30.u32);
	// lis r10,1
	ctx.r10.s64 = 65536;
	// stw r30,15204(r31)
	PPC_STORE_U32(r31.u32 + 15204, r30.u32);
	// ori r11,r11,33744
	r11.u64 = r11.u64 | 33744;
	// stw r30,3976(r31)
	PPC_STORE_U32(r31.u32 + 3976, r30.u32);
	// ori r10,r10,33748
	ctx.r10.u64 = ctx.r10.u64 | 33748;
	// stw r30,3408(r31)
	PPC_STORE_U32(r31.u32 + 3408, r30.u32);
	// stw r30,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r30.u32);
	// stw r30,3452(r31)
	PPC_STORE_U32(r31.u32 + 3452, r30.u32);
	// stw r30,3956(r31)
	PPC_STORE_U32(r31.u32 + 3956, r30.u32);
	// stwx r30,r31,r11
	PPC_STORE_U32(r31.u32 + r11.u32, r30.u32);
	// stwx r30,r31,r10
	PPC_STORE_U32(r31.u32 + ctx.r10.u32, r30.u32);
	// stw r30,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, r30.u32);
	// stw r30,1780(r31)
	PPC_STORE_U32(r31.u32 + 1780, r30.u32);
	// stw r30,376(r31)
	PPC_STORE_U32(r31.u32 + 376, r30.u32);
	// stw r30,380(r31)
	PPC_STORE_U32(r31.u32 + 380, r30.u32);
	// stw r30,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r30.u32);
	// stw r30,388(r31)
	PPC_STORE_U32(r31.u32 + 388, r30.u32);
	// stw r30,15900(r31)
	PPC_STORE_U32(r31.u32 + 15900, r30.u32);
	// stw r30,14756(r31)
	PPC_STORE_U32(r31.u32 + 14756, r30.u32);
	// stw r30,15216(r31)
	PPC_STORE_U32(r31.u32 + 15216, r30.u32);
	// stw r30,15220(r31)
	PPC_STORE_U32(r31.u32 + 15220, r30.u32);
	// stw r30,20848(r31)
	PPC_STORE_U32(r31.u32 + 20848, r30.u32);
	// stw r30,19992(r31)
	PPC_STORE_U32(r31.u32 + 19992, r30.u32);
	// stw r30,19996(r31)
	PPC_STORE_U32(r31.u32 + 19996, r30.u32);
	// stw r30,20000(r31)
	PPC_STORE_U32(r31.u32 + 20000, r30.u32);
	// stw r30,20056(r31)
	PPC_STORE_U32(r31.u32 + 20056, r30.u32);
	// stw r30,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r30.u32);
	// stw r30,19984(r31)
	PPC_STORE_U32(r31.u32 + 19984, r30.u32);
	// stw r30,21000(r31)
	PPC_STORE_U32(r31.u32 + 21000, r30.u32);
	// stw r30,21004(r31)
	PPC_STORE_U32(r31.u32 + 21004, r30.u32);
	// stw r29,20988(r31)
	PPC_STORE_U32(r31.u32 + 20988, r29.u32);
	// stw r29,20996(r31)
	PPC_STORE_U32(r31.u32 + 20996, r29.u32);
	// stw r29,20992(r31)
	PPC_STORE_U32(r31.u32 + 20992, r29.u32);
	// stw r30,20832(r31)
	PPC_STORE_U32(r31.u32 + 20832, r30.u32);
	// stw r29,20836(r31)
	PPC_STORE_U32(r31.u32 + 20836, r29.u32);
	// stw r30,20840(r31)
	PPC_STORE_U32(r31.u32 + 20840, r30.u32);
	// stw r30,21160(r31)
	PPC_STORE_U32(r31.u32 + 21160, r30.u32);
	// stw r30,21164(r31)
	PPC_STORE_U32(r31.u32 + 21164, r30.u32);
	// stw r30,3392(r31)
	PPC_STORE_U32(r31.u32 + 3392, r30.u32);
	// stw r30,21432(r31)
	PPC_STORE_U32(r31.u32 + 21432, r30.u32);
	// stw r30,21468(r31)
	PPC_STORE_U32(r31.u32 + 21468, r30.u32);
	// stw r30,21472(r31)
	PPC_STORE_U32(r31.u32 + 21472, r30.u32);
	// stw r30,21476(r31)
	PPC_STORE_U32(r31.u32 + 21476, r30.u32);
	// stw r29,20972(r31)
	PPC_STORE_U32(r31.u32 + 20972, r29.u32);
	// stw r29,20980(r31)
	PPC_STORE_U32(r31.u32 + 20980, r29.u32);
	// stw r29,20976(r31)
	PPC_STORE_U32(r31.u32 + 20976, r29.u32);
	// stw r30,21440(r31)
	PPC_STORE_U32(r31.u32 + 21440, r30.u32);
	// stw r30,21444(r31)
	PPC_STORE_U32(r31.u32 + 21444, r30.u32);
	// stw r30,21448(r31)
	PPC_STORE_U32(r31.u32 + 21448, r30.u32);
	// stw r30,21452(r31)
	PPC_STORE_U32(r31.u32 + 21452, r30.u32);
	// stw r30,21456(r31)
	PPC_STORE_U32(r31.u32 + 21456, r30.u32);
	// stw r30,21460(r31)
	PPC_STORE_U32(r31.u32 + 21460, r30.u32);
	// stw r30,21464(r31)
	PPC_STORE_U32(r31.u32 + 21464, r30.u32);
	// stw r30,20956(r31)
	PPC_STORE_U32(r31.u32 + 20956, r30.u32);
	// stw r30,20960(r31)
	PPC_STORE_U32(r31.u32 + 20960, r30.u32);
	// stw r30,20964(r31)
	PPC_STORE_U32(r31.u32 + 20964, r30.u32);
	// stw r30,20968(r31)
	PPC_STORE_U32(r31.u32 + 20968, r30.u32);
	// stw r30,21080(r31)
	PPC_STORE_U32(r31.u32 + 21080, r30.u32);
	// stw r30,21084(r31)
	PPC_STORE_U32(r31.u32 + 21084, r30.u32);
	// stw r30,21088(r31)
	PPC_STORE_U32(r31.u32 + 21088, r30.u32);
	// stw r30,21092(r31)
	PPC_STORE_U32(r31.u32 + 21092, r30.u32);
	// stw r30,21096(r31)
	PPC_STORE_U32(r31.u32 + 21096, r30.u32);
	// stw r30,21100(r31)
	PPC_STORE_U32(r31.u32 + 21100, r30.u32);
	// stw r30,21104(r31)
	PPC_STORE_U32(r31.u32 + 21104, r30.u32);
	// stw r30,21108(r31)
	PPC_STORE_U32(r31.u32 + 21108, r30.u32);
	// stw r30,21112(r31)
	PPC_STORE_U32(r31.u32 + 21112, r30.u32);
	// stw r30,21116(r31)
	PPC_STORE_U32(r31.u32 + 21116, r30.u32);
	// stw r30,21120(r31)
	PPC_STORE_U32(r31.u32 + 21120, r30.u32);
	// stw r30,21124(r31)
	PPC_STORE_U32(r31.u32 + 21124, r30.u32);
	// stw r30,21128(r31)
	PPC_STORE_U32(r31.u32 + 21128, r30.u32);
	// stw r30,21132(r31)
	PPC_STORE_U32(r31.u32 + 21132, r30.u32);
	// stw r30,21136(r31)
	PPC_STORE_U32(r31.u32 + 21136, r30.u32);
	// stw r30,21140(r31)
	PPC_STORE_U32(r31.u32 + 21140, r30.u32);
	// stw r30,328(r31)
	PPC_STORE_U32(r31.u32 + 328, r30.u32);
	// stw r30,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r30.u32);
	// stw r29,21200(r31)
	PPC_STORE_U32(r31.u32 + 21200, r29.u32);
	// stw r29,21180(r31)
	PPC_STORE_U32(r31.u32 + 21180, r29.u32);
	// stw r29,21184(r31)
	PPC_STORE_U32(r31.u32 + 21184, r29.u32);
	// stw r30,21188(r31)
	PPC_STORE_U32(r31.u32 + 21188, r30.u32);
	// stw r30,21168(r31)
	PPC_STORE_U32(r31.u32 + 21168, r30.u32);
	// li r11,100
	r11.s64 = 100;
	// stw r30,21172(r31)
	PPC_STORE_U32(r31.u32 + 21172, r30.u32);
	// lis r10,22349
	ctx.r10.s64 = 1464664064;
	// stw r30,21176(r31)
	PPC_STORE_U32(r31.u32 + 21176, r30.u32);
	// stw r30,21196(r31)
	PPC_STORE_U32(r31.u32 + 21196, r30.u32);
	// ori r10,r10,22066
	ctx.r10.u64 = ctx.r10.u64 | 22066;
	// stw r30,21192(r31)
	PPC_STORE_U32(r31.u32 + 21192, r30.u32);
	// stw r30,15368(r31)
	PPC_STORE_U32(r31.u32 + 15368, r30.u32);
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// stw r11,23976(r31)
	PPC_STORE_U32(r31.u32 + 23976, r11.u32);
	// li r11,-1
	r11.s64 = -1;
	// stw r30,21228(r31)
	PPC_STORE_U32(r31.u32 + 21228, r30.u32);
	// stw r30,21480(r31)
	PPC_STORE_U32(r31.u32 + 21480, r30.u32);
	// stw r30,21360(r31)
	PPC_STORE_U32(r31.u32 + 21360, r30.u32);
	// stw r30,21388(r31)
	PPC_STORE_U32(r31.u32 + 21388, r30.u32);
	// stw r30,21212(r31)
	PPC_STORE_U32(r31.u32 + 21212, r30.u32);
	// stw r30,21216(r31)
	PPC_STORE_U32(r31.u32 + 21216, r30.u32);
	// stw r30,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r30.u32);
	// stw r30,21268(r31)
	PPC_STORE_U32(r31.u32 + 21268, r30.u32);
	// stw r30,21204(r31)
	PPC_STORE_U32(r31.u32 + 21204, r30.u32);
	// stw r30,21208(r31)
	PPC_STORE_U32(r31.u32 + 21208, r30.u32);
	// stw r30,21364(r31)
	PPC_STORE_U32(r31.u32 + 21364, r30.u32);
	// stw r30,21368(r31)
	PPC_STORE_U32(r31.u32 + 21368, r30.u32);
	// stw r30,21372(r31)
	PPC_STORE_U32(r31.u32 + 21372, r30.u32);
	// stw r30,21376(r31)
	PPC_STORE_U32(r31.u32 + 21376, r30.u32);
	// stw r30,21528(r31)
	PPC_STORE_U32(r31.u32 + 21528, r30.u32);
	// stw r30,21532(r31)
	PPC_STORE_U32(r31.u32 + 21532, r30.u32);
	// stw r30,21536(r31)
	PPC_STORE_U32(r31.u32 + 21536, r30.u32);
	// stw r29,21544(r31)
	PPC_STORE_U32(r31.u32 + 21544, r29.u32);
	// stw r30,21548(r31)
	PPC_STORE_U32(r31.u32 + 21548, r30.u32);
	// stw r30,1872(r31)
	PPC_STORE_U32(r31.u32 + 1872, r30.u32);
	// stw r30,1876(r31)
	PPC_STORE_U32(r31.u32 + 1876, r30.u32);
	// stw r30,21580(r31)
	PPC_STORE_U32(r31.u32 + 21580, r30.u32);
	// stw r30,21584(r31)
	PPC_STORE_U32(r31.u32 + 21584, r30.u32);
	// stw r29,21588(r31)
	PPC_STORE_U32(r31.u32 + 21588, r29.u32);
	// stw r30,21592(r31)
	PPC_STORE_U32(r31.u32 + 21592, r30.u32);
	// stw r30,21596(r31)
	PPC_STORE_U32(r31.u32 + 21596, r30.u32);
	// stw r30,21632(r31)
	PPC_STORE_U32(r31.u32 + 21632, r30.u32);
	// stw r30,21636(r31)
	PPC_STORE_U32(r31.u32 + 21636, r30.u32);
	// stw r30,15192(r31)
	PPC_STORE_U32(r31.u32 + 15192, r30.u32);
	// stw r29,21700(r31)
	PPC_STORE_U32(r31.u32 + 21700, r29.u32);
	// stw r29,21704(r31)
	PPC_STORE_U32(r31.u32 + 21704, r29.u32);
	// stw r11,15200(r31)
	PPC_STORE_U32(r31.u32 + 15200, r11.u32);
	// beq cr6,0x82607188
	if (cr6.eq) goto loc_82607188;
	// lis r11,30573
	r11.s64 = 2003632128;
	// ori r11,r11,30258
	r11.u64 = r11.u64 | 30258;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607188
	if (cr6.eq) goto loc_82607188;
	// lis r11,22349
	r11.s64 = 1464664064;
	// ori r11,r11,22067
	r11.u64 = r11.u64 | 22067;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607158
	if (cr6.eq) goto loc_82607158;
	// lis r11,30573
	r11.s64 = 2003632128;
	// ori r11,r11,30259
	r11.u64 = r11.u64 | 30259;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607158
	if (cr6.eq) goto loc_82607158;
	// lis r11,22349
	r11.s64 = 1464664064;
	// ori r11,r11,22081
	r11.u64 = r11.u64 | 22081;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607180
	if (cr6.eq) goto loc_82607180;
	// lis r11,30573
	r11.s64 = 2003632128;
	// ori r11,r11,30305
	r11.u64 = r11.u64 | 30305;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607180
	if (cr6.eq) goto loc_82607180;
	// lis r11,22349
	r11.s64 = 1464664064;
	// ori r11,r11,22065
	r11.u64 = r11.u64 | 22065;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607178
	if (cr6.eq) goto loc_82607178;
	// lis r11,30573
	r11.s64 = 2003632128;
	// ori r11,r11,30257
	r11.u64 = r11.u64 | 30257;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607178
	if (cr6.eq) goto loc_82607178;
	// lis r11,19792
	r11.s64 = 1297088512;
	// ori r11,r11,13363
	r11.u64 = r11.u64 | 13363;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607170
	if (cr6.eq) goto loc_82607170;
	// lis r11,28016
	r11.s64 = 1836056576;
	// ori r11,r11,13363
	r11.u64 = r11.u64 | 13363;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607170
	if (cr6.eq) goto loc_82607170;
	// lis r11,19792
	r11.s64 = 1297088512;
	// ori r11,r11,13362
	r11.u64 = r11.u64 | 13362;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607168
	if (cr6.eq) goto loc_82607168;
	// lis r11,28016
	r11.s64 = 1836056576;
	// ori r11,r11,13362
	r11.u64 = r11.u64 | 13362;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607168
	if (cr6.eq) goto loc_82607168;
	// lis r11,19792
	r11.s64 = 1297088512;
	// ori r11,r11,13395
	r11.u64 = r11.u64 | 13395;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607160
	if (cr6.eq) goto loc_82607160;
	// lis r11,28016
	r11.s64 = 1836056576;
	// ori r11,r11,13427
	r11.u64 = r11.u64 | 13427;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607160
	if (cr6.eq) goto loc_82607160;
	// lis r11,22349
	r11.s64 = 1464664064;
	// ori r11,r11,22096
	r11.u64 = r11.u64 | 22096;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607158
	if (cr6.eq) goto loc_82607158;
	// lis r11,30573
	r11.s64 = 2003632128;
	// ori r11,r11,30320
	r11.u64 = r11.u64 | 30320;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607158
	if (cr6.eq) goto loc_82607158;
	// lis r11,22358
	r11.s64 = 1465253888;
	// ori r11,r11,20530
	r11.u64 = r11.u64 | 20530;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607148
	if (cr6.eq) goto loc_82607148;
	// lis r11,30582
	r11.s64 = 2004221952;
	// ori r11,r11,28722
	r11.u64 = r11.u64 | 28722;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607148
	if (cr6.eq) goto loc_82607148;
	// lis r11,22349
	r11.s64 = 1464664064;
	// ori r11,r11,22098
	r11.u64 = r11.u64 | 22098;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607108
	if (cr6.eq) goto loc_82607108;
	// lis r11,30573
	r11.s64 = 2003632128;
	// ori r11,r11,30322
	r11.u64 = r11.u64 | 30322;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x82607108
	if (cr6.eq) goto loc_82607108;
	// li r3,6
	ctx.r3.s64 = 6;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_82607108:
	// li r11,7
	r11.s64 = 7;
	// stw r11,15472(r31)
	PPC_STORE_U32(r31.u32 + 15472, r11.u32);
	// stw r29,21580(r31)
	PPC_STORE_U32(r31.u32 + 21580, r29.u32);
	// lwz r11,21704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21704);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82607134
	if (!cr6.eq) goto loc_82607134;
	// stw r29,21632(r31)
	PPC_STORE_U32(r31.u32 + 21632, r29.u32);
	// stw r29,21596(r31)
	PPC_STORE_U32(r31.u32 + 21596, r29.u32);
	// stw r29,21700(r31)
	PPC_STORE_U32(r31.u32 + 21700, r29.u32);
	// stw r29,15192(r31)
	PPC_STORE_U32(r31.u32 + 15192, r29.u32);
	// b 0x82607190
	goto loc_82607190;
loc_82607134:
	// stw r30,21632(r31)
	PPC_STORE_U32(r31.u32 + 21632, r30.u32);
	// stw r30,21596(r31)
	PPC_STORE_U32(r31.u32 + 21596, r30.u32);
	// stw r30,21700(r31)
	PPC_STORE_U32(r31.u32 + 21700, r30.u32);
	// stw r29,15192(r31)
	PPC_STORE_U32(r31.u32 + 15192, r29.u32);
	// b 0x82607190
	goto loc_82607190;
loc_82607148:
	// li r11,7
	r11.s64 = 7;
	// stw r11,15472(r31)
	PPC_STORE_U32(r31.u32 + 15472, r11.u32);
	// stw r29,15368(r31)
	PPC_STORE_U32(r31.u32 + 15368, r29.u32);
	// b 0x82607190
	goto loc_82607190;
loc_82607158:
	// li r11,6
	r11.s64 = 6;
	// b 0x8260718c
	goto loc_8260718C;
loc_82607160:
	// stw r30,15472(r31)
	PPC_STORE_U32(r31.u32 + 15472, r30.u32);
	// b 0x82607190
	goto loc_82607190;
loc_82607168:
	// li r11,2
	r11.s64 = 2;
	// b 0x8260718c
	goto loc_8260718C;
loc_82607170:
	// li r11,3
	r11.s64 = 3;
	// b 0x8260718c
	goto loc_8260718C;
loc_82607178:
	// li r11,4
	r11.s64 = 4;
	// b 0x8260718c
	goto loc_8260718C;
loc_82607180:
	// li r11,7
	r11.s64 = 7;
	// b 0x8260718c
	goto loc_8260718C;
loc_82607188:
	// li r11,5
	r11.s64 = 5;
loc_8260718C:
	// stw r11,15472(r31)
	PPC_STORE_U32(r31.u32 + 15472, r11.u32);
loc_82607190:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x826071ac
	if (cr6.lt) goto loc_826071AC;
	// stw r29,14788(r31)
	PPC_STORE_U32(r31.u32 + 14788, r29.u32);
	// stw r29,3956(r31)
	PPC_STORE_U32(r31.u32 + 3956, r29.u32);
	// stw r30,3888(r31)
	PPC_STORE_U32(r31.u32 + 3888, r30.u32);
	// stw r29,14756(r31)
	PPC_STORE_U32(r31.u32 + 14756, r29.u32);
loc_826071AC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826071c4
	if (cr6.eq) goto loc_826071C4;
	// stw r30,14788(r31)
	PPC_STORE_U32(r31.u32 + 14788, r30.u32);
	// b 0x826071c8
	goto loc_826071C8;
loc_826071C4:
	// stw r29,14788(r31)
	PPC_STORE_U32(r31.u32 + 14788, r29.u32);
loc_826071C8:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x826071f0
	if (!cr6.eq) goto loc_826071F0;
	// lwz r11,3924(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826071f0
	if (!cr6.eq) goto loc_826071F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826071f4
	if (cr6.eq) goto loc_826071F4;
loc_826071F0:
	// stw r30,15900(r31)
	PPC_STORE_U32(r31.u32 + 15900, r30.u32);
loc_826071F4:
	// stw r29,3356(r31)
	PPC_STORE_U32(r31.u32 + 3356, r29.u32);
	// lwz r11,14772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82607208
	if (!cr6.gt) goto loc_82607208;
	// stw r29,3356(r31)
	PPC_STORE_U32(r31.u32 + 3356, r29.u32);
loc_82607208:
	// lwz r11,3356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// ble cr6,0x8260721c
	if (!cr6.gt) goto loc_8260721C;
	// li r11,2
	r11.s64 = 2;
	// stw r11,3356(r31)
	PPC_STORE_U32(r31.u32 + 3356, r11.u32);
loc_8260721C:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x82607230
	if (!cr6.eq) goto loc_82607230;
	// stw r29,3356(r31)
	PPC_STORE_U32(r31.u32 + 3356, r29.u32);
	// stw r29,20056(r31)
	PPC_STORE_U32(r31.u32 + 20056, r29.u32);
loc_82607230:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// lis r10,9356
	ctx.r10.s64 = 613154816;
	// ori r28,r10,32769
	r28.u64 = ctx.r10.u64 | 32769;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x826072b8
	if (!cr6.eq) goto loc_826072B8;
	// bl 0x82617978
	sub_82617978(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82603ac8
	sub_82603AC8(ctx, base);
	// bl 0x82603be0
	sub_82603BE0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82603d60
	sub_82603D60(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cntlzw r10,r3
	ctx.r10.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// li r11,4096
	r11.s64 = 4096;
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r3,4096
	ctx.r3.s64 = 4096;
	// stw r10,14772(r31)
	PPC_STORE_U32(r31.u32 + 14772, ctx.r10.u32);
	// stw r11,21324(r31)
	PPC_STORE_U32(r31.u32 + 21324, r11.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21316(r31)
	PPC_STORE_U32(r31.u32 + 21316, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r3,21324(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21324);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,21320(r31)
	PPC_STORE_U32(r31.u32 + 21320, ctx.r3.u32);
	// bne cr6,0x826072e8
	if (!cr6.eq) goto loc_826072E8;
loc_826072AC:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_826072B8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826072cc
	if (cr6.eq) goto loc_826072CC;
	// stw r30,14772(r31)
	PPC_STORE_U32(r31.u32 + 14772, r30.u32);
loc_826072CC:
	// bl 0x82617978
	sub_82617978(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82603ac8
	sub_82603AC8(ctx, base);
	// bl 0x82603d60
	sub_82603D60(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82603be0
	sub_82603BE0(ctx, base);
loc_826072E8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82607308
	if (cr6.eq) goto loc_82607308;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82647f38
	sub_82647F38(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82607cfc
	if (!cr6.eq) goto loc_82607CFC;
loc_82607308:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r30,3360(r31)
	PPC_STORE_U32(r31.u32 + 3360, r30.u32);
	// bl 0x82648b58
	sub_82648B58(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82607cfc
	if (!cr6.eq) goto loc_82607CFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r18,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r18.u32);
	// bl 0x82605f20
	sub_82605F20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82607cfc
	if (!cr6.eq) goto loc_82607CFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f3bf0
	sub_825F3BF0(ctx, base);
	// mullw r11,r25,r24
	r11.s64 = int64_t(r25.s32) * int64_t(r24.s32);
	// stw r11,21176(r31)
	PPC_STORE_U32(r31.u32 + 21176, r11.u32);
	// stw r25,21192(r31)
	PPC_STORE_U32(r31.u32 + 21192, r25.u32);
	// stw r25,21352(r31)
	PPC_STORE_U32(r31.u32 + 21352, r25.u32);
	// stw r24,21196(r31)
	PPC_STORE_U32(r31.u32 + 21196, r24.u32);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// stw r24,21356(r31)
	PPC_STORE_U32(r31.u32 + 21356, r24.u32);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// stw r29,3676(r31)
	PPC_STORE_U32(r31.u32 + 3676, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82606168
	sub_82606168(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82607cfc
	if (!cr6.eq) goto loc_82607CFC;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82606a70
	sub_82606A70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82607cfc
	if (!cr6.eq) goto loc_82607CFC;
	// addi r10,r31,1243
	ctx.r10.s64 = r31.s64 + 1243;
	// stw r30,3676(r31)
	PPC_STORE_U32(r31.u32 + 3676, r30.u32);
	// addi r11,r31,603
	r11.s64 = r31.s64 + 603;
	// stw r30,3680(r31)
	PPC_STORE_U32(r31.u32 + 3680, r30.u32);
	// rlwinm r10,r10,0,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFF80;
	// rlwinm r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,1764(r31)
	PPC_STORE_U32(r31.u32 + 1764, ctx.r10.u32);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stw r11,1768(r31)
	PPC_STORE_U32(r31.u32 + 1768, r11.u32);
	// stw r11,1760(r31)
	PPC_STORE_U32(r31.u32 + 1760, r11.u32);
	// stw r10,1756(r31)
	PPC_STORE_U32(r31.u32 + 1756, ctx.r10.u32);
	// stw r30,248(r31)
	PPC_STORE_U32(r31.u32 + 248, r30.u32);
	// lwz r11,3100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3100);
	// stw r11,3104(r31)
	PPC_STORE_U32(r31.u32 + 3104, r11.u32);
	// stw r11,3084(r31)
	PPC_STORE_U32(r31.u32 + 3084, r11.u32);
	// lwz r11,3096(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3096);
	// stw r11,3108(r31)
	PPC_STORE_U32(r31.u32 + 3108, r11.u32);
	// stw r11,3080(r31)
	PPC_STORE_U32(r31.u32 + 3080, r11.u32);
	// bl 0x8260a4f0
	sub_8260A4F0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8262aa88
	sub_8262AA88(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826539c0
	sub_826539C0(ctx, base);
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,3716(r31)
	PPC_STORE_U32(r31.u32 + 3716, r11.u32);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// beq cr6,0x82607aac
	if (cr6.eq) goto loc_82607AAC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-18512
	ctx.r6.s64 = r11.s64 + -18512;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-22920
	ctx.r6.s64 = r11.s64 + -22920;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,-12024
	ctx.r6.s64 = r11.s64 + -12024;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-12544
	ctx.r6.s64 = r11.s64 + -12544;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-14104
	ctx.r6.s64 = r11.s64 + -14104;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-13584
	ctx.r6.s64 = r11.s64 + -13584;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-13064
	ctx.r6.s64 = r11.s64 + -13064;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,-11760
	ctx.r6.s64 = r11.s64 + -11760;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-11272
	ctx.r6.s64 = r11.s64 + -11272;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,9
	ctx.r7.s64 = 9;
	// addi r6,r11,-10784
	ctx.r6.s64 = r11.s64 + -10784;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,-10296
	ctx.r6.s64 = r11.s64 + -10296;
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-27680
	ctx.r6.s64 = r11.s64 + -27680;
	// addi r4,r31,2180
	ctx.r4.s64 = r31.s64 + 2180;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-27000
	ctx.r6.s64 = r11.s64 + -27000;
	// addi r4,r31,2192
	ctx.r4.s64 = r31.s64 + 2192;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-26248
	ctx.r6.s64 = r11.s64 + -26248;
	// addi r4,r31,2204
	ctx.r4.s64 = r31.s64 + 2204;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-25648
	ctx.r6.s64 = r11.s64 + -25648;
	// addi r4,r31,2216
	ctx.r4.s64 = r31.s64 + 2216;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-25112
	ctx.r6.s64 = r11.s64 + -25112;
	// addi r4,r31,2228
	ctx.r4.s64 = r31.s64 + 2228;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-24696
	ctx.r6.s64 = r11.s64 + -24696;
	// addi r4,r31,2240
	ctx.r4.s64 = r31.s64 + 2240;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-24280
	ctx.r6.s64 = r11.s64 + -24280;
	// addi r4,r31,2428
	ctx.r4.s64 = r31.s64 + 2428;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-23576
	ctx.r6.s64 = r11.s64 + -23576;
	// addi r4,r31,2252
	ctx.r4.s64 = r31.s64 + 2252;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r26,r31,2280
	r26.s64 = r31.s64 + 2280;
	// addi r6,r11,-9808
	ctx.r6.s64 = r11.s64 + -9808;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,2292
	r27.s64 = r31.s64 + 2292;
	// addi r6,r11,-9544
	ctx.r6.s64 = r11.s64 + -9544;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,2304
	r29.s64 = r31.s64 + 2304;
	// addi r6,r11,-9280
	ctx.r6.s64 = r11.s64 + -9280;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,2316
	r30.s64 = r31.s64 + 2316;
	// addi r6,r11,-9016
	ctx.r6.s64 = r11.s64 + -9016;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// stw r26,2380(r31)
	PPC_STORE_U32(r31.u32 + 2380, r26.u32);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// stw r27,2384(r31)
	PPC_STORE_U32(r31.u32 + 2384, r27.u32);
	// li r7,138
	ctx.r7.s64 = 138;
	// stw r29,2388(r31)
	PPC_STORE_U32(r31.u32 + 2388, r29.u32);
	// addi r29,r31,2328
	r29.s64 = r31.s64 + 2328;
	// addi r6,r11,-8752
	ctx.r6.s64 = r11.s64 + -8752;
	// stw r30,2392(r31)
	PPC_STORE_U32(r31.u32 + 2392, r30.u32);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r26,r31,2340
	r26.s64 = r31.s64 + 2340;
	// addi r6,r11,-8456
	ctx.r6.s64 = r11.s64 + -8456;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,2352
	r27.s64 = r31.s64 + 2352;
	// addi r6,r11,-8160
	ctx.r6.s64 = r11.s64 + -8160;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,2364
	r30.s64 = r31.s64 + 2364;
	// addi r6,r11,-7864
	ctx.r6.s64 = r11.s64 + -7864;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// stw r29,2396(r31)
	PPC_STORE_U32(r31.u32 + 2396, r29.u32);
	// stw r26,2400(r31)
	PPC_STORE_U32(r31.u32 + 2400, r26.u32);
	// stw r27,2404(r31)
	PPC_STORE_U32(r31.u32 + 2404, r27.u32);
	// stw r30,2408(r31)
	PPC_STORE_U32(r31.u32 + 2408, r30.u32);
	// lwz r11,21596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21596);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826078c0
	if (cr6.eq) goto loc_826078C0;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r26,r31,21652
	r26.s64 = r31.s64 + 21652;
	// addi r6,r11,-7568
	ctx.r6.s64 = r11.s64 + -7568;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r27,r31,21664
	r27.s64 = r31.s64 + 21664;
	// addi r6,r11,-7264
	ctx.r6.s64 = r11.s64 + -7264;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r29,r31,21676
	r29.s64 = r31.s64 + 21676;
	// addi r6,r11,-6960
	ctx.r6.s64 = r11.s64 + -6960;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// addi r30,r31,21688
	r30.s64 = r31.s64 + 21688;
	// addi r6,r11,-6656
	ctx.r6.s64 = r11.s64 + -6656;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// stw r26,2396(r31)
	PPC_STORE_U32(r31.u32 + 2396, r26.u32);
	// stw r27,2400(r31)
	PPC_STORE_U32(r31.u32 + 2400, r27.u32);
	// stw r29,2404(r31)
	PPC_STORE_U32(r31.u32 + 2404, r29.u32);
	// stw r30,2408(r31)
	PPC_STORE_U32(r31.u32 + 2408, r30.u32);
loc_826078C0:
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,-6352
	ctx.r6.s64 = r11.s64 + -6352;
	// addi r4,r31,21640
	ctx.r4.s64 = r31.s64 + 21640;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-6160
	ctx.r6.s64 = r11.s64 + -6160;
	// addi r4,r31,2440
	ctx.r4.s64 = r31.s64 + 2440;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-6224
	ctx.r6.s64 = r11.s64 + -6224;
	// addi r4,r31,2452
	ctx.r4.s64 = r31.s64 + 2452;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-6288
	ctx.r6.s64 = r11.s64 + -6288;
	// addi r4,r31,2464
	ctx.r4.s64 = r31.s64 + 2464;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,-6096
	ctx.r6.s64 = r11.s64 + -6096;
	// addi r4,r31,2480
	ctx.r4.s64 = r31.s64 + 2480;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,-6024
	ctx.r6.s64 = r11.s64 + -6024;
	// addi r4,r31,2492
	ctx.r4.s64 = r31.s64 + 2492;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,-5952
	ctx.r6.s64 = r11.s64 + -5952;
	// addi r4,r31,2504
	ctx.r4.s64 = r31.s64 + 2504;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-5884
	ctx.r6.s64 = r11.s64 + -5884;
	// addi r4,r31,2520
	ctx.r4.s64 = r31.s64 + 2520;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-5848
	ctx.r6.s64 = r11.s64 + -5848;
	// addi r4,r31,2532
	ctx.r4.s64 = r31.s64 + 2532;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,-5812
	ctx.r6.s64 = r11.s64 + -5812;
	// addi r4,r31,2544
	ctx.r4.s64 = r31.s64 + 2544;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655e18
	sub_82655E18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// blt cr6,0x82607ac4
	if (cr6.lt) goto loc_82607AC4;
	// lwz r11,1972(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82607a50
	if (!cr6.eq) goto loc_82607A50;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,3340(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3340);
	// bl 0x8265b810
	sub_8265B810(ctx, base);
	// stw r3,1972(r31)
	PPC_STORE_U32(r31.u32 + 1972, ctx.r3.u32);
loc_82607A50:
	// lwz r11,1972(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// lwz r3,1964(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1964);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82607a78
	if (!cr6.eq) goto loc_82607A78;
	// bl 0x8265a768
	sub_8265A768(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1964(r31)
	PPC_STORE_U32(r31.u32 + 1964, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
loc_82607A78:
	// lwz r11,3180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3180);
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, r11.u32);
	// lwz r11,1968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1968);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82607ac4
	if (!cr6.eq) goto loc_82607AC4;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r4,2
	ctx.r4.s64 = 2;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x8265a440
	sub_8265A440(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,1968(r31)
	PPC_STORE_U32(r31.u32 + 1968, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// b 0x82607ac4
	goto loc_82607AC4;
loc_82607AAC:
	// lwz r11,1964(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1964);
	// lwz r10,3180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3180);
	// stw r10,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r10.u32);
	// bl 0x826026c8
	sub_826026C8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826072ac
	if (!cr6.eq) goto loc_826072AC;
loc_82607AC4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604148
	sub_82604148(ctx, base);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x82607cc8
	if (cr6.lt) goto loc_82607CC8;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82607b00
	if (!cr6.eq) goto loc_82607B00;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,352(r31)
	PPC_STORE_U32(r31.u32 + 352, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
loc_82607B00:
	// lis r11,-32157
	r11.s64 = -2107441152;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r11,r11,672
	r11.s64 = r11.s64 + 672;
	// li r3,64
	ctx.r3.s64 = 64;
	// stw r11,15776(r31)
	PPC_STORE_U32(r31.u32 + 15776, r11.u32);
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,356(r31)
	PPC_STORE_U32(r31.u32 + 356, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r3,72
	ctx.r3.s64 = 72;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,360(r31)
	PPC_STORE_U32(r31.u32 + 360, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r3,288
	ctx.r3.s64 = 288;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,364(r31)
	PPC_STORE_U32(r31.u32 + 364, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r3,576
	ctx.r3.s64 = 576;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,368(r31)
	PPC_STORE_U32(r31.u32 + 368, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r3,1008
	ctx.r3.s64 = 1008;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,372(r31)
	PPC_STORE_U32(r31.u32 + 372, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// lwz r11,1772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82607bd0
	if (!cr6.eq) goto loc_82607BD0;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r3,r11,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// stw r3,1772(r31)
	PPC_STORE_U32(r31.u32 + 1772, ctx.r3.u32);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,1776(r31)
	PPC_STORE_U32(r31.u32 + 1776, r11.u32);
	// lwz r11,1772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
loc_82607BD0:
	// lwz r11,376(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 376);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82607c14
	if (!cr6.eq) goto loc_82607C14;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r3,r11,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,384(r31)
	PPC_STORE_U32(r31.u32 + 384, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,376(r31)
	PPC_STORE_U32(r31.u32 + 376, r11.u32);
loc_82607C14:
	// lwz r11,380(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 380);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82607c4c
	if (!cr6.eq) goto loc_82607C4C;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,388(r31)
	PPC_STORE_U32(r31.u32 + 388, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// stw r3,380(r31)
	PPC_STORE_U32(r31.u32 + 380, ctx.r3.u32);
loc_82607C4C:
	// lwz r11,15216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15216);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82607c7c
	if (!cr6.eq) goto loc_82607C7C;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r3,832
	ctx.r3.s64 = 832;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15216(r31)
	PPC_STORE_U32(r31.u32 + 15216, ctx.r3.u32);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
	// addi r11,r3,31
	r11.s64 = ctx.r3.s64 + 31;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// stw r11,15220(r31)
	PPC_STORE_U32(r31.u32 + 15220, r11.u32);
loc_82607C7C:
	// lwz r11,1780(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82607cc8
	if (!cr6.eq) goto loc_82607CC8;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	sub_82121108(ctx, base);
	// stw r3,1780(r31)
	PPC_STORE_U32(r31.u32 + 1780, ctx.r3.u32);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,1784(r31)
	PPC_STORE_U32(r31.u32 + 1784, r11.u32);
	// lwz r11,1780(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826072ac
	if (cr6.eq) goto loc_826072AC;
loc_82607CC8:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x82607cf8
	if (!cr6.eq) goto loc_82607CF8;
	// addi r29,r31,15920
	r29.s64 = r31.s64 + 15920;
	// li r30,2
	r30.s64 = 2;
loc_82607CDC:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82614d98
	sub_82614D98(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r29,r29,1888
	r29.s64 = r29.s64 + 1888;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82607cdc
	if (!cr6.eq) goto loc_82607CDC;
loc_82607CF8:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
loc_82607CFC:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82607D04"))) PPC_WEAK_FUNC(sub_82607D04);
PPC_FUNC_IMPL(__imp__sub_82607D04) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82607D08"))) PPC_WEAK_FUNC(sub_82607D08);
PPC_FUNC_IMPL(__imp__sub_82607D08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf4
	// lwz r11,156(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// lwz r8,160(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// addi r10,r11,15
	ctx.r10.s64 = r11.s64 + 15;
	// addi r9,r8,15
	ctx.r9.s64 = ctx.r8.s64 + 15;
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// rlwinm r9,r9,0,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF0;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x82607d3c
	if (!cr6.eq) goto loc_82607D3C;
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// li r8,1
	ctx.r8.s64 = 1;
	// beq cr6,0x82607d40
	if (cr6.eq) goto loc_82607D40;
loc_82607D3C:
	// li r8,0
	ctx.r8.s64 = 0;
loc_82607D40:
	// srawi r7,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 1;
	// stw r10,21500(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21500, ctx.r10.u32);
	// lwz r10,204(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// srawi r6,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r9.s32 >> 1;
	// stw r9,21504(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21504, ctx.r9.u32);
	// stw r8,152(r3)
	PPC_STORE_U32(ctx.r3.u32 + 152, ctx.r8.u32);
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r8,136(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// stw r7,21508(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21508, ctx.r7.u32);
	// addi r9,r9,-8
	ctx.r9.s64 = ctx.r9.s64 + -8;
	// lwz r7,208(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r11,1968(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1968);
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r6,21512(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21512, ctx.r6.u32);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
	// stw r9,15176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15176, ctx.r9.u32);
	// stw r9,236(r3)
	PPC_STORE_U32(ctx.r3.u32 + 236, ctx.r9.u32);
	// li r9,2
	ctx.r9.s64 = 2;
	// stw r8,15180(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15180, ctx.r8.u32);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// li r10,2
	ctx.r10.s64 = 2;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// bgt cr6,0x82607da8
	if (cr6.gt) goto loc_82607DA8;
	// li r10,1
	ctx.r10.s64 = 1;
loc_82607DA8:
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,3356(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3356);
	// lwz r31,136(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// lwz r10,188(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// twllei r11,0
	// lwz r9,200(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 200);
	// divwu r30,r31,r11
	r30.u32 = r31.u32 / r11.u32;
	// lwz r29,140(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// divwu r10,r10,r11
	ctx.r10.u32 = ctx.r10.u32 / r11.u32;
	// lwz r7,220(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// divwu r9,r9,r11
	ctx.r9.u32 = ctx.r9.u32 / r11.u32;
	// lwz r6,224(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// divwu r8,r29,r11
	ctx.r8.u32 = r29.u32 / r11.u32;
	// twllei r11,0
	// twllei r11,0
	// stw r30,3816(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3816, r30.u32);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// stw r10,3824(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3824, ctx.r10.u32);
	// stw r7,3836(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3836, ctx.r7.u32);
	// twllei r11,0
	// stw r6,3840(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3840, ctx.r6.u32);
	// stw r9,3832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3832, ctx.r9.u32);
	// stw r8,3812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3812, ctx.r8.u32);
	// blt cr6,0x82607e80
	if (cr6.lt) goto loc_82607E80;
	// lwz r5,204(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// rlwinm r28,r10,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,208(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// stw r10,3856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3856, ctx.r10.u32);
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// stw r9,3864(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3864, ctx.r9.u32);
	// stw r8,3844(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3844, ctx.r8.u32);
	// stw r28,3860(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3860, r28.u32);
	// mullw r11,r9,r4
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// rlwinm r27,r9,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// stw r27,3868(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3868, r27.u32);
	// stw r10,3872(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3872, ctx.r10.u32);
	// stw r11,3876(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3876, r11.u32);
	// bne cr6,0x82607e60
	if (!cr6.eq) goto loc_82607E60;
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r30,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,3848(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3848, r11.u32);
	// stw r10,3852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3852, ctx.r10.u32);
	// b 0x82607e68
	goto loc_82607E68;
loc_82607E60:
	// stw r29,3848(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3848, r29.u32);
	// stw r31,3852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3852, r31.u32);
loc_82607E68:
	// mullw r11,r8,r5
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// mullw r10,r8,r4
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r11,15168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15168, r11.u32);
	// stw r10,15172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15172, ctx.r10.u32);
loc_82607E80:
	// lwz r4,268(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82607f3c
	if (cr6.eq) goto loc_82607F3C;
loc_82607E94:
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// ble cr6,0x82607f2c
	if (!cr6.gt) goto loc_82607F2C;
	// cntlzw r10,r6
	ctx.r10.u64 = ctx.r6.u32 == 0 ? 32 : __builtin_clz(ctx.r6.u32);
	// rlwinm r5,r10,28,30,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0x2;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
loc_82607EBC:
	// lwz r7,136(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cntlzw r31,r11
	r31.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r8,140(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lwz r30,0(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// subf r7,r11,r7
	ctx.r7.s64 = ctx.r7.s64 - r11.s64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// cntlzw r7,r7
	ctx.r7.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r7,r7,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// rlwinm r8,r8,28,30,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0x2;
	// rlwinm r31,r31,27,31,31
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 27) & 0x1;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// or r7,r31,r5
	ctx.r7.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r7,r7,28
	ctx.r7.u64 = ctx.r7.u32 & 0xF;
	// rlwinm r31,r30,0,20,15
	r31.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFFFFFF0FFF;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r8,r8,12,0,19
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFFFF000;
	// or r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 | r31.u64;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,136(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x82607ebc
	if (cr6.lt) goto loc_82607EBC;
loc_82607F2C:
	// lwz r11,140(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplw cr6,r6,r11
	cr6.compare<uint32_t>(ctx.r6.u32, r11.u32, xer);
	// blt cr6,0x82607e94
	if (cr6.lt) goto loc_82607E94;
loc_82607F3C:
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82607F40"))) PPC_WEAK_FUNC(sub_82607F40);
PPC_FUNC_IMPL(__imp__sub_82607F40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcd4
	// lwz r11,3924(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3924);
	// lwz r6,180(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r10,188(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// srawi r11,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r11.s64 = ctx.r6.s32 >> 1;
	// lwz r7,156(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// lwz r5,160(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// addi r11,r11,15
	r11.s64 = r11.s64 + 15;
	// lwz r9,19696(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19696);
	// rlwinm r11,r11,0,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF0;
	// srawi r8,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r8.s64 = r11.s32 >> 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r26,r9,r11
	r26.u64 = ctx.r9.u64 + r11.u64;
	// addi r10,r10,15
	ctx.r10.s64 = ctx.r10.s64 + 15;
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// srawi r29,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r29.s64 = ctx.r10.s32 >> 1;
	// srawi r30,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	r30.s64 = ctx.r7.s32 >> 1;
	// srawi r21,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	r21.s64 = ctx.r5.s32 >> 1;
	// srawi r27,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r27.s64 = r11.s32 >> 4;
	// srawi r22,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	r22.s64 = ctx.r10.s32 >> 4;
	// addi r20,r27,-1
	r20.s64 = r27.s64 + -1;
	// beq cr6,0x82607fa8
	if (cr6.eq) goto loc_82607FA8;
	// srawi r8,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r8.s64 = r11.s32 >> 2;
	// srawi r29,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r29.s64 = ctx.r10.s32 >> 2;
loc_82607FA8:
	// lwz r9,19700(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19700);
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// lwz r7,19696(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19696);
	// stw r6,14832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14832, ctx.r6.u32);
	// rlwinm r6,r9,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,192(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 192);
	// rlwinm r5,r7,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r28,r9,r8
	r28.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r31,r5,r11
	r31.u64 = ctx.r5.u64 + r11.u64;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// stw r4,14836(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14836, ctx.r4.u32);
	// lwz r4,188(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// stw r4,14840(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14840, ctx.r4.u32);
	// lwz r4,200(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 200);
	// stw r4,14844(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14844, ctx.r4.u32);
	// lwz r4,156(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// stw r4,14848(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14848, ctx.r4.u32);
	// lwz r4,160(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// stw r4,14852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14852, ctx.r4.u32);
	// lwz r4,184(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 184);
	// stw r4,14856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14856, ctx.r4.u32);
	// lwz r4,196(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 196);
	// stw r4,14860(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14860, ctx.r4.u32);
	// lwz r4,152(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// stw r4,14864(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14864, ctx.r4.u32);
	// add r4,r6,r8
	ctx.r4.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lwz r25,136(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// add r6,r6,r29
	ctx.r6.u64 = ctx.r6.u64 + r29.u64;
	// addi r24,r4,1
	r24.s64 = ctx.r4.s64 + 1;
	// mullw r9,r24,r9
	ctx.r9.s64 = int64_t(r24.s32) * int64_t(ctx.r9.s32);
	// stw r25,14868(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14868, r25.u32);
	// lwz r25,140(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// stw r25,14872(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14872, r25.u32);
	// lwz r25,144(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 144);
	// stw r25,14876(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14876, r25.u32);
	// lwz r25,148(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 148);
	// stw r25,14880(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14880, r25.u32);
	// lwz r25,204(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// stw r25,14884(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14884, r25.u32);
	// lwz r25,208(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// stw r25,14888(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14888, r25.u32);
	// lwz r25,212(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 212);
	// stw r25,14892(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14892, r25.u32);
	// lwz r25,216(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 216);
	// stw r25,14896(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14896, r25.u32);
	// lwz r25,220(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// stw r25,14900(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14900, r25.u32);
	// addi r25,r31,1
	r25.s64 = r31.s64 + 1;
	// lwz r23,224(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// mullw r7,r25,r7
	ctx.r7.s64 = int64_t(r25.s32) * int64_t(ctx.r7.s32);
	// stw r23,14904(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14904, r23.u32);
	// lwz r24,228(r3)
	r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 228);
	// stw r24,14908(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14908, r24.u32);
	// rlwinm r24,r4,3,0,28
	r24.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r25,232(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 232);
	// stw r25,14912(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14912, r25.u32);
	// rlwinm r25,r31,4,0,27
	r25.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r11,14916(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14916, r11.u32);
	// stw r8,14920(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14920, ctx.r8.u32);
	// lwz r23,188(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// stw r23,14924(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14924, r23.u32);
	// lwz r23,200(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 200);
	// stw r30,14932(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14932, r30.u32);
	// stw r23,14928(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14928, r23.u32);
	// lwz r23,160(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// stw r26,14940(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14940, r26.u32);
	// stw r28,14944(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14944, r28.u32);
	// stw r23,14936(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14936, r23.u32);
	// bne cr6,0x826080d0
	if (!cr6.eq) goto loc_826080D0;
	// lwz r23,188(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// lwz r19,160(r3)
	r19.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// cmpw cr6,r23,r19
	cr6.compare<int32_t>(r23.s32, r19.s32, xer);
	// li r23,1
	r23.s64 = 1;
	// beq cr6,0x826080d4
	if (cr6.eq) goto loc_826080D4;
loc_826080D0:
	// li r23,0
	r23.s64 = 0;
loc_826080D4:
	// stw r23,14948(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14948, r23.u32);
	// stw r27,14952(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14952, r27.u32);
	// lwz r23,140(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// stw r23,14956(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14956, r23.u32);
	// lwz r23,140(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// stw r20,14964(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14964, r20.u32);
	// mullw r23,r23,r27
	r23.s64 = int64_t(r23.s32) * int64_t(r27.s32);
	// stw r31,14968(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14968, r31.u32);
	// stw r4,14972(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14972, ctx.r4.u32);
	// stw r23,14960(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14960, r23.u32);
	// lwz r23,212(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 212);
	// stw r23,14976(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14976, r23.u32);
	// lwz r23,216(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 216);
	// stw r7,14984(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14984, ctx.r7.u32);
	// stw r9,14988(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14988, ctx.r9.u32);
	// stw r25,14992(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14992, r25.u32);
	// stw r24,14996(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14996, r24.u32);
	// stw r23,14980(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14980, r23.u32);
	// lwz r23,180(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// stw r23,15000(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15000, r23.u32);
	// lwz r23,192(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 192);
	// stw r10,15008(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15008, ctx.r10.u32);
	// stw r29,15012(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15012, r29.u32);
	// stw r23,15004(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15004, r23.u32);
	// lwz r23,156(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// stw r21,15020(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15020, r21.u32);
	// stw r23,15016(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15016, r23.u32);
	// lwz r23,184(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 184);
	// stw r23,15024(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15024, r23.u32);
	// lwz r23,196(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 196);
	// stw r23,15028(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15028, r23.u32);
	// lwz r23,180(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// lwz r19,156(r3)
	r19.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// cmpw cr6,r23,r19
	cr6.compare<int32_t>(r23.s32, r19.s32, xer);
	// bne cr6,0x8260816c
	if (!cr6.eq) goto loc_8260816C;
	// cmpw cr6,r10,r21
	cr6.compare<int32_t>(ctx.r10.s32, r21.s32, xer);
	// li r23,1
	r23.s64 = 1;
	// beq cr6,0x82608170
	if (cr6.eq) goto loc_82608170;
loc_8260816C:
	// li r23,0
	r23.s64 = 0;
loc_82608170:
	// stw r23,15032(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15032, r23.u32);
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// lwz r23,136(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// stw r22,15040(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15040, r22.u32);
	// stw r23,15036(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15036, r23.u32);
	// lwz r23,136(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// mullw r23,r23,r22
	r23.s64 = int64_t(r23.s32) * int64_t(r22.s32);
	// stw r23,15044(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15044, r23.u32);
	// lwz r23,148(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 148);
	// stw r23,15048(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15048, r23.u32);
	// lwz r23,204(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// stw r23,15052(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15052, r23.u32);
	// lwz r23,208(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// stw r5,15060(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15060, ctx.r5.u32);
	// stw r6,15064(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15064, ctx.r6.u32);
	// stw r23,15056(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15056, r23.u32);
	// lwz r23,220(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// stw r23,15068(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15068, r23.u32);
	// lwz r23,224(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// stw r23,15072(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15072, r23.u32);
	// lwz r23,228(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 228);
	// stw r23,15076(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15076, r23.u32);
	// lwz r23,232(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 232);
	// stw r23,15080(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15080, r23.u32);
	// stw r11,15084(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15084, r11.u32);
	// stw r8,15088(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15088, ctx.r8.u32);
	// stw r10,15092(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15092, ctx.r10.u32);
	// stw r29,15096(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15096, r29.u32);
	// stw r30,15100(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15100, r30.u32);
	// stw r21,15104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15104, r21.u32);
	// stw r26,15108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15108, r26.u32);
	// stw r28,15112(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15112, r28.u32);
	// bne cr6,0x82608200
	if (!cr6.eq) goto loc_82608200;
	// cmpw cr6,r10,r21
	cr6.compare<int32_t>(ctx.r10.s32, r21.s32, xer);
	// li r11,1
	r11.s64 = 1;
	// beq cr6,0x82608204
	if (cr6.eq) goto loc_82608204;
loc_82608200:
	// li r11,0
	r11.s64 = 0;
loc_82608204:
	// mullw r10,r22,r27
	ctx.r10.s64 = int64_t(r22.s32) * int64_t(r27.s32);
	// stw r11,15116(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15116, r11.u32);
	// stw r27,15120(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15120, r27.u32);
	// stw r22,15124(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15124, r22.u32);
	// stw r20,15132(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15132, r20.u32);
	// stw r31,15136(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15136, r31.u32);
	// stw r10,15128(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15128, ctx.r10.u32);
	// stw r4,15140(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15140, ctx.r4.u32);
	// stw r5,15144(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15144, ctx.r5.u32);
	// stw r6,15148(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15148, ctx.r6.u32);
	// stw r7,15152(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15152, ctx.r7.u32);
	// stw r9,15156(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15156, ctx.r9.u32);
	// stw r25,15160(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15160, r25.u32);
	// stw r24,15164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15164, r24.u32);
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_82608240"))) PPC_WEAK_FUNC(sub_82608240);
PPC_FUNC_IMPL(__imp__sub_82608240) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mulli r11,r4,84
	r11.s64 = ctx.r4.s64 * 84;
	// lwz r10,20056(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20056);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,14832(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14832);
	// stw r10,180(r3)
	PPC_STORE_U32(ctx.r3.u32 + 180, ctx.r10.u32);
	// lwz r10,14836(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14836);
	// lwz r9,180(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// stw r10,192(r3)
	PPC_STORE_U32(ctx.r3.u32 + 192, ctx.r10.u32);
	// lwz r10,14840(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14840);
	// lwz r8,192(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 192);
	// stw r10,188(r3)
	PPC_STORE_U32(ctx.r3.u32 + 188, ctx.r10.u32);
	// lwz r10,14844(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14844);
	// lwz r7,188(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// stw r9,164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 164, ctx.r9.u32);
	// stw r8,168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 168, ctx.r8.u32);
	// stw r10,200(r3)
	PPC_STORE_U32(ctx.r3.u32 + 200, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r7,172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 172, ctx.r7.u32);
	// stw r10,176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 176, ctx.r10.u32);
	// lwz r10,14848(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14848);
	// stw r10,156(r3)
	PPC_STORE_U32(ctx.r3.u32 + 156, ctx.r10.u32);
	// lwz r10,14852(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14852);
	// stw r10,160(r3)
	PPC_STORE_U32(ctx.r3.u32 + 160, ctx.r10.u32);
	// beq cr6,0x826082d4
	if (cr6.eq) goto loc_826082D4;
	// lwz r9,156(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// srawi r10,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// srawi r9,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 1;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 168, ctx.r10.u32);
	// stw r9,176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 176, ctx.r9.u32);
	// stw r8,164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 164, ctx.r8.u32);
	// stw r7,172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 172, ctx.r7.u32);
loc_826082D4:
	// lwz r9,14856(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14856);
	// lwz r10,3732(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3732);
	// stw r9,184(r3)
	PPC_STORE_U32(ctx.r3.u32 + 184, ctx.r9.u32);
	// lwz r9,14860(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14860);
	// stw r9,196(r3)
	PPC_STORE_U32(ctx.r3.u32 + 196, ctx.r9.u32);
	// lwz r9,14864(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14864);
	// stw r9,152(r3)
	PPC_STORE_U32(ctx.r3.u32 + 152, ctx.r9.u32);
	// lwz r9,14868(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14868);
	// stw r9,136(r3)
	PPC_STORE_U32(ctx.r3.u32 + 136, ctx.r9.u32);
	// lwz r9,14872(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14872);
	// stw r9,140(r3)
	PPC_STORE_U32(ctx.r3.u32 + 140, ctx.r9.u32);
	// lwz r9,14876(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14876);
	// stw r9,144(r3)
	PPC_STORE_U32(ctx.r3.u32 + 144, ctx.r9.u32);
	// lwz r9,14880(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14880);
	// stw r9,148(r3)
	PPC_STORE_U32(ctx.r3.u32 + 148, ctx.r9.u32);
	// lwz r9,14884(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14884);
	// stw r9,204(r3)
	PPC_STORE_U32(ctx.r3.u32 + 204, ctx.r9.u32);
	// lwz r9,14888(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14888);
	// stw r9,208(r3)
	PPC_STORE_U32(ctx.r3.u32 + 208, ctx.r9.u32);
	// lwz r9,14892(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14892);
	// stw r9,212(r3)
	PPC_STORE_U32(ctx.r3.u32 + 212, ctx.r9.u32);
	// lwz r9,14896(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14896);
	// stw r9,216(r3)
	PPC_STORE_U32(ctx.r3.u32 + 216, ctx.r9.u32);
	// lwz r9,14900(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14900);
	// stw r9,220(r3)
	PPC_STORE_U32(ctx.r3.u32 + 220, ctx.r9.u32);
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// lwz r8,14904(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 14904);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r8,224(r3)
	PPC_STORE_U32(ctx.r3.u32 + 224, ctx.r8.u32);
	// lwz r9,14908(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14908);
	// stw r9,228(r3)
	PPC_STORE_U32(ctx.r3.u32 + 228, ctx.r9.u32);
	// lwz r11,14912(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 14912);
	// stw r10,3756(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3756, ctx.r10.u32);
	// stw r11,232(r3)
	PPC_STORE_U32(ctx.r3.u32 + 232, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82608360"))) PPC_WEAK_FUNC(sub_82608360);
PPC_FUNC_IMPL(__imp__sub_82608360) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r17,r9
	r17.u64 = ctx.r9.u64;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// mr r22,r8
	r22.u64 = ctx.r8.u64;
	// addi r10,r10,28640
	ctx.r10.s64 = ctx.r10.s64 + 28640;
	// lwz r11,14828(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 14828);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lwz r9,14824(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 14824);
	// addi r8,r10,24
	ctx.r8.s64 = ctx.r10.s64 + 24;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// mulli r11,r11,84
	r11.s64 = r11.s64 * 84;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwzx r19,r9,r8
	r19.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r26,14832(r11)
	r26.u64 = PPC_LOAD_U32(r11.u32 + 14832);
	// mr r24,r27
	r24.u64 = r27.u64;
	// lwz r25,14840(r11)
	r25.u64 = PPC_LOAD_U32(r11.u32 + 14840);
	// mr r21,r22
	r21.u64 = r22.u64;
	// lwz r23,14884(r11)
	r23.u64 = PPC_LOAD_U32(r11.u32 + 14884);
	// mr r15,r17
	r15.u64 = r17.u64;
	// lwz r14,14888(r11)
	r14.u64 = PPC_LOAD_U32(r11.u32 + 14888);
	// cmpwi cr6,r19,2
	cr6.compare<int32_t>(r19.s32, 2, xer);
	// lwzx r18,r9,r10
	r18.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// bne cr6,0x826083ec
	if (!cr6.eq) goto loc_826083EC;
	// addi r11,r26,31
	r11.s64 = r26.s64 + 31;
	// rlwinm r26,r11,0,0,26
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
loc_826083EC:
	// cmpwi cr6,r18,2
	cr6.compare<int32_t>(r18.s32, 2, xer);
	// bne cr6,0x826083fc
	if (!cr6.eq) goto loc_826083FC;
	// addi r11,r25,31
	r11.s64 = r25.s64 + 31;
	// rlwinm r25,r11,0,0,26
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
loc_826083FC:
	// lwz r11,3924(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82608414
	if (cr6.eq) goto loc_82608414;
	// mr r16,r25
	r16.u64 = r25.u64;
	// srawi r20,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r20.s64 = r26.s32 >> 2;
	// b 0x8260841c
	goto loc_8260841C;
loc_82608414:
	// srawi r20,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	r20.s64 = r26.s32 >> 1;
	// srawi r16,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	r16.s64 = r25.s32 >> 1;
loc_8260841C:
	// cmpwi cr6,r19,2
	cr6.compare<int32_t>(r19.s32, 2, xer);
	// bne cr6,0x82608494
	if (!cr6.eq) goto loc_82608494;
	// lwz r10,15812(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 15812);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// lwz r11,15188(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15188);
	// mr r8,r17
	ctx.r8.u64 = r17.u64;
	// stw r14,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r14.u32);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// stw r23,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r23.u32);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// stw r16,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r16.u32);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// stw r20,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r20.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r18,2
	cr6.compare<int32_t>(r18.s32, 2, xer);
	// bne cr6,0x826084d4
	if (!cr6.eq) goto loc_826084D4;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// mr r24,r31
	r24.u64 = r31.u64;
	// mr r21,r30
	r21.u64 = r30.u64;
	// mr r15,r29
	r15.u64 = r29.u64;
	// b 0x8260849c
	goto loc_8260849C;
loc_82608494:
	// cmpwi cr6,r18,2
	cr6.compare<int32_t>(r18.s32, 2, xer);
	// bne cr6,0x826084d4
	if (!cr6.eq) goto loc_826084D4;
loc_8260849C:
	// lwz r11,15188(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15188);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// lwz r31,15816(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 15816);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// stw r14,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r14.u32);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// stw r23,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r23.u32);
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// stw r16,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r16.u32);
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// stw r20,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r20.u32);
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826084D4:
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x82608528
	if (!cr6.gt) goto loc_82608528;
	// mullw r7,r18,r23
	ctx.r7.s64 = int64_t(r18.s32) * int64_t(r23.s32);
loc_826084E8:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x82608510
	if (!cr6.gt) goto loc_82608510;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_826084F8:
	// lbzx r6,r11,r9
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// add r11,r11,r19
	r11.u64 = r11.u64 + r19.u64;
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// stb r6,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r6.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x826084f8
	if (cr6.lt) goto loc_826084F8;
loc_82608510:
	// lwz r11,204(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 204);
	// add r8,r8,r18
	ctx.r8.u64 = ctx.r8.u64 + r18.u64;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// cmpw cr6,r8,r25
	cr6.compare<int32_t>(ctx.r8.s32, r25.s32, xer);
	// blt cr6,0x826084e8
	if (cr6.lt) goto loc_826084E8;
loc_82608528:
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// ble cr6,0x82608580
	if (!cr6.gt) goto loc_82608580;
	// mullw r6,r18,r14
	ctx.r6.s64 = int64_t(r18.s32) * int64_t(r14.s32);
loc_82608540:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x82608568
	if (!cr6.gt) goto loc_82608568;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
loc_82608550:
	// lbzx r5,r11,r9
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// add r11,r11,r19
	r11.u64 = r11.u64 + r19.u64;
	// cmpw cr6,r11,r20
	cr6.compare<int32_t>(r11.s32, r20.s32, xer);
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x82608550
	if (cr6.lt) goto loc_82608550;
loc_82608568:
	// lwz r11,208(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 208);
	// add r8,r8,r18
	ctx.r8.u64 = ctx.r8.u64 + r18.u64;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// cmpw cr6,r8,r16
	cr6.compare<int32_t>(ctx.r8.s32, r16.s32, xer);
	// blt cr6,0x82608540
	if (cr6.lt) goto loc_82608540;
loc_82608580:
	// mr r9,r15
	ctx.r9.u64 = r15.u64;
	// mr r7,r17
	ctx.r7.u64 = r17.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// ble cr6,0x826085d8
	if (!cr6.gt) goto loc_826085D8;
	// mullw r6,r18,r14
	ctx.r6.s64 = int64_t(r18.s32) * int64_t(r14.s32);
loc_82608598:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x826085c0
	if (!cr6.gt) goto loc_826085C0;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
loc_826085A8:
	// lbzx r5,r11,r9
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// add r11,r11,r19
	r11.u64 = r11.u64 + r19.u64;
	// cmpw cr6,r11,r20
	cr6.compare<int32_t>(r11.s32, r20.s32, xer);
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x826085a8
	if (cr6.lt) goto loc_826085A8;
loc_826085C0:
	// lwz r11,208(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 208);
	// add r8,r8,r18
	ctx.r8.u64 = ctx.r8.u64 + r18.u64;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// cmpw cr6,r8,r16
	cr6.compare<int32_t>(ctx.r8.s32, r16.s32, xer);
	// blt cr6,0x82608598
	if (cr6.lt) goto loc_82608598;
loc_826085D8:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826085E0"))) PPC_WEAK_FUNC(sub_826085E0);
PPC_FUNC_IMPL(__imp__sub_826085E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf0
	// addi r27,r6,-1
	r27.s64 = ctx.r6.s64 + -1;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// cmpwi cr6,r27,1
	cr6.compare<int32_t>(r27.s32, 1, xer);
	// ble cr6,0x82608650
	if (!cr6.gt) goto loc_82608650;
	// addi r11,r27,-2
	r11.s64 = r27.s64 + -2;
	// rlwinm r31,r7,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r11,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r8,r5,4
	ctx.r8.s64 = ctx.r5.s64 + 4;
	// add r11,r4,r7
	r11.u64 = ctx.r4.u64 + ctx.r7.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// subf r28,r7,r31
	r28.s64 = r31.s64 - ctx.r7.s64;
loc_82608614:
	// lbz r29,0(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lbzx r30,r28,r11
	r30.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// lbz r26,0(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// rotlwi r29,r26,4
	r29.u64 = __builtin_rotateleft32(r26.u32, 4);
	// mulli r30,r30,-406
	r30.s64 = r30.s64 * -406;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r30.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne cr6,0x82608614
	if (!cr6.eq) goto loc_82608614;
loc_82608650:
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbzx r8,r10,r7
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r7.u32);
	// addi r30,r5,4
	r30.s64 = ctx.r5.s64 + 4;
	// add r29,r11,r5
	r29.u64 = r11.u64 + ctx.r5.u64;
	// mulli r11,r9,-406
	r11.s64 = ctx.r9.s64 * -406;
	// srawi r10,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r10.s64 = r11.s32 >> 3;
	// rotlwi r11,r8,4
	r11.u64 = __builtin_rotateleft32(ctx.r8.u32, 4);
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-4(r29)
	PPC_STORE_U32(r29.u32 + -4, r11.u32);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// mulli r11,r11,-217
	r11.s64 = r11.s64 * -217;
	// srawi r11,r11,10
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FF) != 0);
	r11.s64 = r11.s32 >> 10;
	// rotlwi r10,r10,5
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 5);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// ble cr6,0x826086e8
	if (!cr6.gt) goto loc_826086E8;
	// addi r11,r6,-3
	r11.s64 = ctx.r6.s64 + -3;
	// rlwinm r31,r7,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r11,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r11,r5,12
	r11.s64 = ctx.r5.s64 + 12;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_826086B0:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r4,r31,r4
	ctx.r4.u64 = r31.u64 + ctx.r4.u64;
	// lwz r9,-8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lbz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// mulli r9,r9,-217
	ctx.r9.s64 = ctx.r9.s64 * -217;
	// srawi r9,r9,11
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 11;
	// rotlwi r8,r8,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 5);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r9.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x826086b0
	if (!cr6.eq) goto loc_826086B0;
loc_826086E8:
	// cmpwi cr6,r27,1
	cr6.compare<int32_t>(r27.s32, 1, xer);
	// ble cr6,0x82608730
	if (!cr6.gt) goto loc_82608730;
	// addi r10,r27,-2
	ctx.r10.s64 = r27.s64 + -2;
	// mr r11,r30
	r11.u64 = r30.u64;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_82608700:
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// mulli r9,r9,226
	ctx.r9.s64 = ctx.r9.s64 * 226;
	// srawi r9,r9,9
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1FF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 9;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82608700
	if (!cr6.eq) goto loc_82608700;
loc_82608730:
	// addi r11,r6,-2
	r11.s64 = ctx.r6.s64 + -2;
	// lwz r10,-4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + -4);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lwzx r11,r11,r5
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r5.u32);
	// mulli r11,r11,226
	r11.s64 = r11.s64 * 226;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-4(r29)
	PPC_STORE_U32(r29.u32 + -4, r11.u32);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ble cr6,0x826087d4
	if (!cr6.gt) goto loc_826087D4;
	// addi r11,r6,-1
	r11.s64 = ctx.r6.s64 + -1;
	// rlwinm r4,r7,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r11,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r11,r30
	r11.u64 = r30.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// li r3,0
	ctx.r3.s64 = 0;
loc_82608778:
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r6,-4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// mulli r10,r10,227
	ctx.r10.s64 = ctx.r10.s64 * 227;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// mulli r10,r10,26
	ctx.r10.s64 = ctx.r10.s64 * 26;
	// srawi r10,r10,10
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 10;
	// cmplwi cr6,r10,255
	cr6.compare<uint32_t>(ctx.r10.u32, 255, xer);
	// ble cr6,0x826087b4
	if (!cr6.gt) goto loc_826087B4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// blt cr6,0x826087b4
	if (cr6.lt) goto loc_826087B4;
	// li r10,255
	ctx.r10.s64 = 255;
loc_826087B4:
	// stb r10,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r10.u8);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stbx r3,r9,r7
	PPC_STORE_U8(ctx.r9.u32 + ctx.r7.u32, ctx.r3.u8);
	// add r9,r4,r9
	ctx.r9.u64 = ctx.r4.u64 + ctx.r9.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82608778
	if (!cr6.eq) goto loc_82608778;
loc_826087D4:
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_826087D8"))) PPC_WEAK_FUNC(sub_826087D8);
PPC_FUNC_IMPL(__imp__sub_826087D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// addi r11,r4,-1
	r11.s64 = ctx.r4.s64 + -1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// ble cr6,0x82608828
	if (!cr6.gt) goto loc_82608828;
	// addi r10,r11,-2
	ctx.r10.s64 = r11.s64 + -2;
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_826087F4:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r8,-4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r8,r8,7,0,24
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0xFFFFFF80;
	// rlwinm r7,r7,7,0,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 7) & 0xFFFFFF80;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x826087f4
	if (!cr6.eq) goto loc_826087F4;
loc_82608828:
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r4,-2
	ctx.r10.s64 = ctx.r4.s64 + -2;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r10,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r10.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r10,r11,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r11,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r11.s64 = ctx.r9.s32 >> 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// ble cr6,0x826088b0
	if (!cr6.gt) goto loc_826088B0;
	// addi r10,r4,-3
	ctx.r10.s64 = ctx.r4.s64 + -3;
	// addi r11,r3,8
	r11.s64 = ctx.r3.s64 + 8;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_82608880:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r8,r9,8,0,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82608880
	if (!cr6.eq) goto loc_82608880;
loc_826088B0:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// addi r10,r4,-1
	ctx.r10.s64 = ctx.r4.s64 + -1;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
loc_826088CC:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,128
	ctx.r10.s64 = ctx.r10.s64 + 128;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// cmplwi cr6,r10,255
	cr6.compare<uint32_t>(ctx.r10.u32, 255, xer);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// ble cr6,0x826088f4
	if (!cr6.gt) goto loc_826088F4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// blt cr6,0x826088f4
	if (cr6.lt) goto loc_826088F4;
	// li r10,255
	ctx.r10.s64 = 255;
loc_826088F4:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x826088cc
	if (!cr6.eq) goto loc_826088CC;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82608910"))) PPC_WEAK_FUNC(sub_82608910);
PPC_FUNC_IMPL(__imp__sub_82608910) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r26,292(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// mr r23,r7
	r23.u64 = ctx.r7.u64;
	// mr r21,r8
	r21.u64 = ctx.r8.u64;
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82608974
	if (!cr6.gt) goto loc_82608974;
	// lwz r27,276(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// subf r29,r3,r6
	r29.s64 = ctx.r6.s64 - ctx.r3.s64;
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
loc_8260894C:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r29,r31
	ctx.r3.u64 = r29.u64 + r31.u64;
	// bl 0x826085e0
	sub_826085E0(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r31,r31,r27
	r31.u64 = r31.u64 + r27.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8260894c
	if (!cr6.eq) goto loc_8260894C;
loc_82608974:
	// lwz r30,268(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mr r31,r24
	r31.u64 = r24.u64;
	// lwz r27,284(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// lwz r25,260(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x826089bc
	if (!cr6.gt) goto loc_826089BC;
	// subf r28,r24,r23
	r28.s64 = r23.s64 - r24.s64;
	// mr r29,r30
	r29.u64 = r30.u64;
loc_82608994:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r28,r31
	ctx.r3.u64 = r28.u64 + r31.u64;
	// bl 0x826085e0
	sub_826085E0(ctx, base);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r31,r31,r27
	r31.u64 = r31.u64 + r27.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82608994
	if (!cr6.eq) goto loc_82608994;
loc_826089BC:
	// mr r31,r22
	r31.u64 = r22.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x826089f4
	if (!cr6.gt) goto loc_826089F4;
	// subf r29,r22,r21
	r29.s64 = r21.s64 - r22.s64;
loc_826089CC:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r29,r31
	ctx.r3.u64 = r29.u64 + r31.u64;
	// bl 0x826085e0
	sub_826085E0(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r31,r31,r27
	r31.u64 = r31.u64 + r27.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826089cc
	if (!cr6.eq) goto loc_826089CC;
loc_826089F4:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_826089FC"))) PPC_WEAK_FUNC(sub_826089FC);
PPC_FUNC_IMPL(__imp__sub_826089FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82608A00"))) PPC_WEAK_FUNC(sub_82608A00);
PPC_FUNC_IMPL(__imp__sub_82608A00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,260(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82608aa0
	if (!cr6.gt) goto loc_82608AA0;
	// lwz r29,244(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// subf r6,r31,r6
	ctx.r6.s64 = ctx.r6.s64 - r31.s64;
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
loc_82608A38:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608a60
	if (!cr6.gt) goto loc_82608A60;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_82608A48:
	// lbzx r9,r11,r31
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + r31.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x82608a48
	if (cr6.lt) goto loc_82608A48;
loc_82608A60:
	// bl 0x826087d8
	sub_826087D8(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608a90
	if (!cr6.gt) goto loc_82608A90;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_82608A74:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r9,r6,r11
	ctx.r9.u64 = ctx.r6.u64 + r11.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// stbx r8,r9,r31
	PPC_STORE_U8(ctx.r9.u32 + r31.u32, ctx.r8.u8);
	// blt cr6,0x82608a74
	if (cr6.lt) goto loc_82608A74;
loc_82608A90:
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r31,r31,r29
	r31.u64 = r31.u64 + r29.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82608a38
	if (!cr6.eq) goto loc_82608A38;
loc_82608AA0:
	// lwz r29,236(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// lwz r26,252(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,228(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x82608b28
	if (!cr6.gt) goto loc_82608B28;
	// subf r31,r28,r27
	r31.s64 = r27.s64 - r28.s64;
	// mr r30,r29
	r30.u64 = r29.u64;
loc_82608AC0:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608ae8
	if (!cr6.gt) goto loc_82608AE8;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_82608AD0:
	// lbzx r9,r11,r6
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x82608ad0
	if (cr6.lt) goto loc_82608AD0;
loc_82608AE8:
	// bl 0x826087d8
	sub_826087D8(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608b18
	if (!cr6.gt) goto loc_82608B18;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_82608AFC:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r9,r31,r11
	ctx.r9.u64 = r31.u64 + r11.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// stbx r8,r9,r6
	PPC_STORE_U8(ctx.r9.u32 + ctx.r6.u32, ctx.r8.u8);
	// blt cr6,0x82608afc
	if (cr6.lt) goto loc_82608AFC;
loc_82608B18:
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 + r26.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82608ac0
	if (!cr6.eq) goto loc_82608AC0;
loc_82608B28:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x82608ba4
	if (!cr6.gt) goto loc_82608BA4;
	// subf r5,r5,r25
	ctx.r5.s64 = r25.s64 - ctx.r5.s64;
	// mr r31,r29
	r31.u64 = r29.u64;
loc_82608B3C:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608b64
	if (!cr6.gt) goto loc_82608B64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_82608B4C:
	// lbzx r9,r11,r6
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x82608b4c
	if (cr6.lt) goto loc_82608B4C;
loc_82608B64:
	// bl 0x826087d8
	sub_826087D8(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608b94
	if (!cr6.gt) goto loc_82608B94;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_82608B78:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r9,r5,r11
	ctx.r9.u64 = ctx.r5.u64 + r11.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// stbx r8,r9,r6
	PPC_STORE_U8(ctx.r9.u32 + ctx.r6.u32, ctx.r8.u8);
	// blt cr6,0x82608b78
	if (cr6.lt) goto loc_82608B78;
loc_82608B94:
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// add r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 + r26.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82608b3c
	if (!cr6.eq) goto loc_82608B3C;
loc_82608BA4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_82608BAC"))) PPC_WEAK_FUNC(sub_82608BAC);
PPC_FUNC_IMPL(__imp__sub_82608BAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82608BB0"))) PPC_WEAK_FUNC(sub_82608BB0);
PPC_FUNC_IMPL(__imp__sub_82608BB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r25,276(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
	// mr r26,r10
	r26.u64 = ctx.r10.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82608c10
	if (!cr6.gt) goto loc_82608C10;
	// lwz r7,260(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// subf r27,r6,r3
	r27.s64 = ctx.r3.s64 - ctx.r6.s64;
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
loc_82608BEC:
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// add r4,r27,r29
	ctx.r4.u64 = r27.u64 + r29.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826085e0
	sub_826085E0(ctx, base);
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82608bec
	if (!cr6.eq) goto loc_82608BEC;
loc_82608C10:
	// lwz r26,244(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r7,268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// lwz r27,252(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x82608c84
	if (!cr6.gt) goto loc_82608C84;
	// subf r28,r30,r24
	r28.s64 = r24.s64 - r30.s64;
	// mr r29,r26
	r29.u64 = r26.u64;
loc_82608C2C:
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// add r4,r28,r30
	ctx.r4.u64 = r28.u64 + r30.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x826085e0
	sub_826085E0(ctx, base);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82608c2c
	if (!cr6.eq) goto loc_82608C2C;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x82608c84
	if (!cr6.gt) goto loc_82608C84;
	// subf r29,r31,r23
	r29.s64 = r23.s64 - r31.s64;
	// mr r30,r26
	r30.u64 = r26.u64;
loc_82608C60:
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// add r4,r29,r31
	ctx.r4.u64 = r29.u64 + r31.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826085e0
	sub_826085E0(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82608c60
	if (!cr6.eq) goto loc_82608C60;
loc_82608C84:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82608C8C"))) PPC_WEAK_FUNC(sub_82608C8C);
PPC_FUNC_IMPL(__imp__sub_82608C8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82608C90"))) PPC_WEAK_FUNC(sub_82608C90);
PPC_FUNC_IMPL(__imp__sub_82608C90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r3,260(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82608d3c
	if (!cr6.gt) goto loc_82608D3C;
	// lwz r31,244(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// subf r30,r6,r11
	r30.s64 = r11.s64 - ctx.r6.s64;
	// mr r29,r9
	r29.u64 = ctx.r9.u64;
loc_82608CC8:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608cf8
	if (!cr6.gt) goto loc_82608CF8;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// add r10,r30,r6
	ctx.r10.u64 = r30.u64 + ctx.r6.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_82608CDC:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x82608cdc
	if (!cr6.eq) goto loc_82608CDC;
loc_82608CF8:
	// bl 0x826087d8
	sub_826087D8(ctx, base);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608d2c
	if (!cr6.gt) goto loc_82608D2C;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_82608D10:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// bne cr6,0x82608d10
	if (!cr6.eq) goto loc_82608D10;
loc_82608D2C:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82608cc8
	if (!cr6.eq) goto loc_82608CC8;
loc_82608D3C:
	// lwz r28,228(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// lwz r31,252(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,236(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x82608e58
	if (!cr6.gt) goto loc_82608E58;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// subf r30,r27,r26
	r30.s64 = r26.s64 - r27.s64;
	// mr r29,r28
	r29.u64 = r28.u64;
loc_82608D5C:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608d8c
	if (!cr6.gt) goto loc_82608D8C;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// add r10,r30,r6
	ctx.r10.u64 = r30.u64 + ctx.r6.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_82608D70:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x82608d70
	if (!cr6.eq) goto loc_82608D70;
loc_82608D8C:
	// bl 0x826087d8
	sub_826087D8(ctx, base);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608dc0
	if (!cr6.gt) goto loc_82608DC0;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_82608DA4:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// bne cr6,0x82608da4
	if (!cr6.eq) goto loc_82608DA4;
loc_82608DC0:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82608d5c
	if (!cr6.eq) goto loc_82608D5C;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x82608e58
	if (!cr6.gt) goto loc_82608E58;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// subf r5,r25,r5
	ctx.r5.s64 = ctx.r5.s64 - r25.s64;
	// mr r30,r28
	r30.u64 = r28.u64;
loc_82608DE4:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608e14
	if (!cr6.gt) goto loc_82608E14;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// add r10,r5,r6
	ctx.r10.u64 = ctx.r5.u64 + ctx.r6.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_82608DF8:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x82608df8
	if (!cr6.eq) goto loc_82608DF8;
loc_82608E14:
	// bl 0x826087d8
	sub_826087D8(ctx, base);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82608e48
	if (!cr6.gt) goto loc_82608E48;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_82608E2C:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// bne cr6,0x82608e2c
	if (!cr6.eq) goto loc_82608E2C;
loc_82608E48:
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82608de4
	if (!cr6.eq) goto loc_82608DE4;
loc_82608E58:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_82608E60"))) PPC_WEAK_FUNC(sub_82608E60);
PPC_FUNC_IMPL(__imp__sub_82608E60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// addi r10,r10,28640
	ctx.r10.s64 = ctx.r10.s64 + 28640;
	// lwz r11,14828(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14828);
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// lwz r7,14824(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// addi r30,r10,24
	r30.s64 = ctx.r10.s64 + 24;
	// mulli r9,r11,84
	ctx.r9.s64 = r11.s64 * 84;
	// lwz r29,188(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// add r11,r9,r31
	r11.u64 = ctx.r9.u64 + r31.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r7,14884(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 14884);
	// lwz r28,14888(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 14888);
	// lwzx r29,r9,r10
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// lwzx r30,r9,r30
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + r30.u32);
	// ble cr6,0x82608f14
	if (!cr6.gt) goto loc_82608F14;
	// lwz r9,180(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 180);
loc_82608EC8:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82608ef4
	if (!cr6.gt) goto loc_82608EF4;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
loc_82608ED8:
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stbx r9,r11,r8
	PPC_STORE_U8(r11.u32 + ctx.r8.u32, ctx.r9.u8);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r9,180(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82608ed8
	if (cr6.lt) goto loc_82608ED8;
loc_82608EF4:
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r3,r3,r29
	ctx.r3.u64 = ctx.r3.u64 + r29.u64;
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// cmpw cr6,r3,r10
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r10.s32, xer);
	// blt cr6,0x82608ec8
	if (cr6.lt) goto loc_82608EC8;
loc_82608F14:
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82608f78
	if (!cr6.gt) goto loc_82608F78;
	// lwz r8,192(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 192);
loc_82608F2C:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x82608f58
	if (!cr6.gt) goto loc_82608F58;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
loc_82608F3C:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stbx r8,r11,r9
	PPC_STORE_U8(r11.u32 + ctx.r9.u32, ctx.r8.u8);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r8,192(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// blt cr6,0x82608f3c
	if (cr6.lt) goto loc_82608F3C;
loc_82608F58:
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// add r5,r5,r28
	ctx.r5.u64 = ctx.r5.u64 + r28.u64;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// blt cr6,0x82608f2c
	if (cr6.lt) goto loc_82608F2C;
loc_82608F78:
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82608fdc
	if (!cr6.gt) goto loc_82608FDC;
	// lwz r8,192(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 192);
loc_82608F90:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x82608fbc
	if (!cr6.gt) goto loc_82608FBC;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
loc_82608FA0:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stbx r8,r11,r9
	PPC_STORE_U8(r11.u32 + ctx.r9.u32, ctx.r8.u8);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r8,192(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// blt cr6,0x82608fa0
	if (cr6.lt) goto loc_82608FA0;
loc_82608FBC:
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// add r6,r6,r28
	ctx.r6.u64 = ctx.r6.u64 + r28.u64;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// blt cr6,0x82608f90
	if (cr6.lt) goto loc_82608F90;
loc_82608FDC:
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// bne cr6,0x82609028
	if (!cr6.eq) goto loc_82609028;
	// lwz r11,15188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15188);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// lwz r30,208(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lwz r28,204(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r9,192(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// lwz r8,188(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// lwz r7,180(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r24,15820(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 15820);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// mtctr r24
	ctr.u64 = r24.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82609028:
	// cmpwi cr6,r29,2
	cr6.compare<int32_t>(r29.s32, 2, xer);
	// bne cr6,0x8260906c
	if (!cr6.eq) goto loc_8260906C;
	// lwz r11,15188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15188);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lwz r30,208(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r9,200(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// lwz r8,192(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// lwz r7,188(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// lwz r6,180(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r31,15824(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 15824);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8260906C:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_82609074"))) PPC_WEAK_FUNC(sub_82609074);
PPC_FUNC_IMPL(__imp__sub_82609074) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82609078"))) PPC_WEAK_FUNC(sub_82609078);
PPC_FUNC_IMPL(__imp__sub_82609078) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// addi r4,r6,4
	ctx.r4.s64 = ctx.r6.s64 + 4;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826090c8
	if (!cr6.gt) goto loc_826090C8;
	// addi r11,r5,-1
	r11.s64 = ctx.r5.s64 + -1;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_826090A4:
	// lbz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// mulli r6,r6,315
	ctx.r6.s64 = ctx.r6.s64 * 315;
	// srawi r6,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x826090a4
	if (!cr6.eq) goto loc_826090A4;
loc_826090C8:
	// addi r11,r5,-2
	r11.s64 = ctx.r5.s64 + -2;
	// rlwinm r29,r5,2,0,29
	r29.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r30,r11,2,0,29
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r31,r5,1
	r31.s64 = ctx.r5.s64 + 1;
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// lwzx r11,r30,r4
	r11.u64 = PPC_LOAD_U32(r30.u32 + ctx.r4.u32);
	// stwx r11,r29,r4
	PPC_STORE_U32(r29.u32 + ctx.r4.u32, r11.u32);
	// ble cr6,0x82609120
	if (!cr6.gt) goto loc_82609120;
	// addi r10,r31,-2
	ctx.r10.s64 = r31.s64 + -2;
	// addi r11,r4,8
	r11.s64 = ctx.r4.s64 + 8;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_826090F8:
	// lwz r9,-8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mulli r9,r9,226
	ctx.r9.s64 = ctx.r9.s64 * 226;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stw r9,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r9.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x826090f8
	if (!cr6.eq) goto loc_826090F8;
loc_82609120:
	// addi r11,r4,4
	r11.s64 = ctx.r4.s64 + 4;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,-4(r4)
	PPC_STORE_U32(ctx.r4.u32 + -4, ctx.r10.u32);
	// ble cr6,0x82609174
	if (!cr6.gt) goto loc_82609174;
	// addi r9,r5,-1
	ctx.r9.s64 = ctx.r5.s64 + -1;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// rlwinm r9,r9,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
loc_82609144:
	// lwz r8,-4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r28,0(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mulli r8,r8,217
	ctx.r8.s64 = ctx.r8.s64 * 217;
	// srawi r8,r8,12
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 12;
	// subf r8,r8,r28
	ctx.r8.s64 = r28.s64 - ctx.r8.s64;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x82609144
	if (!cr6.eq) goto loc_82609144;
loc_82609174:
	// lwzx r10,r30,r4
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + ctx.r4.u32);
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// stwx r10,r29,r4
	PPC_STORE_U32(r29.u32 + ctx.r4.u32, ctx.r10.u32);
	// ble cr6,0x826091c0
	if (!cr6.gt) goto loc_826091C0;
	// addi r10,r31,-2
	ctx.r10.s64 = r31.s64 + -2;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_82609190:
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mulli r9,r9,406
	ctx.r9.s64 = ctx.r9.s64 * 406;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// subf r9,r6,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r6.s64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82609190
	if (!cr6.eq) goto loc_82609190;
loc_826091C0:
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x8260920c
	if (!cr6.gt) goto loc_8260920C;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
loc_826091D4:
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// ble cr6,0x826091f4
	if (!cr6.gt) goto loc_826091F4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,255
	r11.s64 = 255;
	// bgt cr6,0x826091f4
	if (cr6.gt) goto loc_826091F4;
	// li r11,0
	r11.s64 = 0;
loc_826091F4:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stb r11,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r11.u8);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826091d4
	if (!cr6.eq) goto loc_826091D4;
loc_8260920C:
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82609210"))) PPC_WEAK_FUNC(sub_82609210);
PPC_FUNC_IMPL(__imp__sub_82609210) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r8
	r29.u64 = ctx.r8.u64;
	// lwz r27,260(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x82609270
	if (!cr6.gt) goto loc_82609270;
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// mullw r26,r30,r11
	r26.s64 = int64_t(r30.s32) * int64_t(r11.s32);
loc_82609250:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x82609078
	sub_82609078(ctx, base);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r3,r26,r3
	ctx.r3.u64 = r26.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r29
	cr6.compare<int32_t>(r31.s32, r29.s32, xer);
	// blt cr6,0x82609250
	if (cr6.lt) goto loc_82609250;
loc_82609270:
	// lwz r26,252(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x826092ac
	if (!cr6.gt) goto loc_826092AC;
	// mullw r29,r30,r26
	r29.s64 = int64_t(r30.s32) * int64_t(r26.s32);
loc_82609288:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x82609078
	sub_82609078(ctx, base);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r3,r29,r3
	ctx.r3.u64 = r29.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
	// blt cr6,0x82609288
	if (cr6.lt) goto loc_82609288;
loc_826092AC:
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x826092e4
	if (!cr6.gt) goto loc_826092E4;
	// mullw r29,r30,r26
	r29.s64 = int64_t(r30.s32) * int64_t(r26.s32);
loc_826092C0:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x82609078
	sub_82609078(ctx, base);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r3,r29,r3
	ctx.r3.u64 = r29.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
	// blt cr6,0x826092c0
	if (cr6.lt) goto loc_826092C0;
loc_826092E4:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_826092EC"))) PPC_WEAK_FUNC(sub_826092EC);
PPC_FUNC_IMPL(__imp__sub_826092EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826092F0"))) PPC_WEAK_FUNC(sub_826092F0);
PPC_FUNC_IMPL(__imp__sub_826092F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce4
	// lwz r23,100(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x826093cc
	if (!cr6.gt) goto loc_826093CC;
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r3,r7,-2
	ctx.r3.s64 = ctx.r7.s64 + -2;
	// add r27,r11,r23
	r27.u64 = r11.u64 + r23.u64;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rlwinm r25,r3,2,0,29
	r25.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r26,r7,-1
	r26.s64 = ctx.r7.s64 + -1;
	// mullw r24,r6,r11
	r24.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
loc_82609328:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82609350
	if (!cr6.gt) goto loc_82609350;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
loc_82609338:
	// lbzx r31,r11,r29
	r31.u64 = PPC_LOAD_U8(r11.u32 + r29.u32);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// stw r31,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r31.u32);
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// blt cr6,0x82609338
	if (cr6.lt) goto loc_82609338;
loc_82609350:
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// ble cr6,0x8260938c
	if (!cr6.gt) goto loc_8260938C;
	// addi r3,r26,-2
	ctx.r3.s64 = r26.s64 + -2;
	// addi r11,r23,8
	r11.s64 = r23.s64 + 8;
	// rlwinm r3,r3,31,1,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
loc_82609368:
	// lwz r31,-8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// stw r31,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, r31.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82609368
	if (!cr6.eq) goto loc_82609368;
loc_8260938C:
	// lwzx r3,r25,r23
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + r23.u32);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r3,-4(r27)
	PPC_STORE_U32(r27.u32 + -4, ctx.r3.u32);
	// ble cr6,0x826093bc
	if (!cr6.gt) goto loc_826093BC;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
loc_826093A4:
	// lwz r31,0(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// stbx r31,r11,r29
	PPC_STORE_U8(r11.u32 + r29.u32, r31.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x826093a4
	if (cr6.lt) goto loc_826093A4;
loc_826093BC:
	// add r28,r28,r6
	r28.u64 = r28.u64 + ctx.r6.u64;
	// add r29,r24,r29
	r29.u64 = r24.u64 + r29.u64;
	// cmpw cr6,r28,r8
	cr6.compare<int32_t>(r28.s32, ctx.r8.s32, xer);
	// blt cr6,0x82609328
	if (cr6.lt) goto loc_82609328;
loc_826093CC:
	// lwz r26,92(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8260949c
	if (!cr6.gt) goto loc_8260949C;
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r8,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r23
	r30.u64 = r11.u64 + r23.u64;
	// addi r29,r9,-1
	r29.s64 = ctx.r9.s64 + -1;
	// mullw r27,r6,r26
	r27.s64 = int64_t(ctx.r6.s32) * int64_t(r26.s32);
loc_826093F8:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82609420
	if (!cr6.gt) goto loc_82609420;
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
loc_82609408:
	// lbzx r7,r11,r3
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + ctx.r3.u32);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// blt cr6,0x82609408
	if (cr6.lt) goto loc_82609408;
loc_82609420:
	// cmpwi cr6,r29,1
	cr6.compare<int32_t>(r29.s32, 1, xer);
	// ble cr6,0x8260945c
	if (!cr6.gt) goto loc_8260945C;
	// addi r8,r29,-2
	ctx.r8.s64 = r29.s64 + -2;
	// addi r11,r23,8
	r11.s64 = r23.s64 + 8;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
loc_82609438:
	// lwz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// stw r7,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r7.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82609438
	if (!cr6.eq) goto loc_82609438;
loc_8260945C:
	// lwzx r8,r28,r23
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + r23.u32);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r8,-4(r30)
	PPC_STORE_U32(r30.u32 + -4, ctx.r8.u32);
	// ble cr6,0x8260948c
	if (!cr6.gt) goto loc_8260948C;
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
loc_82609474:
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// stbx r7,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r7.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82609474
	if (cr6.lt) goto loc_82609474;
loc_8260948C:
	// add r31,r31,r6
	r31.u64 = r31.u64 + ctx.r6.u64;
	// add r3,r27,r3
	ctx.r3.u64 = r27.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r10
	cr6.compare<int32_t>(r31.s32, ctx.r10.s32, xer);
	// blt cr6,0x826093f8
	if (cr6.lt) goto loc_826093F8;
loc_8260949C:
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82609568
	if (!cr6.gt) goto loc_82609568;
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r29,r8,2,0,29
	r29.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r23
	r31.u64 = r11.u64 + r23.u64;
	// addi r30,r9,-1
	r30.s64 = ctx.r9.s64 + -1;
	// mullw r28,r6,r26
	r28.s64 = int64_t(ctx.r6.s32) * int64_t(r26.s32);
loc_826094C4:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x826094ec
	if (!cr6.gt) goto loc_826094EC;
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
loc_826094D4:
	// lbzx r7,r11,r4
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + ctx.r4.u32);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// blt cr6,0x826094d4
	if (cr6.lt) goto loc_826094D4;
loc_826094EC:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// ble cr6,0x82609528
	if (!cr6.gt) goto loc_82609528;
	// addi r8,r30,-2
	ctx.r8.s64 = r30.s64 + -2;
	// addi r11,r23,8
	r11.s64 = r23.s64 + 8;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
loc_82609504:
	// lwz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// stw r7,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r7.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82609504
	if (!cr6.eq) goto loc_82609504;
loc_82609528:
	// lwzx r8,r29,r23
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + r23.u32);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r8,-4(r31)
	PPC_STORE_U32(r31.u32 + -4, ctx.r8.u32);
	// ble cr6,0x82609558
	if (!cr6.gt) goto loc_82609558;
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
loc_82609540:
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// stbx r7,r11,r4
	PPC_STORE_U8(r11.u32 + ctx.r4.u32, ctx.r7.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82609540
	if (cr6.lt) goto loc_82609540;
loc_82609558:
	// add r3,r3,r6
	ctx.r3.u64 = ctx.r3.u64 + ctx.r6.u64;
	// add r4,r28,r4
	ctx.r4.u64 = r28.u64 + ctx.r4.u64;
	// cmpw cr6,r3,r10
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r10.s32, xer);
	// blt cr6,0x826094c4
	if (cr6.lt) goto loc_826094C4;
loc_82609568:
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_8260956C"))) PPC_WEAK_FUNC(sub_8260956C);
PPC_FUNC_IMPL(__imp__sub_8260956C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82609570"))) PPC_WEAK_FUNC(sub_82609570);
PPC_FUNC_IMPL(__imp__sub_82609570) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r29,236(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x826095c0
	if (!cr6.gt) goto loc_826095C0;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
loc_826095A4:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x82609078
	sub_82609078(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826095a4
	if (!cr6.eq) goto loc_826095A4;
loc_826095C0:
	// lwz r7,228(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x82609620
	if (!cr6.gt) goto loc_82609620;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mr r30,r31
	r30.u64 = r31.u64;
loc_826095D4:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x82609078
	sub_82609078(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826095d4
	if (!cr6.eq) goto loc_826095D4;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x82609620
	if (!cr6.gt) goto loc_82609620;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
loc_82609600:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x82609078
	sub_82609078(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82609600
	if (!cr6.eq) goto loc_82609600;
loc_82609620:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_82609628"))) PPC_WEAK_FUNC(sub_82609628);
PPC_FUNC_IMPL(__imp__sub_82609628) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce8
	// lwz r30,92(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x82609718
	if (!cr6.gt) goto loc_82609718;
	// addi r31,r7,-2
	r31.s64 = ctx.r7.s64 + -2;
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r31,2,0,29
	r25.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r26,r11,r30
	r26.u64 = r11.u64 + r30.u64;
	// addi r28,r7,-1
	r28.s64 = ctx.r7.s64 + -1;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
loc_82609658:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82609694
	if (!cr6.gt) goto loc_82609694;
	// addi r11,r7,-1
	r11.s64 = ctx.r7.s64 + -1;
	// rlwinm r31,r10,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82609678:
	// lbz r24,0(r6)
	r24.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r6,r31,r6
	ctx.r6.u64 = r31.u64 + ctx.r6.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r24,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r24.u32);
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// bne cr6,0x82609678
	if (!cr6.eq) goto loc_82609678;
loc_82609694:
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// ble cr6,0x826096d0
	if (!cr6.gt) goto loc_826096D0;
	// addi r6,r28,-2
	ctx.r6.s64 = r28.s64 + -2;
	// addi r11,r30,8
	r11.s64 = r30.s64 + 8;
	// rlwinm r6,r6,31,1,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
loc_826096AC:
	// lwz r3,-8(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// stw r3,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r3.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x826096ac
	if (!cr6.eq) goto loc_826096AC;
loc_826096D0:
	// lwzx r11,r25,r30
	r11.u64 = PPC_LOAD_U32(r25.u32 + r30.u32);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r11,-4(r26)
	PPC_STORE_U32(r26.u32 + -4, r11.u32);
	// ble cr6,0x82609708
	if (!cr6.gt) goto loc_82609708;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_826096EC:
	// lwz r31,0(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r31,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r31.u8);
	// add r3,r3,r10
	ctx.r3.u64 = ctx.r3.u64 + ctx.r10.u64;
	// bne cr6,0x826096ec
	if (!cr6.eq) goto loc_826096EC;
loc_82609708:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82609658
	if (!cr6.eq) goto loc_82609658;
loc_82609718:
	// lwz r27,84(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x826098e0
	if (!cr6.gt) goto loc_826098E0;
	// addi r10,r9,-2
	ctx.r10.s64 = ctx.r9.s64 + -2;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r10,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r11,r30
	r29.u64 = r11.u64 + r30.u64;
	// addi r3,r9,-1
	ctx.r3.s64 = ctx.r9.s64 + -1;
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
loc_8260973C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82609778
	if (!cr6.gt) goto loc_82609778;
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// rlwinm r6,r27,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_8260975C:
	// lbz r26,0(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r26,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r26.u32);
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// bne cr6,0x8260975c
	if (!cr6.eq) goto loc_8260975C;
loc_82609778:
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// ble cr6,0x826097b4
	if (!cr6.gt) goto loc_826097B4;
	// addi r10,r3,-2
	ctx.r10.s64 = ctx.r3.s64 + -2;
	// addi r11,r30,8
	r11.s64 = r30.s64 + 8;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_82609790:
	// lwz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// stw r7,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r7.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82609790
	if (!cr6.eq) goto loc_82609790;
loc_826097B4:
	// lwzx r11,r28,r30
	r11.u64 = PPC_LOAD_U32(r28.u32 + r30.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,-4(r29)
	PPC_STORE_U32(r29.u32 + -4, r11.u32);
	// ble cr6,0x826097ec
	if (!cr6.gt) goto loc_826097EC;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826097D0:
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r6,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, ctx.r6.u8);
	// add r7,r7,r27
	ctx.r7.u64 = ctx.r7.u64 + r27.u64;
	// bne cr6,0x826097d0
	if (!cr6.eq) goto loc_826097D0;
loc_826097EC:
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8260973c
	if (!cr6.eq) goto loc_8260973C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x826098e0
	if (!cr6.gt) goto loc_826098E0;
	// addi r10,r9,-2
	ctx.r10.s64 = ctx.r9.s64 + -2;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// rlwinm r31,r10,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// addi r4,r9,-1
	ctx.r4.s64 = ctx.r9.s64 + -1;
	// mr r5,r8
	ctx.r5.u64 = ctx.r8.u64;
loc_82609820:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8260985c
	if (!cr6.gt) goto loc_8260985C;
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// rlwinm r7,r27,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82609840:
	// lbz r29,0(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r29,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r29.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne cr6,0x82609840
	if (!cr6.eq) goto loc_82609840;
loc_8260985C:
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// ble cr6,0x82609898
	if (!cr6.gt) goto loc_82609898;
	// addi r10,r4,-2
	ctx.r10.s64 = ctx.r4.s64 + -2;
	// addi r11,r30,8
	r11.s64 = r30.s64 + 8;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_82609874:
	// lwz r8,-8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// stw r8,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r8.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82609874
	if (!cr6.eq) goto loc_82609874;
loc_82609898:
	// lwzx r11,r31,r30
	r11.u64 = PPC_LOAD_U32(r31.u32 + r30.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,-4(r3)
	PPC_STORE_U32(ctx.r3.u32 + -4, r11.u32);
	// ble cr6,0x826098d0
	if (!cr6.gt) goto loc_826098D0;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826098B4:
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// add r8,r8,r27
	ctx.r8.u64 = ctx.r8.u64 + r27.u64;
	// bne cr6,0x826098b4
	if (!cr6.eq) goto loc_826098B4;
loc_826098D0:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x82609820
	if (!cr6.eq) goto loc_82609820;
loc_826098E0:
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_826098E4"))) PPC_WEAK_FUNC(sub_826098E4);
PPC_FUNC_IMPL(__imp__sub_826098E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826098E8"))) PPC_WEAK_FUNC(sub_826098E8);
PPC_FUNC_IMPL(__imp__sub_826098E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce4
	// rlwinm r30,r7,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rlwinm r9,r7,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r10,r4,r7
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r7.u32);
	// add r30,r7,r30
	r30.u64 = ctx.r7.u64 + r30.u64;
	// add r11,r9,r4
	r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r31,r8,r10
	r31.u64 = ctx.r8.u64 + ctx.r10.u64;
	// addi r23,r6,-2
	r23.s64 = ctx.r6.s64 + -2;
	// mulli r31,r31,14
	r31.s64 = r31.s64 * 14;
	// lbzx r30,r30,r4
	r30.u64 = PPC_LOAD_U8(r30.u32 + ctx.r4.u32);
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// cmpwi cr6,r23,2
	cr6.compare<int32_t>(r23.s32, 2, xer);
	// mulli r30,r10,11
	r30.s64 = ctx.r10.s64 * 11;
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// subf r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	// addi r10,r10,63
	ctx.r10.s64 = ctx.r10.s64 + 63;
	// srawi r10,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 7;
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// ble cr6,0x826099dc
	if (!cr6.gt) goto loc_826099DC;
	// addi r10,r23,-3
	ctx.r10.s64 = r23.s64 + -3;
	// rlwinm r30,r7,2,0,29
	r30.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r10,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r31,r7,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r7,r30
	r30.u64 = ctx.r7.u64 + r30.u64;
	// addi r29,r5,8
	r29.s64 = ctx.r5.s64 + 8;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// add r31,r31,r4
	r31.u64 = r31.u64 + ctx.r4.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r30,r30,r4
	r30.u64 = r30.u64 + ctx.r4.u64;
loc_82609974:
	// lbz r27,0(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbzx r28,r11,r7
	r28.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r24,r10,r7
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r7.u32);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// lbz r27,0(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// lbz r26,0(r30)
	r26.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// add r31,r31,r9
	r31.u64 = r31.u64 + ctx.r9.u64;
	// mulli r28,r28,14
	r28.s64 = r28.s64 * 14;
	// lbz r25,0(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r28,r28,r24
	r28.u64 = r28.u64 + r24.u64;
	// add r26,r26,r25
	r26.u64 = r26.u64 + r25.u64;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// mulli r26,r26,11
	r26.s64 = r26.s64 * 11;
	// rlwinm r27,r28,2,0,29
	r27.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r30,r9
	r30.u64 = r30.u64 + ctx.r9.u64;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r28,r26,r28
	r28.s64 = r28.s64 - r26.s64;
	// addi r28,r28,63
	r28.s64 = r28.s64 + 63;
	// srawi r28,r28,7
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7F) != 0);
	r28.s64 = r28.s32 >> 7;
	// stw r28,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r28.u32);
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// bne cr6,0x82609974
	if (!cr6.eq) goto loc_82609974;
loc_826099DC:
	// addi r10,r6,-1
	ctx.r10.s64 = ctx.r6.s64 + -1;
	// addi r8,r6,-3
	ctx.r8.s64 = ctx.r6.s64 + -3;
	// addi r31,r6,-4
	r31.s64 = ctx.r6.s64 + -4;
	// mullw r10,r10,r7
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// mullw r11,r23,r7
	r11.s64 = int64_t(r23.s32) * int64_t(ctx.r7.s32);
	// lbzx r11,r11,r4
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r4.u32);
	// mullw r8,r8,r7
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// lbzx r8,r8,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r4.u32);
	// mullw r31,r31,r7
	r31.s64 = int64_t(r31.s32) * int64_t(ctx.r7.s32);
	// lbzx r4,r31,r4
	ctx.r4.u64 = PPC_LOAD_U8(r31.u32 + ctx.r4.u32);
	// add r4,r4,r11
	ctx.r4.u64 = ctx.r4.u64 + r11.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// mulli r4,r4,11
	ctx.r4.s64 = ctx.r4.s64 * 11;
	// mulli r11,r11,14
	r11.s64 = r11.s64 * 14;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r31,r23,2,0,29
	r31.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// addi r11,r11,63
	r11.s64 = r11.s64 + 63;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// stwx r11,r31,r5
	PPC_STORE_U32(r31.u32 + ctx.r5.u32, r11.u32);
	// ble cr6,0x82609a90
	if (!cr6.gt) goto loc_82609A90;
	// addi r10,r6,-1
	ctx.r10.s64 = ctx.r6.s64 + -1;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_82609A58:
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplwi cr6,r8,255
	cr6.compare<uint32_t>(ctx.r8.u32, 255, xer);
	// ble cr6,0x82609a74
	if (!cr6.gt) goto loc_82609A74;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// blt cr6,0x82609a74
	if (cr6.lt) goto loc_82609A74;
	// li r8,255
	ctx.r8.s64 = 255;
loc_82609A74:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// stbx r6,r11,r7
	PPC_STORE_U8(r11.u32 + ctx.r7.u32, ctx.r6.u8);
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82609a58
	if (!cr6.eq) goto loc_82609A58;
loc_82609A90:
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82609A94"))) PPC_WEAK_FUNC(sub_82609A94);
PPC_FUNC_IMPL(__imp__sub_82609A94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82609A98"))) PPC_WEAK_FUNC(sub_82609A98);
PPC_FUNC_IMPL(__imp__sub_82609A98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcfc
	// lbz r10,1(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// addi r11,r4,2
	r11.s64 = ctx.r4.s64 + 2;
	// lbz r7,3(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 3);
	// addi r29,r6,-2
	r29.s64 = ctx.r6.s64 + -2;
	// lbz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmpwi cr6,r29,2
	cr6.compare<int32_t>(r29.s32, 2, xer);
	// mulli r10,r10,14
	ctx.r10.s64 = ctx.r10.s64 * 14;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// mulli r8,r7,11
	ctx.r8.s64 = ctx.r7.s64 * 11;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// srawi r10,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 7;
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// ble cr6,0x82609b5c
	if (!cr6.gt) goto loc_82609B5C;
	// addi r10,r29,-3
	ctx.r10.s64 = r29.s64 + -3;
	// addi r9,r5,8
	ctx.r9.s64 = ctx.r5.s64 + 8;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_82609B00:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r31,-2(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lbz r7,3(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lbz r30,-1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// mulli r8,r8,14
	ctx.r8.s64 = ctx.r8.s64 * 14;
	// mulli r31,r7,11
	r31.s64 = ctx.r7.s64 * 11;
	// lbz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// addi r8,r8,64
	ctx.r8.s64 = ctx.r8.s64 + 64;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// bne cr6,0x82609b00
	if (!cr6.eq) goto loc_82609B00;
loc_82609B5C:
	// add r11,r4,r6
	r11.u64 = ctx.r4.u64 + ctx.r6.u64;
	// rlwinm r7,r29,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lbz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// lbz r9,-1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// lbz r8,-3(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + -3);
	// lbz r11,-4(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -4);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mulli r4,r11,11
	ctx.r4.s64 = r11.s64 * 11;
	// mulli r11,r10,14
	r11.s64 = ctx.r10.s64 * 14;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// stwx r11,r7,r5
	PPC_STORE_U32(ctx.r7.u32 + ctx.r5.u32, r11.u32);
	// ble cr6,0x82609bf8
	if (!cr6.gt) goto loc_82609BF8;
	// addi r10,r6,-1
	ctx.r10.s64 = ctx.r6.s64 + -1;
	// addi r11,r3,1
	r11.s64 = ctx.r3.s64 + 1;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
loc_82609BC0:
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplwi cr6,r10,255
	cr6.compare<uint32_t>(ctx.r10.u32, 255, xer);
	// ble cr6,0x82609bdc
	if (!cr6.gt) goto loc_82609BDC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// blt cr6,0x82609bdc
	if (cr6.lt) goto loc_82609BDC;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82609BDC:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stb r10,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, ctx.r10.u8);
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82609bc0
	if (!cr6.eq) goto loc_82609BC0;
loc_82609BF8:
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82609BFC"))) PPC_WEAK_FUNC(sub_82609BFC);
PPC_FUNC_IMPL(__imp__sub_82609BFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82609C00"))) PPC_WEAK_FUNC(sub_82609C00);
PPC_FUNC_IMPL(__imp__sub_82609C00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// lwz r27,292(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// mr r22,r8
	r22.u64 = ctx.r8.u64;
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82609c60
	if (!cr6.gt) goto loc_82609C60;
	// lwz r28,276(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// subf r29,r3,r11
	r29.s64 = r11.s64 - ctx.r3.s64;
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
loc_82609C40:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r29,r31
	ctx.r3.u64 = r29.u64 + r31.u64;
	// bl 0x82609a98
	sub_82609A98(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r31,r31,r28
	r31.u64 = r31.u64 + r28.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82609c40
	if (!cr6.eq) goto loc_82609C40;
loc_82609C60:
	// lwz r30,268(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mr r31,r25
	r31.u64 = r25.u64;
	// lwz r26,284(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// lwz r6,260(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82609ca0
	if (!cr6.gt) goto loc_82609CA0;
	// subf r28,r25,r24
	r28.s64 = r24.s64 - r25.s64;
	// mr r29,r30
	r29.u64 = r30.u64;
loc_82609C80:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r28,r31
	ctx.r3.u64 = r28.u64 + r31.u64;
	// bl 0x82609a98
	sub_82609A98(ctx, base);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r31,r31,r26
	r31.u64 = r31.u64 + r26.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82609c80
	if (!cr6.eq) goto loc_82609C80;
loc_82609CA0:
	// mr r31,r23
	r31.u64 = r23.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82609cd0
	if (!cr6.gt) goto loc_82609CD0;
	// subf r29,r23,r22
	r29.s64 = r22.s64 - r23.s64;
loc_82609CB0:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r29,r31
	ctx.r3.u64 = r29.u64 + r31.u64;
	// bl 0x82609a98
	sub_82609A98(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r31,r31,r26
	r31.u64 = r31.u64 + r26.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82609cb0
	if (!cr6.eq) goto loc_82609CB0;
loc_82609CD0:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_82609CD8"))) PPC_WEAK_FUNC(sub_82609CD8);
PPC_FUNC_IMPL(__imp__sub_82609CD8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r28,276(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// mr r29,r10
	r29.u64 = ctx.r10.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82609d38
	if (!cr6.gt) goto loc_82609D38;
	// lwz r7,260(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// subf r30,r6,r11
	r30.s64 = r11.s64 - ctx.r6.s64;
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
loc_82609D18:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// add r4,r30,r3
	ctx.r4.u64 = r30.u64 + ctx.r3.u64;
	// bl 0x826098e8
	sub_826098E8(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82609d18
	if (!cr6.eq) goto loc_82609D18;
loc_82609D38:
	// lwz r31,244(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r7,268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// lwz r27,252(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x82609da8
	if (!cr6.gt) goto loc_82609DA8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// subf r29,r26,r25
	r29.s64 = r25.s64 - r26.s64;
	// mr r30,r31
	r30.u64 = r31.u64;
loc_82609D58:
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// add r4,r29,r3
	ctx.r4.u64 = r29.u64 + ctx.r3.u64;
	// bl 0x826098e8
	sub_826098E8(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82609d58
	if (!cr6.eq) goto loc_82609D58;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x82609da8
	if (!cr6.gt) goto loc_82609DA8;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// subf r30,r24,r23
	r30.s64 = r23.s64 - r24.s64;
loc_82609D88:
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// add r4,r30,r3
	ctx.r4.u64 = r30.u64 + ctx.r3.u64;
	// bl 0x826098e8
	sub_826098E8(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82609d88
	if (!cr6.eq) goto loc_82609D88;
loc_82609DA8:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82609DB0"))) PPC_WEAK_FUNC(sub_82609DB0);
PPC_FUNC_IMPL(__imp__sub_82609DB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce0
	// rlwinm r11,r7,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// mulli r28,r9,34
	r28.s64 = ctx.r9.s64 * 34;
	// lbz r31,0(r8)
	r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// rotlwi r29,r31,1
	r29.u64 = __builtin_rotateleft32(r31.u32, 1);
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r31,r31,r29
	r31.u64 = r31.u64 + r29.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r31,r31,r28
	r31.s64 = r28.s64 - r31.s64;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// addi r25,r6,-4
	r25.s64 = ctx.r6.s64 + -4;
	// lbz r30,0(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// cmpwi cr6,r25,4
	cr6.compare<int32_t>(r25.s32, 4, xer);
	// srawi r31,r31,5
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1F) != 0);
	r31.s64 = r31.s32 >> 5;
	// stw r31,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r31.u32);
	// lbz r31,0(r8)
	r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r30,0(r4)
	r30.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rotlwi r29,r31,3
	r29.u64 = __builtin_rotateleft32(r31.u32, 3);
	// mulli r30,r30,25
	r30.s64 = r30.s64 * 25;
	// subf r31,r31,r29
	r31.s64 = r29.s64 - r31.s64;
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// srawi r31,r31,5
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1F) != 0);
	r31.s64 = r31.s32 >> 5;
	// stw r31,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, r31.u32);
	// lbz r30,0(r4)
	r30.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lbz r28,0(r9)
	r28.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// rotlwi r30,r30,1
	r30.u64 = __builtin_rotateleft32(r30.u32, 1);
	// lbz r31,0(r8)
	r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r29,0(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf r30,r28,r30
	r30.s64 = r30.s64 - r28.s64;
	// rotlwi r27,r31,3
	r27.u64 = __builtin_rotateleft32(r31.u32, 3);
	// rlwinm r28,r30,1,0,30
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r27,r31,r27
	r27.s64 = r27.s64 - r31.s64;
	// add r31,r30,r28
	r31.u64 = r30.u64 + r28.u64;
	// rlwinm r30,r27,2,0,29
	r30.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r31,r31,r29
	r31.u64 = r31.u64 + r29.u64;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// srawi r31,r31,5
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1F) != 0);
	r31.s64 = r31.s32 >> 5;
	// stw r31,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, r31.u32);
	// lbz r31,0(r8)
	r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r30,0(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// rotlwi r27,r31,3
	r27.u64 = __builtin_rotateleft32(r31.u32, 3);
	// lbz r28,0(r4)
	r28.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rotlwi r29,r30,1
	r29.u64 = __builtin_rotateleft32(r30.u32, 1);
	// subf r27,r31,r27
	r27.s64 = r27.s64 - r31.s64;
	// add r31,r30,r29
	r31.u64 = r30.u64 + r29.u64;
	// rlwinm r30,r27,1,0,30
	r30.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// subf r31,r28,r31
	r31.s64 = r31.s64 - r28.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r31,r31,5
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1F) != 0);
	r31.s64 = r31.s32 >> 5;
	// stw r31,12(r5)
	PPC_STORE_U32(ctx.r5.u32 + 12, r31.u32);
	// ble cr6,0x82609f6c
	if (!cr6.gt) goto loc_82609F6C;
	// addi r31,r25,-5
	r31.s64 = r25.s64 + -5;
	// rlwinm r29,r7,3,0,28
	r29.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r30,r31,31,1,31
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r31,r5,16
	r31.s64 = ctx.r5.s64 + 16;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// add r29,r29,r4
	r29.u64 = r29.u64 + ctx.r4.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
loc_82609EC8:
	// lbz r26,0(r8)
	r26.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// lbz r23,0(r10)
	r23.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rotlwi r26,r26,1
	r26.u64 = __builtin_rotateleft32(r26.u32, 1);
	// lbz r27,0(r9)
	r27.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r24,0(r29)
	r24.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// subf r26,r23,r26
	r26.s64 = r26.s64 - r23.s64;
	// rotlwi r22,r27,3
	r22.u64 = __builtin_rotateleft32(r27.u32, 3);
	// rlwinm r23,r26,1,0,30
	r23.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r22,r27,r22
	r22.s64 = r22.s64 - r27.s64;
	// add r27,r26,r23
	r27.u64 = r26.u64 + r23.u64;
	// rlwinm r26,r22,2,0,29
	r26.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r29,r11
	r29.u64 = r29.u64 + r11.u64;
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// add r27,r27,r24
	r27.u64 = r27.u64 + r24.u64;
	// addi r27,r27,16
	r27.s64 = r27.s64 + 16;
	// srawi r27,r27,5
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1F) != 0);
	r27.s64 = r27.s32 >> 5;
	// stw r27,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r27.u32);
	// lbz r26,0(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// lbz r23,0(r8)
	r23.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// rotlwi r26,r26,1
	r26.u64 = __builtin_rotateleft32(r26.u32, 1);
	// lbz r27,0(r9)
	r27.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r24,0(r28)
	r24.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// subf r26,r23,r26
	r26.s64 = r26.s64 - r23.s64;
	// rotlwi r22,r27,3
	r22.u64 = __builtin_rotateleft32(r27.u32, 3);
	// rlwinm r23,r26,1,0,30
	r23.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r22,r27,r22
	r22.s64 = r22.s64 - r27.s64;
	// add r27,r26,r23
	r27.u64 = r26.u64 + r23.u64;
	// rlwinm r26,r22,2,0,29
	r26.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r28,r11
	r28.u64 = r28.u64 + r11.u64;
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// add r27,r27,r24
	r27.u64 = r27.u64 + r24.u64;
	// addi r27,r27,16
	r27.s64 = r27.s64 + 16;
	// srawi r27,r27,5
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1F) != 0);
	r27.s64 = r27.s32 >> 5;
	// stw r27,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r27.u32);
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// bne cr6,0x82609ec8
	if (!cr6.eq) goto loc_82609EC8;
loc_82609F6C:
	// addi r11,r6,-6
	r11.s64 = ctx.r6.s64 + -6;
	// mullw r10,r25,r7
	ctx.r10.s64 = int64_t(r25.s32) * int64_t(ctx.r7.s32);
	// mullw r9,r11,r7
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// lbzx r30,r9,r4
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// addi r11,r6,-8
	r11.s64 = ctx.r6.s64 + -8;
	// addi r31,r6,-3
	r31.s64 = ctx.r6.s64 + -3;
	// mullw r28,r11,r7
	r28.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r31,2,0,29
	r27.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r6,-2
	ctx.r8.s64 = ctx.r6.s64 + -2;
	// lbzx r31,r10,r4
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// add r26,r11,r5
	r26.u64 = r11.u64 + ctx.r5.u64;
	// rlwinm r29,r25,2,0,29
	r29.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r11,r8,r7
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// rlwinm r25,r8,2,0,29
	r25.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r8,r31,3
	ctx.r8.u64 = __builtin_rotateleft32(r31.u32, 3);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// rlwinm r31,r8,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rotlwi r8,r30,1
	ctx.r8.u64 = __builtin_rotateleft32(r30.u32, 1);
	// add r8,r30,r8
	ctx.r8.u64 = r30.u64 + ctx.r8.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// lbzx r31,r11,r4
	r31.u64 = PPC_LOAD_U8(r11.u32 + ctx.r4.u32);
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r8,r8,5
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// stwx r8,r29,r5
	PPC_STORE_U32(r29.u32 + ctx.r5.u32, ctx.r8.u32);
	// lbzx r8,r11,r4
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + ctx.r4.u32);
	// lbzx r31,r9,r4
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// rotlwi r29,r8,1
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lbzx r8,r10,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// lbzx r30,r28,r4
	r30.u64 = PPC_LOAD_U8(r28.u32 + ctx.r4.u32);
	// subf r31,r31,r29
	r31.s64 = r29.s64 - r31.s64;
	// rotlwi r28,r8,3
	r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 3);
	// rlwinm r29,r31,1,0,30
	r29.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r28,r8,r28
	r28.s64 = r28.s64 - ctx.r8.s64;
	// add r8,r31,r29
	ctx.r8.u64 = r31.u64 + r29.u64;
	// rlwinm r31,r28,2,0,29
	r31.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// srawi r8,r8,5
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// stwx r8,r27,r5
	PPC_STORE_U32(r27.u32 + ctx.r5.u32, ctx.r8.u32);
	// lbzx r8,r10,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// lbzx r31,r11,r4
	r31.u64 = PPC_LOAD_U8(r11.u32 + ctx.r4.u32);
	// rotlwi r30,r8,3
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 3);
	// mulli r31,r31,25
	r31.s64 = r31.s64 * 25;
	// subf r8,r8,r30
	ctx.r8.s64 = r30.s64 - ctx.r8.s64;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// srawi r8,r8,5
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// stwx r8,r25,r5
	PPC_STORE_U32(r25.u32 + ctx.r5.u32, ctx.r8.u32);
	// lbzx r11,r11,r4
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r4.u32);
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// mulli r8,r11,34
	ctx.r8.s64 = r11.s64 * 34;
	// lbzx r9,r9,r4
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// rotlwi r11,r10,1
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
	// stw r11,-4(r26)
	PPC_STORE_U32(r26.u32 + -4, r11.u32);
	// ble cr6,0x8260a0a8
	if (!cr6.gt) goto loc_8260A0A8;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_8260A074:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// ble cr6,0x8260a090
	if (!cr6.gt) goto loc_8260A090;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// blt cr6,0x8260a090
	if (cr6.lt) goto loc_8260A090;
	// li r11,255
	r11.s64 = 255;
loc_8260A090:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x8260a074
	if (!cr6.eq) goto loc_8260A074;
loc_8260A0A8:
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_8260A0AC"))) PPC_WEAK_FUNC(sub_8260A0AC);
PPC_FUNC_IMPL(__imp__sub_8260A0AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260A0B0"))) PPC_WEAK_FUNC(sub_8260A0B0);
PPC_FUNC_IMPL(__imp__sub_8260A0B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf4
	// lbz r10,2(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// addi r29,r6,-4
	r29.s64 = ctx.r6.s64 + -4;
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// subfic r10,r10,5
	xer.ca = ctx.r10.u32 <= 5;
	ctx.r10.s64 = 5 - ctx.r10.s64;
	// lbz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// mulli r8,r11,34
	ctx.r8.s64 = r11.s64 * 34;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r4,6
	r11.s64 = ctx.r4.s64 + 6;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmpwi cr6,r29,4
	cr6.compare<int32_t>(r29.s32, 4, xer);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srawi r10,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// lbz r10,2(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// lbz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rotlwi r8,r10,3
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// mulli r9,r9,25
	ctx.r9.s64 = ctx.r9.s64 * 25;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,15
	ctx.r10.s64 = ctx.r10.s64 + 15;
	// srawi r10,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stw r10,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r10.u32);
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lbz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// rotlwi r7,r10,1
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// lbz r10,2(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// rotlwi r7,r10,3
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// addi r9,r9,5
	ctx.r9.s64 = ctx.r9.s64 + 5;
	// subf r7,r10,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r10.s64;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// srawi r10,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stw r10,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, ctx.r10.u32);
	// lbz r10,2(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// lbz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// rotlwi r31,r10,3
	r31.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// lbz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rotlwi r8,r9,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// subf r31,r10,r31
	r31.s64 = r31.s64 - ctx.r10.s64;
	// add r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r9,r31,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r10,15
	ctx.r10.s64 = ctx.r10.s64 + 15;
	// srawi r10,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stw r10,12(r5)
	PPC_STORE_U32(ctx.r5.u32 + 12, ctx.r10.u32);
	// ble cr6,0x8260a234
	if (!cr6.gt) goto loc_8260A234;
	// addi r10,r29,-5
	ctx.r10.s64 = r29.s64 + -5;
	// rlwinm r9,r10,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r5,16
	ctx.r10.s64 = ctx.r5.s64 + 16;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
loc_8260A1A0:
	// lbz r31,-4(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + -4);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r30,r31,1
	r30.u64 = __builtin_rotateleft32(r31.u32, 1);
	// lbz r8,-2(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// lbz r31,2(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// subf r7,r7,r30
	ctx.r7.s64 = r30.s64 - ctx.r7.s64;
	// rotlwi r30,r8,3
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 3);
	// addi r7,r7,5
	ctx.r7.s64 = ctx.r7.s64 + 5;
	// subf r30,r8,r30
	r30.s64 = r30.s64 - ctx.r8.s64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r30,r30,2,0,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r8,r8,5
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + -4);
	// rotlwi r31,r8,1
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lbz r8,-2(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// lbz r30,-6(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + -6);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// subf r7,r7,r31
	ctx.r7.s64 = r31.s64 - ctx.r7.s64;
	// rotlwi r31,r8,3
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 3);
	// addi r7,r7,5
	ctx.r7.s64 = ctx.r7.s64 + 5;
	// subf r31,r8,r31
	r31.s64 = r31.s64 - ctx.r8.s64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r31,2,0,29
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r8,r8,5
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x8260a1a0
	if (!cr6.eq) goto loc_8260A1A0;
loc_8260A234:
	// addi r10,r6,-3
	ctx.r10.s64 = ctx.r6.s64 + -3;
	// add r11,r4,r6
	r11.u64 = ctx.r4.u64 + ctx.r6.u64;
	// rlwinm r31,r10,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r29,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r6,-2
	ctx.r9.s64 = ctx.r6.s64 + -2;
	// add r29,r10,r5
	r29.u64 = ctx.r10.u64 + ctx.r5.u64;
	// lbz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -4);
	// rlwinm r30,r9,2,0,29
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r9,-6(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -6);
	// rotlwi r27,r10,3
	r27.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// lbz r28,-2(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// rotlwi r8,r9,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// subf r27,r10,r27
	r27.s64 = r27.s64 - ctx.r10.s64;
	// add r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r9,r27,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// subf r10,r28,r10
	ctx.r10.s64 = ctx.r10.s64 - r28.s64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r10,15
	ctx.r10.s64 = ctx.r10.s64 + 15;
	// srawi r10,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stwx r10,r7,r5
	PPC_STORE_U32(ctx.r7.u32 + ctx.r5.u32, ctx.r10.u32);
	// lbz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// lbz r9,-6(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -6);
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// lbz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -4);
	// lbz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + -8);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// rotlwi r8,r10,3
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// addi r9,r9,5
	ctx.r9.s64 = ctx.r9.s64 + 5;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// srawi r10,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stwx r10,r31,r5
	PPC_STORE_U32(r31.u32 + ctx.r5.u32, ctx.r10.u32);
	// lbz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -4);
	// lbz r9,-2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// rotlwi r8,r10,3
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// mulli r9,r9,25
	ctx.r9.s64 = ctx.r9.s64 * 25;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,15
	ctx.r10.s64 = ctx.r10.s64 + 15;
	// srawi r10,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stwx r10,r30,r5
	PPC_STORE_U32(r30.u32 + ctx.r5.u32, ctx.r10.u32);
	// lbz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -4);
	// lbz r8,-2(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// subfic r10,r10,5
	xer.ca = ctx.r10.u32 <= 5;
	ctx.r10.s64 = 5 - ctx.r10.s64;
	// lbz r9,-6(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -6);
	// mulli r8,r8,34
	ctx.r8.s64 = ctx.r8.s64 * 34;
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
	// stw r11,-4(r29)
	PPC_STORE_U32(r29.u32 + -4, r11.u32);
	// ble cr6,0x8260a358
	if (!cr6.gt) goto loc_8260A358;
loc_8260A328:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// ble cr6,0x8260a344
	if (!cr6.gt) goto loc_8260A344;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// blt cr6,0x8260a344
	if (cr6.lt) goto loc_8260A344;
	// li r11,255
	r11.s64 = 255;
loc_8260A344:
	// stbx r11,r4,r3
	PPC_STORE_U8(ctx.r4.u32 + ctx.r3.u32, r11.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// cmpw cr6,r4,r6
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r6.s32, xer);
	// blt cr6,0x8260a328
	if (cr6.lt) goto loc_8260A328;
loc_8260A358:
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8260A35C"))) PPC_WEAK_FUNC(sub_8260A35C);
PPC_FUNC_IMPL(__imp__sub_8260A35C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260A360"))) PPC_WEAK_FUNC(sub_8260A360);
PPC_FUNC_IMPL(__imp__sub_8260A360) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r8
	r29.u64 = ctx.r8.u64;
	// lwz r27,260(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x8260a3bc
	if (!cr6.gt) goto loc_8260A3BC;
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// mullw r26,r30,r11
	r26.s64 = int64_t(r30.s32) * int64_t(r11.s32);
loc_8260A3A0:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x8260a0b0
	sub_8260A0B0(ctx, base);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r3,r26,r3
	ctx.r3.u64 = r26.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r29
	cr6.compare<int32_t>(r31.s32, r29.s32, xer);
	// blt cr6,0x8260a3a0
	if (cr6.lt) goto loc_8260A3A0;
loc_8260A3BC:
	// lwz r26,252(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x8260a3f4
	if (!cr6.gt) goto loc_8260A3F4;
	// mullw r29,r30,r26
	r29.s64 = int64_t(r30.s32) * int64_t(r26.s32);
loc_8260A3D4:
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x8260a0b0
	sub_8260A0B0(ctx, base);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r3,r29,r3
	ctx.r3.u64 = r29.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
	// blt cr6,0x8260a3d4
	if (cr6.lt) goto loc_8260A3D4;
loc_8260A3F4:
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x8260a428
	if (!cr6.gt) goto loc_8260A428;
	// mullw r29,r30,r26
	r29.s64 = int64_t(r30.s32) * int64_t(r26.s32);
loc_8260A408:
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x8260a0b0
	sub_8260A0B0(ctx, base);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r3,r29,r3
	ctx.r3.u64 = r29.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
	// blt cr6,0x8260a408
	if (cr6.lt) goto loc_8260A408;
loc_8260A428:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_8260A430"))) PPC_WEAK_FUNC(sub_8260A430);
PPC_FUNC_IMPL(__imp__sub_8260A430) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// lwz r28,236(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x8260a488
	if (!cr6.gt) goto loc_8260A488;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
loc_8260A468:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x82609db0
	sub_82609DB0(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8260a468
	if (!cr6.eq) goto loc_8260A468;
loc_8260A488:
	// lwz r7,228(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x8260a4e8
	if (!cr6.gt) goto loc_8260A4E8;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r30,r31
	r30.u64 = r31.u64;
loc_8260A49C:
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x82609db0
	sub_82609DB0(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8260a49c
	if (!cr6.eq) goto loc_8260A49C;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x8260a4e8
	if (!cr6.gt) goto loc_8260A4E8;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
loc_8260A4C8:
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x82609db0
	sub_82609DB0(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8260a4c8
	if (!cr6.eq) goto loc_8260A4C8;
loc_8260A4E8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_8260A4F0"))) PPC_WEAK_FUNC(sub_8260A4F0);
PPC_FUNC_IMPL(__imp__sub_8260A4F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// lis r8,-32159
	ctx.r8.s64 = -2107572224;
	// lis r9,-32159
	ctx.r9.s64 = -2107572224;
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// lis r11,-32159
	r11.s64 = -2107572224;
	// addi r31,r8,-25600
	r31.s64 = ctx.r8.s64 + -25600;
	// addi r30,r9,-25384
	r30.s64 = ctx.r9.s64 + -25384;
	// addi r29,r10,-23712
	r29.s64 = ctx.r10.s64 + -23712;
	// addi r28,r11,-23504
	r28.s64 = r11.s64 + -23504;
	// lis r8,-32159
	ctx.r8.s64 = -2107572224;
	// lis r9,-32159
	ctx.r9.s64 = -2107572224;
	// stw r31,3184(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3184, r31.u32);
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// stw r30,3188(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3188, r30.u32);
	// lis r11,-32159
	r11.s64 = -2107572224;
	// stw r29,3192(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3192, r29.u32);
	// addi r8,r8,-28144
	ctx.r8.s64 = ctx.r8.s64 + -28144;
	// stw r28,3196(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3196, r28.u32);
	// addi r9,r9,-27280
	ctx.r9.s64 = ctx.r9.s64 + -27280;
	// addi r10,r10,-27920
	ctx.r10.s64 = ctx.r10.s64 + -27920;
	// addi r11,r11,-27096
	r11.s64 = r11.s64 + -27096;
	// lis r4,-32159
	ctx.r4.s64 = -2107572224;
	// lis r5,-32159
	ctx.r5.s64 = -2107572224;
	// stw r8,15796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15796, ctx.r8.u32);
	// lis r6,-32159
	ctx.r6.s64 = -2107572224;
	// stw r9,15804(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15804, ctx.r9.u32);
	// lis r7,-32159
	ctx.r7.s64 = -2107572224;
	// stw r10,15800(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15800, ctx.r10.u32);
	// stw r11,15808(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15808, r11.u32);
	// addi r4,r4,-30448
	ctx.r4.s64 = ctx.r4.s64 + -30448;
	// addi r5,r5,-29776
	ctx.r5.s64 = ctx.r5.s64 + -29776;
	// addi r6,r6,-30208
	ctx.r6.s64 = ctx.r6.s64 + -30208;
	// addi r7,r7,-29552
	ctx.r7.s64 = ctx.r7.s64 + -29552;
	// rotlwi r11,r31,0
	r11.u64 = __builtin_rotateleft32(r31.u32, 0);
	// rotlwi r10,r30,0
	ctx.r10.u64 = __builtin_rotateleft32(r30.u32, 0);
	// stw r4,15780(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15780, ctx.r4.u32);
	// rotlwi r9,r29,0
	ctx.r9.u64 = __builtin_rotateleft32(r29.u32, 0);
	// stw r5,15788(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15788, ctx.r5.u32);
	// rotlwi r8,r28,0
	ctx.r8.u64 = __builtin_rotateleft32(r28.u32, 0);
	// stw r6,15784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15784, ctx.r6.u32);
	// stw r7,15792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15792, ctx.r7.u32);
	// stw r11,15812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15812, r11.u32);
	// stw r10,15816(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15816, ctx.r10.u32);
	// stw r9,15820(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15820, ctx.r9.u32);
	// stw r8,15824(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15824, ctx.r8.u32);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8260A5AC"))) PPC_WEAK_FUNC(sub_8260A5AC);
PPC_FUNC_IMPL(__imp__sub_8260A5AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260A5B0"))) PPC_WEAK_FUNC(sub_8260A5B0);
PPC_FUNC_IMPL(__imp__sub_8260A5B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// addi r3,r31,52
	ctx.r3.s64 = r31.s64 + 52;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r11,304(r31)
	PPC_STORE_U32(r31.u32 + 304, r11.u32);
	// stw r11,308(r31)
	PPC_STORE_U32(r31.u32 + 308, r11.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// stw r11,300(r31)
	PPC_STORE_U32(r31.u32 + 300, r11.u32);
	// stw r11,292(r31)
	PPC_STORE_U32(r31.u32 + 292, r11.u32);
	// stw r11,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r11.u32);
	// bl 0x8265bb18
	sub_8265BB18(ctx, base);
	// addi r3,r31,156
	ctx.r3.s64 = r31.s64 + 156;
	// bl 0x8265ba48
	sub_8265BA48(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260A634"))) PPC_WEAK_FUNC(sub_8260A634);
PPC_FUNC_IMPL(__imp__sub_8260A634) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260A638"))) PPC_WEAK_FUNC(sub_8260A638);
PPC_FUNC_IMPL(__imp__sub_8260A638) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r31,52
	ctx.r3.s64 = r31.s64 + 52;
	// bl 0x8265bb38
	sub_8265BB38(ctx, base);
	// addi r3,r31,156
	ctx.r3.s64 = r31.s64 + 156;
	// bl 0x8265ba68
	sub_8265BA68(ctx, base);
	// lwz r3,304(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260a678
	if (cr6.eq) goto loc_8260A678;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,304(r31)
	PPC_STORE_U32(r31.u32 + 304, r30.u32);
loc_8260A678:
	// lwz r3,308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 308);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260a68c
	if (cr6.eq) goto loc_8260A68C;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,308(r31)
	PPC_STORE_U32(r31.u32 + 308, r30.u32);
loc_8260A68C:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260a6a0
	if (cr6.eq) goto loc_8260A6A0;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
loc_8260A6A0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260A6B8"))) PPC_WEAK_FUNC(sub_8260A6B8);
PPC_FUNC_IMPL(__imp__sub_8260A6B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x8260a9cc
	if (cr6.eq) goto loc_8260A9CC;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8260a9cc
	if (cr6.eq) goto loc_8260A9CC;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260a9cc
	if (cr6.eq) goto loc_8260A9CC;
	// lwz r11,296(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 296);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260a704
	if (cr6.eq) goto loc_8260A704;
	// lwz r11,32(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r10,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r10.s64 = temp.s64;
	// b 0x8260a708
	goto loc_8260A708;
loc_8260A704:
	// lwz r10,32(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
loc_8260A708:
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r7,22101
	ctx.r7.s64 = 1448411136;
	// ori r30,r7,22857
	r30.u64 = ctx.r7.u64 | 22857;
	// lwz r7,16(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplw cr6,r7,r30
	cr6.compare<uint32_t>(ctx.r7.u32, r30.u32, xer);
	// beq cr6,0x8260a7b0
	if (cr6.eq) goto loc_8260A7B0;
	// lis r30,12338
	r30.s64 = 808583168;
	// ori r30,r30,13385
	r30.u64 = r30.u64 | 13385;
	// cmplw cr6,r7,r30
	cr6.compare<uint32_t>(ctx.r7.u32, r30.u32, xer);
	// beq cr6,0x8260a7b0
	if (cr6.eq) goto loc_8260A7B0;
	// lis r30,12593
	r30.s64 = 825294848;
	// ori r30,r30,13392
	r30.u64 = r30.u64 | 13392;
	// cmplw cr6,r7,r30
	cr6.compare<uint32_t>(ctx.r7.u32, r30.u32, xer);
	// beq cr6,0x8260a7b0
	if (cr6.eq) goto loc_8260A7B0;
	// lis r30,12849
	r30.s64 = 842072064;
	// ori r30,r30,22105
	r30.u64 = r30.u64 | 22105;
	// cmplw cr6,r7,r30
	cr6.compare<uint32_t>(ctx.r7.u32, r30.u32, xer);
	// beq cr6,0x8260a7b0
	if (cr6.eq) goto loc_8260A7B0;
	// lhz r7,14(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r29,8(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mullw r11,r7,r30
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r30.s32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r7,28(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x8260a7e8
	goto loc_8260A7E8;
loc_8260A7B0:
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
loc_8260A7E8:
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// blt cr6,0x8260a9c0
	if (cr6.lt) goto loc_8260A9C0;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// blt cr6,0x8260a9c0
	if (cr6.lt) goto loc_8260A9C0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8260a9cc
	if (cr6.eq) goto loc_8260A9CC;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x8260a9cc
	if (cr6.eq) goto loc_8260A9CC;
	// lwz r11,292(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 292);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260a994
	if (cr6.eq) goto loc_8260A994;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r9,12850
	ctx.r9.s64 = 842137600;
	// extsw r30,r10
	r30.s64 = ctx.r10.s32;
	// ori r7,r9,13392
	ctx.r7.u64 = ctx.r9.u64 | 13392;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r5,16(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r5,r7
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r7.u32, xer);
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r3,r3,156
	ctx.r3.s64 = ctx.r3.s64 + 156;
	// bne cr6,0x8260a8f4
	if (!cr6.eq) goto loc_8260A8F4;
	// std r30,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r30.u64);
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// lis r5,-32249
	ctx.r5.s64 = -2113470464;
	// extsw r30,r8
	r30.s64 = ctx.r8.s32;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// stw r29,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r29.u32);
	// extsw r29,r11
	r29.s64 = r11.s32;
	// lfd f4,-31368(r5)
	ctx.fpscr.disableFlushMode();
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r5.u32 + -31368);
	// extsw r5,r9
	ctx.r5.s64 = ctx.r9.s32;
	// std r30,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r30.u64);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// fmr f2,f4
	ctx.f2.f64 = ctx.f4.f64;
	// std r5,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r5.u64);
	// mullw r10,r8,r9
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r11,r6
	ctx.r8.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r5,r10,r31
	ctx.r5.u64 = ctx.r10.u64 + r31.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// add r9,r11,r6
	ctx.r9.u64 = r11.u64 + ctx.r6.u64;
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// add r6,r11,r31
	ctx.r6.u64 = r11.u64 + r31.u64;
	// lfd f0,112(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// std r29,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r29.u64);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f12,128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f13,120(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fdiv f3,f13,f0
	ctx.f3.f64 = ctx.f13.f64 / f0.f64;
	// lfd f11,112(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fdiv f1,f12,f11
	ctx.f1.f64 = ctx.f12.f64 / ctx.f11.f64;
	// bl 0x82666238
	sub_82666238(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd4c
	return;
loc_8260A8F4:
	// std r30,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r30.u64);
	// extsw r29,r11
	r29.s64 = r11.s32;
	// stw r5,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r5.u32);
	// lis r5,-32249
	ctx.r5.s64 = -2113470464;
	// extsw r30,r8
	r30.s64 = ctx.r8.s32;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// std r29,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r29.u64);
	// lfd f4,-31368(r5)
	ctx.fpscr.disableFlushMode();
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r5.u32 + -31368);
	// fmr f2,f4
	ctx.f2.f64 = ctx.f4.f64;
	// extsw r5,r9
	ctx.r5.s64 = ctx.r9.s32;
	// mullw r10,r8,r9
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// std r5,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r5.u64);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r6
	ctx.r8.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// add r5,r10,r31
	ctx.r5.u64 = ctx.r10.u64 + r31.u64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// add r9,r11,r6
	ctx.r9.u64 = r11.u64 + ctx.r6.u64;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// add r6,r11,r31
	ctx.r6.u64 = r11.u64 + r31.u64;
	// lfd f0,128(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// std r30,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r30.u64);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f11,112(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// lfd f13,120(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fdiv f3,f13,f0
	ctx.f3.f64 = ctx.f13.f64 / f0.f64;
	// lfd f12,128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// fdiv f1,f12,f11
	ctx.f1.f64 = ctx.f12.f64 / ctx.f11.f64;
	// bl 0x82666238
	sub_82666238(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd4c
	return;
loc_8260A994:
	// lwz r7,28(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// addi r3,r3,52
	ctx.r3.s64 = ctx.r3.s64 + 52;
	// bl 0x826618a8
	sub_826618A8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260a9cc
	if (cr6.eq) goto loc_8260A9CC;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd4c
	return;
loc_8260A9C0:
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
loc_8260A9CC:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8260A9D8"))) PPC_WEAK_FUNC(sub_8260A9D8);
PPC_FUNC_IMPL(__imp__sub_8260A9D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8260aa0c
	if (!cr6.eq) goto loc_8260AA0C;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_8260AA0C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260a638
	sub_8260A638(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260AA34"))) PPC_WEAK_FUNC(sub_8260AA34);
PPC_FUNC_IMPL(__imp__sub_8260AA34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

