#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_824BA910"))) PPC_WEAK_FUNC(sub_824BA910);
PPC_FUNC_IMPL(__imp__sub_824BA910) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
loc_824BA914:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x824ba968
	if (cr6.eq) goto loc_824BA968;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm. r11,r10,0,1,1
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824ba934
	if (!cr0.eq) goto loc_824BA934;
	// rlwinm. r11,r10,0,4,6
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// beq 0x824ba938
	if (cr0.eq) goto loc_824BA938;
loc_824BA934:
	// li r11,0
	r11.s64 = 0;
loc_824BA938:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ba954
	if (cr0.eq) goto loc_824BA954;
	// rlwinm r11,r10,19,20,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 19) & 0xFFF;
	// addi r10,r4,1
	ctx.r10.s64 = ctx.r4.s64 + 1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// li r11,1
	r11.s64 = 1;
	// beq cr6,0x824ba958
	if (cr6.eq) goto loc_824BA958;
loc_824BA954:
	// li r11,0
	r11.s64 = 0;
loc_824BA958:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bnelr 
	if (!cr0.eq) return;
	// lwz r3,4(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// b 0x824ba914
	goto loc_824BA914;
loc_824BA968:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BA970"))) PPC_WEAK_FUNC(sub_824BA970);
PPC_FUNC_IMPL(__imp__sub_824BA970) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
loc_824BA974:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824ba9b0
	if (cr6.eq) goto loc_824BA9B0;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// addi r9,r10,0
	ctx.r9.s64 = ctx.r10.s64 + 0;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824ba9a0
	if (cr0.eq) goto loc_824BA9A0;
	// cmplw cr6,r10,r4
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r4.u32, xer);
	// bne cr6,0x824ba9a8
	if (!cr6.eq) goto loc_824BA9A8;
loc_824BA9A0:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824ba974
	goto loc_824BA974;
loc_824BA9A8:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_824BA9B0:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BA9B8"))) PPC_WEAK_FUNC(sub_824BA9B8);
PPC_FUNC_IMPL(__imp__sub_824BA9B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r7,0
	ctx.r7.s64 = 0;
loc_824BA9C0:
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x824baa2c
	if (cr6.eq) goto loc_824BAA2C;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm. r11,r10,0,1,1
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824ba9e0
	if (!cr0.eq) goto loc_824BA9E0;
	// rlwinm. r11,r10,0,4,6
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// beq 0x824ba9e4
	if (cr0.eq) goto loc_824BA9E4;
loc_824BA9E0:
	// li r11,1
	r11.s64 = 1;
loc_824BA9E4:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824baa24
	if (cr0.eq) goto loc_824BAA24;
	// rlwinm. r11,r10,7,29,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r8,0
	ctx.r8.s64 = 0;
	// beq 0x824baa20
	if (cr0.eq) goto loc_824BAA20;
	// rlwinm r9,r10,27,24,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// li r10,0
	ctx.r10.s64 = 0;
loc_824BAA00:
	// srw r5,r9,r10
	ctx.r5.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r10.u8 & 0x3F));
	// li r4,1
	ctx.r4.s64 = 1;
	// clrlwi r5,r5,30
	ctx.r5.u64 = ctx.r5.u32 & 0x3;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// slw r5,r4,r5
	ctx.r5.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r4.u32 << (ctx.r5.u8 & 0x3F));
	// or r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 | ctx.r8.u64;
	// bne 0x824baa00
	if (!cr0.eq) goto loc_824BAA00;
loc_824BAA20:
	// or r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 | ctx.r7.u64;
loc_824BAA24:
	// lwz r6,8(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// b 0x824ba9c0
	goto loc_824BA9C0;
loc_824BAA2C:
	// lis r10,-28311
	ctx.r10.s64 = -1855389696;
	// lis r9,0
	ctx.r9.s64 = 0;
	// ori r10,r10,5192
	ctx.r10.u64 = ctx.r10.u64 | 5192;
	// ori r9,r9,36262
	ctx.r9.u64 = ctx.r9.u64 | 36262;
	// clrldi r11,r7,32
	r11.u64 = ctx.r7.u64 & 0xFFFFFFFF;
	// rldimi r10,r9,32,0
	ctx.r10.u64 = (__builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r10.u64 & 0xFFFFFFFF);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// srd r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// srd r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// rlwimi r7,r11,13,16,18
	ctx.r7.u64 = (__builtin_rotateleft32(r11.u32, 13) & 0xE000) | (ctx.r7.u64 & 0xFFFFFFFFFFFF1FFF);
	// rlwimi r9,r7,1,27,30
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r7.u32, 1) & 0x1E) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFFE1);
	// rlwimi r9,r7,1,15,17
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r7.u32, 1) & 0x1C000) | (ctx.r9.u64 & 0xFFFFFFFFFFFE3FFF);
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BAA6C"))) PPC_WEAK_FUNC(sub_824BAA6C);
PPC_FUNC_IMPL(__imp__sub_824BAA6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BAA70"))) PPC_WEAK_FUNC(sub_824BAA70);
PPC_FUNC_IMPL(__imp__sub_824BAA70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmplw cr6,r3,r4
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r4.u32, xer);
	// beq cr6,0x824baabc
	if (cr6.eq) goto loc_824BAABC;
	// lwz r11,48(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lwz r10,40(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r10,r10,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne 0x824baac0
	if (!cr0.eq) goto loc_824BAAC0;
loc_824BAABC:
	// li r11,0
	r11.s64 = 0;
loc_824BAAC0:
	// clrlwi r3,r11,24
	ctx.r3.u64 = r11.u32 & 0xFF;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BAAC8"))) PPC_WEAK_FUNC(sub_824BAAC8);
PPC_FUNC_IMPL(__imp__sub_824BAAC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r9,8(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	// li r8,0
	ctx.r8.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
loc_824BAAD4:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824bab30
	if (cr6.eq) goto loc_824BAB30;
	// lwz r11,48(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r5,r11,27
	ctx.r5.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// lwz r6,40(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r6
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// slw r7,r7,r5
	ctx.r7.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r5.u8 & 0x3F));
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bab28
	if (cr0.eq) goto loc_824BAB28;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
loc_824BAB28:
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// b 0x824baad4
	goto loc_824BAAD4;
loc_824BAB30:
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BAB40"))) PPC_WEAK_FUNC(sub_824BAB40);
PPC_FUNC_IMPL(__imp__sub_824BAB40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm. r11,r11,0,10,12
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x380000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bac34
	if (cr0.eq) goto loc_824BAC34;
	// lwz r3,40(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 40);
loc_824BAB68:
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825576c8
	sub_825576C8(ctx, base);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bac28
	if (!cr6.eq) goto loc_824BAC28;
	// li r27,0
	r27.s64 = 0;
loc_824BAB98:
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r31,8(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r11,r31,25,25,31
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 25) & 0x7F;
	// addi r10,r11,-124
	ctx.r10.s64 = r11.s64 + -124;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bac64
	if (!cr0.eq) goto loc_824BAC64;
	// cmplwi cr6,r11,21
	cr6.compare<uint32_t>(r11.u32, 21, xer);
	// blt cr6,0x824babc8
	if (cr6.lt) goto loc_824BABC8;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824babcc
	if (!cr6.gt) goto loc_824BABCC;
loc_824BABC8:
	// mr r11,r27
	r11.u64 = r27.u64;
loc_824BABCC:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824babe0
	if (cr0.eq) goto loc_824BABE0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824bac64
	if (cr6.eq) goto loc_824BAC64;
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
loc_824BABE0:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824baf8c
	if (cr0.eq) goto loc_824BAF8C;
	// rlwinm. r11,r31,0,10,12
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x380000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824baf8c
	if (cr0.eq) goto loc_824BAF8C;
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r3,40(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825576c8
	sub_825576C8(ctx, base);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824bab98
	if (cr6.eq) goto loc_824BAB98;
loc_824BAC28:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BAC34:
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
loc_824BAC38:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x824bab68
	if (cr6.eq) goto loc_824BAB68;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bab68
	if (!cr0.eq) goto loc_824BAB68;
	// lwz r3,4(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// b 0x824bac38
	goto loc_824BAC38;
loc_824BAC64:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm. r11,r11,15,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bad6c
	if (cr0.eq) goto loc_824BAD6C;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// lwz r7,12(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r8,r28,4
	ctx.r8.s64 = r28.s64 + 4;
loc_824BAC80:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bad68
	if (cr0.eq) goto loc_824BAD68;
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r27,r9
	ctx.r10.s64 = ctx.r9.s64 - r27.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bad48
	if (cr0.eq) goto loc_824BAD48;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r6,r10,0,1,1
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq 0x824bacc0
	if (cr0.eq) goto loc_824BACC0;
	// rlwinm. r10,r10,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// beq 0x824bacc4
	if (cr0.eq) goto loc_824BACC4;
loc_824BACC0:
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_824BACC4:
	// clrlwi. r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bad48
	if (cr0.eq) goto loc_824BAD48;
	// lwz r10,8(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// rlwinm r9,r10,25,25,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r9,84
	cr6.compare<uint32_t>(ctx.r9.u32, 84, xer);
	// beq cr6,0x824bacfc
	if (cr6.eq) goto loc_824BACFC;
	// lwz r6,8(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r5,r10,14,18,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 14) & 0x3FFF;
	// rlwinm r6,r6,14,18,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 14) & 0x3FFF;
	// xor r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 ^ ctx.r6.u64;
	// clrlwi. r6,r6,31
	ctx.r6.u64 = ctx.r6.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne 0x824bad5c
	if (!cr0.eq) goto loc_824BAD5C;
	// cmplwi cr6,r9,84
	cr6.compare<uint32_t>(ctx.r9.u32, 84, xer);
	// bne cr6,0x824bad14
	if (!cr6.eq) goto loc_824BAD14;
loc_824BACFC:
	// lwz r9,8(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r10,r10,14,18,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 14) & 0x3FFF;
	// rlwinm r9,r9,14,18,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 14) & 0x3FFF;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bad5c
	if (cr0.eq) goto loc_824BAD5C;
loc_824BAD14:
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824bad24
	goto loc_824BAD24;
loc_824BAD20:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824BAD24:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824bad20
	if (!cr6.eq) goto loc_824BAD20;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, r11.u32);
	// stw r7,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r7.u32);
loc_824BAD48:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824bac80
	if (!cr6.eq) goto loc_824BAC80;
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// b 0x824bac80
	goto loc_824BAC80;
loc_824BAD5C:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BAD68:
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_824BAD6C:
	// addi r7,r28,4
	ctx.r7.s64 = r28.s64 + 4;
loc_824BAD70:
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bae68
	if (cr0.eq) goto loc_824BAE68;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r27,r10
	ctx.r10.s64 = ctx.r10.s64 - r27.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bae48
	if (cr0.eq) goto loc_824BAE48;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r10,r9,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824badb0
	if (cr0.eq) goto loc_824BADB0;
	// rlwinm. r10,r9,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// beq 0x824badb4
	if (cr0.eq) goto loc_824BADB4;
loc_824BADB0:
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_824BADB4:
	// clrlwi. r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bae5c
	if (!cr0.eq) goto loc_824BAE5C;
	// rlwinm r10,r9,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xE000000;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bae48
	if (cr0.eq) goto loc_824BAE48;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824bade8
	goto loc_824BADE8;
loc_824BADE4:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824BADE8:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824bade4
	if (!cr6.eq) goto loc_824BADE4;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r11.u32);
	// stw r8,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r8.u32);
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r8,r10,26
	ctx.r8.u64 = ctx.r10.u32 & 0x3F;
	// or r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 | ctx.r10.u64;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// rlwinm r9,r9,2,26,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3C;
	// rlwinm r6,r6,0,27,18
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFFFFFFFE01F;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// or r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 | ctx.r6.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_824BAE48:
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824bad70
	if (!cr6.eq) goto loc_824BAD70;
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// b 0x824bad70
	goto loc_824BAD70;
loc_824BAE5C:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BAE68:
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-15872
	r11.s64 = r11.s64 + -15872;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824baf78
	if (cr0.eq) goto loc_824BAF78;
loc_824BAE80:
	// lwz r7,4(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 4);
loc_824BAE84:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x824baf78
	if (cr6.eq) goto loc_824BAF78;
	// lwz r31,16(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// subf r11,r27,r31
	r11.s64 = r31.s64 - r27.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824baec0
	if (cr0.eq) goto loc_824BAEC0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824baec8
	if (!cr0.eq) goto loc_824BAEC8;
loc_824BAEC0:
	// lwz r7,8(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// b 0x824bae84
	goto loc_824BAE84;
loc_824BAEC8:
	// addi r6,r31,4
	ctx.r6.s64 = r31.s64 + 4;
loc_824BAECC:
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824baf54
	if (cr0.eq) goto loc_824BAF54;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r27,r10
	ctx.r10.s64 = ctx.r10.s64 - r27.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824baf40
	if (cr0.eq) goto loc_824BAF40;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824baf08
	goto loc_824BAF08;
loc_824BAF04:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824BAF08:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824baf04
	if (!cr6.eq) goto loc_824BAF04;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r11.u32);
	// stw r8,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r8.u32);
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwimi r10,r9,0,27,18
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0xFFFFFFFFFFFFE01F) | (ctx.r10.u64 & 0x1FE0);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_824BAF40:
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824baecc
	if (!cr6.eq) goto loc_824BAECC;
	// addi r6,r11,8
	ctx.r6.s64 = r11.s64 + 8;
	// b 0x824baecc
	goto loc_824BAECC;
loc_824BAF54:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a98e8
	sub_824A98E8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bae80
	if (cr0.eq) goto loc_824BAE80;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8255a3c0
	sub_8255A3C0(ctx, base);
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x824bae80
	goto loc_824BAE80;
loc_824BAF78:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8255a3c0
	sub_8255A3C0(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd40
	return;
loc_824BAF8C:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
}

__attribute__((alias("__imp__sub_824BAF98"))) PPC_WEAK_FUNC(sub_824BAF98);
PPC_FUNC_IMPL(__imp__sub_824BAF98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,36(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// rlwinm. r11,r11,0,12,12
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bafb8
	if (!cr0.eq) goto loc_824BAFB8;
	// lwz r11,36(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb020
	if (!cr0.eq) goto loc_824BB020;
	// lwz r11,32(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	// b 0x824bafc8
	goto loc_824BAFC8;
loc_824BAFB8:
	// lwz r11,28(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb020
	if (!cr0.eq) goto loc_824BB020;
	// lwz r11,24(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
loc_824BAFC8:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r3,r11,-36
	ctx.r3.s64 = r11.s64 + -36;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824bb014
	if (cr6.eq) goto loc_824BB014;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824bb014
	if (cr6.eq) goto loc_824BB014;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824bb014
	if (cr6.eq) goto loc_824BB014;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824bb014
	if (cr6.eq) goto loc_824BB014;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824bb014
	if (cr6.eq) goto loc_824BB014;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824bb018
	if (!cr6.eq) goto loc_824BB018;
loc_824BB014:
	// li r11,1
	r11.s64 = 1;
loc_824BB018:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bnelr 
	if (!cr0.eq) return;
loc_824BB020:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BB028"))) PPC_WEAK_FUNC(sub_824BB028);
PPC_FUNC_IMPL(__imp__sub_824BB028) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// stw r11,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, r11.u32);
	// b 0x824bb040
	goto loc_824BB040;
loc_824BB03C:
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
loc_824BB040:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r11,r4
	cr6.compare<uint32_t>(r11.u32, ctx.r4.u32, xer);
	// bne cr6,0x824bb03c
	if (!cr6.eq) goto loc_824BB03C;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// b 0x824bb064
	goto loc_824BB064;
loc_824BB060:
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
loc_824BB064:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r11,r4
	cr6.compare<uint32_t>(r11.u32, ctx.r4.u32, xer);
	// bne cr6,0x824bb060
	if (!cr6.eq) goto loc_824BB060;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// li r6,26
	ctx.r6.s64 = 26;
	// li r5,20
	ctx.r5.s64 = 20;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x824a9b30
	sub_824A9B30(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_824BB084"))) PPC_WEAK_FUNC(sub_824BB084);
PPC_FUNC_IMPL(__imp__sub_824BB084) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BB088"))) PPC_WEAK_FUNC(sub_824BB088);
PPC_FUNC_IMPL(__imp__sub_824BB088) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// bl 0x824ba910
	sub_824BA910(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824bb0d0
	if (cr0.eq) goto loc_824BB0D0;
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x824bb10c
	if (cr6.eq) goto loc_824BB10C;
	// rlwinm r11,r29,0,0,19
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFF000;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r3,148(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 148);
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BB0D0:
	// li r5,26
	ctx.r5.s64 = 26;
	// li r4,20
	ctx.r4.s64 = 20;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824b0748
	sub_824B0748(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// stw r31,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, r31.u32);
	// rlwimi r10,r11,13,7,18
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 13) & 0x1FFE000) | (ctx.r10.u64 & 0xFFFFFFFFFE001FFF);
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, r11.u32);
	// stw r4,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r4.u32);
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
loc_824BB10C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824BB114"))) PPC_WEAK_FUNC(sub_824BB114);
PPC_FUNC_IMPL(__imp__sub_824BB114) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BB118"))) PPC_WEAK_FUNC(sub_824BB118);
PPC_FUNC_IMPL(__imp__sub_824BB118) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r29,12(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825558f8
	sub_825558F8(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// bne 0x824bb15c
	if (!cr0.eq) goto loc_824BB15C;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BB15C:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// b 0x824bb16c
	goto loc_824BB16C;
loc_824BB168:
	// addi r11,r10,8
	r11.s64 = ctx.r10.s64 + 8;
loc_824BB16C:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// bne cr6,0x824bb168
	if (!cr6.eq) goto loc_824BB168;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r31,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r31.u32);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// not r10,r11
	ctx.r10.u64 = ~r11.u64;
	// rlwimi r10,r11,0,14,12
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFFFFFFBFFFF) | (ctx.r10.u64 & 0x40000);
	// stw r10,8(r28)
	PPC_STORE_U32(r28.u32 + 8, ctx.r10.u32);
	// lbz r11,8(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 8);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb26c
	if (cr0.eq) goto loc_824BB26C;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// oris r11,r11,256
	r11.u64 = r11.u64 | 16777216;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// lwz r11,36(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// rlwinm. r11,r11,0,12,12
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb248
	if (cr0.eq) goto loc_824BB248;
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r10,r29,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r7,r11,-32
	ctx.r7.s64 = r11.s64 + -32;
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r6,r6,0,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r8,32(r6)
	PPC_STORE_U32(ctx.r6.u32 + 32, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r6,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r6.u32);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,32(r8)
	PPC_STORE_U32(ctx.r8.u32 + 32, ctx.r9.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// lbz r11,12(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 12);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb248
	if (cr0.eq) goto loc_824BB248;
	// lwz r11,16(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// clrlwi r11,r11,19
	r11.u64 = r11.u32 & 0x1FFF;
	// lwz r9,12(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// rlwimi r11,r10,0,0,18
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFE000) | (r11.u64 & 0xFFFFFFFF00001FFF);
	// oris r10,r9,256
	ctx.r10.u64 = ctx.r9.u64 | 16777216;
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// stw r10,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r10.u32);
loc_824BB248:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824a98e8
	sub_824A98E8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb26c
	if (cr0.eq) goto loc_824BB26C;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
loc_824BB26C:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_824BB278"))) PPC_WEAK_FUNC(sub_824BB278);
PPC_FUNC_IMPL(__imp__sub_824BB278) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// mr r21,r7
	r21.u64 = ctx.r7.u64;
	// mr r20,r8
	r20.u64 = ctx.r8.u64;
	// li r25,0
	r25.s64 = 0;
	// li r23,0
	r23.s64 = 0;
	// li r24,0
	r24.s64 = 0;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bb4cc
	if (!cr0.eq) goto loc_824BB4CC;
	// mr. r29,r11
	r29.u64 = r11.u64;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// beq 0x824bb4cc
	if (cr0.eq) goto loc_824BB4CC;
	// lwz r11,48(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	// li r28,1
	r28.s64 = 1;
	// clrlwi r27,r11,13
	r27.u64 = r11.u32 & 0x7FFFF;
loc_824BB2C0:
	// clrlwi r9,r27,27
	ctx.r9.u64 = r27.u32 & 0x1F;
	// lwz r10,40(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// rlwinm r11,r27,27,5,31
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r9,r28,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r9,r11
	r11.u64 = ctx.r9.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bb4b0
	if (cr0.eq) goto loc_824BB4B0;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824bb334
	if (cr6.eq) goto loc_824BB334;
	// lwz r11,48(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r28,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bb4b0
	if (!cr0.eq) goto loc_824BB4B0;
loc_824BB334:
	// lwz r11,12(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bb3f4
	if (cr0.eq) goto loc_824BB3F4;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bb3f4
	if (cr0.eq) goto loc_824BB3F4;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_824BB350:
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x824bb3f8
	if (cr6.eq) goto loc_824BB3F8;
	// clrlwi r7,r27,27
	ctx.r7.u64 = r27.u32 & 0x1F;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// rlwinm r10,r27,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 27) & 0x7FFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r11,40(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// slw r7,r28,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r7.u8 & 0x3F));
	// and r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 & ctx.r10.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bb3d0
	if (cr0.eq) goto loc_824BB3D0;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824bb3ec
	if (cr6.eq) goto loc_824BB3EC;
	// lwz r10,48(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// clrlwi r7,r10,27
	ctx.r7.u64 = ctx.r10.u32 & 0x1F;
	// rlwinm r10,r10,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// slw r10,r28,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r7.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bb3ec
	if (cr0.eq) goto loc_824BB3EC;
loc_824BB3D0:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x824bb3e8
	if (cr6.eq) goto loc_824BB3E8;
	// cmplw cr6,r9,r25
	cr6.compare<uint32_t>(ctx.r9.u32, r25.u32, xer);
	// beq cr6,0x824bb3ec
	if (cr6.eq) goto loc_824BB3EC;
loc_824BB3E0:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824bb4ec
	goto loc_824BB4EC;
loc_824BB3E8:
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
loc_824BB3EC:
	// lwz r8,8(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// b 0x824bb350
	goto loc_824BB350;
loc_824BB3F4:
	// mr r23,r28
	r23.u64 = r28.u64;
loc_824BB3F8:
	// clrlwi. r11,r24,24
	r11.u64 = r24.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb4b0
	if (!cr0.eq) goto loc_824BB4B0;
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb428
	if (cr0.eq) goto loc_824BB428;
	// lwz r11,28(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb420
	if (cr0.eq) goto loc_824BB420;
	// addi r30,r29,32
	r30.s64 = r29.s64 + 32;
	// b 0x824bb42c
	goto loc_824BB42C;
loc_824BB420:
	// addi r30,r29,24
	r30.s64 = r29.s64 + 24;
	// b 0x824bb42c
	goto loc_824BB42C;
loc_824BB428:
	// addi r30,r29,16
	r30.s64 = r29.s64 + 16;
loc_824BB42C:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb4a0
	if (cr0.eq) goto loc_824BB4A0;
	// li r31,0
	r31.s64 = 0;
	// b 0x824bb4a0
	goto loc_824BB4A0;
loc_824BB440:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba450
	sub_824BA450(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb4ac
	if (!cr0.eq) goto loc_824BB4AC;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb468
	if (!cr0.eq) goto loc_824BB468;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bb440
	if (!cr6.eq) goto loc_824BB440;
loc_824BB468:
	// addi r10,r29,32
	ctx.r10.s64 = r29.s64 + 32;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x824bb4b0
	if (cr6.eq) goto loc_824BB4B0;
	// addi r11,r29,24
	r11.s64 = r29.s64 + 24;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x824bb490
	if (cr6.eq) goto loc_824BB490;
	// lwz r9,28(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// mr r30,r11
	r30.u64 = r11.u64;
	// clrlwi. r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bb494
	if (cr0.eq) goto loc_824BB494;
loc_824BB490:
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
loc_824BB494:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb4b0
	if (!cr0.eq) goto loc_824BB4B0;
loc_824BB4A0:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bb440
	if (!cr6.eq) goto loc_824BB440;
	// b 0x824bb4b0
	goto loc_824BB4B0;
loc_824BB4AC:
	// mr r24,r28
	r24.u64 = r28.u64;
loc_824BB4B0:
	// rlwinm r11,r29,0,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bb4cc
	if (!cr0.eq) goto loc_824BB4CC;
	// mr r29,r11
	r29.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bb2c0
	if (!cr6.eq) goto loc_824BB2C0;
loc_824BB4CC:
	// clrlwi. r11,r23,24
	r11.u64 = r23.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb4dc
	if (!cr0.eq) goto loc_824BB4DC;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x824bb3e0
	if (cr6.eq) goto loc_824BB3E0;
loc_824BB4DC:
	// stw r25,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r25.u32);
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r24,0(r21)
	PPC_STORE_U8(r21.u32 + 0, r24.u8);
	// stb r23,0(r22)
	PPC_STORE_U8(r22.u32 + 0, r23.u8);
loc_824BB4EC:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_824BB4F4"))) PPC_WEAK_FUNC(sub_824BB4F4);
PPC_FUNC_IMPL(__imp__sub_824BB4F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BB4F8"))) PPC_WEAK_FUNC(sub_824BB4F8);
PPC_FUNC_IMPL(__imp__sub_824BB4F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb528
	if (cr0.eq) goto loc_824BB528;
	// li r31,0
	r31.s64 = 0;
	// b 0x824bb5a0
	goto loc_824BB5A0;
loc_824BB528:
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-36
	xer.ca = r11.u32 > 35;
	r31.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824bb5a0
	if (cr0.eq) goto loc_824BB5A0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824bb574
	if (cr6.eq) goto loc_824BB574;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824bb574
	if (cr6.eq) goto loc_824BB574;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824bb574
	if (cr6.eq) goto loc_824BB574;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824bb574
	if (cr6.eq) goto loc_824BB574;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824bb574
	if (cr6.eq) goto loc_824BB574;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824bb578
	if (!cr6.eq) goto loc_824BB578;
loc_824BB574:
	// li r11,1
	r11.s64 = 1;
loc_824BB578:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb5a0
	if (cr0.eq) goto loc_824BB5A0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r31,-12
	r11.s64 = r31.s64 + -12;
	// lwzx r10,r3,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
loc_824BB5A0:
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// bne cr6,0x824bb5c4
	if (!cr6.eq) goto loc_824BB5C4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba0a8
	sub_824BA0A8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb5cc
	if (cr0.eq) goto loc_824BB5CC;
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// oris r11,r11,1024
	r11.u64 = r11.u64 | 67108864;
	// stw r11,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r11.u32);
loc_824BB5C4:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_824BB5CC:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10624
	cr6.compare<uint32_t>(r11.u32, 10624, xer);
	// beq cr6,0x824bb5c4
	if (cr6.eq) goto loc_824BB5C4;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
}

__attribute__((alias("__imp__sub_824BB5E8"))) PPC_WEAK_FUNC(sub_824BB5E8);
PPC_FUNC_IMPL(__imp__sub_824BB5E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,112(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 112);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,10,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bb63c
	if (cr0.eq) goto loc_824BB63C;
	// b 0x824bb620
	goto loc_824BB620;
loc_824BB614:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,0,9,9
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bb62c
	if (cr0.eq) goto loc_824BB62C;
loc_824BB620:
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bb614
	if (!cr0.eq) goto loc_824BB614;
loc_824BB62C:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,10,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bb620
	if (!cr0.eq) goto loc_824BB620;
	// stw r11,112(r4)
	PPC_STORE_U32(ctx.r4.u32 + 112, r11.u32);
loc_824BB63C:
	// lwz r10,112(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 112);
	// lwz r11,36(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb6b8
	if (!cr0.eq) goto loc_824BB6B8;
	// lwz r11,32(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r3,r11,-36
	xer.ca = r11.u32 > 35;
	ctx.r3.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824bb6b8
	if (cr0.eq) goto loc_824BB6B8;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm. r9,r11,15,31,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bb6b8
	if (cr0.eq) goto loc_824BB6B8;
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824bb6a0
	if (cr6.eq) goto loc_824BB6A0;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824bb6a0
	if (cr6.eq) goto loc_824BB6A0;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824bb6a0
	if (cr6.eq) goto loc_824BB6A0;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824bb6a0
	if (cr6.eq) goto loc_824BB6A0;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824bb6a0
	if (cr6.eq) goto loc_824BB6A0;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824bb6a4
	if (!cr6.eq) goto loc_824BB6A4;
loc_824BB6A0:
	// li r11,1
	r11.s64 = 1;
loc_824BB6A4:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb6b8
	if (cr0.eq) goto loc_824BB6B8;
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// lwz r31,12(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// b 0x824bb6bc
	goto loc_824BB6BC;
loc_824BB6B8:
	// lwz r31,96(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 96);
loc_824BB6BC:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824bb6f0
	if (cr6.eq) goto loc_824BB6F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb6f0
	if (cr0.eq) goto loc_824BB6F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_824BB6F0:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
}

__attribute__((alias("__imp__sub_824BB6FC"))) PPC_WEAK_FUNC(sub_824BB6FC);
PPC_FUNC_IMPL(__imp__sub_824BB6FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BB700"))) PPC_WEAK_FUNC(sub_824BB700);
PPC_FUNC_IMPL(__imp__sub_824BB700) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// lwz r11,76(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb758
	if (cr0.eq) goto loc_824BB758;
	// bl 0x824bb5e8
	sub_824BB5E8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82553608
	sub_82553608(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// b 0x824bb778
	goto loc_824BB778;
loc_824BB758:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r4,1
	ctx.r4.s64 = 1;
	// lfs f4,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f4.f64 = double(temp.f32);
	// fmr f3,f4
	ctx.f3.f64 = ctx.f4.f64;
	// fmr f2,f4
	ctx.f2.f64 = ctx.f4.f64;
	// fmr f1,f4
	ctx.f1.f64 = ctx.f4.f64;
	// bl 0x82557350
	sub_82557350(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
loc_824BB778:
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb78c
	if (cr0.eq) goto loc_824BB78C;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824bb798
	goto loc_824BB798;
loc_824BB78C:
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r3,r11,-36
	ctx.r3.s64 = r11.s64 + -36;
loc_824BB798:
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// lwz r29,12(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmpwi cr6,r11,58
	cr6.compare<int32_t>(r11.s32, 58, xer);
	// beq cr6,0x824bb7ec
	if (cr6.eq) goto loc_824BB7EC;
	// cmpwi cr6,r11,59
	cr6.compare<int32_t>(r11.s32, 59, xer);
	// beq cr6,0x824bb7e4
	if (cr6.eq) goto loc_824BB7E4;
	// cmpwi cr6,r11,60
	cr6.compare<int32_t>(r11.s32, 60, xer);
	// beq cr6,0x824bb7dc
	if (cr6.eq) goto loc_824BB7DC;
	// cmpwi cr6,r11,61
	cr6.compare<int32_t>(r11.s32, 61, xer);
	// beq cr6,0x824bb7d4
	if (cr6.eq) goto loc_824BB7D4;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BB7D4:
	// li r6,24
	ctx.r6.s64 = 24;
	// b 0x824bb7f0
	goto loc_824BB7F0;
loc_824BB7DC:
	// li r6,23
	ctx.r6.s64 = 23;
	// b 0x824bb7f0
	goto loc_824BB7F0;
loc_824BB7E4:
	// li r6,22
	ctx.r6.s64 = 22;
	// b 0x824bb7f0
	goto loc_824BB7F0;
loc_824BB7EC:
	// li r6,21
	ctx.r6.s64 = 21;
loc_824BB7F0:
	// li r8,1
	ctx.r8.s64 = 1;
	// li r7,2
	ctx.r7.s64 = 2;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ori r11,r11,64
	r11.u64 = r11.u64 | 64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// lwz r4,40(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// rlwinm r11,r29,0,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r3,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r3.u32);
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,32(r7)
	PPC_STORE_U32(ctx.r7.u32 + 32, ctx.r9.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r11,76(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb89c
	if (!cr0.eq) goto loc_824BB89C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_824BB89C:
	// addi r8,r29,4
	ctx.r8.s64 = r29.s64 + 4;
loc_824BB8A0:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bb910
	if (cr0.eq) goto loc_824BB910;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// addi r10,r10,0
	ctx.r10.s64 = ctx.r10.s64 + 0;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bb8fc
	if (cr0.eq) goto loc_824BB8FC;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824bb8d8
	goto loc_824BB8D8;
loc_824BB8D4:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824BB8D8:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824bb8d4
	if (!cr6.eq) goto loc_824BB8D4;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r31,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r31.u32);
loc_824BB8FC:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824bb8a0
	if (!cr6.eq) goto loc_824BB8A0;
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// b 0x824bb8a0
	goto loc_824BB8A0;
loc_824BB910:
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_824BB930"))) PPC_WEAK_FUNC(sub_824BB930);
PPC_FUNC_IMPL(__imp__sub_824BB930) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,28(r5)
	r31.u64 = PPC_LOAD_U32(ctx.r5.u32 + 28);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb9bc
	if (!cr0.eq) goto loc_824BB9BC;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824bb9bc
	if (cr0.eq) goto loc_824BB9BC;
loc_824BB958:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r10,r11,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r10,r10,-13952
	ctx.r10.s64 = ctx.r10.s64 + -13952;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bb9a4
	if (cr0.eq) goto loc_824BB9A4;
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bb9a4
	if (cr0.eq) goto loc_824BB9A4;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// bl 0x824c5858
	sub_824C5858(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824c5da0
	sub_824C5DA0(ctx, base);
	// cmplw cr6,r3,r29
	cr6.compare<uint32_t>(ctx.r3.u32, r29.u32, xer);
	// beq cr6,0x824bb9c8
	if (cr6.eq) goto loc_824BB9C8;
loc_824BB9A4:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bb9bc
	if (!cr0.eq) goto loc_824BB9BC;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bb958
	if (!cr6.eq) goto loc_824BB958;
loc_824BB9BC:
	// li r3,0
	ctx.r3.s64 = 0;
loc_824BB9C0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_824BB9C8:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x824bb9c0
	goto loc_824BB9C0;
}

__attribute__((alias("__imp__sub_824BB9D0"))) PPC_WEAK_FUNC(sub_824BB9D0);
PPC_FUNC_IMPL(__imp__sub_824BB9D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r11,20(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bbb88
	if (!cr0.eq) goto loc_824BBB88;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824bbb88
	if (cr0.eq) goto loc_824BBB88;
	// li r25,14208
	r25.s64 = 14208;
loc_824BB9FC:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbb6c
	if (cr0.eq) goto loc_824BBB6C;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lhz r4,14(r30)
	ctx.r4.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,20(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r29,0
	r29.s64 = 0;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bbb6c
	if (!cr0.eq) goto loc_824BBB6C;
	// mr. r31,r11
	r31.u64 = r11.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824bbb6c
	if (cr0.eq) goto loc_824BBB6C;
loc_824BBA3C:
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// beq cr6,0x824bbb6c
	if (cr6.eq) goto loc_824BBB6C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbaf8
	if (cr0.eq) goto loc_824BBAF8;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lhz r4,14(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 14);
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// cmplw cr6,r28,r3
	cr6.compare<uint32_t>(r28.u32, ctx.r3.u32, xer);
	// bne cr6,0x824bbaf8
	if (!cr6.eq) goto loc_824BBAF8;
	// addi r8,r31,4
	ctx.r8.s64 = r31.s64 + 4;
loc_824BBA74:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bbae4
	if (cr0.eq) goto loc_824BBAE4;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// addi r10,r10,0
	ctx.r10.s64 = ctx.r10.s64 + 0;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bbad0
	if (cr0.eq) goto loc_824BBAD0;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824bbaac
	goto loc_824BBAAC;
loc_824BBAA8:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824BBAAC:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824bbaa8
	if (!cr6.eq) goto loc_824BBAA8;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// stw r30,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r30.u32);
loc_824BBAD0:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824bba74
	if (!cr6.eq) goto loc_824BBA74;
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// b 0x824bba74
	goto loc_824BBA74;
loc_824BBAE4:
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r5,616(r27)
	ctx.r5.u64 = PPC_LOAD_U32(r27.u32 + 616);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
loc_824BBAF8:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824bbb40
	if (cr6.eq) goto loc_824BBB40;
	// rlwinm r11,r29,0,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// bne 0x824bbb18
	if (!cr0.eq) goto loc_824BBB18;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_824BBB18:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x824bbb30
	if (!cr6.eq) goto loc_824BBB30;
loc_824BBB20:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r29,r31
	r29.u64 = r31.u64;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// b 0x824bbb5c
	goto loc_824BBB5C;
loc_824BBB30:
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bbb6c
	if (!cr0.eq) goto loc_824BBB6C;
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
	// b 0x824bbb64
	goto loc_824BBB64;
loc_824BBB40:
	// lwz r11,20(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bbb50
	if (cr0.eq) goto loc_824BBB50;
	// li r11,0
	r11.s64 = 0;
loc_824BBB50:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x824bbb20
	if (cr6.eq) goto loc_824BBB20;
	// lwz r31,20(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 20);
loc_824BBB5C:
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bbb6c
	if (!cr0.eq) goto loc_824BBB6C;
loc_824BBB64:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bba3c
	if (!cr6.eq) goto loc_824BBA3C;
loc_824BBB6C:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bbb88
	if (!cr0.eq) goto loc_824BBB88;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bb9fc
	if (!cr6.eq) goto loc_824BB9FC;
loc_824BBB88:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_824BBB90"))) PPC_WEAK_FUNC(sub_824BBB90);
PPC_FUNC_IMPL(__imp__sub_824BBB90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// li r22,0
	r22.s64 = 0;
	// li r20,0
	r20.s64 = 0;
	// li r23,1
	r23.s64 = 1;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bbdfc
	if (!cr0.eq) goto loc_824BBDFC;
	// mr. r28,r11
	r28.u64 = r11.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824bbdfc
	if (cr0.eq) goto loc_824BBDFC;
loc_824BBBD4:
	// lwz r11,48(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 48);
	// lwz r10,40(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r9,r23,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r9,r11
	r11.u64 = ctx.r9.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bbde0
	if (cr0.eq) goto loc_824BBDE0;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824bbc50
	if (cr6.eq) goto loc_824BBC50;
	// lwz r11,48(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r23,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bbde0
	if (!cr0.eq) goto loc_824BBDE0;
loc_824BBC50:
	// lwz r11,48(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 48);
	// rlwinm. r11,r11,0,6,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bbf28
	if (!cr0.eq) goto loc_824BBF28;
	// lwz r11,28(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// li r30,0
	r30.s64 = 0;
	// addi r10,r28,32
	ctx.r10.s64 = r28.s64 + 32;
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bbc74
	if (!cr0.eq) goto loc_824BBC74;
	// addi r10,r28,24
	ctx.r10.s64 = r28.s64 + 24;
loc_824BBC74:
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824bbc84
	if (!cr0.eq) goto loc_824BBC84;
	// mr r30,r11
	r30.u64 = r11.u64;
loc_824BBC84:
	// mr r29,r10
	r29.u64 = ctx.r10.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824bbde0
	if (cr6.eq) goto loc_824BBDE0;
loc_824BBC90:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824a90a0
	sub_824A90A0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bbf28
	if (!cr0.eq) goto loc_824BBF28;
	// lwz r31,8(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm. r11,r31,7,31,31
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 7) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bbf28
	if (!cr0.eq) goto loc_824BBF28;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba0a8
	sub_824BA0A8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbf28
	if (cr0.eq) goto loc_824BBF28;
	// rlwinm r31,r31,25,25,31
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// blt cr6,0x824bbcd4
	if (cr6.lt) goto loc_824BBCD4;
	// cmplwi cr6,r31,101
	cr6.compare<uint32_t>(r31.u32, 101, xer);
	// mr r11,r23
	r11.u64 = r23.u64;
	// ble cr6,0x824bbcd8
	if (!cr6.gt) goto loc_824BBCD8;
loc_824BBCD4:
	// li r11,0
	r11.s64 = 0;
loc_824BBCD8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bbce8
	if (!cr0.eq) goto loc_824BBCE8;
	// cmplwi cr6,r31,104
	cr6.compare<uint32_t>(r31.u32, 104, xer);
	// bne cr6,0x824bbda0
	if (!cr6.eq) goto loc_824BBDA0;
loc_824BBCE8:
	// lwz r11,36(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 36);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// rlwinm. r11,r11,0,12,12
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbd6c
	if (cr0.eq) goto loc_824BBD6C;
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bbd6c
	if (!cr0.eq) goto loc_824BBD6C;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r3,r11,-36
	xer.ca = r11.u32 > 35;
	ctx.r3.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824bbd6c
	if (cr0.eq) goto loc_824BBD6C;
	// cmplwi cr6,r31,31
	cr6.compare<uint32_t>(r31.u32, 31, xer);
	// blt cr6,0x824bbd28
	if (cr6.lt) goto loc_824BBD28;
	// cmplwi cr6,r31,81
	cr6.compare<uint32_t>(r31.u32, 81, xer);
	// mr r11,r23
	r11.u64 = r23.u64;
	// ble cr6,0x824bbd2c
	if (!cr6.gt) goto loc_824BBD2C;
loc_824BBD28:
	// li r11,0
	r11.s64 = 0;
loc_824BBD2C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbd6c
	if (cr0.eq) goto loc_824BBD6C;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824bbd50
	if (cr6.lt) goto loc_824BBD50;
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// mr r11,r23
	r11.u64 = r23.u64;
	// ble cr6,0x824bbd54
	if (!cr6.gt) goto loc_824BBD54;
loc_824BBD50:
	// li r11,0
	r11.s64 = 0;
loc_824BBD54:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbd6c
	if (cr0.eq) goto loc_824BBD6C;
	// bl 0x824a9c08
	sub_824A9C08(ctx, base);
	// cmplw cr6,r30,r3
	cr6.compare<uint32_t>(r30.u32, ctx.r3.u32, xer);
	// bne cr6,0x824bbd6c
	if (!cr6.eq) goto loc_824BBD6C;
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
loc_824BBD6C:
	// cmplwi cr6,r31,95
	cr6.compare<uint32_t>(r31.u32, 95, xer);
	// blt cr6,0x824bbd80
	if (cr6.lt) goto loc_824BBD80;
	// cmplwi cr6,r31,101
	cr6.compare<uint32_t>(r31.u32, 101, xer);
	// mr r11,r23
	r11.u64 = r23.u64;
	// ble cr6,0x824bbd84
	if (!cr6.gt) goto loc_824BBD84;
loc_824BBD80:
	// li r11,0
	r11.s64 = 0;
loc_824BBD84:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bbd9c
	if (!cr0.eq) goto loc_824BBD9C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824d19d8
	sub_824D19D8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbda0
	if (cr0.eq) goto loc_824BBDA0;
loc_824BBD9C:
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
loc_824BBDA0:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bbdbc
	if (!cr0.eq) goto loc_824BBDBC;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bbc90
	if (!cr6.eq) goto loc_824BBC90;
loc_824BBDBC:
	// addi r11,r28,32
	r11.s64 = r28.s64 + 32;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// beq cr6,0x824bbde0
	if (cr6.eq) goto loc_824BBDE0;
	// mr r29,r11
	r29.u64 = r11.u64;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bbde0
	if (!cr0.eq) goto loc_824BBDE0;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// bne 0x824bbc90
	if (!cr0.eq) goto loc_824BBC90;
loc_824BBDE0:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bbdfc
	if (!cr0.eq) goto loc_824BBDFC;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bbbd4
	if (!cr6.eq) goto loc_824BBBD4;
loc_824BBDFC:
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// beq cr6,0x824bbf28
	if (cr6.eq) goto loc_824BBF28;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// beq cr6,0x824bbf18
	if (cr6.eq) goto loc_824BBF18;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// beq cr6,0x824bbf0c
	if (cr6.eq) goto loc_824BBF0C;
	// cmpwi cr6,r3,7
	cr6.compare<int32_t>(ctx.r3.s32, 7, xer);
	// ble cr6,0x824bbe30
	if (!cr6.gt) goto loc_824BBE30;
	// cmpwi cr6,r3,9
	cr6.compare<int32_t>(ctx.r3.s32, 9, xer);
	// ble cr6,0x824bbf28
	if (!cr6.gt) goto loc_824BBF28;
loc_824BBE30:
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824bbe4c
	if (cr6.lt) goto loc_824BBE4C;
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// mr r11,r23
	r11.u64 = r23.u64;
	// ble cr6,0x824bbe50
	if (!cr6.gt) goto loc_824BBE50;
loc_824BBE4C:
	// li r11,0
	r11.s64 = 0;
loc_824BBE50:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbe60
	if (cr0.eq) goto loc_824BBE60;
	// lwz r11,44(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 44);
	// b 0x824bbe64
	goto loc_824BBE64;
loc_824BBE60:
	// lwz r11,40(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 40);
loc_824BBE64:
	// lwz r31,12(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-15744
	r11.s64 = r11.s64 + -15744;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbebc
	if (cr0.eq) goto loc_824BBEBC;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r31,-8
	r11.s64 = r31.s64 + -8;
	// lwz r10,12(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 12);
	// lwzx r11,r3,r11
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// clrlwi r11,r11,17
	r11.u64 = r11.u32 & 0x7FFF;
	// mulli r11,r11,40
	r11.s64 = r11.s64 * 40;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm. r11,r11,0,25,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbf0c
	if (cr0.eq) goto loc_824BBF0C;
loc_824BBEBC:
	// lwz r11,24(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 24);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r11,r11,6,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bbee0
	if (!cr0.eq) goto loc_824BBEE0;
	// lwz r11,36(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 36);
	// rlwinm. r11,r11,0,12,12
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbf28
	if (cr0.eq) goto loc_824BBF28;
	// cmplwi cr6,r22,12
	cr6.compare<uint32_t>(r22.u32, 12, xer);
	// bgt cr6,0x824bbf28
	if (cr6.gt) goto loc_824BBF28;
loc_824BBEE0:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x824bbf04
	if (!cr6.eq) goto loc_824BBF04;
	// lwz r11,36(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 36);
	// rlwinm. r11,r11,0,12,12
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bbefc
	if (cr0.eq) goto loc_824BBEFC;
	// cmplwi cr6,r22,1
	cr6.compare<uint32_t>(r22.u32, 1, xer);
	// bgt cr6,0x824bbf04
	if (cr6.gt) goto loc_824BBF04;
loc_824BBEFC:
	// li r11,0
	r11.s64 = 0;
	// b 0x824bbf1c
	goto loc_824BBF1C;
loc_824BBF04:
	// mr r11,r23
	r11.u64 = r23.u64;
	// b 0x824bbf1c
	goto loc_824BBF1C;
loc_824BBF0C:
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r23,0(r19)
	PPC_STORE_U8(r19.u32 + 0, r23.u8);
	// b 0x824bbf2c
	goto loc_824BBF2C;
loc_824BBF18:
	// li r11,0
	r11.s64 = 0;
loc_824BBF1C:
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,0(r19)
	PPC_STORE_U8(r19.u32 + 0, r11.u8);
	// b 0x824bbf2c
	goto loc_824BBF2C;
loc_824BBF28:
	// li r3,0
	ctx.r3.s64 = 0;
loc_824BBF2C:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_824BBF34"))) PPC_WEAK_FUNC(sub_824BBF34);
PPC_FUNC_IMPL(__imp__sub_824BBF34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BBF38"))) PPC_WEAK_FUNC(sub_824BBF38);
PPC_FUNC_IMPL(__imp__sub_824BBF38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// mr r20,r6
	r20.u64 = ctx.r6.u64;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// lwz r31,4(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// mr r21,r8
	r21.u64 = ctx.r8.u64;
	// mr r25,r26
	r25.u64 = r26.u64;
	// li r23,0
	r23.s64 = 0;
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bc1c8
	if (!cr0.eq) goto loc_824BC1C8;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824bc1c8
	if (cr0.eq) goto loc_824BC1C8;
	// li r28,1
	r28.s64 = 1;
loc_824BBF7C:
	// lwz r11,48(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 48);
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r9,r28,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r9,r11
	r11.u64 = ctx.r9.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc14c
	if (cr0.eq) goto loc_824BC14C;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x824bbff8
	if (cr6.eq) goto loc_824BBFF8;
	// lwz r11,48(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r28,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bc14c
	if (!cr0.eq) goto loc_824BC14C;
loc_824BBFF8:
	// lwz r7,48(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// clrlwi r9,r22,24
	ctx.r9.u64 = r22.u32 & 0xFF;
	// lwz r6,76(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// clrlwi r8,r21,24
	ctx.r8.u64 = r21.u32 & 0xFF;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r26,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r26.u32);
	// rlwimi r7,r8,24,7,7
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 24) & 0x1000000) | (ctx.r7.u64 & 0xFFFFFFFFFEFFFFFF);
	// rlwimi r6,r9,23,8,8
	ctx.r6.u64 = (__builtin_rotateleft32(ctx.r9.u32, 23) & 0x800000) | (ctx.r6.u64 & 0xFFFFFFFFFF7FFFFF);
	// oris r9,r7,128
	ctx.r9.u64 = ctx.r7.u64 | 8388608;
	// oris r8,r6,64
	ctx.r8.u64 = ctx.r6.u64 | 4194304;
	// rlwinm r10,r25,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r25,r31
	r25.u64 = r31.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r9,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r9.u32);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// stw r8,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r8.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r8,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bc140
	if (!cr0.eq) goto loc_824BC140;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r29,r11,-36
	xer.ca = r11.u32 > 35;
	r29.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// beq 0x824bc140
	if (cr0.eq) goto loc_824BC140;
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824bc0cc
	if (cr6.eq) goto loc_824BC0CC;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824bc0cc
	if (cr6.eq) goto loc_824BC0CC;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824bc0cc
	if (cr6.eq) goto loc_824BC0CC;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824bc0cc
	if (cr6.eq) goto loc_824BC0CC;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824bc0cc
	if (cr6.eq) goto loc_824BC0CC;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824bc0d0
	if (!cr6.eq) goto loc_824BC0D0;
loc_824BC0CC:
	// mr r11,r28
	r11.u64 = r28.u64;
loc_824BC0D0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bc140
	if (cr0.eq) goto loc_824BC140;
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r10,48(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 48);
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// clrlwi r30,r10,13
	r30.u64 = ctx.r10.u32 & 0x7FFFF;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r10,r29,-12
	ctx.r10.s64 = r29.s64 + -12;
	// clrlwi r9,r30,27
	ctx.r9.u64 = r30.u32 & 0x1F;
	// rlwinm r11,r30,27,5,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwzx r10,r3,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r10.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,40(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// slw r10,r28,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bc14c
	if (!cr0.eq) goto loc_824BC14C;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// oris r11,r11,1024
	r11.u64 = r11.u64 | 67108864;
	// b 0x824bc148
	goto loc_824BC148;
loc_824BC140:
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// oris r11,r11,2048
	r11.u64 = r11.u64 | 134217728;
loc_824BC148:
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
loc_824BC14C:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x824bc180
	if (cr6.eq) goto loc_824BC180;
	// rlwinm r11,r23,0,0,30
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// bne 0x824bc16c
	if (!cr0.eq) goto loc_824BC16C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_824BC16C:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x824bc19c
	if (cr6.eq) goto loc_824BC19C;
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bc1c8
	if (!cr0.eq) goto loc_824BC1C8;
	// b 0x824bc1bc
	goto loc_824BC1BC;
loc_824BC180:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// li r11,0
	r11.s64 = 0;
	// clrlwi. r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824bc194
	if (!cr0.eq) goto loc_824BC194;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_824BC194:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x824bc1b4
	if (!cr6.eq) goto loc_824BC1B4;
loc_824BC19C:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r23,r31
	r23.u64 = r31.u64;
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bc1c8
	if (!cr0.eq) goto loc_824BC1C8;
	// b 0x824bc1c0
	goto loc_824BC1C0;
loc_824BC1B4:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x824bc1c8
	if (!cr6.eq) goto loc_824BC1C8;
loc_824BC1BC:
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
loc_824BC1C0:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bbf7c
	if (!cr6.eq) goto loc_824BBF7C;
loc_824BC1C8:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_824BC1D0"))) PPC_WEAK_FUNC(sub_824BC1D0);
PPC_FUNC_IMPL(__imp__sub_824BC1D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,20(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bc224
	if (!cr0.eq) goto loc_824BC224;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc224
	if (cr0.eq) goto loc_824BC224;
loc_824BC1F4:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// addi r10,r10,-14208
	ctx.r10.s64 = ctx.r10.s64 + -14208;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bc2b0
	if (cr0.eq) goto loc_824BC2B0;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bc224
	if (!cr0.eq) goto loc_824BC224;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bc1f4
	if (!cr6.eq) goto loc_824BC1F4;
loc_824BC224:
	// lwz r11,28(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bc25c
	if (!cr0.eq) goto loc_824BC25C;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc25c
	if (cr0.eq) goto loc_824BC25C;
loc_824BC238:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm. r10,r10,7,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bc2b0
	if (!cr0.eq) goto loc_824BC2B0;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bc25c
	if (!cr0.eq) goto loc_824BC25C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bc238
	if (!cr6.eq) goto loc_824BC238;
loc_824BC25C:
	// lwz r31,36(r4)
	r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bc298
	if (!cr0.eq) goto loc_824BC298;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824bc298
	if (cr0.eq) goto loc_824BC298;
loc_824BC270:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bc2b0
	if (cr0.eq) goto loc_824BC2B0;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bc298
	if (!cr0.eq) goto loc_824BC298;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bc270
	if (!cr6.eq) goto loc_824BC270;
loc_824BC298:
	// li r3,1
	ctx.r3.s64 = 1;
loc_824BC29C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_824BC2B0:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824bc29c
	goto loc_824BC29C;
}

__attribute__((alias("__imp__sub_824BC2B8"))) PPC_WEAK_FUNC(sub_824BC2B8);
PPC_FUNC_IMPL(__imp__sub_824BC2B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,24(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,112(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r9,r9,10,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bc30c
	if (cr0.eq) goto loc_824BC30C;
	// b 0x824bc2f0
	goto loc_824BC2F0;
loc_824BC2E4:
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r9,r9,0,9,9
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bc2fc
	if (cr0.eq) goto loc_824BC2FC;
loc_824BC2F0:
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bc2e4
	if (!cr0.eq) goto loc_824BC2E4;
loc_824BC2FC:
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r9,r9,10,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824bc2f0
	if (!cr0.eq) goto loc_824BC2F0;
	// stw r11,112(r10)
	PPC_STORE_U32(ctx.r10.u32 + 112, r11.u32);
loc_824BC30C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,112(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	// bl 0x824baf98
	sub_824BAF98(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x824bc328
	if (!cr0.eq) goto loc_824BC328;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x824bc340
	goto loc_824BC340;
loc_824BC328:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// addi r11,r3,-1
	r11.s64 = ctx.r3.s64 + -1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r3,r11,1
	ctx.r3.u64 = r11.u64 ^ 1;
loc_824BC340:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BC354"))) PPC_WEAK_FUNC(sub_824BC354);
PPC_FUNC_IMPL(__imp__sub_824BC354) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BC358"))) PPC_WEAK_FUNC(sub_824BC358);
PPC_FUNC_IMPL(__imp__sub_824BC358) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	// addi r10,r4,4
	ctx.r10.s64 = ctx.r4.s64 + 4;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bnelr 
	if (!cr0.eq) return;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r7,r3,-32
	ctx.r7.s64 = ctx.r3.s64 + -32;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r6,r4,-32
	ctx.r6.s64 = ctx.r4.s64 + -32;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// ori r7,r7,1
	ctx.r7.u64 = ctx.r7.u64 | 1;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// addi r8,r11,-32
	ctx.r8.s64 = r11.s64 + -32;
	// addi r11,r9,4
	r11.s64 = ctx.r9.s64 + 4;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// ori r6,r6,1
	ctx.r6.u64 = ctx.r6.u64 | 1;
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// ori r5,r10,1
	ctx.r5.u64 = ctx.r10.u64 | 1;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// stw r5,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r5.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BC3C0"))) PPC_WEAK_FUNC(sub_824BC3C0);
PPC_FUNC_IMPL(__imp__sub_824BC3C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// mulli r9,r9,28
	ctx.r9.s64 = ctx.r9.s64 * 28;
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r3,r9,16
	ctx.r3.s64 = ctx.r9.s64 + 16;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// clrlwi. r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bc400
	if (cr0.eq) goto loc_824BC400;
	// li r10,0
	ctx.r10.s64 = 0;
loc_824BC400:
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BC410"))) PPC_WEAK_FUNC(sub_824BC410);
PPC_FUNC_IMPL(__imp__sub_824BC410) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r4,10
	r11.s64 = ctx.r4.s64 + 10;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// rlwinm r30,r11,2,0,29
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// lwzx r4,r30,r31
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + r31.u32);
	// bl 0x824bb028
	sub_824BB028(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// stwx r3,r30,r31
	PPC_STORE_U32(r30.u32 + r31.u32, ctx.r3.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824BC450"))) PPC_WEAK_FUNC(sub_824BC450);
PPC_FUNC_IMPL(__imp__sub_824BC450) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// li r8,4
	ctx.r8.s64 = 4;
	// li r7,2
	ctx.r7.s64 = 2;
	// li r6,3
	ctx.r6.s64 = 3;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// li r26,0
	r26.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// subf r11,r26,r11
	r11.s64 = r11.s64 - r26.s64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc4b8
	if (cr0.eq) goto loc_824BC4B8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC4B8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// stw r3,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r3.u32);
	// lwz r11,16(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// subf r11,r26,r11
	r11.s64 = r11.s64 - r26.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc4f4
	if (cr0.eq) goto loc_824BC4F4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r27)
	ctx.r5.u64 = PPC_LOAD_U32(r27.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC4F4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// stw r3,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r3.u32);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,7,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x7;
	// rlwimi r10,r11,14,15,17
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 14) & 0x1C000) | (ctx.r10.u64 & 0xFFFFFFFFFFFE3FFF);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// beq cr6,0x824bc554
	if (cr6.eq) goto loc_824BC554;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r9,0(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// addi r10,r28,-32
	ctx.r10.s64 = r28.s64 + -32;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// ori r8,r10,1
	ctx.r8.u64 = ctx.r10.u64 | 1;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// lwz r9,0(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
loc_824BC554:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_824BC560"))) PPC_WEAK_FUNC(sub_824BC560);
PPC_FUNC_IMPL(__imp__sub_824BC560) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// li r8,4
	ctx.r8.s64 = 4;
	// li r7,3
	ctx.r7.s64 = 3;
	// li r6,13
	ctx.r6.s64 = 13;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// li r25,0
	r25.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc5cc
	if (cr0.eq) goto loc_824BC5CC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC5CC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// stw r3,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r3.u32);
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc608
	if (cr0.eq) goto loc_824BC608;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC608:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// stw r3,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r3.u32);
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc644
	if (cr0.eq) goto loc_824BC644;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC644:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,7,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x7;
	// rlwimi r10,r11,14,15,17
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 14) & 0x1C000) | (ctx.r10.u64 & 0xFFFFFFFFFFFE3FFF);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// beq cr6,0x824bc6a4
	if (cr6.eq) goto loc_824BC6A4;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// addi r10,r27,-32
	ctx.r10.s64 = r27.s64 + -32;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// ori r8,r10,1
	ctx.r8.u64 = ctx.r10.u64 | 1;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// stw r10,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r10.u32);
loc_824BC6A4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_824BC6B0"))) PPC_WEAK_FUNC(sub_824BC6B0);
PPC_FUNC_IMPL(__imp__sub_824BC6B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// li r8,4
	ctx.r8.s64 = 4;
	// li r7,3
	ctx.r7.s64 = 3;
	// li r6,14
	ctx.r6.s64 = 14;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// li r25,0
	r25.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc71c
	if (cr0.eq) goto loc_824BC71C;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC71C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// stw r3,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r3.u32);
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc758
	if (cr0.eq) goto loc_824BC758;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC758:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// stw r3,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r3.u32);
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc794
	if (cr0.eq) goto loc_824BC794;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC794:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,7,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x7;
	// rlwimi r10,r11,14,15,17
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 14) & 0x1C000) | (ctx.r10.u64 & 0xFFFFFFFFFFFE3FFF);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// beq cr6,0x824bc7f4
	if (cr6.eq) goto loc_824BC7F4;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// addi r10,r27,-32
	ctx.r10.s64 = r27.s64 + -32;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// ori r8,r10,1
	ctx.r8.u64 = ctx.r10.u64 | 1;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// stw r10,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r10.u32);
loc_824BC7F4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_824BC800"))) PPC_WEAK_FUNC(sub_824BC800);
PPC_FUNC_IMPL(__imp__sub_824BC800) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// li r8,4
	ctx.r8.s64 = 4;
	// li r7,3
	ctx.r7.s64 = 3;
	// li r6,15
	ctx.r6.s64 = 15;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// li r25,0
	r25.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc86c
	if (cr0.eq) goto loc_824BC86C;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC86C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// stw r3,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r3.u32);
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc8a8
	if (cr0.eq) goto loc_824BC8A8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC8A8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// stw r3,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r3.u32);
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bc8e4
	if (cr0.eq) goto loc_824BC8E4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BC8E4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,7,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x7;
	// rlwimi r10,r11,14,15,17
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 14) & 0x1C000) | (ctx.r10.u64 & 0xFFFFFFFFFFFE3FFF);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// beq cr6,0x824bc944
	if (cr6.eq) goto loc_824BC944;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// addi r10,r27,-32
	ctx.r10.s64 = r27.s64 + -32;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// ori r8,r10,1
	ctx.r8.u64 = ctx.r10.u64 | 1;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// stw r10,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r10.u32);
loc_824BC944:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_824BC950"))) PPC_WEAK_FUNC(sub_824BC950);
PPC_FUNC_IMPL(__imp__sub_824BC950) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// li r8,1
	ctx.r8.s64 = 1;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,64
	ctx.r6.s64 = 64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// lwz r4,24(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x824b0928
	sub_824B0928(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824BC9A4"))) PPC_WEAK_FUNC(sub_824BC9A4);
PPC_FUNC_IMPL(__imp__sub_824BC9A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BC9A8"))) PPC_WEAK_FUNC(sub_824BC9A8);
PPC_FUNC_IMPL(__imp__sub_824BC9A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// li r8,1
	ctx.r8.s64 = 1;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,65
	ctx.r6.s64 = 65;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r11,r11,0
	r11.s64 = r11.s64 + 0;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bca08
	if (cr0.eq) goto loc_824BCA08;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,12(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_824BCA08:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// addi r11,r28,24
	r11.s64 = r28.s64 + 24;
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r7,r11,-32
	ctx.r7.s64 = r11.s64 + -32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// stw r9,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r9.u32);
	// ori r7,r7,1
	ctx.r7.u64 = ctx.r7.u64 | 1;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824BCA5C"))) PPC_WEAK_FUNC(sub_824BCA5C);
PPC_FUNC_IMPL(__imp__sub_824BCA5C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BCA60"))) PPC_WEAK_FUNC(sub_824BCA60);
PPC_FUNC_IMPL(__imp__sub_824BCA60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r21,r3
	r21.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r24,r6
	r24.u64 = ctx.r6.u64;
	// mr r23,r7
	r23.u64 = ctx.r7.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r22,r8
	r22.u64 = ctx.r8.u64;
	// rlwinm r11,r10,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// bne cr6,0x824bca9c
	if (!cr6.eq) goto loc_824BCA9C;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x824bcd44
	goto loc_824BCD44;
loc_824BCA9C:
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// bne cr6,0x824bcaac
	if (!cr6.eq) goto loc_824BCAAC;
	// li r3,2
	ctx.r3.s64 = 2;
	// b 0x824bcd44
	goto loc_824BCD44;
loc_824BCAAC:
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824bcabc
	if (cr6.eq) goto loc_824BCABC;
loc_824BCAB4:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824bcd44
	goto loc_824BCD44;
loc_824BCABC:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// rlwinm r25,r10,14,31,31
	r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 14) & 0x1;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r10,r31,-12
	ctx.r10.s64 = r31.s64 + -12;
	// lwz r11,12(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 12);
	// lwzx r31,r3,r10
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r10.u32);
loc_824BCAE4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824bcb00
	if (cr6.eq) goto loc_824BCB00;
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// bne cr6,0x824bcb04
	if (!cr6.eq) goto loc_824BCB04;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824bcae4
	goto loc_824BCAE4;
loc_824BCB00:
	// li r30,0
	r30.s64 = 0;
loc_824BCB04:
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// li r28,1
	r28.s64 = 1;
	// lwz r11,40(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// clrlwi r27,r10,13
	r27.u64 = ctx.r10.u32 & 0x7FFFF;
	// clrlwi r9,r27,27
	ctx.r9.u64 = r27.u32 & 0x1F;
	// rlwinm r10,r27,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 27) & 0x7FFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// slw r9,r28,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r9.u8 & 0x3F));
	// and r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ctx.r10.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bcb58
	if (cr0.eq) goto loc_824BCB58;
	// stb r25,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r25.u8);
	// li r3,5
	ctx.r3.s64 = 5;
	// stw r31,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r31.u32);
	// stw r30,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r30.u32);
	// b 0x824bcd44
	goto loc_824BCD44;
loc_824BCB58:
	// lwz r10,48(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// clrlwi r29,r10,13
	r29.u64 = ctx.r10.u32 & 0x7FFFF;
	// rlwinm r10,r29,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r9,r29,27
	ctx.r9.u64 = r29.u32 & 0x1F;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// slw r10,r28,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bcbb0
	if (cr0.eq) goto loc_824BCBB0;
	// li r3,5
	ctx.r3.s64 = 5;
loc_824BCB94:
	// clrlwi r11,r25,24
	r11.u64 = r25.u32 & 0xFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stb r11,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r11.u8);
	// stw r30,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r30.u32);
	// stw r31,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r31.u32);
	// b 0x824bcd44
	goto loc_824BCD44;
loc_824BCBB0:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824baa70
	sub_824BAA70(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bcbd8
	if (!cr0.eq) goto loc_824BCBD8;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824baa70
	sub_824BAA70(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bcab4
	if (cr0.eq) goto loc_824BCAB4;
loc_824BCBD8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_824BCBDC:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824bcc24
	if (cr6.eq) goto loc_824BCC24;
	// clrlwi r8,r29,27
	ctx.r8.u64 = r29.u32 & 0x1F;
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// rlwinm r9,r29,27,5,31
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 27) & 0x7FFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r7,40(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r7.u32);
	// slw r8,r28,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r8.u8 & 0x3F));
	// and r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ctx.r8.u64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne 0x824bcc94
	if (!cr0.eq) goto loc_824BCC94;
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// b 0x824bcbdc
	goto loc_824BCBDC;
loc_824BCC24:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
loc_824BCC28:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824bcc70
	if (cr6.eq) goto loc_824BCC70;
	// clrlwi r8,r27,27
	ctx.r8.u64 = r27.u32 & 0x1F;
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// rlwinm r9,r27,27,5,31
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 27) & 0x7FFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r7,40(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r7.u32);
	// slw r8,r28,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r28.u32 << (ctx.r8.u8 & 0x3F));
	// and r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ctx.r8.u64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne 0x824bcc78
	if (!cr0.eq) goto loc_824BCC78;
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// b 0x824bcc28
	goto loc_824BCC28;
loc_824BCC70:
	// li r11,0
	r11.s64 = 0;
	// b 0x824bcc94
	goto loc_824BCC94;
loc_824BCC78:
	// clrlwi r9,r25,24
	ctx.r9.u64 = r25.u32 & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// mr r31,r30
	r31.u64 = r30.u64;
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
	// clrlwi r25,r9,24
	r25.u64 = ctx.r9.u32 & 0xFF;
loc_824BCC94:
	// stb r25,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r25.u8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r31,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r31.u32);
	// stw r30,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r30.u32);
	// bne cr6,0x824bcd40
	if (!cr6.eq) goto loc_824BCD40;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824baa70
	sub_824BAA70(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bcd40
	if (cr0.eq) goto loc_824BCD40;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824baa70
	sub_824BAA70(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bcd40
	if (cr0.eq) goto loc_824BCD40;
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,81
	ctx.r6.s64 = ctx.r1.s64 + 81;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824bb278
	sub_824BB278(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bccfc
	if (!cr0.eq) goto loc_824BCCFC;
loc_824BCCF4:
	// li r3,4
	ctx.r3.s64 = 4;
	// b 0x824bcd44
	goto loc_824BCD44;
loc_824BCCFC:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824bcd40
	if (cr6.eq) goto loc_824BCD40;
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// addi r7,r1,81
	ctx.r7.s64 = ctx.r1.s64 + 81;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824bb278
	sub_824BB278(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bccf4
	if (cr0.eq) goto loc_824BCCF4;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bccf4
	if (!cr6.eq) goto loc_824BCCF4;
	// li r3,3
	ctx.r3.s64 = 3;
	// b 0x824bcb94
	goto loc_824BCB94;
loc_824BCD40:
	// li r3,3
	ctx.r3.s64 = 3;
loc_824BCD44:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_824BCD4C"))) PPC_WEAK_FUNC(sub_824BCD4C);
PPC_FUNC_IMPL(__imp__sub_824BCD4C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BCD50"))) PPC_WEAK_FUNC(sub_824BCD50);
PPC_FUNC_IMPL(__imp__sub_824BCD50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc4
	// stfd f30,-160(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -160, f30.u64);
	// stfd f31,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, f31.u64);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r19,r4
	r19.u64 = ctx.r4.u64;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r11,36(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bcdb8
	if (!cr0.eq) goto loc_824BCDB8;
	// mr. r17,r11
	r17.u64 = r11.u64;
	cr0.compare<int32_t>(r17.s32, 0, xer);
	// beq 0x824bcdb8
	if (cr0.eq) goto loc_824BCDB8;
loc_824BCD80:
	// lwz r11,8(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10496
	cr6.compare<uint32_t>(r11.u32, 10496, xer);
	// beq cr6,0x824bcdb0
	if (cr6.eq) goto loc_824BCDB0;
	// rlwinm r11,r17,0,0,30
	r11.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bcdb8
	if (!cr0.eq) goto loc_824BCDB8;
	// mr r17,r11
	r17.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bcd80
	if (!cr6.eq) goto loc_824BCD80;
	// b 0x824bcdb8
	goto loc_824BCDB8;
loc_824BCDB0:
	// cmplwi cr6,r17,0
	cr6.compare<uint32_t>(r17.u32, 0, xer);
	// bne cr6,0x824bcdc4
	if (!cr6.eq) goto loc_824BCDC4;
loc_824BCDB8:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BCDC4:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lwz r15,8(r19)
	r15.u64 = PPC_LOAD_U32(r19.u32 + 8);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f30,2552(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	f30.f64 = double(temp.f32);
	// lfs f31,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f31.f64 = double(temp.f32);
loc_824BCDD8:
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// beq cr6,0x824bd1c4
	if (cr6.eq) goto loc_824BD1C4;
	// lwz r20,4(r15)
	r20.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// addi r16,r20,32
	r16.s64 = r20.s64 + 32;
loc_824BCDE8:
	// lwz r11,4(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 4);
	// li r21,0
	r21.s64 = 0;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bcdfc
	if (!cr0.eq) goto loc_824BCDFC;
loc_824BCDF8:
	// mr r21,r11
	r21.u64 = r11.u64;
loc_824BCDFC:
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x824bd1b0
	if (cr6.eq) goto loc_824BD1B0;
	// lwz r11,8(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,14848
	cr6.compare<uint32_t>(r11.u32, 14848, xer);
	// bne cr6,0x824bd180
	if (!cr6.eq) goto loc_824BD180;
	// mr r18,r21
	r18.u64 = r21.u64;
loc_824BCE18:
	// lwz r22,0(r18)
	r22.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// cmplwi r22,0
	cr0.compare<uint32_t>(r22.u32, 0, xer);
	// beq 0x824bd174
	if (cr0.eq) goto loc_824BD174;
	// lwz r8,0(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// rlwinm r11,r8,0,4,6
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bd160
	if (cr0.eq) goto loc_824BD160;
	// lwz r23,12(r22)
	r23.u64 = PPC_LOAD_U32(r22.u32 + 12);
	// lwz r11,8(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 8);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bd160
	if (cr0.eq) goto loc_824BD160;
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// li r11,0
	r11.s64 = 0;
loc_824BCE58:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824bce98
	if (cr6.eq) goto loc_824BCE98;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r7,r10,0,4,6
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	// cntlzw r7,r7
	ctx.r7.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// rlwinm r7,r7,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// xori r7,r7,1
	ctx.r7.u64 = ctx.r7.u64 ^ 1;
	// cmplwi r7,0
	cr0.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq 0x824bce90
	if (cr0.eq) goto loc_824BCE90;
	// xor r7,r10,r8
	ctx.r7.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// rlwinm. r7,r7,0,7,14
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x1FE0000;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x824bce90
	if (!cr0.eq) goto loc_824BCE90;
	// rlwinm r10,r10,19,28,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 19) & 0xF;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
loc_824BCE90:
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x824bce58
	goto loc_824BCE58;
loc_824BCE98:
	// rlwimi r11,r8,19,20,27
	r11.u64 = (__builtin_rotateleft32(ctx.r8.u32, 19) & 0xFF0) | (r11.u64 & 0xFFFFFFFFFFFFF00F);
	// lwz r31,0(r17)
	r31.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// clrlwi r27,r11,20
	r27.u64 = r11.u32 & 0xFFF;
loc_824BCEA4:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824bcef0
	if (cr6.eq) goto loc_824BCEF0;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r10,r11,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bcee8
	if (cr0.eq) goto loc_824BCEE8;
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bcee8
	if (cr0.eq) goto loc_824BCEE8;
	// rlwinm r11,r11,19,20,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 19) & 0xFF0;
	// rlwinm r10,r27,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFF0;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x824bcef0
	if (cr6.eq) goto loc_824BCEF0;
loc_824BCEE8:
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x824bcea4
	goto loc_824BCEA4;
loc_824BCEF0:
	// lis r10,-28311
	ctx.r10.s64 = -1855389696;
	// lis r9,0
	ctx.r9.s64 = 0;
	// ori r10,r10,5192
	ctx.r10.u64 = ctx.r10.u64 | 5192;
	// ori r9,r9,36262
	ctx.r9.u64 = ctx.r9.u64 | 36262;
	// clrldi r11,r27,60
	r11.u64 = r27.u64 & 0xF;
	// rldimi r10,r9,32,0
	ctx.r10.u64 = (__builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r10.u64 & 0xFFFFFFFF);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// srd r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// srd r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// clrlwi r30,r11,29
	r30.u64 = r11.u32 & 0x7;
	// bne cr6,0x824bcfe4
	if (!cr6.eq) goto loc_824BCFE4;
	// addi r28,r19,24
	r28.s64 = r19.s64 + 24;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// bl 0x825536c8
	sub_825536C8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824bcf7c
	if (cr6.eq) goto loc_824BCF7C;
	// addi r29,r31,40
	r29.s64 = r31.s64 + 40;
loc_824BCF48:
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82553608
	sub_82553608(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// bne 0x824bcf48
	if (!cr0.eq) goto loc_824BCF48;
loc_824BCF7C:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// bl 0x824bc450
	sub_824BC450(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// ori r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 | 32;
	// lis r11,16384
	r11.s64 = 1073741824;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// rlwimi r11,r27,13,7,18
	r11.u64 = (__builtin_rotateleft32(r27.u32, 13) & 0x1FFE000) | (r11.u64 & 0xFFFFFFFFFE001FFF);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r10,r10,0,19,6
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFE001FFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
loc_824BCFE4:
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r11,8(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 8);
	// lwz r9,0(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// rlwinm r31,r9,19,28,31
	r31.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 19) & 0xF;
	// lwz r10,40(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// lwz r24,12(r10)
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// bne cr6,0x824bd0bc
	if (!cr6.eq) goto loc_824BD0BC;
	// lwz r28,40(r23)
	r28.u64 = PPC_LOAD_U32(r23.u32 + 40);
	// li r29,0
	r29.s64 = 0;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// rlwinm. r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bd144
	if (cr0.eq) goto loc_824BD144;
loc_824BD01C:
	// addi r11,r31,-1
	r11.s64 = r31.s64 + -1;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// andc r11,r31,r11
	r11.u64 = r31.u64 & ~r11.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// subf r31,r11,r31
	r31.s64 = r31.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// subfic r26,r11,31
	xer.ca = r11.u32 <= 31;
	r26.s64 = 31 - r11.s64;
	// bl 0x8251ec48
	sub_8251EC48(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmplw cr6,r11,r20
	cr6.compare<uint32_t>(r11.u32, r20.u32, xer);
	// beq cr6,0x824bd07c
	if (cr6.eq) goto loc_824BD07C;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82553608
	sub_82553608(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_824BD07C:
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rlwinm r11,r11,13,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// bge cr6,0x824bd1b8
	if (!cr6.lt) goto loc_824BD1B8;
	// addi r11,r26,10
	r11.s64 = r26.s64 + 10;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r24
	r11.u64 = PPC_LOAD_U32(r11.u32 + r24.u32);
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// rlwinm r11,r11,7,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x7;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// blt cr6,0x824bd01c
	if (cr6.lt) goto loc_824BD01C;
	// b 0x824bd144
	goto loc_824BD144;
loc_824BD0BC:
	// cmplwi cr6,r11,120
	cr6.compare<uint32_t>(r11.u32, 120, xer);
	// bne cr6,0x824bd0cc
	if (!cr6.eq) goto loc_824BD0CC;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// b 0x824bd0d0
	goto loc_824BD0D0;
loc_824BD0CC:
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
loc_824BD0D0:
	// li r4,1
	ctx.r4.s64 = 1;
	// fmr f4,f31
	ctx.fpscr.disableFlushMode();
	ctx.f4.f64 = f31.f64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// fmr f3,f31
	ctx.f3.f64 = f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// bl 0x82557350
	sub_82557350(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82553608
	sub_82553608(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// cntlzw r11,r31
	r11.u64 = r31.u32 == 0 ? 32 : __builtin_clz(r31.u32);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// subfic r11,r11,41
	xer.ca = r11.u32 <= 41;
	r11.s64 = 41 - r11.s64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r24
	r11.u64 = PPC_LOAD_U32(r11.u32 + r24.u32);
	// lwz r31,12(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
loc_824BD144:
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824bb028
	sub_824BB028(ctx, base);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8255a3c0
	sub_8255A3C0(ctx, base);
loc_824BD160:
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// cmplw cr6,r11,r22
	cr6.compare<uint32_t>(r11.u32, r22.u32, xer);
	// bne cr6,0x824bce18
	if (!cr6.eq) goto loc_824BCE18;
	// addi r18,r22,4
	r18.s64 = r22.s64 + 4;
	// b 0x824bce18
	goto loc_824BCE18;
loc_824BD174:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x824bd198
	if (cr6.eq) goto loc_824BD198;
loc_824BD180:
	// rlwinm r11,r21,0,0,30
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bcdf8
	if (cr0.eq) goto loc_824BCDF8;
	// li r21,0
	r21.s64 = 0;
	// b 0x824bcdfc
	goto loc_824BCDFC;
loc_824BD198:
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// b 0x824bcde8
	goto loc_824BCDE8;
loc_824BD1B0:
	// lwz r15,12(r15)
	r15.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// b 0x824bcdd8
	goto loc_824BCDD8;
loc_824BD1B8:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BD1C4:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f30,-160(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// lfd f31,-152(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// b 0x8239bd14
	return;
}

__attribute__((alias("__imp__sub_824BD1D4"))) PPC_WEAK_FUNC(sub_824BD1D4);
PPC_FUNC_IMPL(__imp__sub_824BD1D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BD1D8"))) PPC_WEAK_FUNC(sub_824BD1D8);
PPC_FUNC_IMPL(__imp__sub_824BD1D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x824bd274
	goto loc_824BD274;
loc_824BD1F4:
	// lwz r31,12(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 12);
loc_824BD1F8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824bd25c
	if (cr6.eq) goto loc_824BD25C;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r9,40(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// lwz r11,48(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r8,r11,27
	ctx.r8.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// slw r10,r10,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r8.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bd254
	if (cr0.eq) goto loc_824BD254;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824baa70
	sub_824BAA70(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bd298
	if (cr0.eq) goto loc_824BD298;
loc_824BD254:
	// lwz r31,8(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// b 0x824bd1f8
	goto loc_824BD1F8;
loc_824BD25C:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8255e890
	sub_8255E890(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bd668
	if (cr0.eq) goto loc_824BD668;
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
loc_824BD274:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bd290
	if (!cr0.eq) goto loc_824BD290;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r28,r11
	r28.u64 = r11.u64;
	// bne cr6,0x824bd1f4
	if (!cr6.eq) goto loc_824BD1F4;
	// b 0x824bd668
	goto loc_824BD668;
loc_824BD290:
	// li r28,0
	r28.s64 = 0;
	// b 0x824bd668
	goto loc_824BD668;
loc_824BD298:
	// lwz r11,28(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r11,r28,32
	r11.s64 = r28.s64 + 32;
	// bne 0x824bd2ac
	if (!cr0.eq) goto loc_824BD2AC;
	// addi r11,r28,24
	r11.s64 = r28.s64 + 24;
loc_824BD2AC:
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r31,31
	ctx.r10.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bd2bc
	if (cr0.eq) goto loc_824BD2BC;
	// li r31,0
	r31.s64 = 0;
loc_824BD2BC:
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824bd668
	if (cr6.eq) goto loc_824BD668;
loc_824BD2C8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b0570
	sub_824B0570(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bd2e8
	if (cr0.eq) goto loc_824BD2E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a90a0
	sub_824A90A0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bd328
	if (cr0.eq) goto loc_824BD328;
loc_824BD2E8:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bd300
	if (!cr0.eq) goto loc_824BD300;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bd2c8
	if (!cr6.eq) goto loc_824BD2C8;
loc_824BD300:
	// addi r11,r28,32
	r11.s64 = r28.s64 + 32;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x824bd668
	if (cr6.eq) goto loc_824BD668;
	// mr r30,r11
	r30.u64 = r11.u64;
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bd668
	if (!cr0.eq) goto loc_824BD668;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x824bd2c8
	if (!cr0.eq) goto loc_824BD2C8;
	// b 0x824bd668
	goto loc_824BD668;
loc_824BD328:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824bd668
	if (cr6.eq) goto loc_824BD668;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x824b09b8
	sub_824B09B8(ctx, base);
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r29,r28,12
	r29.s64 = r28.s64 + 12;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_824BD36C:
	// lwz r30,0(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824bd39c
	if (cr6.eq) goto loc_824BD39C;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8255e4c0
	sub_8255E4C0(ctx, base);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bne cr6,0x824bd36c
	if (!cr6.eq) goto loc_824BD36C;
	// addi r29,r30,8
	r29.s64 = r30.s64 + 8;
	// b 0x824bd36c
	goto loc_824BD36C;
loc_824BD39C:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8255e6b0
	sub_8255E6B0(ctx, base);
	// addi r30,r31,32
	r30.s64 = r31.s64 + 32;
	// addi r4,r28,32
	ctx.r4.s64 = r28.s64 + 32;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824bc358
	sub_824BC358(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bd3ec
	if (!cr0.eq) goto loc_824BD3EC;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bd3ec
	if (cr0.eq) goto loc_824BD3EC;
loc_824BD3D0:
	// rlwinm r10,r11,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r31,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r31.u32);
	// lwz r11,36(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bd3ec
	if (!cr0.eq) goto loc_824BD3EC;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bd3d0
	if (!cr6.eq) goto loc_824BD3D0;
loc_824BD3EC:
	// lwz r30,28(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// li r29,0
	r29.s64 = 0;
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bd4e4
	if (!cr0.eq) goto loc_824BD4E4;
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x824bd4e4
	if (cr0.eq) goto loc_824BD4E4;
loc_824BD404:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824a90a0
	sub_824A90A0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bd470
	if (cr0.eq) goto loc_824BD470;
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r10,r31,24
	ctx.r10.s64 = r31.s64 + 24;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// addi r9,r10,-32
	ctx.r9.s64 = ctx.r10.s64 + -32;
	// addi r6,r11,-32
	ctx.r6.s64 = r11.s64 + -32;
	// ori r8,r9,1
	ctx.r8.u64 = ctx.r9.u64 | 1;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r5,r5,0,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r7,32(r5)
	PPC_STORE_U32(ctx.r5.u32 + 32, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r5,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r5.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r31,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r31.u32);
loc_824BD470:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824bd4b8
	if (cr6.eq) goto loc_824BD4B8;
	// rlwinm r11,r29,0,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// bne 0x824bd490
	if (!cr0.eq) goto loc_824BD490;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_824BD490:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x824bd4a8
	if (!cr6.eq) goto loc_824BD4A8;
loc_824BD498:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r29,r30
	r29.u64 = r30.u64;
	// lwz r30,36(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// b 0x824bd4d4
	goto loc_824BD4D4;
loc_824BD4A8:
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bd4e4
	if (!cr0.eq) goto loc_824BD4E4;
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
	// b 0x824bd4dc
	goto loc_824BD4DC;
loc_824BD4B8:
	// lwz r11,28(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bd4c8
	if (cr0.eq) goto loc_824BD4C8;
	// li r11,0
	r11.s64 = 0;
loc_824BD4C8:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x824bd498
	if (cr6.eq) goto loc_824BD498;
	// lwz r30,28(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 28);
loc_824BD4D4:
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bd4e4
	if (!cr0.eq) goto loc_824BD4E4;
loc_824BD4DC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824bd404
	if (!cr6.eq) goto loc_824BD404;
loc_824BD4E4:
	// lwz r11,84(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// lwz r10,584(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 584);
	// lwz r11,580(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 580);
	// rlwinm r10,r10,6,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 6) & 0xFFFFFFC0;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// b 0x824bd5ec
	goto loc_824BD5EC;
loc_824BD500:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bd538
	if (cr0.eq) goto loc_824BD538;
	// lwz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r9,r9,10,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bd538
	if (cr0.eq) goto loc_824BD538;
	// b 0x824bd528
	goto loc_824BD528;
loc_824BD51C:
	// lwz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r9,r9,0,9,9
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bd534
	if (cr0.eq) goto loc_824BD534;
loc_824BD528:
	// lwz r10,80(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x824bd51c
	if (!cr0.eq) goto loc_824BD51C;
loc_824BD534:
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
loc_824BD538:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// bne cr6,0x824bd548
	if (!cr6.eq) goto loc_824BD548;
	// stw r31,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r31.u32);
loc_824BD548:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bd580
	if (cr0.eq) goto loc_824BD580;
	// lwz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r9,r9,10,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bd580
	if (cr0.eq) goto loc_824BD580;
	// b 0x824bd570
	goto loc_824BD570;
loc_824BD564:
	// lwz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r9,r9,0,9,9
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bd57c
	if (cr0.eq) goto loc_824BD57C;
loc_824BD570:
	// lwz r10,80(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x824bd564
	if (!cr0.eq) goto loc_824BD564;
loc_824BD57C:
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
loc_824BD580:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// bne cr6,0x824bd590
	if (!cr6.eq) goto loc_824BD590;
	// stw r31,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r31.u32);
loc_824BD590:
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bd5c8
	if (cr0.eq) goto loc_824BD5C8;
	// lwz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r9,r9,10,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bd5c8
	if (cr0.eq) goto loc_824BD5C8;
	// b 0x824bd5b8
	goto loc_824BD5B8;
loc_824BD5AC:
	// lwz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r9,r9,0,9,9
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bd5c4
	if (cr0.eq) goto loc_824BD5C4;
loc_824BD5B8:
	// lwz r10,80(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x824bd5ac
	if (!cr0.eq) goto loc_824BD5AC;
loc_824BD5C4:
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
loc_824BD5C8:
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// bne cr6,0x824bd5d8
	if (!cr6.eq) goto loc_824BD5D8;
	// stw r31,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r31.u32);
loc_824BD5D8:
	// lwz r10,584(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 584);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// lwz r9,580(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 580);
	// rlwinm r10,r10,6,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 6) & 0xFFFFFFC0;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_824BD5EC:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824bd500
	if (cr6.lt) goto loc_824BD500;
	// lwz r10,48(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 48);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// rlwimi r10,r9,0,9,7
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0xFFFFFFFFFF7FFFFF) | (ctx.r10.u64 & 0x800000);
	// stw r10,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r10.u32);
	// lwz r10,76(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 76);
	// rlwimi r11,r10,0,9,9
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x400000) | (r11.u64 & 0xFFFFFFFFFFBFFFFF);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// stw r11,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r11.u32);
	// lwz r11,76(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 76);
	// rlwimi r10,r11,0,8,8
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0x800000) | (ctx.r10.u64 & 0xFFFFFFFFFF7FFFFF);
	// stw r10,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r10.u32);
	// lwz r10,112(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 112);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// stw r10,112(r31)
	PPC_STORE_U32(r31.u32 + 112, ctx.r10.u32);
	// lwz r10,76(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 76);
	// rlwimi r11,r10,0,7,7
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x1000000) | (r11.u64 & 0xFFFFFFFFFEFFFFFF);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// stw r11,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r11.u32);
	// lwz r11,76(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 76);
	// rlwimi r10,r11,0,6,6
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0x2000000) | (ctx.r10.u64 & 0xFFFFFFFFFDFFFFFF);
	// stw r10,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r10.u32);
	// lwz r11,108(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 108);
	// stw r11,108(r31)
	PPC_STORE_U32(r31.u32 + 108, r11.u32);
	// lwz r11,72(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 72);
	// mr r28,r31
	r28.u64 = r31.u64;
	// stw r11,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r11.u32);
	// bl 0x8251cbe0
	sub_8251CBE0(ctx, base);
loc_824BD668:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_824BD674"))) PPC_WEAK_FUNC(sub_824BD674);
PPC_FUNC_IMPL(__imp__sub_824BD674) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BD678"))) PPC_WEAK_FUNC(sub_824BD678);
PPC_FUNC_IMPL(__imp__sub_824BD678) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// li r8,1
	ctx.r8.s64 = 1;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x8251aa40
	sub_8251AA40(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r8,1
	ctx.r8.s64 = 1;
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ori r11,r11,64
	r11.u64 = r11.u64 | 64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// addi r11,r29,32
	r11.s64 = r29.s64 + 32;
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r3,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r3.u32);
	// addi r7,r11,-32
	ctx.r7.s64 = r11.s64 + -32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// ori r7,r7,1
	ctx.r7.u64 = ctx.r7.u64 | 1;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r5,96(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 96);
	// cmplwi r5,0
	cr0.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq 0x824bd744
	if (cr0.eq) goto loc_824BD744;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824bb088
	sub_824BB088(ctx, base);
loc_824BD744:
	// stw r31,96(r29)
	PPC_STORE_U32(r29.u32 + 96, r31.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// oris r11,r11,512
	r11.u64 = r11.u64 | 33554432;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824BD760"))) PPC_WEAK_FUNC(sub_824BD760);
PPC_FUNC_IMPL(__imp__sub_824BD760) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x824bc950
	sub_824BC950(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r11,1
	r11.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwimi r10,r11,17,13,14
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 17) & 0x60000) | (ctx.r10.u64 & 0xFFFFFFFFFFF9FFFF);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r4,12(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// addi r8,r4,4
	ctx.r8.s64 = ctx.r4.s64 + 4;
	// b 0x824bd7d0
	goto loc_824BD7D0;
loc_824BD7CC:
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
loc_824BD7D0:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x824bd7cc
	if (!cr6.eq) goto loc_824BD7CC;
	// lwz r7,8(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r11,r4,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// rlwinm r9,r31,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r6,r11,-32
	ctx.r6.s64 = r11.s64 + -32;
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r5,7
	ctx.r5.s64 = 7;
	// addi r8,r9,-32
	ctx.r8.s64 = ctx.r9.s64 + -32;
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// stw r31,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, r31.u32);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r30,r30,0,0,30
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r7,32(r30)
	PPC_STORE_U32(r30.u32 + 32, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r30,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r30.u32);
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// bl 0x824b31e8
	sub_824B31E8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824BD86C"))) PPC_WEAK_FUNC(sub_824BD86C);
PPC_FUNC_IMPL(__imp__sub_824BD86C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BD870"))) PPC_WEAK_FUNC(sub_824BD870);
PPC_FUNC_IMPL(__imp__sub_824BD870) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r24,r6
	r24.u64 = ctx.r6.u64;
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r11,12(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 12);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,15232
	cr6.compare<uint32_t>(r11.u32, 15232, xer);
	// beq cr6,0x824bdaa0
	if (cr6.eq) goto loc_824BDAA0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b09b8
	sub_824B09B8(ctx, base);
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// rlwinm r10,r30,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r9,48(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// lwz r11,76(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 76);
	// rlwimi r9,r10,0,8,8
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x800000) | (ctx.r9.u64 & 0xFFFFFFFFFF7FFFFF);
	// stw r9,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r9.u32);
	// lwz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// rlwimi r11,r10,0,9,9
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x400000) | (r11.u64 & 0xFFFFFFFFFFBFFFFF);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// stw r11,76(r30)
	PPC_STORE_U32(r30.u32 + 76, r11.u32);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// rlwimi r10,r11,0,8,8
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0x800000) | (ctx.r10.u64 & 0xFFFFFFFFFF7FFFFF);
	// stw r10,76(r30)
	PPC_STORE_U32(r30.u32 + 76, ctx.r10.u32);
	// lwz r10,112(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// lwz r11,76(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 76);
	// stw r10,112(r30)
	PPC_STORE_U32(r30.u32 + 112, ctx.r10.u32);
	// lwz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// rlwimi r11,r10,0,7,7
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x1000000) | (r11.u64 & 0xFFFFFFFFFEFFFFFF);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// stw r11,76(r30)
	PPC_STORE_U32(r30.u32 + 76, r11.u32);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// rlwimi r10,r11,0,6,6
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0x2000000) | (ctx.r10.u64 & 0xFFFFFFFFFDFFFFFF);
	// stw r10,76(r30)
	PPC_STORE_U32(r30.u32 + 76, ctx.r10.u32);
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// stw r11,108(r30)
	PPC_STORE_U32(r30.u32 + 108, r11.u32);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// stw r11,72(r30)
	PPC_STORE_U32(r30.u32 + 72, r11.u32);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bd960
	if (cr0.eq) goto loc_824BD960;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_824BD960:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8255e6b0
	sub_8255E6B0(ctx, base);
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwimi r11,r10,0,4,4
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x8000000) | (r11.u64 & 0xFFFFFFFFF7FFFFFF);
	// stw r11,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r11.u32);
	// lwz r4,12(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// bl 0x8255e500
	sub_8255E500(ctx, base);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// oris r11,r11,2048
	r11.u64 = r11.u64 | 134217728;
	// li r6,119
	ctx.r6.s64 = 119;
	// addi r5,r30,24
	ctx.r5.s64 = r30.s64 + 24;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// rlwinm r11,r27,0,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// addi r28,r11,32
	r28.s64 = r11.s64 + 32;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// ori r11,r11,64
	r11.u64 = r11.u64 | 64;
	// stw r11,8(r26)
	PPC_STORE_U32(r26.u32 + 8, r11.u32);
	// lwz r10,4(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,32(r10)
	PPC_STORE_U32(ctx.r10.u32 + 32, r11.u32);
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// lwz r10,4(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x8255e770
	sub_8255E770(ctx, base);
	// addi r11,r30,32
	r11.s64 = r30.s64 + 32;
	// addi r7,r28,-32
	ctx.r7.s64 = r28.s64 + -32;
	// addi r8,r11,-32
	ctx.r8.s64 = r11.s64 + -32;
	// addi r10,r28,4
	ctx.r10.s64 = r28.s64 + 4;
	// ori r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 | 1;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r9,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r9.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r5,96(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 96);
	// cmplwi r5,0
	cr0.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq 0x824bda4c
	if (cr0.eq) goto loc_824BDA4C;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x824bb088
	sub_824BB088(ctx, base);
loc_824BDA4C:
	// stw r27,96(r30)
	PPC_STORE_U32(r30.u32 + 96, r27.u32);
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// stw r30,24(r27)
	PPC_STORE_U32(r27.u32 + 24, r30.u32);
	// oris r11,r11,512
	r11.u64 = r11.u64 | 33554432;
	// stw r11,8(r27)
	PPC_STORE_U32(r27.u32 + 8, r11.u32);
	// lwz r11,12(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 12);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// b 0x824bda70
	goto loc_824BDA70;
loc_824BDA6C:
	// addi r11,r10,8
	r11.s64 = ctx.r10.s64 + 8;
loc_824BDA70:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r10,r25
	cr6.compare<uint32_t>(ctx.r10.u32, r25.u32, xer);
	// bne cr6,0x824bda6c
	if (!cr6.eq) goto loc_824BDA6C;
	// lwz r10,8(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// stw r11,8(r25)
	PPC_STORE_U32(r25.u32 + 8, r11.u32);
	// stw r25,4(r26)
	PPC_STORE_U32(r26.u32 + 4, r25.u32);
	// stw r26,12(r25)
	PPC_STORE_U32(r25.u32 + 12, r26.u32);
	// bl 0x8251cbe0
	sub_8251CBE0(ctx, base);
	// mr r31,r30
	r31.u64 = r30.u64;
loc_824BDAA0:
	// addi r28,r31,24
	r28.s64 = r31.s64 + 24;
	// li r8,1
	ctx.r8.s64 = 1;
	// li r7,2
	ctx.r7.s64 = 2;
	// li r6,107
	ctx.r6.s64 = 107;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lfs f4,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f4.f64 = double(temp.f32);
	// lis r11,-32253
	r11.s64 = -2113732608;
	// fmr f3,f4
	ctx.f3.f64 = ctx.f4.f64;
	// fmr f2,f4
	ctx.f2.f64 = ctx.f4.f64;
	// lfs f1,-11132(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -11132);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82557350
	sub_82557350(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r11.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// lwz r10,8(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// rlwimi r10,r11,0,14,12
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFFFFFFBFFFF) | (ctx.r10.u64 & 0x40000);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x824ba888
	sub_824BA888(ctx, base);
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,116
	ctx.r6.s64 = 116;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// oris r11,r11,512
	r11.u64 = r11.u64 | 33554432;
	// stw r11,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r11.u32);
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_824BDBB8"))) PPC_WEAK_FUNC(sub_824BDBB8);
PPC_FUNC_IMPL(__imp__sub_824BDBB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r23,r7
	r23.u64 = ctx.r7.u64;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
	// lwz r11,48(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// li r22,1
	r22.s64 = 1;
	// lwz r10,40(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 40);
	// mr r29,r26
	r29.u64 = r26.u64;
	// b 0x824bdbf4
	goto loc_824BDBF4;
loc_824BDBEC:
	// lwz r29,52(r29)
	r29.u64 = PPC_LOAD_U32(r29.u32 + 52);
	// lwz r11,48(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 48);
loc_824BDBF4:
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r9,r22,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r9,r11
	r11.u64 = ctx.r9.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bdbec
	if (cr0.eq) goto loc_824BDBEC;
	// lwz r9,8(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 8);
loc_824BDC2C:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824bdd18
	if (cr6.eq) goto loc_824BDD18;
	// lwz r11,48(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 48);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r7,r11,27
	ctx.r7.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// lwz r8,40(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 40);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r8
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// slw r8,r22,r7
	ctx.r8.u64 = ctx.r7.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r7.u8 & 0x3F));
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bdc7c
	if (!cr0.eq) goto loc_824BDC7C;
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// b 0x824bdc2c
	goto loc_824BDC2C;
loc_824BDC7C:
	// lwz r11,48(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r22,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bdd18
	if (!cr0.eq) goto loc_824BDD18;
	// rlwinm r11,r29,0,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bdd0c
	if (!cr0.eq) goto loc_824BDD0C;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r29,r11,-4
	xer.ca = r11.u32 > 3;
	r29.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// beq 0x824bdd0c
	if (cr0.eq) goto loc_824BDD0C;
	// lwz r11,36(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bdd0c
	if (!cr0.eq) goto loc_824BDD0C;
	// lwz r11,36(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bdcf0
	if (cr0.eq) goto loc_824BDCF0;
	// li r11,0
	r11.s64 = 0;
	// b 0x824bdcfc
	goto loc_824BDCFC;
loc_824BDCF0:
	// lwz r11,32(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,-36
	r11.s64 = r11.s64 + -36;
loc_824BDCFC:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10624
	cr6.compare<uint32_t>(r11.u32, 10624, xer);
	// beq cr6,0x824bdd18
	if (cr6.eq) goto loc_824BDD18;
loc_824BDD0C:
	// li r4,3535
	ctx.r4.s64 = 3535;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BDD18:
	// lwz r11,36(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bdecc
	if (!cr0.eq) goto loc_824BDECC;
	// mr. r27,r11
	r27.u64 = r11.u64;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// beq 0x824bdecc
	if (cr0.eq) goto loc_824BDECC;
loc_824BDD2C:
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,8320
	cr6.compare<uint32_t>(r11.u32, 8320, xer);
	// bne cr6,0x824bdea4
	if (!cr6.eq) goto loc_824BDEA4;
	// lwz r31,28(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bdea4
	if (!cr0.eq) goto loc_824BDEA4;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824bdea4
	if (cr0.eq) goto loc_824BDEA4;
loc_824BDD50:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r10,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,108
	cr6.compare<uint32_t>(r11.u32, 108, xer);
	// beq cr6,0x824bdd70
	if (cr6.eq) goto loc_824BDD70;
	// cmplwi cr6,r11,65
	cr6.compare<uint32_t>(r11.u32, 65, xer);
	// bne cr6,0x824bde80
	if (!cr6.eq) goto loc_824BDE80;
	// rlwinm. r11,r10,0,0,0
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bde80
	if (cr0.eq) goto loc_824BDE80;
loc_824BDD70:
	// lwz r4,40(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r3,40(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 40);
	// bl 0x824a91e8
	sub_824A91E8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bde80
	if (cr0.eq) goto loc_824BDE80;
	// rlwinm r11,r26,0,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xFFFFFFFE;
	// li r30,0
	r30.s64 = 0;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bde80
	if (!cr0.eq) goto loc_824BDE80;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824bde80
	if (cr6.eq) goto loc_824BDE80;
loc_824BDDA4:
	// lwz r11,40(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// lwz r10,48(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi r8,r10,27
	ctx.r8.u64 = ctx.r10.u32 & 0x1F;
	// rlwinm r10,r10,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r9,r9,13
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFF;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,31
	ctx.r9.s64 = ctx.r9.s64 + 31;
	// rlwinm r10,r9,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFC;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// slw r10,r22,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r8.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bde78
	if (cr0.eq) goto loc_824BDE78;
	// lwz r30,36(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bde08
	if (cr0.eq) goto loc_824BDE08;
loc_824BDE00:
	// li r30,0
	r30.s64 = 0;
	// b 0x824bde5c
	goto loc_824BDE5C;
loc_824BDE08:
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x824bde5c
	if (cr0.eq) goto loc_824BDE5C;
loc_824BDE10:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,8192
	cr6.compare<uint32_t>(r11.u32, 8192, xer);
	// bne cr6,0x824bde34
	if (!cr6.eq) goto loc_824BDE34;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824ba210
	sub_824BA210(ctx, base);
	// cmplw cr6,r3,r31
	cr6.compare<uint32_t>(ctx.r3.u32, r31.u32, xer);
	// beq cr6,0x824bde54
	if (cr6.eq) goto loc_824BDE54;
loc_824BDE34:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bde00
	if (!cr0.eq) goto loc_824BDE00;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bde10
	if (!cr6.eq) goto loc_824BDE10;
	// b 0x824bde5c
	goto loc_824BDE5C;
loc_824BDE54:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824bde9c
	if (!cr6.eq) goto loc_824BDE9C;
loc_824BDE5C:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bde78
	if (!cr0.eq) goto loc_824BDE78;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bdda4
	if (!cr6.eq) goto loc_824BDDA4;
loc_824BDE78:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824bde9c
	if (!cr6.eq) goto loc_824BDE9C;
loc_824BDE80:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bdea4
	if (!cr0.eq) goto loc_824BDEA4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bdd50
	if (!cr6.eq) goto loc_824BDD50;
	// b 0x824bdea4
	goto loc_824BDEA4;
loc_824BDE9C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bdec4
	if (!cr6.eq) goto loc_824BDEC4;
loc_824BDEA4:
	// rlwinm r11,r27,0,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bdecc
	if (!cr0.eq) goto loc_824BDECC;
	// mr r27,r11
	r27.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bdd2c
	if (!cr6.eq) goto loc_824BDD2C;
	// b 0x824bdecc
	goto loc_824BDECC;
loc_824BDEC4:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x824be050
	if (!cr6.eq) goto loc_824BE050;
loc_824BDECC:
	// lwz r11,24(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 24);
	// mr r31,r24
	r31.u64 = r24.u64;
	// b 0x824bdf00
	goto loc_824BDF00;
loc_824BDED8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// beq 0x824bdf58
	if (cr0.eq) goto loc_824BDF58;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x824ba210
	sub_824BA210(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824bdf60
	if (cr0.eq) goto loc_824BDF60;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
loc_824BDF00:
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r10,40(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r22,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bded8
	if (cr0.eq) goto loc_824BDED8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bne 0x824bdf6c
	if (!cr0.eq) goto loc_824BDF6C;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BDF58:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BDF60:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BDF6C:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x824ba210
	sub_824BA210(ctx, base);
	// mr. r28,r3
	r28.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bne 0x824bdf88
	if (!cr0.eq) goto loc_824BDF88;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BDF88:
	// addi r31,r29,24
	r31.s64 = r29.s64 + 24;
	// li r8,1
	ctx.r8.s64 = 1;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,119
	ctx.r6.s64 = 119;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r8,1
	ctx.r8.s64 = 1;
	// li r7,2
	ctx.r7.s64 = 2;
	// li r6,108
	ctx.r6.s64 = 108;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r23,0,0,30
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r3,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r3.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824be030
	if (cr0.eq) goto loc_824BE030;
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x824be038
	goto loc_824BE038;
loc_824BE030:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r6,r11,-4
	ctx.r6.s64 = r11.s64 + -4;
loc_824BE038:
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// li r7,65
	ctx.r7.s64 = 65;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824bd678
	sub_824BD678(ctx, base);
loc_824BE050:
	// addi r29,r21,32
	r29.s64 = r21.s64 + 32;
	// lwz r30,4(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be0b0
	if (!cr0.eq) goto loc_824BE0B0;
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x824be0b0
	if (cr0.eq) goto loc_824BE0B0;
loc_824BE068:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,8192
	cr6.compare<uint32_t>(r11.u32, 8192, xer);
	// bne cr6,0x824be08c
	if (!cr6.eq) goto loc_824BE08C;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824ba210
	sub_824BA210(ctx, base);
	// cmplw cr6,r3,r31
	cr6.compare<uint32_t>(ctx.r3.u32, r31.u32, xer);
	// beq cr6,0x824be0a8
	if (cr6.eq) goto loc_824BE0A8;
loc_824BE08C:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r30,36(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be0b0
	if (!cr0.eq) goto loc_824BE0B0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824be068
	if (!cr6.eq) goto loc_824BE068;
	// b 0x824be0b0
	goto loc_824BE0B0;
loc_824BE0A8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824be224
	if (!cr6.eq) goto loc_824BE224;
loc_824BE0B0:
	// li r8,1
	ctx.r8.s64 = 1;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,64
	ctx.r6.s64 = 64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// oris r11,r11,6
	r11.u64 = r11.u64 | 393216;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824be110
	if (cr0.eq) goto loc_824BE110;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_824BE110:
	// mr r31,r21
	r31.u64 = r21.u64;
loc_824BE114:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824be130
	if (cr0.eq) goto loc_824BE130;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r11,r22
	r11.u64 = r22.u64;
	// bne cr6,0x824be134
	if (!cr6.eq) goto loc_824BE134;
loc_824BE130:
	// li r11,0
	r11.s64 = 0;
loc_824BE134:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be168
	if (!cr0.eq) goto loc_824BE168;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824baf98
	sub_824BAF98(ctx, base);
	// mr. r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq 0x824be168
	if (cr0.eq) goto loc_824BE168;
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10752
	cr6.compare<uint32_t>(r11.u32, 10752, xer);
	// bne cr6,0x824be168
	if (!cr6.eq) goto loc_824BE168;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8255a3c0
	sub_8255A3C0(ctx, base);
loc_824BE168:
	// lwz r31,52(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x824be114
	if (!cr0.eq) goto loc_824BE114;
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r8,r29,-32
	ctx.r8.s64 = r29.s64 + -32;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// ori r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 | 1;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// addi r7,r11,-32
	ctx.r7.s64 = r11.s64 + -32;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// stw r10,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r10.u32);
	// lwz r4,96(r21)
	ctx.r4.u64 = PPC_LOAD_U32(r21.u32 + 96);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne 0x824be1e4
	if (!cr0.eq) goto loc_824BE1E4;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,119
	ctx.r6.s64 = 119;
	// addi r5,r21,24
	ctx.r5.s64 = r21.s64 + 24;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ori r11,r11,64
	r11.u64 = r11.u64 | 64;
	// stw r11,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, r11.u32);
loc_824BE1E4:
	// li r5,7
	ctx.r5.s64 = 7;
	// bl 0x824b31e8
	sub_824B31E8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r5,96(r21)
	ctx.r5.u64 = PPC_LOAD_U32(r21.u32 + 96);
	// cmplwi r5,0
	cr0.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq 0x824be214
	if (cr0.eq) goto loc_824BE214;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824bb088
	sub_824BB088(ctx, base);
loc_824BE214:
	// stw r30,96(r21)
	PPC_STORE_U32(r21.u32 + 96, r30.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// oris r11,r11,512
	r11.u64 = r11.u64 | 33554432;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
loc_824BE224:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_824BE22C"))) PPC_WEAK_FUNC(sub_824BE22C);
PPC_FUNC_IMPL(__imp__sub_824BE22C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BE230"))) PPC_WEAK_FUNC(sub_824BE230);
PPC_FUNC_IMPL(__imp__sub_824BE230) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lhz r4,14(r4)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r4.u32 + 14);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c5da0
	sub_824C5DA0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x824bb930
	sub_824BB930(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824be288
	if (cr0.eq) goto loc_824BE288;
loc_824BE280:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x824be2d8
	goto loc_824BE2D8;
loc_824BE288:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824bb930
	sub_824BB930(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be280
	if (!cr0.eq) goto loc_824BE280;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824bb930
	sub_824BB930(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be280
	if (!cr0.eq) goto loc_824BE280;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824bb930
	sub_824BB930(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r3,r11,1
	ctx.r3.u64 = r11.u64 ^ 1;
loc_824BE2D8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_824BE2E0"))) PPC_WEAK_FUNC(sub_824BE2E0);
PPC_FUNC_IMPL(__imp__sub_824BE2E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82503970
	sub_82503970(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be314
	if (!cr0.eq) goto loc_824BE314;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824be324
	goto loc_824BE324;
loc_824BE314:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824bb9d0
	sub_824BB9D0(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
loc_824BE324:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BE33C"))) PPC_WEAK_FUNC(sub_824BE33C);
PPC_FUNC_IMPL(__imp__sub_824BE33C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BE340"))) PPC_WEAK_FUNC(sub_824BE340);
PPC_FUNC_IMPL(__imp__sub_824BE340) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r28,24(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824be44c
	if (!cr0.eq) goto loc_824BE44C;
loc_824BE374:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r31,r11,-36
	r31.s64 = r11.s64 + -36;
loc_824BE37C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824be424
	if (cr6.eq) goto loc_824BE424;
loc_824BE384:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824be408
	if (cr0.eq) goto loc_824BE408;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824be3c4
	if (cr0.eq) goto loc_824BE3C4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// beq 0x824be45c
	if (cr0.eq) goto loc_824BE45C;
	// lhz r4,14(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 14);
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// cmplw cr6,r3,r27
	cr6.compare<uint32_t>(ctx.r3.u32, r27.u32, xer);
	// beq cr6,0x824be464
	if (cr6.eq) goto loc_824BE464;
loc_824BE3C4:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824ba210
	sub_824BA210(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824be408
	if (cr0.eq) goto loc_824BE408;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824be408
	if (cr0.eq) goto loc_824BE408;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// beq 0x824be470
	if (cr0.eq) goto loc_824BE470;
	// lhz r4,14(r30)
	ctx.r4.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// cmplw cr6,r3,r27
	cr6.compare<uint32_t>(ctx.r3.u32, r27.u32, xer);
	// beq cr6,0x824be464
	if (cr6.eq) goto loc_824BE464;
loc_824BE408:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824be424
	if (!cr0.eq) goto loc_824BE424;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-36
	xer.ca = r11.u32 > 35;
	r31.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x824be384
	if (!cr0.eq) goto loc_824BE384;
loc_824BE424:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824be478
	if (!cr0.eq) goto loc_824BE478;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r28,r11,-4
	xer.ca = r11.u32 > 3;
	r28.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824be478
	if (cr0.eq) goto loc_824BE478;
	// lwz r11,28(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824be454
	if (cr0.eq) goto loc_824BE454;
loc_824BE44C:
	// li r31,0
	r31.s64 = 0;
	// b 0x824be37c
	goto loc_824BE37C;
loc_824BE454:
	// lwz r11,24(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 24);
	// b 0x824be374
	goto loc_824BE374;
loc_824BE45C:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BE464:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_824BE470:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BE478:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
}

__attribute__((alias("__imp__sub_824BE484"))) PPC_WEAK_FUNC(sub_824BE484);
PPC_FUNC_IMPL(__imp__sub_824BE484) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BE488"))) PPC_WEAK_FUNC(sub_824BE488);
PPC_FUNC_IMPL(__imp__sub_824BE488) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r29,r9
	r29.u64 = ctx.r9.u64;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
	// bl 0x824bca60
	sub_824BCA60(ctx, base);
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// stw r7,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r7.u32);
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// beq cr6,0x824be4e8
	if (cr6.eq) goto loc_824BE4E8;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// beq cr6,0x824be4e4
	if (cr6.eq) goto loc_824BE4E4;
loc_824BE4DC:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824be5b0
	goto loc_824BE5B0;
loc_824BE4E4:
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
loc_824BE4E8:
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824be4fc
	if (!cr6.eq) goto loc_824BE4FC;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x824be504
	goto loc_824BE504;
loc_824BE4FC:
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
loc_824BE504:
	// lwz r10,228(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// stw r31,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r31.u32);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824be530
	if (cr6.eq) goto loc_824BE530;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bne cr6,0x824be534
	if (!cr6.eq) goto loc_824BE534;
loc_824BE530:
	// li r10,0
	ctx.r10.s64 = 0;
loc_824BE534:
	// clrlwi. r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824be4dc
	if (!cr0.eq) goto loc_824BE4DC;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// bne cr6,0x824be6cc
	if (!cr6.eq) goto loc_824BE6CC;
	// lwz r10,12(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824be564
	if (cr0.eq) goto loc_824BE564;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x824be568
	if (!cr6.eq) goto loc_824BE568;
loc_824BE564:
	// li r11,0
	r11.s64 = 0;
loc_824BE568:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be4dc
	if (!cr0.eq) goto loc_824BE4DC;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824be580
	if (!cr6.eq) goto loc_824BE580;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x824be588
	goto loc_824BE588;
loc_824BE580:
	// lwz r11,12(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_824BE588:
	// rlwinm r11,r7,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x824be5b8
	if (!cr6.eq) goto loc_824BE5B8;
loc_824BE59C:
	// beq 0x824be5a4
	if (cr0.eq) goto loc_824BE5A4;
	// li r11,0
	r11.s64 = 0;
loc_824BE5A4:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x824be4dc
	if (!cr6.eq) goto loc_824BE4DC;
loc_824BE5AC:
	// li r3,1
	ctx.r3.s64 = 1;
loc_824BE5B0:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd44
	return;
loc_824BE5B8:
	// beq 0x824be5c0
	if (cr0.eq) goto loc_824BE5C0;
	// li r11,0
	r11.s64 = 0;
loc_824BE5C0:
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x824be4dc
	if (!cr6.eq) goto loc_824BE4DC;
	// lwz r10,12(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824be5e4
	if (cr0.eq) goto loc_824BE5E4;
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x824be5e8
	if (!cr6.eq) goto loc_824BE5E8;
loc_824BE5E4:
	// li r11,0
	r11.s64 = 0;
loc_824BE5E8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be4dc
	if (!cr0.eq) goto loc_824BE4DC;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// beq cr6,0x824be600
	if (cr6.eq) goto loc_824BE600;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
loc_824BE600:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x824be4dc
	if (!cr6.eq) goto loc_824BE4DC;
	// rlwinm r11,r8,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824be61c
	if (cr0.eq) goto loc_824BE61C;
	// li r11,0
	r11.s64 = 0;
loc_824BE61C:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x824be4dc
	if (!cr6.eq) goto loc_824BE4DC;
	// lwz r11,28(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824be4dc
	if (cr0.eq) goto loc_824BE4DC;
	// lwz r9,36(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 36);
	// clrlwi. r11,r9,31
	r11.u64 = ctx.r9.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be4dc
	if (!cr0.eq) goto loc_824BE4DC;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824be4dc
	if (cr0.eq) goto loc_824BE4DC;
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,7936
	cr6.compare<uint32_t>(r11.u32, 7936, xer);
	// bne cr6,0x824be4dc
	if (!cr6.eq) goto loc_824BE4DC;
	// rlwinm r11,r9,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824be66c
	if (!cr0.eq) goto loc_824BE66C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824be4dc
	if (!cr6.eq) goto loc_824BE4DC;
loc_824BE66C:
	// lwz r11,20(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824be4dc
	if (!cr0.eq) goto loc_824BE4DC;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824be4dc
	if (cr0.eq) goto loc_824BE4DC;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// addi r10,r10,-14208
	ctx.r10.s64 = ctx.r10.s64 + -14208;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824be4dc
	if (cr0.eq) goto loc_824BE4DC;
	// rlwinm r10,r11,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// clrlwi. r7,r10,31
	ctx.r7.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x824be6b0
	if (!cr0.eq) goto loc_824BE6B0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824be4dc
	if (!cr6.eq) goto loc_824BE4DC;
loc_824BE6B0:
	// lwz r10,40(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x824be4dc
	if (!cr6.eq) goto loc_824BE4DC;
	// stw r8,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r8.u32);
	// stw r8,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r8.u32);
	// b 0x824be5ac
	goto loc_824BE5AC;
loc_824BE6CC:
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// bne cr6,0x824be5ac
	if (!cr6.eq) goto loc_824BE5AC;
	// cmplw cr6,r7,r9
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r9.u32, xer);
	// bne cr6,0x824be4dc
	if (!cr6.eq) goto loc_824BE4DC;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// b 0x824be59c
	goto loc_824BE59C;
}

__attribute__((alias("__imp__sub_824BE6EC"))) PPC_WEAK_FUNC(sub_824BE6EC);
PPC_FUNC_IMPL(__imp__sub_824BE6EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BE6F0"))) PPC_WEAK_FUNC(sub_824BE6F0);
PPC_FUNC_IMPL(__imp__sub_824BE6F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824be7fc
	if (!cr0.eq) goto loc_824BE7FC;
	// mr. r26,r11
	r26.u64 = r11.u64;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// beq 0x824be7fc
	if (cr0.eq) goto loc_824BE7FC;
loc_824BE714:
	// lwz r31,28(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 28);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be7e0
	if (!cr0.eq) goto loc_824BE7E0;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824be7e0
	if (cr0.eq) goto loc_824BE7E0;
loc_824BE728:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,108
	cr6.compare<uint32_t>(r11.u32, 108, xer);
	// beq cr6,0x824be740
	if (cr6.eq) goto loc_824BE740;
	// cmplwi cr6,r11,107
	cr6.compare<uint32_t>(r11.u32, 107, xer);
	// bne cr6,0x824be7c8
	if (!cr6.eq) goto loc_824BE7C8;
loc_824BE740:
	// addi r11,r11,-108
	r11.s64 = r11.s64 + -108;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// subfic r9,r11,11
	xer.ca = r11.u32 <= 11;
	ctx.r9.s64 = 11 - r11.s64;
	// subfic r10,r11,1
	xer.ca = r11.u32 <= 1;
	ctx.r10.s64 = 1 - r11.s64;
	// addi r11,r11,10
	r11.s64 = r11.s64 + 10;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// lwzx r28,r11,r31
	r28.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// addi r11,r10,10
	r11.s64 = ctx.r10.s64 + 10;
	// rlwinm r30,r11,2,0,29
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r29,12(r9)
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwzx r4,r30,r31
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + r31.u32);
	// bl 0x824bb028
	sub_824BB028(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stwx r11,r30,r31
	PPC_STORE_U32(r30.u32 + r31.u32, r11.u32);
	// bl 0x824a98e8
	sub_824A98E8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824be7b8
	if (cr0.eq) goto loc_824BE7B8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8255a3c0
	sub_8255A3C0(ctx, base);
loc_824BE7B8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r10,3
	ctx.r10.s64 = 3;
	// rlwimi r11,r10,7,18,24
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 7) & 0x3F80) | (r11.u64 & 0xFFFFFFFFFFFFC07F);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_824BE7C8:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be7e0
	if (!cr0.eq) goto loc_824BE7E0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824be728
	if (!cr6.eq) goto loc_824BE728;
loc_824BE7E0:
	// rlwinm r11,r26,0,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824be7fc
	if (!cr0.eq) goto loc_824BE7FC;
	// mr r26,r11
	r26.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824be714
	if (!cr6.eq) goto loc_824BE714;
loc_824BE7FC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_824BE804"))) PPC_WEAK_FUNC(sub_824BE804);
PPC_FUNC_IMPL(__imp__sub_824BE804) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BE808"))) PPC_WEAK_FUNC(sub_824BE808);
PPC_FUNC_IMPL(__imp__sub_824BE808) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r11,r5,0,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFE0;
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// rlwinm r10,r10,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// cmplwi cr6,r10,32
	cr6.compare<uint32_t>(ctx.r10.u32, 32, xer);
	// beq cr6,0x824be8a0
	if (cr6.eq) goto loc_824BE8A0;
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// bge cr6,0x824be8a0
	if (!cr6.lt) goto loc_824BE8A0;
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// clrlwi r5,r5,27
	ctx.r5.u64 = ctx.r5.u32 & 0x1F;
	// addi r31,r11,-1
	r31.s64 = r11.s64 + -1;
	// clrlwi r7,r4,27
	ctx.r7.u64 = ctx.r4.u32 & 0x1F;
	// rlwinm r30,r4,29,3,29
	r30.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r31,r31,27
	r31.u64 = r31.u32 & 0x1F;
	// clrlwi r8,r11,27
	ctx.r8.u64 = r11.u32 & 0x1F;
	// rlwinm r3,r11,29,3,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// subf r4,r4,r11
	ctx.r4.s64 = r11.s64 - ctx.r4.s64;
	// li r10,-1
	ctx.r10.s64 = -1;
	// slw r11,r6,r5
	r11.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
	// addi r5,r11,-1
	ctx.r5.s64 = r11.s64 + -1;
	// slw r11,r6,r31
	r11.u64 = r31.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r31.u8 & 0x3F));
	// lwzx r6,r3,r9
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r9.u32);
	// lwzx r9,r30,r9
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + ctx.r9.u32);
	// slw r3,r10,r8
	ctx.r3.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r8.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 & ctx.r6.u64;
	// and r11,r11,r9
	r11.u64 = r11.u64 & ctx.r9.u64;
	// and r9,r6,r3
	ctx.r9.u64 = ctx.r6.u64 & ctx.r3.u64;
	// slw r10,r10,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r7.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// srw r10,r9,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r8.u8 & 0x3F));
	// srw r11,r11,r7
	r11.u64 = ctx.r7.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r7.u8 & 0x3F));
	// slw r10,r10,r4
	ctx.r10.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r4.u8 & 0x3F));
	// or r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 | r11.u64;
	// b 0x824be8d4
	goto loc_824BE8D4;
loc_824BE8A0:
	// clrlwi r9,r5,27
	ctx.r9.u64 = ctx.r5.u32 & 0x1F;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r8,2
	ctx.r8.s64 = 2;
	// clrlwi r11,r4,27
	r11.u64 = ctx.r4.u32 & 0x1F;
	// li r7,-1
	ctx.r7.s64 = -1;
	// rlwinm r6,r4,29,3,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 29) & 0x1FFFFFFC;
	// lwzx r6,r6,r10
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r10.u32);
	// slw r10,r8,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r9.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// slw r9,r7,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (r11.u8 & 0x3F));
	// and r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 & ctx.r6.u64;
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// srw r3,r10,r11
	ctx.r3.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
loc_824BE8D4:
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BE8E0"))) PPC_WEAK_FUNC(sub_824BE8E0);
PPC_FUNC_IMPL(__imp__sub_824BE8E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r4,4(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be960
	if (!cr0.eq) goto loc_824BE960;
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x824be960
	if (cr0.eq) goto loc_824BE960;
loc_824BE90C:
	// rlwinm r11,r4,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r31,31
	ctx.r10.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824be920
	if (cr0.eq) goto loc_824BE920;
	// li r31,0
	r31.s64 = 0;
loc_824BE920:
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// mulli r11,r11,28
	r11.s64 = r11.s64 * 28;
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// bl 0x82496e38
	sub_82496E38(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824be90c
	if (!cr6.eq) goto loc_824BE90C;
loc_824BE960:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BE978"))) PPC_WEAK_FUNC(sub_824BE978);
PPC_FUNC_IMPL(__imp__sub_824BE978) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r4,4(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824be9f8
	if (!cr0.eq) goto loc_824BE9F8;
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x824be9f8
	if (cr0.eq) goto loc_824BE9F8;
loc_824BE9A4:
	// rlwinm r11,r4,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r31,31
	ctx.r10.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824be9b8
	if (cr0.eq) goto loc_824BE9B8;
	// li r31,0
	r31.s64 = 0;
loc_824BE9B8:
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82496e38
	sub_82496E38(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824be9a4
	if (!cr6.eq) goto loc_824BE9A4;
loc_824BE9F8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BEA10"))) PPC_WEAK_FUNC(sub_824BEA10);
PPC_FUNC_IMPL(__imp__sub_824BEA10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,10
	cr6.compare<uint32_t>(ctx.r4.u32, 10, xer);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bgt cr6,0x824bea38
	if (cr6.gt) goto loc_824BEA38;
	// li r30,10
	r30.s64 = 10;
loc_824BEA38:
	// mulli r11,r30,28
	r11.s64 = r30.s64 * 28;
	// addi r4,r11,16
	ctx.r4.s64 = r11.s64 + 16;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9e88
	sub_824A9E88(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r30,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r30.u32);
	// rlwinm r11,r3,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFFFFFE;
	// ori r9,r31,1
	ctx.r9.u64 = r31.u64 | 1;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BEA94"))) PPC_WEAK_FUNC(sub_824BEA94);
PPC_FUNC_IMPL(__imp__sub_824BEA94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BEA98"))) PPC_WEAK_FUNC(sub_824BEA98);
PPC_FUNC_IMPL(__imp__sub_824BEA98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,10
	cr6.compare<uint32_t>(ctx.r4.u32, 10, xer);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bgt cr6,0x824beac0
	if (cr6.gt) goto loc_824BEAC0;
	// li r30,10
	r30.s64 = 10;
loc_824BEAC0:
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x824a9e88
	sub_824A9E88(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r30,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r30.u32);
	// rlwinm r11,r3,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFFFFFE;
	// ori r9,r31,1
	ctx.r9.u64 = r31.u64 | 1;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824BEB1C"))) PPC_WEAK_FUNC(sub_824BEB1C);
PPC_FUNC_IMPL(__imp__sub_824BEB1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824BEB20"))) PPC_WEAK_FUNC(sub_824BEB20);
PPC_FUNC_IMPL(__imp__sub_824BEB20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f31.u64);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824beb5c
	if (!cr0.eq) goto loc_824BEB5C;
loc_824BEB54:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824bec14
	goto loc_824BEC14;
loc_824BEB5C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,12(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x82558498
	sub_82558498(ctx, base);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// rlwinm. r10,r11,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bec10
	if (cr0.eq) goto loc_824BEC10;
	// li r31,0
	r31.s64 = 0;
loc_824BEB84:
	// rlwinm r10,r11,27,24,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0xFF;
	// li r8,1
	ctx.r8.s64 = 1;
	// srw r10,r10,r31
	ctx.r10.u64 = r31.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r31.u8 & 0x3F));
	// clrlwi r9,r10,30
	ctx.r9.u64 = ctx.r10.u32 & 0x3;
	// slw r10,r8,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r9.u8 & 0x3F));
	// and. r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 & r28.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824beb54
	if (cr0.eq) goto loc_824BEB54;
	// clrlwi r5,r11,27
	ctx.r5.u64 = r11.u32 & 0x1F;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r9,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// li r8,2
	ctx.r8.s64 = 2;
	// li r7,-1
	ctx.r7.s64 = -1;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lfdx f1,r11,r9
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + ctx.r9.u32);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// clrlwi r11,r10,27
	r11.u64 = ctx.r10.u32 & 0x1F;
	// clrlwi r9,r9,27
	ctx.r9.u64 = ctx.r9.u32 & 0x1F;
	// rlwinm r10,r10,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFC;
	// lwzx r6,r10,r6
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// slw r10,r8,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r9.u8 & 0x3F));
	// slw r9,r7,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (r11.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 & ctx.r6.u64;
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// srw r4,r10,r11
	ctx.r4.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// bl 0x825507b8
	sub_825507B8(ctx, base);
	// fcmpu cr6,f1,f31
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, f31.f64);
	// bne cr6,0x824beb54
	if (!cr6.eq) goto loc_824BEB54;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// rlwinm r10,r11,7,29,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x7;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// blt cr6,0x824beb84
	if (cr6.lt) goto loc_824BEB84;
loc_824BEC10:
	// li r3,1
	ctx.r3.s64 = 1;
loc_824BEC14:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824BEC20"))) PPC_WEAK_FUNC(sub_824BEC20);
PPC_FUNC_IMPL(__imp__sub_824BEC20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
	// lwz r31,4(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// lwz r11,76(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 76);
	// mr r16,r9
	r16.u64 = ctx.r9.u64;
	// li r17,0
	r17.s64 = 0;
	// rlwinm r18,r11,10,31,31
	r18.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	// mr r25,r27
	r25.u64 = r27.u64;
	// li r22,0
	r22.s64 = 0;
	// li r23,0
	r23.s64 = 0;
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824befc8
	if (!cr0.eq) goto loc_824BEFC8;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824befc8
	if (cr0.eq) goto loc_824BEFC8;
	// li r26,1
	r26.s64 = 1;
loc_824BEC78:
	// lwz r11,48(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 48);
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r8,r11,27
	ctx.r8.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// rlwinm r11,r11,29,3,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// slw r8,r26,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r26.u32 << (ctx.r8.u8 & 0x3F));
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bef98
	if (cr0.eq) goto loc_824BEF98;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bed30
	if (!cr0.eq) goto loc_824BED30;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-36
	xer.ca = r11.u32 > 35;
	r11.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bed30
	if (cr0.eq) goto loc_824BED30;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824bed20
	if (cr6.eq) goto loc_824BED20;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824bed20
	if (cr6.eq) goto loc_824BED20;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824bed20
	if (cr6.eq) goto loc_824BED20;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824bed20
	if (cr6.eq) goto loc_824BED20;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824bed20
	if (cr6.eq) goto loc_824BED20;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824bed24
	if (!cr6.eq) goto loc_824BED24;
loc_824BED20:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_824BED24:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bed30
	if (cr0.eq) goto loc_824BED30;
	// mr r17,r26
	r17.u64 = r26.u64;
loc_824BED30:
	// lwz r30,12(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 12);
loc_824BED34:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824bede0
	if (cr6.eq) goto loc_824BEDE0;
	// lwz r11,48(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 48);
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r8,r11,27
	ctx.r8.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// lwz r10,40(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 40);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// rlwinm r11,r11,29,3,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// slw r8,r26,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r26.u32 << (ctx.r8.u8 & 0x3F));
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bedd8
	if (!cr0.eq) goto loc_824BEDD8;
	// lwz r11,48(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 48);
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r26,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r26.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bedd8
	if (!cr0.eq) goto loc_824BEDD8;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824bb4f8
	sub_824BB4F8(ctx, base);
loc_824BEDD8:
	// lwz r30,8(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// b 0x824bed34
	goto loc_824BED34;
loc_824BEDE0:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8255e890
	sub_8255E890(ctx, base);
	// clrlwi. r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824bee58
	if (cr0.eq) goto loc_824BEE58;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r10,r27,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplw cr6,r27,r25
	cr6.compare<uint32_t>(r27.u32, r25.u32, xer);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r6,r6,0,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r7,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r6,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r6.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r9.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// bne cr6,0x824bee4c
	if (!cr6.eq) goto loc_824BEE4C;
	// mr r25,r31
	r25.u64 = r31.u64;
loc_824BEE4C:
	// mr r27,r31
	r27.u64 = r31.u64;
	// stw r21,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r21.u32);
	// b 0x824beeb0
	goto loc_824BEEB0;
loc_824BEE58:
	// clrlwi. r11,r18,24
	r11.u64 = r18.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824beeb0
	if (!cr0.eq) goto loc_824BEEB0;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r10,r25,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// mr r25,r31
	r25.u64 = r31.u64;
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r6,r6,0,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r7,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r6,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r6.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r9.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
loc_824BEEB0:
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x824beec4
	if (!cr6.eq) goto loc_824BEEC4;
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bef80
	if (!cr0.eq) goto loc_824BEF80;
loc_824BEEC4:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r9,48(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 48);
	// rlwinm r4,r9,1,12,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFE;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r5,r4,1
	ctx.r5.s64 = ctx.r4.s64 + 1;
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// addi r9,r10,31
	ctx.r9.s64 = ctx.r10.s64 + 31;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,29,3,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x824be808
	sub_824BE808(ctx, base);
	// cmplwi cr6,r3,3
	cr6.compare<uint32_t>(ctx.r3.u32, 3, xer);
	// beq cr6,0x824bef18
	if (cr6.eq) goto loc_824BEF18;
	// cmplw cr6,r3,r20
	cr6.compare<uint32_t>(ctx.r3.u32, r20.u32, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824bef1c
	if (!cr6.eq) goto loc_824BEF1C;
loc_824BEF18:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_824BEF1C:
	// clrlwi r29,r11,24
	r29.u64 = r11.u32 & 0xFF;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bef5c
	if (!cr0.eq) goto loc_824BEF5C;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824bef5c
	if (cr0.eq) goto loc_824BEF5C;
loc_824BEF34:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba0a8
	sub_824BA0A8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf03c
	if (cr0.eq) goto loc_824BF03C;
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r30,36(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bef5c
	if (!cr0.eq) goto loc_824BEF5C;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824bef34
	if (!cr6.eq) goto loc_824BEF34;
loc_824BEF5C:
	// lwz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwimi r10,r11,23,8,8
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 23) & 0x800000) | (ctx.r10.u64 & 0xFFFFFFFFFF7FFFFF);
	// stw r28,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r28.u32);
	// oris r11,r9,128
	r11.u64 = ctx.r9.u64 | 8388608;
	// oris r10,r10,64
	ctx.r10.u64 = ctx.r10.u64 | 4194304;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// stw r10,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r10.u32);
loc_824BEF80:
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x824bef94
	if (cr6.eq) goto loc_824BEF94;
	// lwz r11,48(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 48);
	// oris r11,r11,2048
	r11.u64 = r11.u64 | 134217728;
	// stw r11,48(r22)
	PPC_STORE_U32(r22.u32 + 48, r11.u32);
loc_824BEF94:
	// mr r22,r31
	r22.u64 = r31.u64;
loc_824BEF98:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x824beff0
	if (cr6.eq) goto loc_824BEFF0;
	// rlwinm r11,r23,0,0,30
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// bne 0x824befb8
	if (!cr0.eq) goto loc_824BEFB8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_824BEFB8:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x824bf00c
	if (cr6.eq) goto loc_824BF00C;
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf02c
	if (cr0.eq) goto loc_824BF02C;
loc_824BEFC8:
	// rlwinm r11,r27,0,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r25,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r25.u32);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824befe0
	if (cr0.eq) goto loc_824BEFE0;
	// li r11,0
	r11.s64 = 0;
loc_824BEFE0:
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// stw r11,0(r16)
	PPC_STORE_U32(r16.u32 + 0, r11.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd18
	return;
loc_824BEFF0:
	// lwz r10,4(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// li r11,0
	r11.s64 = 0;
	// clrlwi. r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824bf004
	if (!cr0.eq) goto loc_824BF004;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_824BF004:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x824bf024
	if (!cr6.eq) goto loc_824BF024;
loc_824BF00C:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r23,r31
	r23.u64 = r31.u64;
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824befc8
	if (!cr0.eq) goto loc_824BEFC8;
	// b 0x824bf030
	goto loc_824BF030;
loc_824BF024:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x824befc8
	if (!cr6.eq) goto loc_824BEFC8;
loc_824BF02C:
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
loc_824BF030:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bec78
	if (!cr6.eq) goto loc_824BEC78;
	// b 0x824befc8
	goto loc_824BEFC8;
loc_824BF03C:
	// li r4,3541
	ctx.r4.s64 = 3541;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
}

__attribute__((alias("__imp__sub_824BF048"))) PPC_WEAK_FUNC(sub_824BF048);
PPC_FUNC_IMPL(__imp__sub_824BF048) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -160, f31.u64);
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r17,r3
	r17.u64 = ctx.r3.u64;
	// lwz r11,40(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 40);
	// rlwinm. r11,r11,0,16,16
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf070
	if (cr0.eq) goto loc_824BF070;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824c0160
	goto loc_824C0160;
loc_824BF070:
	// lwz r11,36(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 36);
	// li r14,0
	r14.s64 = 0;
	// mr r16,r14
	r16.u64 = r14.u64;
	// rlwinm. r11,r11,0,23,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x100;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bfea4
	if (cr0.eq) goto loc_824BFEA4;
	// lwz r11,4(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bf0fc
	if (!cr0.eq) goto loc_824BF0FC;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824bf0fc
	if (cr0.eq) goto loc_824BF0FC;
loc_824BF098:
	// lwz r31,28(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf0e0
	if (!cr0.eq) goto loc_824BF0E0;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824bf0e0
	if (cr0.eq) goto loc_824BF0E0;
loc_824BF0AC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf0c8
	if (cr0.eq) goto loc_824BF0C8;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// clrlwi r11,r11,19
	r11.u64 = r11.u32 & 0x1FFF;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
loc_824BF0C8:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf0e0
	if (!cr0.eq) goto loc_824BF0E0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bf0ac
	if (!cr6.eq) goto loc_824BF0AC;
loc_824BF0E0:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bf0fc
	if (!cr0.eq) goto loc_824BF0FC;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bf098
	if (!cr6.eq) goto loc_824BF098;
loc_824BF0FC:
	// lwz r11,4(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 4);
	// mr r29,r14
	r29.u64 = r14.u64;
	// li r15,15872
	r15.s64 = 15872;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bf348
	if (!cr0.eq) goto loc_824BF348;
	// mr. r28,r11
	r28.u64 = r11.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824bf348
	if (cr0.eq) goto loc_824BF348;
loc_824BF11C:
	// lwz r31,28(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf2e8
	if (!cr0.eq) goto loc_824BF2E8;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824bf2e8
	if (cr0.eq) goto loc_824BF2E8;
loc_824BF130:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf2d0
	if (cr0.eq) goto loc_824BF2D0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// bgt cr6,0x824bf1a0
	if (cr6.gt) goto loc_824BF1A0;
	// beq cr6,0x824bf180
	if (cr6.eq) goto loc_824BF180;
	// cmpwi cr6,r11,21
	cr6.compare<int32_t>(r11.s32, 21, xer);
	// blt cr6,0x824bf1b8
	if (cr6.lt) goto loc_824BF1B8;
	// cmpwi cr6,r11,24
	cr6.compare<int32_t>(r11.s32, 24, xer);
	// ble cr6,0x824bf18c
	if (!cr6.gt) goto loc_824BF18C;
	// cmpwi cr6,r11,62
	cr6.compare<int32_t>(r11.s32, 62, xer);
	// beq cr6,0x824bf180
	if (cr6.eq) goto loc_824BF180;
	// cmpwi cr6,r11,63
	cr6.compare<int32_t>(r11.s32, 63, xer);
	// bne cr6,0x824bf1b8
	if (!cr6.eq) goto loc_824BF1B8;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// oris r11,r11,6144
	r11.u64 = r11.u64 | 402653184;
	// b 0x824bf198
	goto loc_824BF198;
loc_824BF180:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwinm r11,r11,0,5,2
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFE7FFFFFF;
	// b 0x824bf198
	goto loc_824BF198;
loc_824BF18C:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// li r10,1
	ctx.r10.s64 = 1;
	// rlwimi r11,r10,27,3,4
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 27) & 0x18000000) | (r11.u64 & 0xFFFFFFFFE7FFFFFF);
loc_824BF198:
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// b 0x824bf288
	goto loc_824BF288;
loc_824BF1A0:
	// cmpwi cr6,r11,65
	cr6.compare<int32_t>(r11.s32, 65, xer);
	// beq cr6,0x824bf1c4
	if (cr6.eq) goto loc_824BF1C4;
	// cmpwi cr6,r11,107
	cr6.compare<int32_t>(r11.s32, 107, xer);
	// beq cr6,0x824bf180
	if (cr6.eq) goto loc_824BF180;
	// cmpwi cr6,r11,108
	cr6.compare<int32_t>(r11.s32, 108, xer);
	// beq cr6,0x824bf180
	if (cr6.eq) goto loc_824BF180;
loc_824BF1B8:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BF1C4:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// rlwinm r11,r11,0,5,2
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFE7FFFFFF;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// addi r6,r1,116
	ctx.r6.s64 = ctx.r1.s64 + 116;
	// addi r5,r1,108
	ctx.r5.s64 = ctx.r1.s64 + 108;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// bl 0x825576c8
	sub_825576C8(ctx, base);
	// lwz r30,108(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r29,8(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r29,0,18,24
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x3F80;
	// subf r11,r15,r11
	r11.s64 = r11.s64 - r15.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf234
	if (cr0.eq) goto loc_824BF234;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x825509c8
	sub_825509C8(ctx, base);
	// addi r11,r1,100
	r11.s64 = ctx.r1.s64 + 100;
	// fctidz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r29,100(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// b 0x824bf288
	goto loc_824BF288;
loc_824BF234:
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf318
	if (cr0.eq) goto loc_824BF318;
	// rlwinm. r11,r29,9,31,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf260
	if (cr0.eq) goto loc_824BF260;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lhz r5,14(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824be340
	sub_824BE340(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r30,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r30.u32);
loc_824BF260:
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// rlwinm r29,r11,19,18,31
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 19) & 0x3FFF;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// rlwinm. r10,r11,0,3,4
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x18000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bf288
	if (cr0.eq) goto loc_824BF288;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// rlwinm r11,r11,5,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x3;
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// addi r29,r11,2
	r29.s64 = r11.s64 + 2;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
loc_824BF288:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwimi r11,r29,13,5,18
	r11.u64 = (__builtin_rotateleft32(r29.u32, 13) & 0x7FFE000) | (r11.u64 & 0xFFFFFFFFF8001FFF);
	// rlwinm r10,r11,5,30,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x3;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// blt cr6,0x824bf2d0
	if (cr6.lt) goto loc_824BF2D0;
	// beq cr6,0x824bf2c0
	if (cr6.eq) goto loc_824BF2C0;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// bne cr6,0x824bf324
	if (!cr6.eq) goto loc_824BF324;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824bf330
	if (cr6.eq) goto loc_824BF330;
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// b 0x824bf2d0
	goto loc_824BF2D0;
loc_824BF2C0:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmplwi cr6,r29,16383
	cr6.compare<uint32_t>(r29.u32, 16383, xer);
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// bgt cr6,0x824bf33c
	if (cr6.gt) goto loc_824BF33C;
loc_824BF2D0:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf2e8
	if (!cr0.eq) goto loc_824BF2E8;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bf130
	if (!cr6.eq) goto loc_824BF130;
loc_824BF2E8:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bf304
	if (!cr0.eq) goto loc_824BF304;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bf11c
	if (!cr6.eq) goto loc_824BF11C;
loc_824BF304:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824bf348
	if (cr6.eq) goto loc_824BF348;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BF318:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BF324:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BF330:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BF33C:
	// li r4,3535
	ctx.r4.s64 = 3535;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BF348:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f31,-31368(r11)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(r11.u32 + -31368);
loc_824BF350:
	// lwz r11,4(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 4);
	// mr r21,r14
	r21.u64 = r14.u64;
	// li r30,16384
	r30.s64 = 16384;
	// mr r18,r14
	r18.u64 = r14.u64;
	// mr r26,r14
	r26.u64 = r14.u64;
	// li r24,1
	r24.s64 = 1;
	// li r20,1
	r20.s64 = 1;
	// li r19,1
	r19.s64 = 1;
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfac0
	if (!cr0.eq) goto loc_824BFAC0;
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
loc_824BF37C:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r23,r11,-4
	r23.s64 = r11.s64 + -4;
loc_824BF384:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x824bfdc4
	if (cr6.eq) goto loc_824BFDC4;
	// lwz r11,28(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 28);
	// mr r22,r14
	r22.u64 = r14.u64;
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfab0
	if (!cr0.eq) goto loc_824BFAB0;
	// lwz r11,24(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r28,r11,-36
	xer.ca = r11.u32 > 35;
	r28.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824bfab0
	if (cr0.eq) goto loc_824BFAB0;
loc_824BF3AC:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf900
	if (cr0.eq) goto loc_824BF900;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824a90a0
	sub_824A90A0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf3e4
	if (cr0.eq) goto loc_824BF3E4;
	// clrlwi r11,r21,24
	r11.u64 = r21.u32 & 0xFF;
	// lwz r10,16(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwimi r10,r11,26,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 26) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r10,16(r28)
	PPC_STORE_U32(r28.u32 + 16, ctx.r10.u32);
	// b 0x824bf3f0
	goto loc_824BF3F0;
loc_824BF3E4:
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,16(r28)
	PPC_STORE_U32(r28.u32 + 16, r11.u32);
loc_824BF3F0:
	// lwz r10,16(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// rlwinm r29,r10,19,18,31
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 19) & 0x3FFF;
	// rlwinm. r11,r10,5,30,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0x3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf408
	if (cr0.eq) goto loc_824BF408;
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// addi r29,r11,2
	r29.s64 = r11.s64 + 2;
loc_824BF408:
	// cmplw cr6,r29,r30
	cr6.compare<uint32_t>(r29.u32, r30.u32, xer);
	// beq cr6,0x824bf420
	if (cr6.eq) goto loc_824BF420;
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// bne cr6,0x824bf420
	if (!cr6.eq) goto loc_824BF420;
	// oris r11,r10,16384
	r11.u64 = ctx.r10.u64 | 1073741824;
	// stw r11,16(r28)
	PPC_STORE_U32(r28.u32 + 16, r11.u32);
loc_824BF420:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r27,r14
	r27.u64 = r14.u64;
	// mr r25,r14
	r25.u64 = r14.u64;
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,21
	cr6.compare<uint32_t>(r11.u32, 21, xer);
	// stw r27,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r27.u32);
	// stw r25,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r25.u32);
	// blt cr6,0x824bf44c
	if (cr6.lt) goto loc_824BF44C;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824bf450
	if (!cr6.gt) goto loc_824BF450;
loc_824BF44C:
	// mr r11,r14
	r11.u64 = r14.u64;
loc_824BF450:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf5e4
	if (cr0.eq) goto loc_824BF5E4;
	// addi r6,r1,104
	ctx.r6.s64 = ctx.r1.s64 + 104;
	// addi r5,r1,100
	ctx.r5.s64 = ctx.r1.s64 + 100;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824ba318
	sub_824BA318(ctx, base);
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// lwz r25,104(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// rlwinm. r10,r11,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bf49c
	if (cr0.eq) goto loc_824BF49C;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x824bfda0
	if (!cr6.eq) goto loc_824BFDA0;
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x824bfda0
	if (!cr6.eq) goto loc_824BFDA0;
loc_824BF490:
	// oris r11,r11,8192
	r11.u64 = r11.u64 | 536870912;
	// stw r11,16(r28)
	PPC_STORE_U32(r28.u32 + 16, r11.u32);
	// b 0x824bf5e4
	goto loc_824BF5E4;
loc_824BF49C:
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x824bf4cc
	if (!cr6.eq) goto loc_824BF4CC;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x824bf4c4
	if (!cr6.eq) goto loc_824BF4C4;
	// lwz r10,16(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// rlwinm r10,r10,19,18,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 19) & 0x3FFF;
	// cmplw cr6,r10,r29
	cr6.compare<uint32_t>(ctx.r10.u32, r29.u32, xer);
	// blt cr6,0x824bf490
	if (cr6.lt) goto loc_824BF490;
	// b 0x824bf4cc
	goto loc_824BF4CC;
loc_824BF4C4:
	// cmplw cr6,r25,r26
	cr6.compare<uint32_t>(r25.u32, r26.u32, xer);
	// beq cr6,0x824bf490
	if (cr6.eq) goto loc_824BF490;
loc_824BF4CC:
	// clrlwi. r11,r19,24
	r11.u64 = r19.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf5e4
	if (!cr0.eq) goto loc_824BF5E4;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x824bf5e4
	if (cr6.eq) goto loc_824BF5E4;
	// lwz r11,16(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf5e4
	if (cr0.eq) goto loc_824BF5E4;
	// cmplw cr6,r26,r27
	cr6.compare<uint32_t>(r26.u32, r27.u32, xer);
	// bne cr6,0x824bf5e4
	if (!cr6.eq) goto loc_824BF5E4;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x824bf5e4
	if (!cr6.eq) goto loc_824BF5E4;
	// lwz r3,40(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x824beb20
	sub_824BEB20(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf5e4
	if (cr0.eq) goto loc_824BF5E4;
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// oris r11,r11,8192
	r11.u64 = r11.u64 | 536870912;
	// stw r11,16(r28)
	PPC_STORE_U32(r28.u32 + 16, r11.u32);
	// lwz r11,24(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 24);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x824bf5cc
	goto loc_824BF5CC;
loc_824BF524:
	// lwz r11,76(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf5e4
	if (cr0.eq) goto loc_824BF5E4;
	// lwz r11,112(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 112);
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,10,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bf56c
	if (cr0.eq) goto loc_824BF56C;
	// b 0x824bf550
	goto loc_824BF550;
loc_824BF544:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,0,9,9
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bf55c
	if (cr0.eq) goto loc_824BF55C;
loc_824BF550:
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bf544
	if (!cr0.eq) goto loc_824BF544;
loc_824BF55C:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,10,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bf550
	if (!cr0.eq) goto loc_824BF550;
	// stw r11,112(r30)
	PPC_STORE_U32(r30.u32 + 112, r11.u32);
loc_824BF56C:
	// lwz r11,112(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 112);
	// lwz r10,24(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 24);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x824bf5e4
	if (!cr6.eq) goto loc_824BF5E4;
	// lwz r31,28(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf5c8
	if (!cr0.eq) goto loc_824BF5C8;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824bf5c8
	if (cr0.eq) goto loc_824BF5C8;
loc_824BF590:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a90a0
	sub_824A90A0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf5bc
	if (!cr0.eq) goto loc_824BF5BC;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf5c8
	if (!cr0.eq) goto loc_824BF5C8;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bf590
	if (!cr6.eq) goto loc_824BF590;
	// b 0x824bf5c8
	goto loc_824BF5C8;
loc_824BF5BC:
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// rlwinm r11,r11,0,3,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFDFFFFFFF;
	// stw r11,16(r28)
	PPC_STORE_U32(r28.u32 + 16, r11.u32);
loc_824BF5C8:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
loc_824BF5CC:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bf5e4
	if (!cr0.eq) goto loc_824BF5E4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r30,r11
	r30.u64 = r11.u64;
	// bne cr6,0x824bf524
	if (!cr6.eq) goto loc_824BF524;
loc_824BF5E4:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824bf640
	if (cr6.eq) goto loc_824BF640;
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm r11,r10,0,18,24
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,8320
	cr6.compare<uint32_t>(r11.u32, 8320, xer);
	// bne cr6,0x824bf640
	if (!cr6.eq) goto loc_824BF640;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,8192
	cr6.compare<uint32_t>(r11.u32, 8192, xer);
	// beq cr6,0x824bf640
	if (cr6.eq) goto loc_824BF640;
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// rlwinm r9,r11,19,18,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 19) & 0x3FFF;
	// cmplw cr6,r9,r29
	cr6.compare<uint32_t>(ctx.r9.u32, r29.u32, xer);
	// bne cr6,0x824bf640
	if (!cr6.eq) goto loc_824BF640;
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfac8
	if (!cr0.eq) goto loc_824BFAC8;
	// clrlwi. r11,r20,24
	r11.u64 = r20.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf640
	if (!cr0.eq) goto loc_824BF640;
	// rlwinm. r11,r10,0,0,0
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bfac8
	if (cr0.eq) goto loc_824BFAC8;
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x824bfac8
	if (cr6.eq) goto loc_824BFAC8;
loc_824BF640:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,65
	cr6.compare<uint32_t>(r11.u32, 65, xer);
	// beq cr6,0x824bf658
	if (cr6.eq) goto loc_824BF658;
	// cmplwi cr6,r11,108
	cr6.compare<uint32_t>(r11.u32, 108, xer);
	// bne cr6,0x824bf6d8
	if (!cr6.eq) goto loc_824BF6D8;
loc_824BF658:
	// clrlwi. r11,r24,24
	r11.u64 = r24.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf6d8
	if (!cr0.eq) goto loc_824BF6D8;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824bf6d8
	if (cr6.eq) goto loc_824BF6D8;
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,8192
	cr6.compare<uint32_t>(r11.u32, 8192, xer);
	// bne cr6,0x824bf6d8
	if (!cr6.eq) goto loc_824BF6D8;
	// lwz r31,0(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 0);
loc_824BF67C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824bfac8
	if (cr6.eq) goto loc_824BFAC8;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bf6d0
	if (cr0.eq) goto loc_824BF6D0;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,132
	ctx.r7.s64 = ctx.r1.s64 + 132;
	// addi r6,r1,136
	ctx.r6.s64 = ctx.r1.s64 + 136;
	// addi r5,r1,120
	ctx.r5.s64 = ctx.r1.s64 + 120;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825576c8
	sub_825576C8(ctx, base);
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// bne cr6,0x824bf6d8
	if (!cr6.eq) goto loc_824BF6D8;
loc_824BF6D0:
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x824bf67c
	goto loc_824BF67C;
loc_824BF6D8:
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// rlwinm. r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bf730
	if (cr0.eq) goto loc_824BF730;
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf730
	if (cr0.eq) goto loc_824BF730;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r10,r11,25,25,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r10,31
	cr6.compare<uint32_t>(ctx.r10.u32, 31, xer);
	// blt cr6,0x824bf708
	if (cr6.lt) goto loc_824BF708;
	// cmplwi cr6,r10,81
	cr6.compare<uint32_t>(ctx.r10.u32, 81, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824bf70c
	if (!cr6.gt) goto loc_824BF70C;
loc_824BF708:
	// mr r11,r14
	r11.u64 = r14.u64;
loc_824BF70C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf730
	if (cr0.eq) goto loc_824BF730;
	// cmplwi cr6,r10,62
	cr6.compare<uint32_t>(ctx.r10.u32, 62, xer);
	// bne cr6,0x824bfae0
	if (!cr6.eq) goto loc_824BFAE0;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824bc2b8
	sub_824BC2B8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfae0
	if (!cr0.eq) goto loc_824BFAE0;
loc_824BF730:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,7936
	cr6.compare<uint32_t>(r11.u32, 7936, xer);
	// bne cr6,0x824bf778
	if (!cr6.eq) goto loc_824BF778;
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf778
	if (cr0.eq) goto loc_824BF778;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824bf778
	if (cr6.eq) goto loc_824BF778;
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,8064
	cr6.compare<uint32_t>(r11.u32, 8064, xer);
	// bne cr6,0x824bf778
	if (!cr6.eq) goto loc_824BF778;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824bc2b8
	sub_824BC2B8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfae0
	if (!cr0.eq) goto loc_824BFAE0;
loc_824BF778:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,21
	cr6.compare<uint32_t>(r11.u32, 21, xer);
	// blt cr6,0x824bf794
	if (cr6.lt) goto loc_824BF794;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824bf798
	if (!cr6.gt) goto loc_824BF798;
loc_824BF794:
	// mr r11,r14
	r11.u64 = r14.u64;
loc_824BF798:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf854
	if (cr0.eq) goto loc_824BF854;
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf854
	if (cr0.eq) goto loc_824BF854;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824bf854
	if (cr6.eq) goto loc_824BF854;
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,7936
	cr6.compare<uint32_t>(r11.u32, 7936, xer);
	// bne cr6,0x824bf854
	if (!cr6.eq) goto loc_824BF854;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824bc2b8
	sub_824BC2B8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf854
	if (cr0.eq) goto loc_824BF854;
	// lwz r10,24(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 24);
	// lwz r11,112(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r9,r9,10,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bf818
	if (cr0.eq) goto loc_824BF818;
	// b 0x824bf7fc
	goto loc_824BF7FC;
loc_824BF7F0:
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r9,r9,0,9,9
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bf808
	if (cr0.eq) goto loc_824BF808;
loc_824BF7FC:
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bf7f0
	if (!cr0.eq) goto loc_824BF7F0;
loc_824BF808:
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r9,r9,10,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824bf7fc
	if (!cr0.eq) goto loc_824BF7FC;
	// stw r11,112(r10)
	PPC_STORE_U32(ctx.r10.u32 + 112, r11.u32);
loc_824BF818:
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lwz r4,112(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	// bl 0x824baf98
	sub_824BAF98(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824bfae8
	if (cr0.eq) goto loc_824BFAE8;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// cmpwi cr6,r3,9
	cr6.compare<int32_t>(ctx.r3.s32, 9, xer);
	// beq cr6,0x824bf854
	if (cr6.eq) goto loc_824BF854;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// cmpwi cr6,r3,8
	cr6.compare<int32_t>(ctx.r3.s32, 8, xer);
	// bne cr6,0x824bfae8
	if (!cr6.eq) goto loc_824BFAE8;
loc_824BF854:
	// lwz r10,16(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// rlwinm. r11,r10,0,2,2
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf888
	if (cr0.eq) goto loc_824BF888;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x824bf874
	if (cr6.eq) goto loc_824BF874;
	// lwz r11,16(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 16);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf888
	if (cr0.eq) goto loc_824BF888;
loc_824BF874:
	// lwz r11,44(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 44);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bfaf8
	if (cr0.eq) goto loc_824BFAF8;
loc_824BF888:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm. r30,r10,19,18,31
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 19) & 0x3FFF;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// rlwinm r10,r11,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r10,r10,-8192
	ctx.r10.s64 = ctx.r10.s64 + -8192;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// xori r24,r10,1
	r24.u64 = ctx.r10.u64 ^ 1;
	// bne 0x824bf8f8
	if (!cr0.eq) goto loc_824BF8F8;
	// rlwinm. r11,r11,0,10,12
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x380000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf8f8
	if (cr0.eq) goto loc_824BF8F8;
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r3,40(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,140
	ctx.r7.s64 = ctx.r1.s64 + 140;
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// addi r5,r1,124
	ctx.r5.s64 = ctx.r1.s64 + 124;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825576c8
	sub_825576C8(ctx, base);
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r15,r11
	r11.s64 = r11.s64 - r15.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf8f8
	if (cr0.eq) goto loc_824BF8F8;
	// li r30,16384
	r30.s64 = 16384;
loc_824BF8F8:
	// mr r26,r28
	r26.u64 = r28.u64;
	// b 0x824bf978
	goto loc_824BF978;
loc_824BF900:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r10,r11,25,25,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r10,84
	cr6.compare<uint32_t>(ctx.r10.u32, 84, xer);
	// bne cr6,0x824bf924
	if (!cr6.eq) goto loc_824BF924;
	// lwz r11,48(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 48);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf978
	if (cr0.eq) goto loc_824BF978;
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// b 0x824bf978
	goto loc_824BF978;
loc_824BF924:
	// cmplwi cr6,r10,83
	cr6.compare<uint32_t>(ctx.r10.u32, 83, xer);
	// bne cr6,0x824bf978
	if (!cr6.eq) goto loc_824BF978;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// li r4,83
	ctx.r4.s64 = 83;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r28,-12
	r11.s64 = r28.s64 + -12;
	// lwzx r11,r3,r11
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bf960
	if (cr0.eq) goto loc_824BF960;
	// mr r11,r14
	r11.u64 = r14.u64;
	// b 0x824bf968
	goto loc_824BF968;
loc_824BF960:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
loc_824BF968:
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf978
	if (cr0.eq) goto loc_824BF978;
	// addi r18,r18,-1
	r18.s64 = r18.s64 + -1;
loc_824BF978:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824a90a0
	sub_824A90A0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bf9bc
	if (cr0.eq) goto loc_824BF9BC;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r21,r14
	r21.u64 = r14.u64;
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// addi r10,r11,-65
	ctx.r10.s64 = r11.s64 + -65;
	// addi r11,r11,-62
	r11.s64 = r11.s64 + -62;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// xori r20,r10,1
	r20.u64 = ctx.r10.u64 ^ 1;
	// xori r19,r11,1
	r19.u64 = r11.u64 ^ 1;
loc_824BF9BC:
	// lwz r31,8(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm. r11,r31,15,31,31
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bf9f4
	if (!cr0.eq) goto loc_824BF9F4;
	// lwz r11,76(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bfa10
	if (cr0.eq) goto loc_824BFA10;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824b0570
	sub_824B0570(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bfa10
	if (cr0.eq) goto loc_824BFA10;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfa10
	if (!cr0.eq) goto loc_824BFA10;
loc_824BF9F4:
	// rlwinm r11,r31,0,18,24
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,11392
	cr6.compare<uint32_t>(r11.u32, 11392, xer);
	// bne cr6,0x824bfa0c
	if (!cr6.eq) goto loc_824BFA0C;
	// lwz r11,48(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 48);
	// rlwinm. r11,r11,6,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfa10
	if (!cr0.eq) goto loc_824BFA10;
loc_824BFA0C:
	// li r21,1
	r21.s64 = 1;
loc_824BFA10:
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x824bfa68
	if (cr6.eq) goto loc_824BFA68;
	// rlwinm r11,r22,0,0,30
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bfa30
	if (cr0.eq) goto loc_824BFA30;
	// mr r11,r14
	r11.u64 = r14.u64;
	// b 0x824bfa38
	goto loc_824BFA38;
loc_824BFA30:
	// rlwinm r11,r10,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,-36
	r11.s64 = r11.s64 + -36;
loc_824BFA38:
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x824bfa58
	if (!cr6.eq) goto loc_824BFA58;
loc_824BFA40:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r22,r28
	r22.u64 = r28.u64;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bfab0
	if (!cr0.eq) goto loc_824BFAB0;
	// b 0x824bfaa0
	goto loc_824BFAA0;
loc_824BFA58:
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfab0
	if (!cr0.eq) goto loc_824BFAB0;
	// rlwinm r11,r10,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x824bfaa4
	goto loc_824BFAA4;
loc_824BFA68:
	// lwz r11,28(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bfa7c
	if (cr0.eq) goto loc_824BFA7C;
	// mr r11,r14
	r11.u64 = r14.u64;
	// b 0x824bfa88
	goto loc_824BFA88;
loc_824BFA7C:
	// lwz r11,24(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,-36
	r11.s64 = r11.s64 + -36;
loc_824BFA88:
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x824bfa40
	if (cr6.eq) goto loc_824BFA40;
	// lwz r11,28(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfab0
	if (!cr0.eq) goto loc_824BFAB0;
	// lwz r11,24(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 24);
loc_824BFAA0:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
loc_824BFAA4:
	// addi r28,r11,-36
	r28.s64 = r11.s64 + -36;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x824bf3ac
	if (!cr6.eq) goto loc_824BF3AC;
loc_824BFAB0:
	// rlwinm r11,r23,0,0,30
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bf37c
	if (cr0.eq) goto loc_824BF37C;
loc_824BFAC0:
	// mr r23,r14
	r23.u64 = r14.u64;
	// b 0x824bf384
	goto loc_824BF384;
loc_824BFAC8:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
loc_824BFACC:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824bab40
	sub_824BAB40(ctx, base);
loc_824BFAD8:
	// li r16,1
	r16.s64 = 1;
	// b 0x824bf350
	goto loc_824BF350;
loc_824BFAE0:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// b 0x824bfacc
	goto loc_824BFACC;
loc_824BFAE8:
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82550b20
	sub_82550B20(ctx, base);
	// b 0x824bfac8
	goto loc_824BFAC8;
loc_824BFAF8:
	// addi r29,r28,4
	r29.s64 = r28.s64 + 4;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
loc_824BFB00:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824bfdac
	if (cr6.eq) goto loc_824BFDAC;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r9,r14,r10
	ctx.r9.s64 = ctx.r10.s64 - r14.s64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824bfb34
	if (cr0.eq) goto loc_824BFB34;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r10,11392
	cr6.compare<uint32_t>(ctx.r10.u32, 11392, xer);
	// beq cr6,0x824bfb3c
	if (cr6.eq) goto loc_824BFB3C;
loc_824BFB34:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824bfb00
	goto loc_824BFB00;
loc_824BFB3C:
	// lwz r10,8(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r30,16(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// rlwinm r11,r10,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmpwi cr6,r11,21
	cr6.compare<int32_t>(r11.s32, 21, xer);
	// beq cr6,0x824bfb80
	if (cr6.eq) goto loc_824BFB80;
	// cmpwi cr6,r11,22
	cr6.compare<int32_t>(r11.s32, 22, xer);
	// beq cr6,0x824bfb78
	if (cr6.eq) goto loc_824BFB78;
	// cmpwi cr6,r11,23
	cr6.compare<int32_t>(r11.s32, 23, xer);
	// beq cr6,0x824bfb70
	if (cr6.eq) goto loc_824BFB70;
	// cmpwi cr6,r11,24
	cr6.compare<int32_t>(r11.s32, 24, xer);
	// bne cr6,0x824bfdb8
	if (!cr6.eq) goto loc_824BFDB8;
	// li r6,61
	ctx.r6.s64 = 61;
	// b 0x824bfb84
	goto loc_824BFB84;
loc_824BFB70:
	// li r6,60
	ctx.r6.s64 = 60;
	// b 0x824bfb84
	goto loc_824BFB84;
loc_824BFB78:
	// li r6,59
	ctx.r6.s64 = 59;
	// b 0x824bfb84
	goto loc_824BFB84;
loc_824BFB80:
	// li r6,58
	ctx.r6.s64 = 58;
loc_824BFB84:
	// li r8,1
	ctx.r8.s64 = 1;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r7,r11,-32
	ctx.r7.s64 = r11.s64 + -32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,-32
	ctx.r10.s64 = ctx.r10.s64 + -32;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ori r11,r11,64
	r11.u64 = r11.u64 | 64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r4,44(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 44);
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// stw r3,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r3.u32);
	// beq cr6,0x824bfc90
	if (cr6.eq) goto loc_824BFC90;
	// lwz r11,24(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 24);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// b 0x824bfc70
	goto loc_824BFC70;
loc_824BFC08:
	// lwz r9,76(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// rlwinm. r11,r9,10,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bfc80
	if (cr0.eq) goto loc_824BFC80;
	// lwz r11,112(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	// lwz r8,48(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r8,r8,10,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824bfc50
	if (cr0.eq) goto loc_824BFC50;
	// b 0x824bfc34
	goto loc_824BFC34;
loc_824BFC28:
	// lwz r8,48(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r8,r8,0,9,9
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824bfc40
	if (cr0.eq) goto loc_824BFC40;
loc_824BFC34:
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bfc28
	if (!cr0.eq) goto loc_824BFC28;
loc_824BFC40:
	// lwz r8,48(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r8,r8,10,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x824bfc34
	if (!cr0.eq) goto loc_824BFC34;
	// stw r11,112(r10)
	PPC_STORE_U32(ctx.r10.u32 + 112, r11.u32);
loc_824BFC50:
	// lwz r11,112(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	// lwz r8,24(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 24);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x824bfc80
	if (!cr6.eq) goto loc_824BFC80;
	// rlwinm r11,r9,0,9,7
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFF7FFFFF;
	// rlwinm r9,r10,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,76(r10)
	PPC_STORE_U32(ctx.r10.u32 + 76, r11.u32);
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
loc_824BFC70:
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfc80
	if (!cr0.eq) goto loc_824BFC80;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824bfc08
	if (!cr6.eq) goto loc_824BFC08;
loc_824BFC80:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824bab40
	sub_824BAB40(ctx, base);
loc_824BFC90:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x824bfca8
	if (cr6.eq) goto loc_824BFCA8;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824bab40
	sub_824BAB40(ctx, base);
loc_824BFCA8:
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
loc_824BFCAC:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824bfd40
	if (cr0.eq) goto loc_824BFD40;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r14,r10
	ctx.r10.s64 = ctx.r10.s64 - r14.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bfd2c
	if (cr0.eq) goto loc_824BFD2C;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r9,r10,0,1,1
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824bfcec
	if (cr0.eq) goto loc_824BFCEC;
	// rlwinm. r10,r10,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// beq 0x824bfcf0
	if (cr0.eq) goto loc_824BFCF0;
loc_824BFCEC:
	// mr r10,r14
	ctx.r10.u64 = r14.u64;
loc_824BFCF0:
	// clrlwi. r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824bfd2c
	if (cr0.eq) goto loc_824BFD2C;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824bfd08
	goto loc_824BFD08;
loc_824BFD04:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824BFD08:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824bfd04
	if (!cr6.eq) goto loc_824BFD04;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r31,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r31.u32);
loc_824BFD2C:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824bfcac
	if (!cr6.eq) goto loc_824BFCAC;
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// b 0x824bfcac
	goto loc_824BFCAC;
loc_824BFD40:
	// lwz r30,0(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 0);
loc_824BFD44:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824bfd7c
	if (cr6.eq) goto loc_824BFD7C;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824bfd74
	if (!cr0.eq) goto loc_824BFD74;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
loc_824BFD74:
	// lwz r30,4(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// b 0x824bfd44
	goto loc_824BFD44;
loc_824BFD7C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba9b8
	sub_824BA9B8(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// oris r11,r11,256
	r11.u64 = r11.u64 | 16777216;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bl 0x8255a3c0
	sub_8255A3C0(ctx, base);
	// b 0x824bfad8
	goto loc_824BFAD8;
loc_824BFDA0:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BFDAC:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BFDB8:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824BFDC4:
	// clrlwi. r11,r16,24
	r11.u64 = r16.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bfea4
	if (cr0.eq) goto loc_824BFEA4;
	// lwz r11,4(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bfea4
	if (!cr0.eq) goto loc_824BFEA4;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824bfea4
	if (cr0.eq) goto loc_824BFEA4;
loc_824BFDE0:
	// lwz r31,28(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfe88
	if (!cr0.eq) goto loc_824BFE88;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824bfe88
	if (cr0.eq) goto loc_824BFE88;
loc_824BFDF4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824bfe70
	if (cr0.eq) goto loc_824BFE70;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
loc_824BFE08:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824bfe50
	if (cr6.eq) goto loc_824BFE50;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r14,r10
	ctx.r10.s64 = ctx.r10.s64 - r14.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824bfe48
	if (cr0.eq) goto loc_824BFE48;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x824bfe70
	if (!cr0.eq) goto loc_824BFE70;
loc_824BFE48:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824bfe08
	goto loc_824BFE08;
loc_824BFE50:
	// lis r12,-130
	r12.s64 = -8519680;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// ori r12,r12,16353
	r12.u64 = r12.u64 | 16353;
	// rlwinm r10,r10,0,16,12
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFF8FFFF;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_824BFE70:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824bfe88
	if (!cr0.eq) goto loc_824BFE88;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824bfdf4
	if (!cr6.eq) goto loc_824BFDF4;
loc_824BFE88:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824bfea4
	if (!cr0.eq) goto loc_824BFEA4;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bfde0
	if (!cr6.eq) goto loc_824BFDE0;
loc_824BFEA4:
	// lwz r11,4(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c015c
	if (!cr0.eq) goto loc_824C015C;
	// mr. r26,r11
	r26.u64 = r11.u64;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// beq 0x824c015c
	if (cr0.eq) goto loc_824C015C;
loc_824BFEB8:
	// lwz r11,48(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// rlwinm. r11,r11,6,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0140
	if (!cr0.eq) goto loc_824C0140;
	// lwz r11,28(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0140
	if (!cr0.eq) goto loc_824C0140;
	// lwz r11,24(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 24);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-36
	xer.ca = r11.u32 > 35;
	r31.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824c0140
	if (cr0.eq) goto loc_824C0140;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r10,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824bff1c
	if (cr6.eq) goto loc_824BFF1C;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824bff1c
	if (cr6.eq) goto loc_824BFF1C;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824bff1c
	if (cr6.eq) goto loc_824BFF1C;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824bff1c
	if (cr6.eq) goto loc_824BFF1C;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824bff1c
	if (cr6.eq) goto loc_824BFF1C;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// bne cr6,0x824bff20
	if (!cr6.eq) goto loc_824BFF20;
loc_824BFF1C:
	// li r11,1
	r11.s64 = 1;
loc_824BFF20:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0140
	if (cr0.eq) goto loc_824C0140;
	// rlwinm. r11,r10,15,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0140
	if (cr0.eq) goto loc_824C0140;
	// addi r11,r1,148
	r11.s64 = ctx.r1.s64 + 148;
	// addi r10,r1,152
	ctx.r10.s64 = ctx.r1.s64 + 152;
	// addi r9,r1,100
	ctx.r9.s64 = ctx.r1.s64 + 100;
	// addi r8,r1,104
	ctx.r8.s64 = ctx.r1.s64 + 104;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824be488
	sub_824BE488(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0140
	if (cr0.eq) goto loc_824C0140;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// beq cr6,0x824c0140
	if (cr6.eq) goto loc_824C0140;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// ble cr6,0x824bff98
	if (!cr6.gt) goto loc_824BFF98;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// ble cr6,0x824c0140
	if (!cr6.gt) goto loc_824C0140;
	// cmpwi cr6,r3,7
	cr6.compare<int32_t>(ctx.r3.s32, 7, xer);
	// ble cr6,0x824bff98
	if (!cr6.gt) goto loc_824BFF98;
	// cmpwi cr6,r3,9
	cr6.compare<int32_t>(ctx.r3.s32, 9, xer);
	// ble cr6,0x824c0140
	if (!cr6.gt) goto loc_824C0140;
loc_824BFF98:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// lwz r30,12(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bne cr6,0x824c00ac
	if (!cr6.eq) goto loc_824C00AC;
	// lwz r28,104(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r8,r1,97
	ctx.r8.s64 = ctx.r1.s64 + 97;
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x824bbb90
	sub_824BBB90(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0140
	if (cr0.eq) goto loc_824C0140;
	// lbz r29,96(r1)
	r29.u64 = PPC_LOAD_U8(ctx.r1.u32 + 96);
	// mr. r11,r29
	r11.u64 = r29.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0048
	if (cr0.eq) goto loc_824C0048;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
loc_824BFFEC:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c0028
	if (cr6.eq) goto loc_824C0028;
	// lwz r11,16(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// subf r9,r14,r11
	ctx.r9.s64 = r11.s64 - r14.s64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824c0018
	if (cr0.eq) goto loc_824C0018;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bne cr6,0x824c0020
	if (!cr6.eq) goto loc_824C0020;
loc_824C0018:
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// b 0x824bffec
	goto loc_824BFFEC;
loc_824C0020:
	// li r11,1
	r11.s64 = 1;
	// b 0x824c002c
	goto loc_824C002C;
loc_824C0028:
	// mr r11,r14
	r11.u64 = r14.u64;
loc_824C002C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0140
	if (!cr0.eq) goto loc_824C0140;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824bb118
	sub_824BB118(ctx, base);
	// mr r29,r14
	r29.u64 = r14.u64;
	// stb r29,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, r29.u8);
loc_824C0048:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r31,-12
	r11.s64 = r31.s64 + -12;
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// beq cr6,0x824c0084
	if (cr6.eq) goto loc_824C0084;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// rlwimi r9,r10,0,31,29
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFFFFFFFFFFFD) | (ctx.r9.u64 & 0x2);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
loc_824C0084:
	// stw r28,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r28.u32);
	// clrlwi r10,r29,24
	ctx.r10.u64 = r29.u32 & 0xFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r7,1
	ctx.r7.s64 = 1;
	// lbz r8,97(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 97);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// rlwimi r11,r10,18,13,13
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 18) & 0x40000) | (r11.u64 & 0xFFFFFFFFFFFBFFFF);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x824c0128
	goto loc_824C0128;
loc_824C00AC:
	// lwz r29,104(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r8,r1,98
	ctx.r8.s64 = ctx.r1.s64 + 98;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x824bbb90
	sub_824BBB90(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0140
	if (cr0.eq) goto loc_824C0140;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// lwz r30,100(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r8,r1,99
	ctx.r8.s64 = ctx.r1.s64 + 99;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824bbb90
	sub_824BBB90(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0140
	if (cr0.eq) goto loc_824C0140;
	// lbz r31,96(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + 96);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lbz r8,98(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 98);
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824bbf38
	sub_824BBF38(ctx, base);
	// mr r11,r31
	r11.u64 = r31.u64;
	// lbz r8,99(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 99);
	// li r6,0
	ctx.r6.s64 = 0;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
loc_824C0128:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824bbf38
	sub_824BBF38(ctx, base);
	// lwz r11,48(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// oris r11,r11,3072
	r11.u64 = r11.u64 | 201326592;
	// stw r11,48(r26)
	PPC_STORE_U32(r26.u32 + 48, r11.u32);
loc_824C0140:
	// rlwinm r11,r26,0,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c015c
	if (!cr0.eq) goto loc_824C015C;
	// mr r26,r11
	r26.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824bfeb8
	if (!cr6.eq) goto loc_824BFEB8;
loc_824C015C:
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
loc_824C0160:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// lfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_824C016C"))) PPC_WEAK_FUNC(sub_824C016C);
PPC_FUNC_IMPL(__imp__sub_824C016C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C0170"))) PPC_WEAK_FUNC(sub_824C0170);
PPC_FUNC_IMPL(__imp__sub_824C0170) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r15,r4
	r15.u64 = ctx.r4.u64;
	// stb r6,335(r1)
	PPC_STORE_U8(ctx.r1.u32 + 335, ctx.r6.u8);
	// mr r17,r3
	r17.u64 = ctx.r3.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r14,r7
	r14.u64 = ctx.r7.u64;
	// lwz r11,48(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 48);
	// rlwinm. r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c05e8
	if (!cr0.eq) goto loc_824C05E8;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwinm. r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c05e8
	if (!cr0.eq) goto loc_824C05E8;
	// lwz r11,76(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 76);
	// lwz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c05e8
	if (cr0.eq) goto loc_824C05E8;
	// lwz r11,36(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c0208
	if (!cr0.eq) goto loc_824C0208;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c0208
	if (cr0.eq) goto loc_824C0208;
loc_824C01D8:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,25,25,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r10,89
	cr6.compare<uint32_t>(ctx.r10.u32, 89, xer);
	// beq cr6,0x824c01f0
	if (cr6.eq) goto loc_824C01F0;
	// cmplwi cr6,r10,63
	cr6.compare<uint32_t>(ctx.r10.u32, 63, xer);
	// bne cr6,0x824c05e8
	if (!cr6.eq) goto loc_824C05E8;
loc_824C01F0:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c0208
	if (!cr0.eq) goto loc_824C0208;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c01d8
	if (!cr6.eq) goto loc_824C01D8;
loc_824C0208:
	// li r5,35
	ctx.r5.s64 = 35;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// li r16,1
	r16.s64 = 1;
	// bl 0x824a9858
	sub_824A9858(ctx, base);
	// mr r21,r3
	r21.u64 = ctx.r3.u64;
	// li r19,14208
	r19.s64 = 14208;
	// addi r20,r21,4
	r20.s64 = r21.s64 + 4;
	// ori r11,r21,1
	r11.u64 = r21.u64 | 1;
	// ori r10,r20,1
	ctx.r10.u64 = r20.u64 | 1;
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// stw r10,0(r21)
	PPC_STORE_U32(r21.u32 + 0, ctx.r10.u32);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c027c
	if (!cr0.eq) goto loc_824C027C;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c027c
	if (cr0.eq) goto loc_824C027C;
loc_824C024C:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// subf r10,r19,r10
	ctx.r10.s64 = ctx.r10.s64 - r19.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c05c4
	if (cr0.eq) goto loc_824C05C4;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c027c
	if (!cr0.eq) goto loc_824C027C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c024c
	if (!cr6.eq) goto loc_824C024C;
loc_824C027C:
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c05c8
	if (!cr0.eq) goto loc_824C05C8;
	// mr. r23,r11
	r23.u64 = r11.u64;
	cr0.compare<int32_t>(r23.s32, 0, xer);
	// beq 0x824c05c8
	if (cr0.eq) goto loc_824C05C8;
	// li r18,13952
	r18.s64 = 13952;
loc_824C0294:
	// lwz r11,8(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r18,r11
	r11.s64 = r11.s64 - r18.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c05a4
	if (cr0.eq) goto loc_824C05A4;
	// addi r22,r23,40
	r22.s64 = r23.s64 + 40;
	// li r24,0
	r24.s64 = 0;
	// li r29,0
	r29.s64 = 0;
	// addi r31,r1,112
	r31.s64 = ctx.r1.s64 + 112;
	// mr r30,r22
	r30.u64 = r22.u64;
loc_824C02C0:
	// lwz r11,8(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 8);
	// rlwinm r11,r11,13,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bge cr6,0x824c032c
	if (!cr6.lt) goto loc_824C032C;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r19,r11
	r11.s64 = r11.s64 - r19.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0320
	if (cr0.eq) goto loc_824C0320;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r6,r1,92
	ctx.r6.s64 = ctx.r1.s64 + 92;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825576c8
	sub_825576C8(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
loc_824C0320:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// b 0x824c02c0
	goto loc_824C02C0;
loc_824C032C:
	// lwz r11,8(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 8);
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// rlwinm r11,r11,13,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// subf r28,r24,r11
	r28.s64 = r11.s64 - r24.s64;
	// beq cr6,0x824c0404
	if (cr6.eq) goto loc_824C0404;
	// addi r29,r1,112
	r29.s64 = ctx.r1.s64 + 112;
loc_824C0348:
	// lwz r31,0(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824c0378
	if (cr6.eq) goto loc_824C0378;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
loc_824C035C:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// beq cr6,0x824c0378
	if (cr6.eq) goto loc_824C0378;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// blt cr6,0x824c035c
	if (cr6.lt) goto loc_824C035C;
loc_824C0378:
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bne cr6,0x824c03f4
	if (!cr6.eq) goto loc_824C03F4;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// rlwinm r11,r11,18,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bl 0x824b4bf0
	sub_824B4BF0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c05c4
	if (!cr0.eq) goto loc_824C05C4;
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c03d0
	if (!cr0.eq) goto loc_824C03D0;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r3,r11,-4
	xer.ca = r11.u32 > 3;
	ctx.r3.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824c03d0
	if (cr0.eq) goto loc_824C03D0;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x824c03dc
	if (!cr6.gt) goto loc_824C03DC;
loc_824C03D0:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
loc_824C03DC:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stwx r31,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r31.u32);
loc_824C03F4:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplw cr6,r30,r24
	cr6.compare<uint32_t>(r30.u32, r24.u32, xer);
	// blt cr6,0x824c0348
	if (cr6.lt) goto loc_824C0348;
loc_824C0404:
	// cmplwi cr6,r28,4
	cr6.compare<uint32_t>(r28.u32, 4, xer);
	// bgt cr6,0x824c05c4
	if (cr6.gt) goto loc_824C05C4;
	// lwz r11,28(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 28);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c05a4
	if (!cr0.eq) goto loc_824C05A4;
	// mr. r28,r11
	r28.u64 = r11.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824c05a4
	if (cr0.eq) goto loc_824C05A4;
loc_824C0420:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r18,r11
	r11.s64 = r11.s64 - r18.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0588
	if (cr0.eq) goto loc_824C0588;
	// li r25,0
	r25.s64 = 0;
	// li r26,0
	r26.s64 = 0;
	// addi r27,r28,40
	r27.s64 = r28.s64 + 40;
loc_824C0444:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r11,r11,13,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// bge cr6,0x824c0574
	if (!cr6.lt) goto loc_824C0574;
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,100
	ctx.r6.s64 = ctx.r1.s64 + 100;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825576c8
	sub_825576C8(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x824c04ac
	if (cr6.eq) goto loc_824C04AC;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
loc_824C048C:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// beq cr6,0x824c05c4
	if (cr6.eq) goto loc_824C05C4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplw cr6,r11,r24
	cr6.compare<uint32_t>(r11.u32, r24.u32, xer);
	// blt cr6,0x824c048c
	if (cr6.lt) goto loc_824C048C;
loc_824C04AC:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// subf r10,r19,r10
	ctx.r10.s64 = ctx.r10.s64 - r19.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c0568
	if (cr0.eq) goto loc_824C0568;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lhz r4,14(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// mr r31,r22
	r31.u64 = r22.u64;
loc_824C04E0:
	// lwz r11,8(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 8);
	// rlwinm r11,r11,13,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bge cr6,0x824c0568
	if (!cr6.lt) goto loc_824C0568;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// subf r10,r19,r10
	ctx.r10.s64 = ctx.r10.s64 - r19.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c055c
	if (cr0.eq) goto loc_824C055C;
	// lbz r10,335(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 335);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c0544
	if (cr0.eq) goto loc_824C0544;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c0538
	if (cr0.eq) goto loc_824C0538;
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bne cr6,0x824c053c
	if (!cr6.eq) goto loc_824C053C;
loc_824C0538:
	// li r10,0
	ctx.r10.s64 = 0;
loc_824C053C:
	// clrlwi. r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c055c
	if (!cr0.eq) goto loc_824C055C;
loc_824C0544:
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lhz r4,14(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// cmplw cr6,r3,r29
	cr6.compare<uint32_t>(ctx.r3.u32, r29.u32, xer);
	// bne cr6,0x824c055c
	if (!cr6.eq) goto loc_824C055C;
	// li r25,1
	r25.s64 = 1;
loc_824C055C:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// b 0x824c04e0
	goto loc_824C04E0;
loc_824C0568:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// b 0x824c0444
	goto loc_824C0444;
loc_824C0574:
	// clrlwi. r11,r25,24
	r11.u64 = r25.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0588
	if (cr0.eq) goto loc_824C0588;
	// lwz r11,0(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r14)
	PPC_STORE_U32(r14.u32 + 0, r11.u32);
loc_824C0588:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c05a4
	if (!cr0.eq) goto loc_824C05A4;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c0420
	if (!cr6.eq) goto loc_824C0420;
loc_824C05A4:
	// rlwinm r11,r23,0,0,30
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c05c8
	if (!cr0.eq) goto loc_824C05C8;
	// mr r23,r11
	r23.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c0294
	if (!cr6.eq) goto loc_824C0294;
	// b 0x824c05c8
	goto loc_824C05C8;
loc_824C05C4:
	// li r16,0
	r16.s64 = 0;
loc_824C05C8:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824b4b58
	sub_824B4B58(ctx, base);
	// addi r11,r17,924
	r11.s64 = r17.s64 + 924;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r21)
	PPC_STORE_U32(r21.u32 + 0, ctx.r10.u32);
	// stw r21,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r21.u32);
	// b 0x824c05ec
	goto loc_824C05EC;
loc_824C05E8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_824C05EC:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_824C05F4"))) PPC_WEAK_FUNC(sub_824C05F4);
PPC_FUNC_IMPL(__imp__sub_824C05F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C05F8"))) PPC_WEAK_FUNC(sub_824C05F8);
PPC_FUNC_IMPL(__imp__sub_824C05F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r10,r11,6,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c0678
	if (!cr0.eq) goto loc_824C0678;
	// oris r11,r11,1024
	r11.u64 = r11.u64 | 67108864;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r3,616(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 616);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0654
	if (!cr0.eq) goto loc_824C0654;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-4
	xer.ca = r11.u32 > 3;
	r11.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0654
	if (cr0.eq) goto loc_824C0654;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x824c0660
	if (!cr6.gt) goto loc_824C0660;
loc_824C0654:
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_824C0660:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// stwx r31,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r31.u32);
loc_824C0678:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C068C"))) PPC_WEAK_FUNC(sub_824C068C);
PPC_FUNC_IMPL(__imp__sub_824C068C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C0690"))) PPC_WEAK_FUNC(sub_824C0690);
PPC_FUNC_IMPL(__imp__sub_824C0690) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824c06e4
	if (cr6.eq) goto loc_824C06E4;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824c06e4
	if (cr6.eq) goto loc_824C06E4;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824c06e4
	if (cr6.eq) goto loc_824C06E4;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824c06e4
	if (cr6.eq) goto loc_824C06E4;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824c06e4
	if (cr6.eq) goto loc_824C06E4;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824c06e8
	if (!cr6.eq) goto loc_824C06E8;
loc_824C06E4:
	// li r11,1
	r11.s64 = 1;
loc_824C06E8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0718
	if (cr0.eq) goto loc_824C0718;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// cmpwi cr6,r3,8
	cr6.compare<int32_t>(ctx.r3.s32, 8, xer);
	// beq cr6,0x824c0778
	if (cr6.eq) goto loc_824C0778;
	// cmpwi cr6,r3,9
	cr6.compare<int32_t>(ctx.r3.s32, 9, xer);
	// beq cr6,0x824c0778
	if (cr6.eq) goto loc_824C0778;
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// stw r11,36(r30)
	PPC_STORE_U32(r30.u32 + 36, r11.u32);
loc_824C0718:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// oris r11,r11,2048
	r11.u64 = r11.u64 | 134217728;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0754
	if (!cr0.eq) goto loc_824C0754;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r3,r11,-4
	xer.ca = r11.u32 > 3;
	ctx.r3.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824c0754
	if (cr0.eq) goto loc_824C0754;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x824c0760
	if (!cr6.gt) goto loc_824C0760;
loc_824C0754:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
loc_824C0760:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stwx r31,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r31.u32);
loc_824C0778:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824C0780"))) PPC_WEAK_FUNC(sub_824C0780);
PPC_FUNC_IMPL(__imp__sub_824C0780) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c07c0
	if (cr0.eq) goto loc_824C07C0;
	// lwz r11,28(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c07b8
	if (cr0.eq) goto loc_824C07B8;
	// addi r10,r27,32
	ctx.r10.s64 = r27.s64 + 32;
	// b 0x824c07c4
	goto loc_824C07C4;
loc_824C07B8:
	// addi r10,r27,24
	ctx.r10.s64 = r27.s64 + 24;
	// b 0x824c07c4
	goto loc_824C07C4;
loc_824C07C0:
	// addi r10,r27,16
	ctx.r10.s64 = r27.s64 + 16;
loc_824C07C4:
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// li r30,0
	r30.s64 = 0;
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824c07d8
	if (!cr0.eq) goto loc_824C07D8;
	// mr r30,r11
	r30.u64 = r11.u64;
loc_824C07D8:
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// b 0x824c0958
	goto loc_824C0958;
loc_824C07E0:
	// lwz r29,4(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 4);
loc_824C07E4:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824c0900
	if (cr6.eq) goto loc_824C0900;
	// lwz r31,16(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// addi r11,r31,0
	r11.s64 = r31.s64 + 0;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c08f8
	if (cr0.eq) goto loc_824C08F8;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c08f8
	if (cr0.eq) goto loc_824C08F8;
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r8,0,18,24
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c08f8
	if (cr0.eq) goto loc_824C08F8;
	// rlwinm. r11,r8,5,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c08f8
	if (!cr0.eq) goto loc_824C08F8;
	// lwz r11,48(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r6,r11,27
	ctx.r6.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// lwz r10,40(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// rlwinm r11,r11,29,3,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// slw r7,r7,r6
	ctx.r7.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r6.u8 & 0x3F));
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c08f8
	if (!cr0.eq) goto loc_824C08F8;
	// oris r11,r8,2048
	r11.u64 = ctx.r8.u64 | 134217728;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c08d4
	if (!cr0.eq) goto loc_824C08D4;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r3,r11,-4
	xer.ca = r11.u32 > 3;
	ctx.r3.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824c08d4
	if (cr0.eq) goto loc_824C08D4;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x824c08e0
	if (!cr6.gt) goto loc_824C08E0;
loc_824C08D4:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
loc_824C08E0:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stwx r31,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r31.u32);
loc_824C08F8:
	// lwz r29,8(r29)
	r29.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// b 0x824c07e4
	goto loc_824C07E4;
loc_824C0900:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c091c
	if (!cr0.eq) goto loc_824C091C;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c07e0
	if (!cr6.eq) goto loc_824C07E0;
loc_824C091C:
	// addi r10,r27,32
	ctx.r10.s64 = r27.s64 + 32;
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// beq cr6,0x824c0960
	if (cr6.eq) goto loc_824C0960;
	// addi r11,r27,24
	r11.s64 = r27.s64 + 24;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x824c0944
	if (cr6.eq) goto loc_824C0944;
	// lwz r9,28(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 28);
	// mr r28,r11
	r28.u64 = r11.u64;
	// clrlwi. r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824c0948
	if (cr0.eq) goto loc_824C0948;
loc_824C0944:
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
loc_824C0948:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c0960
	if (!cr0.eq) goto loc_824C0960;
	// mr r30,r11
	r30.u64 = r11.u64;
loc_824C0958:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824c07e0
	if (!cr6.eq) goto loc_824C07E0;
loc_824C0960:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_824C0968"))) PPC_WEAK_FUNC(sub_824C0968);
PPC_FUNC_IMPL(__imp__sub_824C0968) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r10,104(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// addi r30,r10,1
	r30.s64 = ctx.r10.s64 + 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r30,104(r31)
	PPC_STORE_U32(r31.u32 + 104, r30.u32);
	// stw r30,44(r11)
	PPC_STORE_U32(r11.u32 + 44, r30.u32);
	// bl 0x824f9258
	sub_824F9258(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c0a38
	if (!cr0.eq) goto loc_824C0A38;
	// mr. r29,r11
	r29.u64 = r11.u64;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// beq 0x824c0a38
	if (cr0.eq) goto loc_824C0A38;
loc_824C09B0:
	// lwz r11,44(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 44);
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0a1c
	if (cr0.eq) goto loc_824C0A1C;
	// lwz r11,36(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0a1c
	if (!cr0.eq) goto loc_824C0A1C;
	// lwz r11,32(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r4,r11,-36
	xer.ca = r11.u32 > 35;
	ctx.r4.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq 0x824c0a1c
	if (cr0.eq) goto loc_824C0A1C;
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm r11,r10,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,82
	cr6.compare<uint32_t>(r11.u32, 82, xer);
	// blt cr6,0x824c09fc
	if (cr6.lt) goto loc_824C09FC;
	// cmplwi cr6,r11,94
	cr6.compare<uint32_t>(r11.u32, 94, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824c0a00
	if (!cr6.gt) goto loc_824C0A00;
loc_824C09FC:
	// li r11,0
	r11.s64 = 0;
loc_824C0A00:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0a1c
	if (cr0.eq) goto loc_824C0A1C;
	// rlwinm. r11,r10,5,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0a1c
	if (!cr0.eq) goto loc_824C0A1C;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c0690
	sub_824C0690(ctx, base);
loc_824C0A1C:
	// rlwinm r11,r29,0,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c0a38
	if (!cr0.eq) goto loc_824C0A38;
	// mr r29,r11
	r29.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c09b0
	if (!cr6.eq) goto loc_824C09B0;
loc_824C0A38:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824C0A40"))) PPC_WEAK_FUNC(sub_824C0A40);
PPC_FUNC_IMPL(__imp__sub_824C0A40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// lwz r11,104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r28,r11
	r28.u64 = r11.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r11,104(r31)
	PPC_STORE_U32(r31.u32 + 104, r11.u32);
	// stw r28,44(r10)
	PPC_STORE_U32(ctx.r10.u32 + 44, r28.u32);
	// bl 0x824f9258
	sub_824F9258(ctx, base);
	// lwz r11,104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// stw r29,104(r31)
	PPC_STORE_U32(r31.u32 + 104, r29.u32);
	// stw r29,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r29.u32);
	// bl 0x824f9258
	sub_824F9258(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c0b48
	if (!cr0.eq) goto loc_824C0B48;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824c0b48
	if (cr0.eq) goto loc_824C0B48;
loc_824C0AB0:
	// lwz r11,44(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 44);
	// subf r10,r11,r28
	ctx.r10.s64 = r28.s64 - r11.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c0b2c
	if (cr0.eq) goto loc_824C0B2C;
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0b2c
	if (!cr0.eq) goto loc_824C0B2C;
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0b2c
	if (!cr0.eq) goto loc_824C0B2C;
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r4,r11,-36
	xer.ca = r11.u32 > 35;
	ctx.r4.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq 0x824c0b2c
	if (cr0.eq) goto loc_824C0B2C;
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm r11,r10,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,82
	cr6.compare<uint32_t>(r11.u32, 82, xer);
	// blt cr6,0x824c0b0c
	if (cr6.lt) goto loc_824C0B0C;
	// cmplwi cr6,r11,94
	cr6.compare<uint32_t>(r11.u32, 94, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824c0b10
	if (!cr6.gt) goto loc_824C0B10;
loc_824C0B0C:
	// li r11,0
	r11.s64 = 0;
loc_824C0B10:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0b2c
	if (cr0.eq) goto loc_824C0B2C;
	// rlwinm. r11,r10,5,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0b2c
	if (!cr0.eq) goto loc_824C0B2C;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c0690
	sub_824C0690(ctx, base);
loc_824C0B2C:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c0b48
	if (!cr0.eq) goto loc_824C0B48;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c0ab0
	if (!cr6.eq) goto loc_824C0AB0;
loc_824C0B48:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_824C0B50"))) PPC_WEAK_FUNC(sub_824C0B50);
PPC_FUNC_IMPL(__imp__sub_824C0B50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd0
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// lwz r11,8(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 8);
	// li r18,0
	r18.s64 = 0;
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,21
	cr6.compare<uint32_t>(r11.u32, 21, xer);
	// blt cr6,0x824c0b90
	if (cr6.lt) goto loc_824C0B90;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// ble cr6,0x824c0b94
	if (!cr6.gt) goto loc_824C0B94;
loc_824C0B90:
	// mr r10,r18
	ctx.r10.u64 = r18.u64;
loc_824C0B94:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// lwz r11,4(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// addi r10,r10,10
	ctx.r10.s64 = ctx.r10.s64 + 10;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r23,r10,r20
	r23.u64 = PPC_LOAD_U32(ctx.r10.u32 + r20.u32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c0bc4
	if (cr0.eq) goto loc_824C0BC4;
	// mr r11,r18
	r11.u64 = r18.u64;
loc_824C0BC4:
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r18,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r18.u32);
loc_824C0BCC:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c0bec
	if (cr6.eq) goto loc_824C0BEC;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// mr r11,r18
	r11.u64 = r18.u64;
	// bne cr6,0x824c0bf0
	if (!cr6.eq) goto loc_824C0BF0;
loc_824C0BEC:
	// li r11,1
	r11.s64 = 1;
loc_824C0BF0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0fa0
	if (!cr0.eq) goto loc_824C0FA0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824bc3c0
	sub_824BC3C0(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// lwz r5,12(r23)
	ctx.r5.u64 = PPC_LOAD_U32(r23.u32 + 12);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824b3d28
	sub_824B3D28(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r10,8(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwimi r11,r10,9,4,6
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 9) & 0xE000000) | (r11.u64 & 0xFFFFFFFFF1FFFFFF);
	// rlwinm r10,r11,27,30,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// rlwinm r11,r11,0,27,18
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFE01F;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// or r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 | ctx.r10.u64;
	// rlwinm r9,r9,2,26,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3C;
	// or r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 | ctx.r10.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// clrlwi r30,r11,24
	r30.u64 = r11.u32 & 0xFF;
	// rlwinm r27,r11,16,29,31
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0x7;
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwimi r11,r27,25,4,6
	r11.u64 = (__builtin_rotateleft32(r27.u32, 25) & 0xE000000) | (r11.u64 & 0xFFFFFFFFF1FFFFFF);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// bl 0x824ba148
	sub_824BA148(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// mr r27,r31
	r27.u64 = r31.u64;
	// rlwimi r11,r30,5,19,26
	r11.u64 = (__builtin_rotateleft32(r30.u32, 5) & 0x1FE0) | (r11.u64 & 0xFFFFFFFFFFFFE01F);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lhz r11,8(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 8);
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lbz r26,10(r28)
	r26.u64 = PPC_LOAD_U8(r28.u32 + 10);
	// clrlwi r25,r11,29
	r25.u64 = r11.u32 & 0x7;
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwimi r11,r25,25,4,6
	r11.u64 = (__builtin_rotateleft32(r25.u32, 25) & 0xE000000) | (r11.u64 & 0xFFFFFFFFF1FFFFFF);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// bl 0x824ba148
	sub_824BA148(ctx, base);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// rlwimi r11,r26,5,19,26
	r11.u64 = (__builtin_rotateleft32(r26.u32, 5) & 0x1FE0) | (r11.u64 & 0xFFFFFFFFFFFFE01F);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// clrlwi. r10,r21,24
	ctx.r10.u64 = r21.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c0ce0
	if (!cr0.eq) goto loc_824C0CE0;
	// mr r27,r30
	r27.u64 = r30.u64;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
loc_824C0CE0:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm. r11,r11,0,12,12
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0cf8
	if (!cr0.eq) goto loc_824C0CF8;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
loc_824C0CF8:
	// lwz r11,8(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmpwi cr6,r11,58
	cr6.compare<int32_t>(r11.s32, 58, xer);
	// bgt cr6,0x824c0d38
	if (cr6.gt) goto loc_824C0D38;
	// beq cr6,0x824c0d2c
	if (cr6.eq) goto loc_824C0D2C;
	// cmpwi cr6,r11,21
	cr6.compare<int32_t>(r11.s32, 21, xer);
	// beq cr6,0x824c0d2c
	if (cr6.eq) goto loc_824C0D2C;
	// cmpwi cr6,r11,22
	cr6.compare<int32_t>(r11.s32, 22, xer);
	// beq cr6,0x824c0d90
	if (cr6.eq) goto loc_824C0D90;
	// cmpwi cr6,r11,23
	cr6.compare<int32_t>(r11.s32, 23, xer);
	// beq cr6,0x824c0d70
	if (cr6.eq) goto loc_824C0D70;
	// cmpwi cr6,r11,24
	cr6.compare<int32_t>(r11.s32, 24, xer);
	// b 0x824c0d4c
	goto loc_824C0D4C;
loc_824C0D2C:
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// b 0x824c0d94
	goto loc_824C0D94;
loc_824C0D38:
	// cmpwi cr6,r11,59
	cr6.compare<int32_t>(r11.s32, 59, xer);
	// beq cr6,0x824c0d90
	if (cr6.eq) goto loc_824C0D90;
	// cmpwi cr6,r11,60
	cr6.compare<int32_t>(r11.s32, 60, xer);
	// beq cr6,0x824c0d70
	if (cr6.eq) goto loc_824C0D70;
	// cmpwi cr6,r11,61
	cr6.compare<int32_t>(r11.s32, 61, xer);
loc_824C0D4C:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bne cr6,0x824c0f98
	if (!cr6.eq) goto loc_824C0F98;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// bl 0x824bc6b0
	sub_824BC6B0(ctx, base);
	// b 0x824c0da8
	goto loc_824C0DA8;
loc_824C0D70:
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824bc800
	sub_824BC800(ctx, base);
	// b 0x824c0da8
	goto loc_824C0DA8;
loc_824C0D90:
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
loc_824C0D94:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824bc560
	sub_824BC560(ctx, base);
loc_824C0DA8:
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r22,24
	r11.s64 = r22.s64 + 24;
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// ori r6,r11,1
	ctx.r6.u64 = r11.u64 | 1;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,32(r7)
	PPC_STORE_U32(ctx.r7.u32 + 32, ctx.r9.u32);
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm. r11,r11,0,28,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0e20
	if (cr0.eq) goto loc_824C0E20;
	// lwz r11,36(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 36);
	// rlwinm. r11,r11,0,17,17
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0e20
	if (cr0.eq) goto loc_824C0E20;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824ba1b0
	sub_824BA1B0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// bl 0x824b31e8
	sub_824B31E8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
loc_824C0E20:
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// beq cr6,0x824c0e94
	if (cr6.eq) goto loc_824C0E94;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// oris r11,r11,128
	r11.u64 = r11.u64 | 8388608;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// sth r11,14(r31)
	PPC_STORE_U16(r31.u32 + 14, r11.u16);
	// lwz r11,4(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c0e70
	if (!cr0.eq) goto loc_824C0E70;
	// lwz r11,0(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r3,r11,-4
	xer.ca = r11.u32 > 3;
	ctx.r3.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824c0e70
	if (cr0.eq) goto loc_824C0E70;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x824c0e7c
	if (!cr6.gt) goto loc_824C0E7C;
loc_824C0E70:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
loc_824C0E7C:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stwx r31,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r31.u32);
loc_824C0E94:
	// lhz r11,8(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 8);
	// mr r30,r18
	r30.u64 = r18.u64;
	// clrlwi. r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c0bcc
	if (cr0.eq) goto loc_824C0BCC;
	// addi r29,r28,12
	r29.s64 = r28.s64 + 12;
loc_824C0EA8:
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r7,r4,4
	ctx.r7.s64 = ctx.r4.s64 + 4;
loc_824C0EB0:
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c0f6c
	if (cr0.eq) goto loc_824C0F6C;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r18,r10
	ctx.r10.s64 = ctx.r10.s64 - r18.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c0f58
	if (cr0.eq) goto loc_824C0F58;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// b 0x824c0ee8
	goto loc_824C0EE8;
loc_824C0EE4:
	// addi r9,r10,8
	ctx.r9.s64 = ctx.r10.s64 + 8;
loc_824C0EE8:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824c0ee4
	if (!cr6.eq) goto loc_824C0EE4;
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// ori r10,r30,228
	ctx.r10.u64 = r30.u64 | 228;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r31,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r31.u32);
	// rlwinm r9,r8,27,24,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0xFF;
	// rlwinm r8,r8,0,27,18
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFE01F;
	// rlwinm r6,r9,27,29,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x6;
	// rlwinm r5,r9,29,29,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x6;
	// rlwinm r3,r9,31,29,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x6;
	// rlwinm r9,r9,1,29,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x6;
	// srw r6,r10,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r6.u8 & 0x3F));
	// srw r5,r10,r5
	ctx.r5.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r5.u8 & 0x3F));
	// srw r3,r10,r3
	ctx.r3.u64 = ctx.r3.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r3.u8 & 0x3F));
	// rlwimi r5,r6,2,28,29
	ctx.r5.u64 = (__builtin_rotateleft32(ctx.r6.u32, 2) & 0xC) | (ctx.r5.u64 & 0xFFFFFFFFFFFFFFF3);
	// srw r10,r10,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r9.u8 & 0x3F));
	// clrlwi r9,r5,28
	ctx.r9.u64 = ctx.r5.u32 & 0xF;
	// rlwimi r3,r9,2,0,29
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r9.u32, 2) & 0xFFFFFFFC) | (ctx.r3.u64 & 0xFFFFFFFF00000003);
	// rlwimi r10,r3,2,0,29
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r3.u32, 2) & 0xFFFFFFFC) | (ctx.r10.u64 & 0xFFFFFFFF00000003);
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// or r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 | ctx.r8.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_824C0F58:
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824c0eb0
	if (!cr6.eq) goto loc_824C0EB0;
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// b 0x824c0eb0
	goto loc_824C0EB0;
loc_824C0F6C:
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// lhz r11,8(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 8);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x824c0ea8
	if (cr6.lt) goto loc_824C0EA8;
	// b 0x824c0bcc
	goto loc_824C0BCC;
loc_824C0F98:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C0FA0:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_824C0FA8"))) PPC_WEAK_FUNC(sub_824C0FA8);
PPC_FUNC_IMPL(__imp__sub_824C0FA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// li r5,8
	ctx.r5.s64 = 8;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// mr r23,r9
	r23.u64 = ctx.r9.u64;
	// bl 0x824a9858
	sub_824A9858(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r26,r30,4
	r26.s64 = r30.s64 + 4;
	// ori r11,r30,1
	r11.u64 = r30.u64 | 1;
	// ori r10,r26,1
	ctx.r10.u64 = r26.u64 | 1;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// lwz r29,20(r23)
	r29.u64 = PPC_LOAD_U32(r23.u32 + 20);
	// clrlwi. r11,r29,31
	r11.u64 = r29.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c118c
	if (!cr0.eq) goto loc_824C118C;
	// cmplwi r29,0
	cr0.compare<uint32_t>(r29.u32, 0, xer);
	// beq 0x824c118c
	if (cr0.eq) goto loc_824C118C;
loc_824C1004:
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1168
	if (cr0.eq) goto loc_824C1168;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c1038
	if (cr0.eq) goto loc_824C1038;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x824c103c
	if (!cr6.eq) goto loc_824C103C;
loc_824C1038:
	// li r11,0
	r11.s64 = 0;
loc_824C103C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1168
	if (cr0.eq) goto loc_824C1168;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmplw cr6,r31,r27
	cr6.compare<uint32_t>(r31.u32, r27.u32, xer);
	// beq cr6,0x824c1168
	if (cr6.eq) goto loc_824C1168;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c10b4
	if (!cr0.eq) goto loc_824C10B4;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-4
	xer.ca = r11.u32 > 3;
	r11.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c10b4
	if (cr0.eq) goto loc_824C10B4;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x824c10c4
	if (!cr6.gt) goto loc_824C10C4;
loc_824C10B4:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824bea10
	sub_824BEA10(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_824C10C4:
	// lwz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// li r20,9
	r20.s64 = 9;
	// mulli r31,r5,28
	r31.s64 = ctx.r5.s64 * 28;
	// add r31,r31,r11
	r31.u64 = r31.u64 + r11.u64;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// stw r5,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r5.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// stw r29,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r29.u32);
	// rlwimi r11,r20,16,12,15
	r11.u64 = (__builtin_rotateleft32(r20.u32, 16) & 0xF0000) | (r11.u64 & 0xFFFFFFFFFFF0FFFF);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bl 0x825576c8
	sub_825576C8(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// ori r11,r11,228
	r11.u64 = r11.u64 | 228;
	// stb r11,11(r31)
	PPC_STORE_U8(r31.u32 + 11, r11.u8);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c1184
	if (!cr6.eq) goto loc_824C1184;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r31,4
	ctx.r5.s64 = r31.s64 + 4;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825576c8
	sub_825576C8(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// ori r11,r11,228
	r11.u64 = r11.u64 | 228;
	// stb r11,10(r31)
	PPC_STORE_U8(r31.u32 + 10, r11.u8);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c1184
	if (!cr6.eq) goto loc_824C1184;
loc_824C1168:
	// rlwinm r11,r29,0,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r29,36(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r29,31
	r11.u64 = r29.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c118c
	if (!cr0.eq) goto loc_824C118C;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x824c1004
	if (!cr6.eq) goto loc_824C1004;
	// b 0x824c118c
	goto loc_824C118C;
loc_824C1184:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824be8e0
	sub_824BE8E0(ctx, base);
loc_824C118C:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c119c
	if (cr0.eq) goto loc_824C119C;
	// li r11,0
	r11.s64 = 0;
loc_824C119C:
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c11b0
	if (cr0.eq) goto loc_824C11B0;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824c11e8
	goto loc_824C11E8;
loc_824C11B0:
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c0b50
	sub_824C0B50(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824be8e0
	sub_824BE8E0(ctx, base);
	// addi r11,r28,924
	r11.s64 = r28.s64 + 924;
	// li r3,1
	ctx.r3.s64 = 1;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
loc_824C11E8:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_824C11F0"))) PPC_WEAK_FUNC(sub_824C11F0);
PPC_FUNC_IMPL(__imp__sub_824C11F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r5,37
	ctx.r5.s64 = 37;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// bl 0x824a9858
	sub_824A9858(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r22,0
	r22.s64 = 0;
	// addi r26,r27,4
	r26.s64 = r27.s64 + 4;
	// ori r11,r27,1
	r11.u64 = r27.u64 | 1;
	// ori r10,r26,1
	ctx.r10.u64 = r26.u64 | 1;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// stw r10,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r10.u32);
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c1238
	if (cr0.eq) goto loc_824C1238;
	// mr r11,r22
	r11.u64 = r22.u64;
loc_824C1238:
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// li r23,1
	r23.s64 = 1;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c12f8
	if (!cr0.eq) goto loc_824C12F8;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824c12f8
	if (cr0.eq) goto loc_824C12F8;
loc_824C1250:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r10,r11,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r10,r10,-14336
	ctx.r10.s64 = ctx.r10.s64 + -14336;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c12dc
	if (cr0.eq) goto loc_824C12DC;
	// oris r11,r11,2048
	r11.u64 = r11.u64 | 134217728;
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
loc_824C1274:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c12dc
	if (cr6.eq) goto loc_824C12DC;
	// lwz r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r11,r22,r4
	r11.s64 = ctx.r4.s64 - r22.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c12d4
	if (cr0.eq) goto loc_824C12D4;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm. r10,r11,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c12b0
	if (!cr0.eq) goto loc_824C12B0;
	// rlwinm. r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r22
	r11.u64 = r22.u64;
	// beq 0x824c12b4
	if (cr0.eq) goto loc_824C12B4;
loc_824C12B0:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_824C12B4:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c12d4
	if (cr0.eq) goto loc_824C12D4;
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm. r11,r11,5,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c12d4
	if (!cr0.eq) goto loc_824C12D4;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824c0690
	sub_824C0690(ctx, base);
loc_824C12D4:
	// lwz r31,8(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// b 0x824c1274
	goto loc_824C1274;
loc_824C12DC:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c12f8
	if (!cr0.eq) goto loc_824C12F8;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c1250
	if (!cr6.eq) goto loc_824C1250;
loc_824C12F8:
	// lwz r11,36(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 36);
	// rlwinm. r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c13cc
	if (cr0.eq) goto loc_824C13CC;
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c13cc
	if (!cr0.eq) goto loc_824C13CC;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824c13cc
	if (cr0.eq) goto loc_824C13CC;
loc_824C1318:
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c13b0
	if (!cr0.eq) goto loc_824C13B0;
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-36
	xer.ca = r11.u32 > 35;
	r31.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824c13b0
	if (cr0.eq) goto loc_824C13B0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r10,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824c1370
	if (cr6.eq) goto loc_824C1370;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824c1370
	if (cr6.eq) goto loc_824C1370;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824c1370
	if (cr6.eq) goto loc_824C1370;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824c1370
	if (cr6.eq) goto loc_824C1370;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824c1370
	if (cr6.eq) goto loc_824C1370;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// mr r11,r22
	r11.u64 = r22.u64;
	// bne cr6,0x824c1374
	if (!cr6.eq) goto loc_824C1374;
loc_824C1370:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_824C1374:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c13b0
	if (cr0.eq) goto loc_824C13B0;
	// rlwinm. r11,r10,5,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c13b0
	if (!cr0.eq) goto loc_824C13B0;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// beq cr6,0x824c13a0
	if (cr6.eq) goto loc_824C13A0;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// bne cr6,0x824c13b0
	if (!cr6.eq) goto loc_824C13B0;
loc_824C13A0:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824c0690
	sub_824C0690(ctx, base);
loc_824C13B0:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c13cc
	if (!cr0.eq) goto loc_824C13CC;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c1318
	if (!cr6.eq) goto loc_824C1318;
loc_824C13CC:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c13dc
	if (cr0.eq) goto loc_824C13DC;
	// mr r11,r22
	r11.u64 = r22.u64;
loc_824C13DC:
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1578
	if (!cr0.eq) goto loc_824C1578;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c13f8
	if (cr6.eq) goto loc_824C13F8;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// b 0x824c1404
	goto loc_824C1404;
loc_824C13F8:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r4,r11,-4
	ctx.r4.s64 = r11.s64 + -4;
loc_824C1404:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// addic. r10,r11,-1
	xer.ca = r11.u32 > 0;
	ctx.r10.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r29,r11,r4
	r29.u64 = PPC_LOAD_U32(r11.u32 + ctx.r4.u32);
	// stw r10,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r10.u32);
	// bne 0x824c1458
	if (!cr0.eq) goto loc_824C1458;
	// rlwinm r11,r4,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82496e38
	sub_82496E38(ctx, base);
loc_824C1458:
	// lwz r30,24(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c13cc
	if (!cr0.eq) goto loc_824C13CC;
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r11,r11,13,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c150c
	if (cr0.eq) goto loc_824C150C;
	// lwz r31,4(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c150c
	if (!cr0.eq) goto loc_824C150C;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824c150c
	if (cr0.eq) goto loc_824C150C;
loc_824C1490:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r10,48(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi r8,r10,27
	ctx.r8.u64 = ctx.r10.u32 & 0x1F;
	// rlwinm r10,r10,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r9,r9,13
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFF;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,31
	ctx.r9.s64 = ctx.r9.s64 + 31;
	// rlwinm r10,r9,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFC;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// slw r10,r23,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r8.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c14f4
	if (cr0.eq) goto loc_824C14F4;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824c0780
	sub_824C0780(ctx, base);
loc_824C14F4:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c150c
	if (!cr0.eq) goto loc_824C150C;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c1490
	if (!cr6.eq) goto loc_824C1490;
loc_824C150C:
	// lwz r31,4(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 4);
loc_824C1510:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c13cc
	if (cr6.eq) goto loc_824C13CC;
	// lwz r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r11,r22,r4
	r11.s64 = ctx.r4.s64 - r22.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c1570
	if (cr0.eq) goto loc_824C1570;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm. r10,r11,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c154c
	if (!cr0.eq) goto loc_824C154C;
	// rlwinm. r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r22
	r11.u64 = r22.u64;
	// beq 0x824c1550
	if (cr0.eq) goto loc_824C1550;
loc_824C154C:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_824C1550:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1570
	if (cr0.eq) goto loc_824C1570;
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm. r11,r11,5,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1570
	if (!cr0.eq) goto loc_824C1570;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824c0690
	sub_824C0690(ctx, base);
loc_824C1570:
	// lwz r31,8(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// b 0x824c1510
	goto loc_824C1510;
loc_824C1578:
	// lwz r11,36(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 36);
	// rlwinm. r11,r11,0,23,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x100;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c18fc
	if (cr0.eq) goto loc_824C18FC;
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c18e0
	if (!cr0.eq) goto loc_824C18E0;
	// mr. r28,r11
	r28.u64 = r11.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824c18e0
	if (cr0.eq) goto loc_824C18E0;
loc_824C1598:
	// lwz r11,36(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c18c4
	if (!cr0.eq) goto loc_824C18C4;
	// lwz r11,32(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-36
	xer.ca = r11.u32 > 35;
	r31.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824c18c4
	if (cr0.eq) goto loc_824C18C4;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10496
	cr6.compare<uint32_t>(r11.u32, 10496, xer);
	// bne cr6,0x824c16b0
	if (!cr6.eq) goto loc_824C16B0;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824f8d88
	sub_824F8D88(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c16b0
	if (cr0.eq) goto loc_824C16B0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r10,r11,5,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c15ec
	if (!cr0.eq) goto loc_824C15EC;
	// oris r11,r11,2048
	r11.u64 = r11.u64 | 134217728;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_824C15EC:
	// lwz r11,40(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 40);
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// stw r11,40(r24)
	PPC_STORE_U32(r24.u32 + 40, r11.u32);
	// lwz r31,64(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 64);
	// b 0x824c16a8
	goto loc_824C16A8;
loc_824C1600:
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1614
	if (cr0.eq) goto loc_824C1614;
	// mr r30,r22
	r30.u64 = r22.u64;
	// b 0x824c1620
	goto loc_824C1620;
loc_824C1614:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r30,r11,-36
	r30.s64 = r11.s64 + -36;
loc_824C1620:
	// lwz r29,8(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm. r11,r29,5,31,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1694
	if (!cr0.eq) goto loc_824C1694;
	// rlwinm r11,r29,25,25,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824c1664
	if (cr6.eq) goto loc_824C1664;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824c1664
	if (cr6.eq) goto loc_824C1664;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824c1664
	if (cr6.eq) goto loc_824C1664;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824c1664
	if (cr6.eq) goto loc_824C1664;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824c1664
	if (cr6.eq) goto loc_824C1664;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// mr r11,r22
	r11.u64 = r22.u64;
	// bne cr6,0x824c1668
	if (!cr6.eq) goto loc_824C1668;
loc_824C1664:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_824C1668:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c168c
	if (cr0.eq) goto loc_824C168C;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// cmpwi cr6,r3,8
	cr6.compare<int32_t>(ctx.r3.s32, 8, xer);
	// beq cr6,0x824c1928
	if (cr6.eq) goto loc_824C1928;
	// cmpwi cr6,r3,9
	cr6.compare<int32_t>(ctx.r3.s32, 9, xer);
	// beq cr6,0x824c1928
	if (cr6.eq) goto loc_824C1928;
loc_824C168C:
	// oris r11,r29,2048
	r11.u64 = r29.u64 | 134217728;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
loc_824C1694:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824c0968
	sub_824C0968(ctx, base);
	// lwz r31,64(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 64);
loc_824C16A8:
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x824c1600
	if (!cr0.eq) goto loc_824C1600;
loc_824C16B0:
	// lwz r10,8(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c16cc
	if (cr0.eq) goto loc_824C16CC;
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r11,r23
	r11.u64 = r23.u64;
	// bne cr6,0x824c16d0
	if (!cr6.eq) goto loc_824C16D0;
loc_824C16CC:
	// mr r11,r22
	r11.u64 = r22.u64;
loc_824C16D0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c18c4
	if (cr0.eq) goto loc_824C18C4;
	// lwz r11,40(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c18c4
	if (!cr0.eq) goto loc_824C18C4;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
loc_824C16F8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c176c
	if (cr6.eq) goto loc_824C176C;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,40(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1760
	if (!cr0.eq) goto loc_824C1760;
	// lwz r11,48(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r23,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c1768
	if (!cr0.eq) goto loc_824C1768;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x824c175c
	if (!cr6.eq) goto loc_824C175C;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// b 0x824c1760
	goto loc_824C1760;
loc_824C175C:
	// bl 0x8255e720
	sub_8255E720(ctx, base);
loc_824C1760:
	// lwz r31,12(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// b 0x824c16f8
	goto loc_824C16F8;
loc_824C1768:
	// mr r30,r23
	r30.u64 = r23.u64;
loc_824C176C:
	// clrlwi. r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c18c4
	if (!cr0.eq) goto loc_824C18C4;
	// lwz r9,8(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_824C177C:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c17b8
	if (cr6.eq) goto loc_824C17B8;
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r8,40(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// lwz r8,4(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// clrlwi. r8,r8,31
	ctx.r8.u64 = ctx.r8.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x824c17ac
	if (!cr0.eq) goto loc_824C17AC;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// beq cr6,0x824c17ac
	if (cr6.eq) goto loc_824C17AC;
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bne cr6,0x824c17b4
	if (!cr6.eq) goto loc_824C17B4;
loc_824C17AC:
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// b 0x824c177c
	goto loc_824C177C;
loc_824C17B4:
	// mr r29,r23
	r29.u64 = r23.u64;
loc_824C17B8:
	// clrlwi. r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c18c4
	if (cr0.eq) goto loc_824C18C4;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
loc_824C17C4:
	// lwz r11,36(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1828
	if (!cr0.eq) goto loc_824C1828;
	// lwz r11,32(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-36
	xer.ca = r11.u32 > 35;
	r11.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1828
	if (cr0.eq) goto loc_824C1828;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824c181c
	if (cr6.eq) goto loc_824C181C;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824c181c
	if (cr6.eq) goto loc_824C181C;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824c181c
	if (cr6.eq) goto loc_824C181C;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824c181c
	if (cr6.eq) goto loc_824C181C;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824c181c
	if (cr6.eq) goto loc_824C181C;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// mr r11,r22
	r11.u64 = r22.u64;
	// bne cr6,0x824c1820
	if (!cr6.eq) goto loc_824C1820;
loc_824C181C:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_824C1820:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1840
	if (!cr0.eq) goto loc_824C1840;
loc_824C1828:
	// lwz r8,52(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 52);
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne 0x824c17c4
	if (!cr0.eq) goto loc_824C17C4;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C1840:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824c18c4
	if (cr6.eq) goto loc_824C18C4;
	// lwz r11,4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// beq cr6,0x824c18a8
	if (cr6.eq) goto loc_824C18A8;
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c18a8
	if (!cr0.eq) goto loc_824C18A8;
	// b 0x824c189c
	goto loc_824C189C;
loc_824C1868:
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c187c
	if (cr0.eq) goto loc_824C187C;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// b 0x824c1888
	goto loc_824C1888;
loc_824C187C:
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r10,r10,-36
	ctx.r10.s64 = ctx.r10.s64 + -36;
loc_824C1888:
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm. r10,r10,5,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c18b0
	if (!cr0.eq) goto loc_824C18B0;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// beq cr6,0x824c18a8
	if (cr6.eq) goto loc_824C18A8;
loc_824C189C:
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c1868
	if (!cr0.eq) goto loc_824C1868;
loc_824C18A8:
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// b 0x824c1840
	goto loc_824C1840;
loc_824C18B0:
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824c0a40
	sub_824C0A40(ctx, base);
loc_824C18C4:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c18e0
	if (!cr0.eq) goto loc_824C18E0;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c1598
	if (!cr6.eq) goto loc_824C1598;
loc_824C18E0:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c18f0
	if (cr0.eq) goto loc_824C18F0;
	// mr r11,r22
	r11.u64 = r22.u64;
loc_824C18F0:
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c13cc
	if (cr0.eq) goto loc_824C13CC;
loc_824C18FC:
	// addi r11,r24,924
	r11.s64 = r24.s64 + 924;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r10.u32);
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
loc_824C190C:
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// mr r25,r22
	r25.u64 = r22.u64;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c1920
	if (cr0.eq) goto loc_824C1920;
	// mr r11,r22
	r11.u64 = r22.u64;
loc_824C1920:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x824c1bec
	goto loc_824C1BEC;
loc_824C1928:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r4,3500
	ctx.r4.s64 = 3500;
	// addi r5,r11,27992
	ctx.r5.s64 = r11.s64 + 27992;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C193C:
	// lwz r11,36(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1964
	if (cr0.eq) goto loc_824C1964;
	// lwz r11,28(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c195c
	if (cr0.eq) goto loc_824C195C;
	// addi r11,r26,16
	r11.s64 = r26.s64 + 16;
	// b 0x824c1968
	goto loc_824C1968;
loc_824C195C:
	// addi r11,r26,24
	r11.s64 = r26.s64 + 24;
	// b 0x824c1968
	goto loc_824C1968;
loc_824C1964:
	// addi r11,r26,32
	r11.s64 = r26.s64 + 32;
loc_824C1968:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c197c
	if (cr0.eq) goto loc_824C197C;
	// mr r28,r22
	r28.u64 = r22.u64;
	// b 0x824c1988
	goto loc_824C1988;
loc_824C197C:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r28,r10,-36
	r28.s64 = ctx.r10.s64 + -36;
loc_824C1988:
	// mr r27,r11
	r27.u64 = r11.u64;
	// b 0x824c1be0
	goto loc_824C1BE0;
loc_824C1990:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-12288
	r11.s64 = r11.s64 + -12288;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1a48
	if (cr0.eq) goto loc_824C1A48;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r28,-24
	r11.s64 = r28.s64 + -24;
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm. r10,r10,0,3,3
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x10000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c1a48
	if (cr0.eq) goto loc_824C1A48;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1a48
	if (!cr0.eq) goto loc_824C1A48;
	// lwz r9,0(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 0);
loc_824C19E4:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824c1a48
	if (cr6.eq) goto loc_824C1A48;
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c1a40
	if (cr0.eq) goto loc_824C1A40;
	// lwz r11,12(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm. r8,r10,0,3,3
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x10000000;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x824c1a40
	if (!cr0.eq) goto loc_824C1A40;
	// oris r8,r10,4096
	ctx.r8.u64 = ctx.r10.u64 | 268435456;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// clrlwi. r8,r11,31
	ctx.r8.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c1a34
	if (cr0.eq) goto loc_824C1A34;
	// mr r11,r22
	r11.u64 = r22.u64;
loc_824C1A34:
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x824c1a40
	if (cr6.eq) goto loc_824C1A40;
	// mr r25,r23
	r25.u64 = r23.u64;
loc_824C1A40:
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x824c19e4
	goto loc_824C19E4;
loc_824C1A48:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm. r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1b80
	if (cr0.eq) goto loc_824C1B80;
	// lwz r9,0(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 0);
loc_824C1A58:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824c1aa0
	if (cr6.eq) goto loc_824C1AA0;
	// lwz r11,12(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm. r8,r10,0,3,3
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x10000000;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x824c1a98
	if (!cr0.eq) goto loc_824C1A98;
	// oris r8,r10,4096
	ctx.r8.u64 = ctx.r10.u64 | 268435456;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// clrlwi. r8,r11,31
	ctx.r8.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c1a8c
	if (cr0.eq) goto loc_824C1A8C;
	// mr r11,r22
	r11.u64 = r22.u64;
loc_824C1A8C:
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x824c1a98
	if (cr6.eq) goto loc_824C1A98;
	// mr r25,r23
	r25.u64 = r23.u64;
loc_824C1A98:
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x824c1a58
	goto loc_824C1A58;
loc_824C1AA0:
	// lwz r29,52(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + 52);
	// b 0x824c1b78
	goto loc_824C1B78;
loc_824C1AA8:
	// lwz r11,36(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1b74
	if (!cr0.eq) goto loc_824C1B74;
	// lwz r11,32(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r30,r11,-36
	xer.ca = r11.u32 > 35;
	r30.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824c1b74
	if (cr0.eq) goto loc_824C1B74;
	// lwz r31,8(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm. r11,r31,0,3,3
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x10000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1b74
	if (!cr0.eq) goto loc_824C1B74;
	// rlwinm r4,r31,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r4,85
	cr6.compare<uint32_t>(ctx.r4.u32, 85, xer);
	// beq cr6,0x824c1b08
	if (cr6.eq) goto loc_824C1B08;
	// cmplwi cr6,r4,86
	cr6.compare<uint32_t>(ctx.r4.u32, 86, xer);
	// beq cr6,0x824c1b08
	if (cr6.eq) goto loc_824C1B08;
	// cmplwi cr6,r4,88
	cr6.compare<uint32_t>(ctx.r4.u32, 88, xer);
	// beq cr6,0x824c1b08
	if (cr6.eq) goto loc_824C1B08;
	// cmplwi cr6,r4,89
	cr6.compare<uint32_t>(ctx.r4.u32, 89, xer);
	// beq cr6,0x824c1b08
	if (cr6.eq) goto loc_824C1B08;
	// cmplwi cr6,r4,83
	cr6.compare<uint32_t>(ctx.r4.u32, 83, xer);
	// beq cr6,0x824c1b08
	if (cr6.eq) goto loc_824C1B08;
	// cmplwi cr6,r4,84
	cr6.compare<uint32_t>(ctx.r4.u32, 84, xer);
	// mr r11,r22
	r11.u64 = r22.u64;
	// bne cr6,0x824c1b0c
	if (!cr6.eq) goto loc_824C1B0C;
loc_824C1B08:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_824C1B0C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1b74
	if (cr0.eq) goto loc_824C1B74;
	// rlwinm r6,r31,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 18) & 0x7;
	// rlwinm r5,r31,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 13) & 0x7;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r30,-12
	r11.s64 = r30.s64 + -12;
	// lwz r10,40(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// lwzx r11,r3,r11
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r23,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c1b74
	if (!cr0.eq) goto loc_824C1B74;
	// oris r11,r31,4096
	r11.u64 = r31.u64 | 268435456;
	// mr r25,r23
	r25.u64 = r23.u64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
loc_824C1B74:
	// lwz r29,52(r29)
	r29.u64 = PPC_LOAD_U32(r29.u32 + 52);
loc_824C1B78:
	// cmplwi r29,0
	cr0.compare<uint32_t>(r29.u32, 0, xer);
	// bne 0x824c1aa8
	if (!cr0.eq) goto loc_824C1AA8;
loc_824C1B80:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c1b9c
	if (!cr0.eq) goto loc_824C1B9C;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r28,r11,-36
	xer.ca = r11.u32 > 35;
	r28.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// bne 0x824c1990
	if (!cr0.eq) goto loc_824C1990;
loc_824C1B9C:
	// addi r10,r26,16
	ctx.r10.s64 = r26.s64 + 16;
	// cmplw cr6,r27,r10
	cr6.compare<uint32_t>(r27.u32, ctx.r10.u32, xer);
	// beq cr6,0x824c1be8
	if (cr6.eq) goto loc_824C1BE8;
	// addi r11,r26,24
	r11.s64 = r26.s64 + 24;
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// beq cr6,0x824c1bc4
	if (cr6.eq) goto loc_824C1BC4;
	// lwz r9,28(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 28);
	// mr r27,r11
	r27.u64 = r11.u64;
	// clrlwi. r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824c1bc8
	if (cr0.eq) goto loc_824C1BC8;
loc_824C1BC4:
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
loc_824C1BC8:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1be8
	if (!cr0.eq) goto loc_824C1BE8;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r28,r11,-36
	r28.s64 = r11.s64 + -36;
loc_824C1BE0:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x824c1990
	if (!cr6.eq) goto loc_824C1990;
loc_824C1BE8:
	// rlwinm r11,r26,0,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xFFFFFFFE;
loc_824C1BEC:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c1c04
	if (!cr0.eq) goto loc_824C1C04;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r26,r11
	r26.u64 = r11.u64;
	// bne cr6,0x824c193c
	if (!cr6.eq) goto loc_824C193C;
loc_824C1C04:
	// clrlwi. r11,r25,24
	r11.u64 = r25.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1c18
	if (cr0.eq) goto loc_824C1C18;
	// lwz r10,36(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 36);
	// oris r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 8388608;
	// stw r10,36(r24)
	PPC_STORE_U32(r24.u32 + 36, ctx.r10.u32);
loc_824C1C18:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c190c
	if (!cr6.eq) goto loc_824C190C;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_824C1C28"))) PPC_WEAK_FUNC(sub_824C1C28);
PPC_FUNC_IMPL(__imp__sub_824C1C28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,36(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// rlwinm r11,r11,0,8,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF00;
	// rlwinm. r11,r11,0,23,8
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFF8001FF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beqlr 
	if (cr0.eq) return;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c1d0c
	if (!cr0.eq) goto loc_824C1D0C;
	// mr. r8,r11
	ctx.r8.u64 = r11.u64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c1d0c
	if (cr0.eq) goto loc_824C1D0C;
loc_824C1C4C:
	// lwz r11,20(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1c74
	if (cr0.eq) goto loc_824C1C74;
	// lwz r11,28(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1c6c
	if (cr0.eq) goto loc_824C1C6C;
	// addi r10,r8,32
	ctx.r10.s64 = ctx.r8.s64 + 32;
	// b 0x824c1c78
	goto loc_824C1C78;
loc_824C1C6C:
	// addi r10,r8,24
	ctx.r10.s64 = ctx.r8.s64 + 24;
	// b 0x824c1c78
	goto loc_824C1C78;
loc_824C1C74:
	// addi r10,r8,16
	ctx.r10.s64 = ctx.r8.s64 + 16;
loc_824C1C78:
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824c1ce8
	if (cr0.eq) goto loc_824C1CE8;
	// li r11,0
	r11.s64 = 0;
	// b 0x824c1ce8
	goto loc_824C1CE8;
loc_824C1C8C:
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r9,r11,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r7,r7,0,5,2
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFE7FFFFFF;
	// stw r7,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r7.u32);
	// lwz r11,36(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824c1cb0
	if (!cr0.eq) goto loc_824C1CB0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c1c8c
	if (!cr6.eq) goto loc_824C1C8C;
loc_824C1CB0:
	// addi r9,r8,32
	ctx.r9.s64 = ctx.r8.s64 + 32;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x824c1cf0
	if (cr6.eq) goto loc_824C1CF0;
	// addi r11,r8,24
	r11.s64 = ctx.r8.s64 + 24;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x824c1cd8
	if (cr6.eq) goto loc_824C1CD8;
	// lwz r10,28(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// beq 0x824c1cdc
	if (cr0.eq) goto loc_824C1CDC;
loc_824C1CD8:
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_824C1CDC:
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824c1cf0
	if (!cr0.eq) goto loc_824C1CF0;
loc_824C1CE8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c1c8c
	if (!cr6.eq) goto loc_824C1C8C;
loc_824C1CF0:
	// rlwinm r11,r8,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c1d0c
	if (!cr0.eq) goto loc_824C1D0C;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c1c4c
	if (!cr6.eq) goto loc_824C1C4C;
loc_824C1D0C:
	// b 0x824c11f0
	sub_824C11F0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_824C1D10"))) PPC_WEAK_FUNC(sub_824C1D10);
PPC_FUNC_IMPL(__imp__sub_824C1D10) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C1D14"))) PPC_WEAK_FUNC(sub_824C1D14);
PPC_FUNC_IMPL(__imp__sub_824C1D14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C1D18"))) PPC_WEAK_FUNC(sub_824C1D18);
PPC_FUNC_IMPL(__imp__sub_824C1D18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stfd f31,-144(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -144, f31.u64);
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// li r16,1
	r16.s64 = 1;
	// lwz r11,40(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 40);
	// rlwinm. r11,r11,0,23,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x100;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1d48
	if (!cr0.eq) goto loc_824C1D48;
	// lwz r11,36(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 36);
	// rlwinm. r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c21cc
	if (cr0.eq) goto loc_824C21CC;
loc_824C1D48:
	// lwz r11,4(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c21cc
	if (!cr0.eq) goto loc_824C21CC;
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// b 0x824c21c0
	goto loc_824C21C0;
loc_824C1D5C:
	// addi r27,r28,32
	r27.s64 = r28.s64 + 32;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c21b0
	if (!cr0.eq) goto loc_824C21B0;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r26,r11,-36
	xer.ca = r11.u32 > 35;
	r26.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// beq 0x824c21b0
	if (cr0.eq) goto loc_824C21B0;
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm r10,r11,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r10,10496
	cr6.compare<uint32_t>(ctx.r10.u32, 10496, xer);
	// bne cr6,0x824c21b0
	if (!cr6.eq) goto loc_824C21B0;
	// rlwinm. r11,r11,5,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r29,0
	r29.s64 = 0;
	// beq 0x824c1e8c
	if (cr0.eq) goto loc_824C1E8C;
	// lwz r30,64(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 64);
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x824c1e8c
	if (cr0.eq) goto loc_824C1E8C;
loc_824C1DA4:
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1db8
	if (cr0.eq) goto loc_824C1DB8;
	// li r31,0
	r31.s64 = 0;
	// b 0x824c1dc4
	goto loc_824C1DC4;
loc_824C1DB8:
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r31,r11,-36
	r31.s64 = r11.s64 + -36;
loc_824C1DC4:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r10,r11,5,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c1e74
	if (!cr0.eq) goto loc_824C1E74;
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10624
	cr6.compare<uint32_t>(r11.u32, 10624, xer);
	// bne cr6,0x824c1e84
	if (!cr6.eq) goto loc_824C1E84;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,83
	ctx.r4.s64 = 83;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r31,-16
	r11.s64 = r31.s64 + -16;
	// lwz r10,580(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 580);
	// lwzx r11,r3,r11
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// rlwinm r11,r11,1,21,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x7C0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x824c1e2c
	goto loc_824C1E2C;
loc_824C1E08:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824c1e1c
	goto loc_824C1E1C;
loc_824C1E10:
	// lwz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r9,r9,0,9,9
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824c1e28
	if (cr0.eq) goto loc_824C1E28;
loc_824C1E1C:
	// lwz r10,80(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x824c1e10
	if (!cr0.eq) goto loc_824C1E10;
loc_824C1E28:
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
loc_824C1E2C:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r10,48(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r10,r10,10,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c1e08
	if (!cr0.eq) goto loc_824C1E08;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c23bc
	if (!cr0.eq) goto loc_824C23BC;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-36
	xer.ca = r11.u32 > 35;
	r11.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c23bc
	if (cr0.eq) goto loc_824C23BC;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r11,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r10,10752
	cr6.compare<uint32_t>(ctx.r10.u32, 10752, xer);
	// bne cr6,0x824c23bc
	if (!cr6.eq) goto loc_824C23BC;
	// rlwinm. r11,r11,5,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1e84
	if (cr0.eq) goto loc_824C1E84;
loc_824C1E74:
	// mr r29,r30
	r29.u64 = r30.u64;
	// lwz r30,64(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 64);
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// bne 0x824c1da4
	if (!cr0.eq) goto loc_824C1DA4;
loc_824C1E84:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x824c1ee4
	if (!cr6.eq) goto loc_824C1EE4;
loc_824C1E8C:
	// lwz r31,64(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 64);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824c21b0
	if (cr0.eq) goto loc_824C21B0;
loc_824C1E98:
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1eac
	if (cr0.eq) goto loc_824C1EAC;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824c1eb8
	goto loc_824C1EB8;
loc_824C1EAC:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r3,r11,-36
	ctx.r3.s64 = r11.s64 + -36;
loc_824C1EB8:
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// beq cr6,0x824c1ed8
	if (cr6.eq) goto loc_824C1ED8;
	// lwz r31,64(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x824c1e98
	if (!cr0.eq) goto loc_824C1E98;
	// b 0x824c1edc
	goto loc_824C1EDC;
loc_824C1ED8:
	// mr r29,r31
	r29.u64 = r31.u64;
loc_824C1EDC:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824c21b0
	if (cr6.eq) goto loc_824C21B0;
loc_824C1EE4:
	// lwz r11,4(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c23c8
	if (!cr0.eq) goto loc_824C23C8;
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-4
	xer.ca = r11.u32 > 3;
	r31.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824c23c8
	if (cr0.eq) goto loc_824C23C8;
	// lwz r11,48(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r10,r11,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// slw r9,r16,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r9.u8 & 0x3F));
loc_824C1F14:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// and r11,r11,r9
	r11.u64 = r11.u64 & ctx.r9.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c1f64
	if (cr0.eq) goto loc_824C1F64;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c1f64
	if (!cr0.eq) goto loc_824C1F64;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-36
	xer.ca = r11.u32 > 35;
	r11.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c1f64
	if (cr0.eq) goto loc_824C1F64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10496
	cr6.compare<uint32_t>(r11.u32, 10496, xer);
	// beq cr6,0x824c1f80
	if (cr6.eq) goto loc_824C1F80;
loc_824C1F64:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r8,r11,31
	ctx.r8.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x824c23c8
	if (!cr0.eq) goto loc_824C23C8;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-4
	xer.ca = r11.u32 > 3;
	r31.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x824c1f14
	if (!cr0.eq) goto loc_824C1F14;
loc_824C1F80:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c23c8
	if (cr6.eq) goto loc_824C23C8;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c20c4
	if (!cr0.eq) goto loc_824C20C4;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// mr r30,r31
	r30.u64 = r31.u64;
	// bl 0x824b09b8
	sub_824B09B8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// lwz r8,48(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// oris r8,r8,32768
	ctx.r8.u64 = ctx.r8.u64 | 2147483648;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r8,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x8255e6b0
	sub_8255E6B0(ctx, base);
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,82
	ctx.r6.s64 = 82;
	// addi r5,r31,32
	ctx.r5.s64 = r31.s64 + 32;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// addi r5,r30,32
	ctx.r5.s64 = r30.s64 + 32;
	// lwz r11,4(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c2020
	if (cr0.eq) goto loc_824C2020;
	// li r29,0
	r29.s64 = 0;
	// b 0x824c202c
	goto loc_824C202C;
loc_824C2020:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r29,r11,-36
	r29.s64 = r11.s64 + -36;
loc_824C202C:
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,116
	ctx.r6.s64 = 116;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// oris r11,r11,512
	r11.u64 = r11.u64 | 33554432;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
loc_824C2054:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c20a8
	if (cr0.eq) goto loc_824C20A8;
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// b 0x824c206c
	goto loc_824C206C;
loc_824C2068:
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
loc_824C206C:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824c2068
	if (!cr6.eq) goto loc_824C2068;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r3,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r3.u32);
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824c2054
	if (!cr6.eq) goto loc_824C2054;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// b 0x824c2054
	goto loc_824C2054;
loc_824C20A8:
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8251cbe0
	sub_8251CBE0(ctx, base);
loc_824C20C4:
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
loc_824C20C8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c20ec
	if (cr6.eq) goto loc_824C20EC;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// beq cr6,0x824c20e4
	if (cr6.eq) goto loc_824C20E4;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824c20c8
	goto loc_824C20C8;
loc_824C20E4:
	// mr r11,r16
	r11.u64 = r16.u64;
	// b 0x824c20f0
	goto loc_824C20F0;
loc_824C20EC:
	// li r11,0
	r11.s64 = 0;
loc_824C20F0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c21b0
	if (!cr0.eq) goto loc_824C21B0;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8255e6b0
	sub_8255E6B0(ctx, base);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x824c2194
	if (cr6.eq) goto loc_824C2194;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,116
	ctx.r6.s64 = 116;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// oris r11,r11,512
	r11.u64 = r11.u64 | 33554432;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
loc_824C2140:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c2194
	if (cr0.eq) goto loc_824C2194;
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// b 0x824c2158
	goto loc_824C2158;
loc_824C2154:
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
loc_824C2158:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824c2154
	if (!cr6.eq) goto loc_824C2154;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r3,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r3.u32);
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824c2140
	if (!cr6.eq) goto loc_824C2140;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// b 0x824c2140
	goto loc_824C2140;
loc_824C2194:
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8251cbe0
	sub_8251CBE0(ctx, base);
loc_824C21B0:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c21cc
	if (!cr0.eq) goto loc_824C21CC;
loc_824C21C0:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r28,r11,-4
	xer.ca = r11.u32 > 3;
	r28.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// bne 0x824c1d5c
	if (!cr0.eq) goto loc_824C1D5C;
loc_824C21CC:
	// lwz r11,36(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 36);
	// andi. r11,r11,260
	r11.u64 = r11.u64 & 260;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c3b3c
	if (cr0.eq) goto loc_824C3B3C;
	// lwz r30,4(r20)
	r30.u64 = PPC_LOAD_U32(r20.u32 + 4);
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c240c
	if (!cr0.eq) goto loc_824C240C;
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x824c240c
	if (cr0.eq) goto loc_824C240C;
loc_824C21F0:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r11,r11,13,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c23f0
	if (cr0.eq) goto loc_824C23F0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bd1d8
	sub_824BD1D8(ctx, base);
	// mr. r26,r3
	r26.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// beq 0x824c23f0
	if (cr0.eq) goto loc_824C23F0;
	// lwz r11,40(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// lwz r9,48(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// rlwinm r4,r9,1,12,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFE;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r5,r4,1
	ctx.r5.s64 = ctx.r4.s64 + 1;
	// clrlwi r10,r8,13
	ctx.r10.u64 = ctx.r8.u32 & 0x7FFFF;
	// addi r9,r10,31
	ctx.r9.s64 = ctx.r10.s64 + 31;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// rlwinm r10,r9,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// bl 0x824be808
	sub_824BE808(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824c23f0
	if (cr0.eq) goto loc_824C23F0;
	// cmplwi cr6,r3,3
	cr6.compare<uint32_t>(ctx.r3.u32, 3, xer);
	// beq cr6,0x824c23f0
	if (cr6.eq) goto loc_824C23F0;
	// lwz r11,40(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 40);
	// mr r29,r30
	r29.u64 = r30.u64;
	// ori r11,r11,32768
	r11.u64 = r11.u64 | 32768;
	// stw r11,40(r20)
	PPC_STORE_U32(r20.u32 + 40, r11.u32);
loc_824C2274:
	// cmplw cr6,r29,r30
	cr6.compare<uint32_t>(r29.u32, r30.u32, xer);
	// beq cr6,0x824c22cc
	if (cr6.eq) goto loc_824C22CC;
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// lwz r10,48(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi r8,r10,27
	ctx.r8.u64 = ctx.r10.u32 & 0x1F;
	// rlwinm r10,r10,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r9,r9,13
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFF;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,31
	ctx.r9.s64 = ctx.r9.s64 + 31;
	// rlwinm r10,r9,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFC;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// slw r10,r16,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r8.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c23f0
	if (cr0.eq) goto loc_824C23F0;
loc_824C22CC:
	// cmplw cr6,r29,r26
	cr6.compare<uint32_t>(r29.u32, r26.u32, xer);
	// beq cr6,0x824c23d4
	if (cr6.eq) goto loc_824C23D4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824baf98
	sub_824BAF98(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824c23d4
	if (cr0.eq) goto loc_824C23D4;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10752
	cr6.compare<uint32_t>(r11.u32, 10752, xer);
	// beq cr6,0x824c23d4
	if (cr6.eq) goto loc_824C23D4;
	// addi r27,r29,12
	r27.s64 = r29.s64 + 12;
loc_824C22FC:
	// lwz r28,0(r27)
	r28.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824c23d4
	if (cr6.eq) goto loc_824C23D4;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmplw cr6,r31,r26
	cr6.compare<uint32_t>(r31.u32, r26.u32, xer);
	// beq cr6,0x824c23a8
	if (cr6.eq) goto loc_824C23A8;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r16,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c23a8
	if (!cr0.eq) goto loc_824C23A8;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c236c
	if (cr0.eq) goto loc_824C236C;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r11,r16
	r11.u64 = r16.u64;
	// bne cr6,0x824c2370
	if (!cr6.eq) goto loc_824C2370;
loc_824C236C:
	// li r11,0
	r11.s64 = 0;
loc_824C2370:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c23a8
	if (cr0.eq) goto loc_824C23A8;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8255e890
	sub_8255E890(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c23a8
	if (!cr0.eq) goto loc_824C23A8;
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8251aa40
	sub_8251AA40(ctx, base);
loc_824C23A8:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// bne cr6,0x824c22fc
	if (!cr6.eq) goto loc_824C22FC;
	// addi r27,r28,8
	r27.s64 = r28.s64 + 8;
	// b 0x824c22fc
	goto loc_824C22FC;
loc_824C23BC:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C23C8:
	// li r4,3535
	ctx.r4.s64 = 3535;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C23D4:
	// rlwinm r11,r29,0,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c23f0
	if (!cr0.eq) goto loc_824C23F0;
	// mr r29,r11
	r29.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c2274
	if (!cr6.eq) goto loc_824C2274;
loc_824C23F0:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c240c
	if (!cr0.eq) goto loc_824C240C;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c21f0
	if (!cr6.eq) goto loc_824C21F0;
loc_824C240C:
	// lwz r11,4(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 4);
	// lwz r17,88(r20)
	r17.u64 = PPC_LOAD_U32(r20.u32 + 88);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c370c
	if (!cr0.eq) goto loc_824C370C;
	// mr. r19,r11
	r19.u64 = r11.u64;
	cr0.compare<int32_t>(r19.s32, 0, xer);
	// beq 0x824c370c
	if (cr0.eq) goto loc_824C370C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f31,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f31.f64 = double(temp.f32);
loc_824C242C:
	// lwz r11,40(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 40);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r11,r11,13,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c36f0
	if (cr0.eq) goto loc_824C36F0;
	// lwz r11,36(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c37b4
	if (!cr0.eq) goto loc_824C37B4;
	// lwz r11,32(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r25,r11,-36
	xer.ca = r11.u32 > 35;
	r25.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r25.s32, 0, xer);
	// beq 0x824c37b4
	if (cr0.eq) goto loc_824C37B4;
	// lwz r31,8(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// rlwinm r4,r31,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r4,85
	cr6.compare<uint32_t>(ctx.r4.u32, 85, xer);
	// beq cr6,0x824c2494
	if (cr6.eq) goto loc_824C2494;
	// cmplwi cr6,r4,86
	cr6.compare<uint32_t>(ctx.r4.u32, 86, xer);
	// beq cr6,0x824c2494
	if (cr6.eq) goto loc_824C2494;
	// cmplwi cr6,r4,88
	cr6.compare<uint32_t>(ctx.r4.u32, 88, xer);
	// beq cr6,0x824c2494
	if (cr6.eq) goto loc_824C2494;
	// cmplwi cr6,r4,89
	cr6.compare<uint32_t>(ctx.r4.u32, 89, xer);
	// beq cr6,0x824c2494
	if (cr6.eq) goto loc_824C2494;
	// cmplwi cr6,r4,83
	cr6.compare<uint32_t>(ctx.r4.u32, 83, xer);
	// beq cr6,0x824c2494
	if (cr6.eq) goto loc_824C2494;
	// cmplwi cr6,r4,84
	cr6.compare<uint32_t>(ctx.r4.u32, 84, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824c2498
	if (!cr6.eq) goto loc_824C2498;
loc_824C2494:
	// mr r11,r16
	r11.u64 = r16.u64;
loc_824C2498:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c37b4
	if (cr0.eq) goto loc_824C37B4;
	// cmplwi cr6,r4,83
	cr6.compare<uint32_t>(ctx.r4.u32, 83, xer);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// beq cr6,0x824c3160
	if (cr6.eq) goto loc_824C3160;
	// rlwinm r6,r31,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 18) & 0x7;
	// rlwinm r5,r31,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 13) & 0x7;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r25,-12
	r11.s64 = r25.s64 + -12;
	// add r21,r3,r11
	r21.u64 = ctx.r3.u64 + r11.u64;
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// clrlwi. r10,r11,30
	ctx.r10.u64 = r11.u32 & 0x3;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c24d4
	if (cr0.eq) goto loc_824C24D4;
	// li r22,7
	r22.s64 = 7;
	// b 0x824c24f0
	goto loc_824C24F0;
loc_824C24D4:
	// rlwinm r22,r11,30,28,31
	r22.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0xF;
	// cmpwi cr6,r22,2
	cr6.compare<int32_t>(r22.s32, 2, xer);
	// beq cr6,0x824c2504
	if (cr6.eq) goto loc_824C2504;
	// cmpwi cr6,r22,3
	cr6.compare<int32_t>(r22.s32, 3, xer);
	// beq cr6,0x824c2504
	if (cr6.eq) goto loc_824C2504;
	// cmpwi cr6,r22,4
	cr6.compare<int32_t>(r22.s32, 4, xer);
	// beq cr6,0x824c2504
	if (cr6.eq) goto loc_824C2504;
loc_824C24F0:
	// lwz r11,76(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c2504
	if (!cr0.eq) goto loc_824C2504;
	// rlwinm. r11,r31,5,31,31
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c36f0
	if (cr0.eq) goto loc_824C36F0;
loc_824C2504:
	// lwz r11,48(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 48);
	// rlwinm r18,r19,0,0,30
	r18.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r23,r11,13
	r23.u64 = r11.u32 & 0x7FFFF;
loc_824C2510:
	// lwz r31,4(r18)
	r31.u64 = PPC_LOAD_U32(r18.u32 + 4);
	// li r24,0
	r24.s64 = 0;
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c2954
	if (!cr0.eq) goto loc_824C2954;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c2954
	if (cr6.eq) goto loc_824C2954;
	// lwz r11,48(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r10,r11,27
	ctx.r10.u64 = r11.u32 & 0x1F;
	// rlwinm r30,r11,29,3,29
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// slw r29,r16,r10
	r29.u64 = ctx.r10.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r10.u8 & 0x3F));
loc_824C253C:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// rlwinm r10,r10,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFC;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// and r11,r11,r29
	r11.u64 = r11.u64 & r29.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c25c4
	if (cr0.eq) goto loc_824C25C4;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// rlwinm r4,r23,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// addi r5,r4,1
	ctx.r5.s64 = ctx.r4.s64 + 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// addi r9,r10,31
	ctx.r9.s64 = ctx.r10.s64 + 31;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r10.u32);
	// rlwinm r10,r9,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// bl 0x824be808
	sub_824BE808(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824c25c4
	if (cr0.eq) goto loc_824C25C4;
	// cmplwi cr6,r3,3
	cr6.compare<uint32_t>(ctx.r3.u32, 3, xer);
	// beq cr6,0x824c25c4
	if (cr6.eq) goto loc_824C25C4;
	// or r24,r3,r24
	r24.u64 = ctx.r3.u64 | r24.u64;
loc_824C25C4:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c25dc
	if (!cr0.eq) goto loc_824C25DC;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c253c
	if (!cr6.eq) goto loc_824C253C;
loc_824C25DC:
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x824c2954
	if (cr6.eq) goto loc_824C2954;
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// rlwinm. r11,r11,15,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3768
	if (cr0.eq) goto loc_824C3768;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// lwz r26,12(r3)
	r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,15232
	cr6.compare<uint32_t>(r11.u32, 15232, xer);
	// beq cr6,0x824c36f0
	if (cr6.eq) goto loc_824C36F0;
	// cmpwi cr6,r22,2
	cr6.compare<int32_t>(r22.s32, 2, xer);
	// bne cr6,0x824c28e0
	if (!cr6.eq) goto loc_824C28E0;
	// cmplwi cr6,r24,3
	cr6.compare<uint32_t>(r24.u32, 3, xer);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// beq cr6,0x824c2738
	if (cr6.eq) goto loc_824C2738;
	// addi r8,r1,92
	ctx.r8.s64 = ctx.r1.s64 + 92;
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r6,r1,82
	ctx.r6.s64 = ctx.r1.s64 + 82;
	// bl 0x824bca60
	sub_824BCA60(ctx, base);
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// bne cr6,0x824c2898
	if (!cr6.eq) goto loc_824C2898;
	// lwz r29,88(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lwz r30,92(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// addi r7,r1,81
	ctx.r7.s64 = ctx.r1.s64 + 81;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bb278
	sub_824BB278(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c2898
	if (cr0.eq) goto loc_824C2898;
	// lbz r11,80(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c2898
	if (!cr0.eq) goto loc_824C2898;
	// lbz r11,81(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c2898
	if (!cr0.eq) goto loc_824C2898;
	// lwz r31,96(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c2898
	if (cr6.eq) goto loc_824C2898;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// bne cr6,0x824c2898
	if (!cr6.eq) goto loc_824C2898;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c26b0
	if (cr0.eq) goto loc_824C26B0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bcd50
	sub_824BCD50(ctx, base);
loc_824C26B0:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824baac8
	sub_824BAAC8(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x824c26e0
	if (!cr0.eq) goto loc_824C26E0;
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8251aa40
	sub_8251AA40(ctx, base);
loc_824C26E0:
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// lbz r6,82(r1)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r1.u32 + 82);
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824c0fa8
	sub_824C0FA8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c2898
	if (cr0.eq) goto loc_824C2898;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x8255e540
	sub_8255E540(ctx, base);
loc_824C2718:
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8251cbe0
	sub_8251CBE0(ctx, base);
	// b 0x824c36f0
	goto loc_824C36F0;
loc_824C2738:
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// addi r7,r1,104
	ctx.r7.s64 = ctx.r1.s64 + 104;
	// addi r6,r1,87
	ctx.r6.s64 = ctx.r1.s64 + 87;
	// bl 0x824bca60
	sub_824BCA60(ctx, base);
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// bne cr6,0x824c2898
	if (!cr6.eq) goto loc_824C2898;
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r8,r1,108
	ctx.r8.s64 = ctx.r1.s64 + 108;
	// addi r7,r1,85
	ctx.r7.s64 = ctx.r1.s64 + 85;
	// addi r6,r1,83
	ctx.r6.s64 = ctx.r1.s64 + 83;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bb278
	sub_824BB278(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c2898
	if (cr0.eq) goto loc_824C2898;
	// lwz r28,104(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// addi r7,r1,86
	ctx.r7.s64 = ctx.r1.s64 + 86;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bb278
	sub_824BB278(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c2898
	if (cr0.eq) goto loc_824C2898;
	// lbz r11,83(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 83);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c2898
	if (!cr0.eq) goto loc_824C2898;
	// lbz r11,84(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 84);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c2898
	if (!cr0.eq) goto loc_824C2898;
	// lbz r11,85(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 85);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c2898
	if (!cr0.eq) goto loc_824C2898;
	// lbz r11,86(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 86);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c2898
	if (!cr0.eq) goto loc_824C2898;
	// lwz r31,108(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x824c2898
	if (!cr6.eq) goto loc_824C2898;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c27f8
	if (cr0.eq) goto loc_824C27F8;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bcd50
	sub_824BCD50(ctx, base);
loc_824C27F8:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824baac8
	sub_824BAAC8(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824baac8
	sub_824BAAC8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x824c2848
	if (!cr6.eq) goto loc_824C2848;
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8251aa40
	sub_8251AA40(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
loc_824C2848:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824c2870
	if (!cr6.eq) goto loc_824C2870;
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8251aa40
	sub_8251AA40(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_824C2870:
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// lbz r6,87(r1)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r1.u32 + 87);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824c0fa8
	sub_824C0FA8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c29d0
	if (!cr0.eq) goto loc_824C29D0;
loc_824C2898:
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// clrlwi. r10,r11,30
	ctx.r10.u64 = r11.u32 & 0x3;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c28ac
	if (cr0.eq) goto loc_824C28AC;
	// mr r11,r16
	r11.u64 = r16.u64;
	// b 0x824c28b4
	goto loc_824C28B4;
loc_824C28AC:
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r11,r11,25,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1;
loc_824C28B4:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c28c8
	if (cr0.eq) goto loc_824C28C8;
	// li r4,3588
	ctx.r4.s64 = 3588;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496da0
	sub_82496DA0(ctx, base);
loc_824C28C8:
	// lwz r11,76(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c28e0
	if (!cr0.eq) goto loc_824C28E0;
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// rlwinm. r11,r11,5,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c36f0
	if (cr0.eq) goto loc_824C36F0;
loc_824C28E0:
	// cmplwi cr6,r24,3
	cr6.compare<uint32_t>(r24.u32, 3, xer);
	// beq cr6,0x824c2a04
	if (cr6.eq) goto loc_824C2A04;
	// lwz r10,8(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 8);
	// rlwinm r11,r10,31,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x1;
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// slw r11,r16,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r16.u32 << (r11.u8 & 0x3F));
	// cmplw cr6,r24,r11
	cr6.compare<uint32_t>(r24.u32, r11.u32, xer);
	// beq cr6,0x824c2a04
	if (cr6.eq) goto loc_824C2A04;
	// lwz r9,12(r19)
	ctx.r9.u64 = PPC_LOAD_U32(r19.u32 + 12);
loc_824C2904:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824c2924
	if (cr6.eq) goto loc_824C2924;
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r8,0(r21)
	ctx.r8.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x824c2928
	if (!cr6.eq) goto loc_824C2928;
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// b 0x824c2904
	goto loc_824C2904;
loc_824C2924:
	// li r11,0
	r11.s64 = 0;
loc_824C2928:
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// rlwimi r9,r10,0,31,29
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFFFFFFFFFFFD) | (ctx.r9.u64 & 0x2);
	// stw r9,8(r21)
	PPC_STORE_U32(r21.u32 + 8, ctx.r9.u32);
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// not r10,r11
	ctx.r10.u64 = ~r11.u64;
	// rlwimi r10,r11,0,14,12
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFFFFFFBFFFF) | (ctx.r10.u64 & 0x40000);
	// stw r10,8(r25)
	PPC_STORE_U32(r25.u32 + 8, ctx.r10.u32);
	// bl 0x8251cbe0
	sub_8251CBE0(ctx, base);
	// b 0x824c2510
	goto loc_824C2510;
loc_824C2954:
	// lwz r11,12(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 12);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c2970
	if (cr0.eq) goto loc_824C2970;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r11,r16
	r11.u64 = r16.u64;
	// bne cr6,0x824c2974
	if (!cr6.eq) goto loc_824C2974;
loc_824C2970:
	// li r11,0
	r11.s64 = 0;
loc_824C2974:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c2994
	if (cr0.eq) goto loc_824C2994;
	// lwz r11,76(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c375c
	if (!cr0.eq) goto loc_824C375C;
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// rlwinm. r11,r11,5,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c375c
	if (!cr0.eq) goto loc_824C375C;
loc_824C2994:
	// cmpwi cr6,r22,2
	cr6.compare<int32_t>(r22.s32, 2, xer);
	// bne cr6,0x824c36f0
	if (!cr6.eq) goto loc_824C36F0;
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// clrlwi. r10,r11,30
	ctx.r10.u64 = r11.u32 & 0x3;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c29b0
	if (cr0.eq) goto loc_824C29B0;
	// mr r11,r16
	r11.u64 = r16.u64;
	// b 0x824c29b8
	goto loc_824C29B8;
loc_824C29B0:
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r11,r11,25,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1;
loc_824C29B8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c36f0
	if (cr0.eq) goto loc_824C36F0;
	// li r4,3588
	ctx.r4.s64 = 3588;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496da0
	sub_82496DA0(ctx, base);
	// b 0x824c36f0
	goto loc_824C36F0;
loc_824C29D0:
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x8255e540
	sub_8255E540(ctx, base);
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8255e540
	sub_8255E540(ctx, base);
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8255e6b0
	sub_8255E6B0(ctx, base);
	// b 0x824c2718
	goto loc_824C2718;
loc_824C2A04:
	// lwz r11,104(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 104);
	// addi r9,r1,116
	ctx.r9.s64 = ctx.r1.s64 + 116;
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// stw r29,104(r20)
	PPC_STORE_U32(r20.u32 + 104, r29.u32);
	// lwz r11,8(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 8);
	// rlwinm r11,r11,31,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x1;
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// slw r5,r16,r11
	ctx.r5.u64 = r11.u8 & 0x20 ? 0 : (r16.u32 << (r11.u8 & 0x3F));
	// bl 0x824bec20
	sub_824BEC20(ctx, base);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplw cr6,r11,r19
	cr6.compare<uint32_t>(r11.u32, r19.u32, xer);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// beq cr6,0x824c3774
	if (cr6.eq) goto loc_824C3774;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// bl 0x824bd1d8
	sub_824BD1D8(ctx, base);
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// rlwinm. r11,r11,14,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 14) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c2a74
	if (cr0.eq) goto loc_824C2A74;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bb118
	sub_824BB118(ctx, base);
loc_824C2A74:
	// lwz r11,48(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 48);
	// clrlwi. r10,r31,24
	ctx.r10.u64 = r31.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// oris r11,r11,3072
	r11.u64 = r11.u64 | 201326592;
	// stw r11,48(r19)
	PPC_STORE_U32(r19.u32 + 48, r11.u32);
	// bne 0x824c2a94
	if (!cr0.eq) goto loc_824C2A94;
	// lwz r11,76(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3150
	if (cr0.eq) goto loc_824C3150;
loc_824C2A94:
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bb700
	sub_824BB700(ctx, base);
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r11,r9,31
	r11.u64 = ctx.r9.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c2b68
	if (!cr0.eq) goto loc_824C2B68;
	// rlwinm r11,r9,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x824c2b60
	goto loc_824C2B60;
loc_824C2ABC:
	// cmplw cr6,r4,r19
	cr6.compare<uint32_t>(ctx.r4.u32, r19.u32, xer);
	// beq cr6,0x824c2b68
	if (cr6.eq) goto loc_824C2B68;
	// lwz r11,48(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// cmplw cr6,r11,r17
	cr6.compare<uint32_t>(r11.u32, r17.u32, xer);
	// bge cr6,0x824c2b4c
	if (!cr6.lt) goto loc_824C2B4C;
	// lwz r10,12(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
loc_824C2AD8:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c2b4c
	if (cr6.eq) goto loc_824C2B4C;
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c2b34
	if (!cr0.eq) goto loc_824C2B34;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r8,40(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r7,r11,27
	ctx.r7.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r8
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// slw r8,r16,r7
	ctx.r8.u64 = ctx.r7.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r7.u8 & 0x3F));
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c2b74
	if (cr0.eq) goto loc_824C2B74;
loc_824C2B34:
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// cmplw cr6,r11,r17
	cr6.compare<uint32_t>(r11.u32, r17.u32, xer);
	// bge cr6,0x824c2b74
	if (!cr6.lt) goto loc_824C2B74;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// b 0x824c2ad8
	goto loc_824C2AD8;
loc_824C2B4C:
	// rlwinm r11,r4,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c2b68
	if (!cr0.eq) goto loc_824C2B68;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
loc_824C2B60:
	// addic. r4,r11,-4
	xer.ca = r11.u32 > 3;
	ctx.r4.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne 0x824c2abc
	if (!cr0.eq) goto loc_824C2ABC;
loc_824C2B68:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C2B74:
	// clrlwi. r11,r9,31
	r11.u64 = ctx.r9.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c2ba4
	if (!cr0.eq) goto loc_824C2BA4;
	// rlwinm r11,r9,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x824c2bb0
	goto loc_824C2BB0;
loc_824C2B84:
	// lwz r11,48(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// cmplw cr6,r11,r17
	cr6.compare<uint32_t>(r11.u32, r17.u32, xer);
	// blt cr6,0x824c2bbc
	if (cr6.lt) goto loc_824C2BBC;
	// rlwinm r11,r6,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c2bac
	if (cr0.eq) goto loc_824C2BAC;
loc_824C2BA4:
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x824c2bb4
	goto loc_824C2BB4;
loc_824C2BAC:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
loc_824C2BB0:
	// addi r6,r11,-4
	ctx.r6.s64 = r11.s64 + -4;
loc_824C2BB4:
	// cmplw cr6,r6,r4
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r4.u32, xer);
	// bne cr6,0x824c2b84
	if (!cr6.eq) goto loc_824C2B84;
loc_824C2BBC:
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// li r7,63
	ctx.r7.s64 = 63;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bd678
	sub_824BD678(ctx, base);
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// lwz r10,48(r19)
	ctx.r10.u64 = PPC_LOAD_U32(r19.u32 + 48);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi r8,r10,27
	ctx.r8.u64 = ctx.r10.u32 & 0x1F;
	// rlwinm r10,r10,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r9,r9,13
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFF;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,31
	ctx.r9.s64 = ctx.r9.s64 + 31;
	// rlwinm r10,r9,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFC;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// slw r10,r16,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r8.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c2d68
	if (cr0.eq) goto loc_824C2D68;
	// lwz r11,44(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 44);
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c2c3c
	if (!cr0.eq) goto loc_824C2C3C;
	// stw r29,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r29.u32);
loc_824C2C3C:
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c2c54
	if (cr0.eq) goto loc_824C2C54;
	// stw r29,44(r25)
	PPC_STORE_U32(r25.u32 + 44, r29.u32);
loc_824C2C54:
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// mr r11,r25
	r11.u64 = r25.u64;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// stw r10,84(r25)
	PPC_STORE_U32(r25.u32 + 84, ctx.r10.u32);
	// beq cr6,0x824c2ca4
	if (cr6.eq) goto loc_824C2CA4;
loc_824C2C68:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// lwz r9,76(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// oris r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 8388608;
	// stw r19,112(r11)
	PPC_STORE_U32(r11.u32 + 112, r19.u32);
	// oris r9,r9,192
	ctx.r9.u64 = ctx.r9.u64 | 12582912;
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// stw r9,76(r11)
	PPC_STORE_U32(r11.u32 + 76, ctx.r9.u32);
	// beq cr6,0x824c2ca4
	if (cr6.eq) goto loc_824C2CA4;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c2ca4
	if (!cr0.eq) goto loc_824C2CA4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c2c68
	if (!cr6.eq) goto loc_824C2C68;
loc_824C2CA4:
	// lwz r31,4(r18)
	r31.u64 = PPC_LOAD_U32(r18.u32 + 4);
	// b 0x824c2d58
	goto loc_824C2D58;
loc_824C2CAC:
	// cmplw cr6,r31,r25
	cr6.compare<uint32_t>(r31.u32, r25.u32, xer);
	// beq cr6,0x824c2d68
	if (cr6.eq) goto loc_824C2D68;
	// lwz r30,12(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 12);
loc_824C2CB8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824c2d50
	if (cr6.eq) goto loc_824C2D50;
	// lwz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,44(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 44);
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c2d48
	if (!cr0.eq) goto loc_824C2D48;
	// cmplw cr6,r7,r25
	cr6.compare<uint32_t>(ctx.r7.u32, r25.u32, xer);
	// bne cr6,0x824c2cf8
	if (!cr6.eq) goto loc_824C2CF8;
	// lwz r11,12(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 12);
	// li r7,0
	ctx.r7.s64 = 0;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c2d34
	if (cr0.eq) goto loc_824C2D34;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x824c2d34
	goto loc_824C2D34;
loc_824C2CF8:
	// lwz r11,48(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r16,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c2d48
	if (!cr0.eq) goto loc_824C2D48;
loc_824C2D34:
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bdbb8
	sub_824BDBB8(ctx, base);
loc_824C2D48:
	// lwz r30,8(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// b 0x824c2cb8
	goto loc_824C2CB8;
loc_824C2D50:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
loc_824C2D58:
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c2d68
	if (!cr0.eq) goto loc_824C2D68;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c2cac
	if (!cr6.eq) goto loc_824C2CAC;
loc_824C2D68:
	// lwz r10,8(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 8);
	// li r27,0
	r27.s64 = 0;
	// lwz r11,4(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 4);
	// rlwinm r10,r10,31,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x1;
	// subfic r10,r10,1
	xer.ca = ctx.r10.u32 <= 1;
	ctx.r10.s64 = 1 - ctx.r10.s64;
	// slw r26,r16,r10
	r26.u64 = ctx.r10.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r10.u8 & 0x3F));
	// b 0x824c2df8
	goto loc_824C2DF8;
loc_824C2D84:
	// lwz r9,76(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// rlwinm. r10,r9,10,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c2df0
	if (cr0.eq) goto loc_824C2DF0;
	// lwz r10,112(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 112);
	// lwz r8,48(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r8,r8,10,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c2dcc
	if (cr0.eq) goto loc_824C2DCC;
	// b 0x824c2db0
	goto loc_824C2DB0;
loc_824C2DA4:
	// lwz r8,48(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r8,r8,0,9,9
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c2dbc
	if (cr0.eq) goto loc_824C2DBC;
loc_824C2DB0:
	// lwz r10,80(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x824c2da4
	if (!cr0.eq) goto loc_824C2DA4;
loc_824C2DBC:
	// lwz r8,48(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r8,r8,10,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x824c2db0
	if (!cr0.eq) goto loc_824C2DB0;
	// stw r10,112(r11)
	PPC_STORE_U32(r11.u32 + 112, ctx.r10.u32);
loc_824C2DCC:
	// lwz r10,112(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 112);
	// cmplw cr6,r10,r19
	cr6.compare<uint32_t>(ctx.r10.u32, r19.u32, xer);
	// bne cr6,0x824c2df0
	if (!cr6.eq) goto loc_824C2DF0;
	// oris r10,r9,192
	ctx.r10.u64 = ctx.r9.u64 | 12582912;
	// stw r19,112(r11)
	PPC_STORE_U32(r11.u32 + 112, r19.u32);
	// stw r10,76(r11)
	PPC_STORE_U32(r11.u32 + 76, ctx.r10.u32);
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// oris r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 8388608;
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
loc_824C2DF0:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
loc_824C2DF8:
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c2e08
	if (!cr0.eq) goto loc_824C2E08;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c2d84
	if (!cr6.eq) goto loc_824C2D84;
loc_824C2E08:
	// lwz r11,4(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c3150
	if (!cr0.eq) goto loc_824C3150;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c3150
	if (cr6.eq) goto loc_824C3150;
loc_824C2E20:
	// cmplw cr6,r28,r25
	cr6.compare<uint32_t>(r28.u32, r25.u32, xer);
	// beq cr6,0x824c313c
	if (cr6.eq) goto loc_824C313C;
	// lwz r11,40(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// addi r3,r1,136
	ctx.r3.s64 = ctx.r1.s64 + 136;
	// lwz r9,48(r19)
	ctx.r9.u64 = PPC_LOAD_U32(r19.u32 + 48);
	// rlwinm r4,r9,1,12,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFE;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r5,r4,1
	ctx.r5.s64 = ctx.r4.s64 + 1;
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// addi r9,r10,31
	ctx.r9.s64 = ctx.r10.s64 + 31;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,29,3,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// stw r10,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r10.u32);
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// bl 0x824be808
	sub_824BE808(ctx, base);
	// cmplw cr6,r26,r3
	cr6.compare<uint32_t>(r26.u32, ctx.r3.u32, xer);
	// beq cr6,0x824c3120
	if (cr6.eq) goto loc_824C3120;
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c2e88
	if (cr0.eq) goto loc_824C2E88;
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x824c2e90
	goto loc_824C2E90;
loc_824C2E88:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r6,r11,-4
	ctx.r6.s64 = r11.s64 + -4;
loc_824C2E90:
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// li r7,62
	ctx.r7.s64 = 62;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bd678
	sub_824BD678(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c3100
	if (!cr0.eq) goto loc_824C3100;
	// mr. r31,r11
	r31.u64 = r11.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824c3100
	if (cr0.eq) goto loc_824C3100;
loc_824C2EC8:
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
loc_824C2ED0:
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x824c2f38
	if (cr6.eq) goto loc_824C2F38;
	// lwz r10,12(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r11,48(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// lwz r10,24(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// rlwinm r9,r11,27,5,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r6,r11,27
	ctx.r6.u64 = r11.u32 & 0x1F;
	// lwz r11,40(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// addi r10,r9,1
	ctx.r10.s64 = ctx.r9.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// rlwinm r10,r10,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// slw r6,r16,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r6.u8 & 0x3F));
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c3088
	if (!cr0.eq) goto loc_824C3088;
	// lwz r8,4(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x824c2ed0
	goto loc_824C2ED0;
loc_824C2F38:
	// lwz r11,4(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x824c377c
	if (!cr6.eq) goto loc_824C377C;
	// addi r6,r31,4
	ctx.r6.s64 = r31.s64 + 4;
loc_824C2F48:
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c3064
	if (cr0.eq) goto loc_824C3064;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// addi r10,r10,0
	ctx.r10.s64 = ctx.r10.s64 + 0;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c3050
	if (cr0.eq) goto loc_824C3050;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r8,12(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824c2f84
	goto loc_824C2F84;
loc_824C2F80:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824C2F84:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824c2f80
	if (!cr6.eq) goto loc_824C2F80;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r11.u32);
	// stw r8,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r8.u32);
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi r10,r10,27
	ctx.r10.u64 = ctx.r10.u32 & 0x1F;
	// clrlwi r9,r9,27
	ctx.r9.u64 = ctx.r9.u32 & 0x1F;
	// rlwinm. r8,r10,0,29,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c2fcc
	if (cr0.eq) goto loc_824C2FCC;
	// rlwinm. r8,r9,0,30,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c2fcc
	if (cr0.eq) goto loc_824C2FCC;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
loc_824C2FCC:
	// and r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 & ctx.r10.u64;
	// rlwinm. r8,r8,0,29,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c2fe0
	if (cr0.eq) goto loc_824C2FE0;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
loc_824C2FE0:
	// clrlwi. r8,r10,31
	ctx.r8.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c2ff4
	if (cr0.eq) goto loc_824C2FF4;
	// rlwinm. r8,r9,0,30,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c2ff4
	if (cr0.eq) goto loc_824C2FF4;
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
loc_824C2FF4:
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwimi r10,r8,0,0,26
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r8.u32, 0) & 0xFFFFFFE0) | (ctx.r10.u64 & 0xFFFFFFFF0000001F);
	// rlwinm r9,r10,27,24,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// rlwinm r8,r10,0,27,18
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFE01F;
	// rlwinm r5,r9,27,29,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x6;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// rlwinm r4,r9,29,29,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x6;
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// rlwinm r3,r9,31,29,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x6;
	// rlwinm r9,r9,1,29,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x6;
	// rlwinm r10,r10,27,24,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// srw r5,r10,r5
	ctx.r5.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r5.u8 & 0x3F));
	// srw r4,r10,r4
	ctx.r4.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r4.u8 & 0x3F));
	// srw r3,r10,r3
	ctx.r3.u64 = ctx.r3.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r3.u8 & 0x3F));
	// rlwimi r4,r5,2,28,29
	ctx.r4.u64 = (__builtin_rotateleft32(ctx.r5.u32, 2) & 0xC) | (ctx.r4.u64 & 0xFFFFFFFFFFFFFFF3);
	// srw r10,r10,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r9.u8 & 0x3F));
	// clrlwi r9,r4,28
	ctx.r9.u64 = ctx.r4.u32 & 0xF;
	// rlwimi r3,r9,2,0,29
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r9.u32, 2) & 0xFFFFFFFC) | (ctx.r3.u64 & 0xFFFFFFFF00000003);
	// rlwimi r10,r3,2,0,29
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r3.u32, 2) & 0xFFFFFFFC) | (ctx.r10.u64 & 0xFFFFFFFF00000003);
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// or r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 | ctx.r8.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_824C3050:
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824c2f48
	if (!cr6.eq) goto loc_824C2F48;
	// addi r6,r11,8
	ctx.r6.s64 = r11.s64 + 8;
	// b 0x824c2f48
	goto loc_824C2F48;
loc_824C3064:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a98e8
	sub_824A98E8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// beq 0x824c3788
	if (cr0.eq) goto loc_824C3788;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
loc_824C3088:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824c30d0
	if (cr6.eq) goto loc_824C30D0;
	// rlwinm r11,r29,0,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// bne 0x824c30a8
	if (!cr0.eq) goto loc_824C30A8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_824C30A8:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x824c30c0
	if (!cr6.eq) goto loc_824C30C0;
loc_824C30B0:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r29,r31
	r29.u64 = r31.u64;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// b 0x824c30ec
	goto loc_824C30EC;
loc_824C30C0:
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c3100
	if (!cr0.eq) goto loc_824C3100;
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
	// b 0x824c30f8
	goto loc_824C30F8;
loc_824C30D0:
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c30e0
	if (cr0.eq) goto loc_824C30E0;
	// li r11,0
	r11.s64 = 0;
loc_824C30E0:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x824c30b0
	if (cr6.eq) goto loc_824C30B0;
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 20);
loc_824C30EC:
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c3100
	if (!cr0.eq) goto loc_824C3100;
	// mr r31,r11
	r31.u64 = r11.u64;
loc_824C30F8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c2ec8
	if (!cr6.eq) goto loc_824C2EC8;
loc_824C3100:
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// subfic r26,r26,3
	xer.ca = r26.u32 <= 3;
	r26.s64 = 3 - r26.s64;
	// lwz r10,76(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 76);
	// oris r11,r11,2176
	r11.u64 = r11.u64 | 142606336;
	// stw r19,112(r30)
	PPC_STORE_U32(r30.u32 + 112, r19.u32);
	// oris r10,r10,192
	ctx.r10.u64 = ctx.r10.u64 | 12582912;
	// stw r11,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r11.u32);
	// stw r10,76(r30)
	PPC_STORE_U32(r30.u32 + 76, ctx.r10.u32);
loc_824C3120:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c313c
	if (!cr0.eq) goto loc_824C313C;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c2e20
	if (!cr6.eq) goto loc_824C2E20;
loc_824C313C:
	// cmplwi cr6,r27,1
	cr6.compare<uint32_t>(r27.u32, 1, xer);
	// ble cr6,0x824c3150
	if (!cr6.gt) goto loc_824C3150;
	// lwz r11,40(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 40);
	// ori r11,r11,32768
	r11.u64 = r11.u64 | 32768;
	// stw r11,40(r20)
	PPC_STORE_U32(r20.u32 + 40, r11.u32);
loc_824C3150:
	// lwz r11,36(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 36);
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// stw r11,36(r20)
	PPC_STORE_U32(r20.u32 + 36, r11.u32);
	// b 0x824c36f0
	goto loc_824C36F0;
loc_824C3160:
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,83
	ctx.r4.s64 = 83;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r25,-16
	r11.s64 = r25.s64 + -16;
	// lwz r10,580(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 580);
	// lwzx r11,r3,r11
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// rlwinm r11,r11,1,21,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x7C0;
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c31c8
	if (cr0.eq) goto loc_824C31C8;
	// b 0x824c31bc
	goto loc_824C31BC;
loc_824C3194:
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// b 0x824c31a8
	goto loc_824C31A8;
loc_824C319C:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,0,9,9
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c31b4
	if (cr0.eq) goto loc_824C31B4;
loc_824C31A8:
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c319c
	if (!cr0.eq) goto loc_824C319C;
loc_824C31B4:
	// stw r11,16(r28)
	PPC_STORE_U32(r28.u32 + 16, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
loc_824C31BC:
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c3194
	if (!cr0.eq) goto loc_824C3194;
loc_824C31C8:
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c3234
	if (cr0.eq) goto loc_824C3234;
	// b 0x824c3200
	goto loc_824C3200;
loc_824C31D8:
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// b 0x824c31ec
	goto loc_824C31EC;
loc_824C31E0:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,0,9,9
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c31f8
	if (cr0.eq) goto loc_824C31F8;
loc_824C31EC:
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c31e0
	if (!cr0.eq) goto loc_824C31E0;
loc_824C31F8:
	// stw r11,12(r28)
	PPC_STORE_U32(r28.u32 + 12, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
loc_824C3200:
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c31d8
	if (!cr0.eq) goto loc_824C31D8;
	// b 0x824c3234
	goto loc_824C3234;
loc_824C3210:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// b 0x824c3224
	goto loc_824C3224;
loc_824C3218:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,0,9,9
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c3230
	if (cr0.eq) goto loc_824C3230;
loc_824C3224:
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c3218
	if (!cr0.eq) goto loc_824C3218;
loc_824C3230:
	// stw r11,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r11.u32);
loc_824C3234:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c3210
	if (!cr0.eq) goto loc_824C3210;
	// lwz r10,8(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r11,36(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c37a8
	if (!cr0.eq) goto loc_824C37A8;
	// lwz r11,32(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-36
	xer.ca = r11.u32 > 35;
	r31.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824c37a8
	if (cr0.eq) goto loc_824C37A8;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r9,0,18,24
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10752
	cr6.compare<uint32_t>(r11.u32, 10752, xer);
	// bne cr6,0x824c37a8
	if (!cr6.eq) goto loc_824C37A8;
	// lwz r11,76(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c34cc
	if (cr0.eq) goto loc_824C34CC;
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c3790
	if (cr0.eq) goto loc_824C3790;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x824c3790
	if (!cr6.eq) goto loc_824C3790;
	// rlwinm. r11,r9,15,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c338c
	if (!cr0.eq) goto loc_824C338C;
	// lwz r11,36(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c338c
	if (!cr0.eq) goto loc_824C338C;
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,119
	ctx.r6.s64 = 119;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r10,r30,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// lwz r7,8(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r8,r11,-32
	ctx.r8.s64 = r11.s64 + -32;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// ori r7,r7,64
	ctx.r7.u64 = ctx.r7.u64 | 64;
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// li r5,7
	ctx.r5.s64 = 7;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r7,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// bl 0x824b31e8
	sub_824B31E8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r5,0
	ctx.r5.s64 = 0;
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r11,76(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 76);
	// rlwinm r11,r11,9,24,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0xFF;
	// rlwimi r10,r11,18,13,13
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 18) & 0x40000) | (ctx.r10.u64 & 0xFFFFFFFFFFFBFFFF);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
	// rlwimi r11,r10,30,7,18
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 30) & 0x1FFE000) | (r11.u64 & 0xFFFFFFFFFE001FFF);
	// rlwimi r11,r10,30,1,1
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 30) & 0x40000000) | (r11.u64 & 0xFFFFFFFFBFFFFFFF);
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwimi r11,r16,1,27,30
	r11.u64 = (__builtin_rotateleft32(r16.u32, 1) & 0x1E) | (r11.u64 & 0xFFFFFFFFFFFFFFE1);
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// b 0x824c36f0
	goto loc_824C36F0;
loc_824C338C:
	// lwz r11,20(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 20);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c33a4
	if (!cr6.eq) goto loc_824C33A4;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824ff620
	sub_824FF620(ctx, base);
loc_824C33A4:
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bb5e8
	sub_824BB5E8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r7,65
	ctx.r7.s64 = 65;
	// lwz r6,8(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// lwz r5,20(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 20);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// bl 0x824bd678
	sub_824BD678(ctx, base);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82553608
	sub_82553608(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bc9a8
	sub_824BC9A8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// lwz r5,96(r19)
	ctx.r5.u64 = PPC_LOAD_U32(r19.u32 + 96);
	// cmplwi r5,0
	cr0.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq 0x824c3440
	if (cr0.eq) goto loc_824C3440;
	// mr r6,r20
	ctx.r6.u64 = r20.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x824bb088
	sub_824BB088(ctx, base);
loc_824C3440:
	// stw r30,96(r19)
	PPC_STORE_U32(r19.u32 + 96, r30.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// oris r11,r11,512
	r11.u64 = r11.u64 | 33554432;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// lwz r11,4(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c34ac
	if (!cr0.eq) goto loc_824C34AC;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c34ac
	if (cr0.eq) goto loc_824C34AC;
loc_824C3464:
	// lwz r10,580(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 580);
	// subf r10,r10,r28
	ctx.r10.s64 = r28.s64 - ctx.r10.s64;
	// srawi r10,r10,6
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 6;
	// rlwinm r9,r10,27,5,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r10,r10,27
	ctx.r10.u64 = ctx.r10.u32 & 0x1F;
	// addi r9,r9,21
	ctx.r9.s64 = ctx.r9.s64 + 21;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// srw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r10.u8 & 0x3F));
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c3494
	if (cr0.eq) goto loc_824C3494;
	// stw r19,112(r11)
	PPC_STORE_U32(r11.u32 + 112, r19.u32);
loc_824C3494:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c34ac
	if (!cr0.eq) goto loc_824C34AC;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c3464
	if (!cr6.eq) goto loc_824C3464;
loc_824C34AC:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r11,r11,15,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c36f0
	if (cr0.eq) goto loc_824C36F0;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bd760
	sub_824BD760(ctx, base);
	// b 0x824c36f0
	goto loc_824C36F0;
loc_824C34CC:
	// lwz r11,36(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c36f0
	if (cr0.eq) goto loc_824C36F0;
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c379c
	if (cr0.eq) goto loc_824C379C;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x824c379c
	if (!cr6.eq) goto loc_824C379C;
	// rlwinm. r11,r9,15,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c35c0
	if (!cr0.eq) goto loc_824C35C0;
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,119
	ctx.r6.s64 = 119;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r10,r30,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// lwz r7,8(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r8,r11,-32
	ctx.r8.s64 = r11.s64 + -32;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// ori r7,r7,64
	ctx.r7.u64 = ctx.r7.u64 | 64;
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// li r5,7
	ctx.r5.s64 = 7;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r7,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// bl 0x824b31e8
	sub_824B31E8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r5,0
	ctx.r5.s64 = 0;
	// oris r11,r11,6
	r11.u64 = r11.u64 | 393216;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
	// rlwimi r11,r10,30,7,18
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 30) & 0x1FFE000) | (r11.u64 & 0xFFFFFFFFFE001FFF);
	// rlwimi r11,r10,30,1,1
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 30) & 0x40000000) | (r11.u64 & 0xFFFFFFFFBFFFFFFF);
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwimi r11,r16,1,27,30
	r11.u64 = (__builtin_rotateleft32(r16.u32, 1) & 0x1E) | (r11.u64 & 0xFFFFFFFFFFFFFFE1);
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
loc_824C35C0:
	// lwz r11,4(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c3634
	if (!cr0.eq) goto loc_824C3634;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c3634
	if (cr0.eq) goto loc_824C3634;
loc_824C35D4:
	// lwz r10,580(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 580);
	// subf r10,r10,r28
	ctx.r10.s64 = r28.s64 - ctx.r10.s64;
	// srawi r10,r10,6
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 6;
	// rlwinm r9,r10,27,5,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r10,r10,27
	ctx.r10.u64 = ctx.r10.u32 & 0x1F;
	// addi r9,r9,21
	ctx.r9.s64 = ctx.r9.s64 + 21;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// srw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r10.u8 & 0x3F));
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c361c
	if (cr0.eq) goto loc_824C361C;
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r9,76(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// oris r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 8388608;
	// stw r19,112(r11)
	PPC_STORE_U32(r11.u32 + 112, r19.u32);
	// oris r9,r9,192
	ctx.r9.u64 = ctx.r9.u64 | 12582912;
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// stw r9,76(r11)
	PPC_STORE_U32(r11.u32 + 76, ctx.r9.u32);
loc_824C361C:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c3634
	if (!cr0.eq) goto loc_824C3634;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c35d4
	if (!cr6.eq) goto loc_824C35D4;
loc_824C3634:
	// li r4,1
	ctx.r4.s64 = 1;
	// fmr f4,f31
	ctx.fpscr.disableFlushMode();
	ctx.f4.f64 = f31.f64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// fmr f3,f31
	ctx.f3.f64 = f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x82557350
	sub_82557350(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bc9a8
	sub_824BC9A8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// lwz r5,96(r19)
	ctx.r5.u64 = PPC_LOAD_U32(r19.u32 + 96);
	// cmplwi r5,0
	cr0.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq 0x824c369c
	if (cr0.eq) goto loc_824C369C;
	// mr r6,r20
	ctx.r6.u64 = r20.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x824bb088
	sub_824BB088(ctx, base);
loc_824C369C:
	// stw r30,96(r19)
	PPC_STORE_U32(r19.u32 + 96, r30.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// oris r11,r11,512
	r11.u64 = r11.u64 | 33554432;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// bl 0x824bd760
	sub_824BD760(ctx, base);
	// lwz r11,20(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 20);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c36d4
	if (!cr6.eq) goto loc_824C36D4;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824ff620
	sub_824FF620(ctx, base);
loc_824C36D4:
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lwz r6,8(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// li r7,65
	ctx.r7.s64 = 65;
	// lwz r5,20(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 20);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bd678
	sub_824BD678(ctx, base);
loc_824C36F0:
	// rlwinm r11,r19,0,0,30
	r11.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c370c
	if (!cr0.eq) goto loc_824C370C;
	// mr r19,r11
	r19.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c242c
	if (!cr6.eq) goto loc_824C242C;
loc_824C370C:
	// lwz r11,40(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 40);
	// rlwinm. r11,r11,0,16,16
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3b34
	if (cr0.eq) goto loc_824C3B34;
	// lwz r11,4(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c39bc
	if (!cr0.eq) goto loc_824C39BC;
	// mr. r28,r11
	r28.u64 = r11.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824c39bc
	if (cr0.eq) goto loc_824C39BC;
loc_824C372C:
	// lwz r29,96(r28)
	r29.u64 = PPC_LOAD_U32(r28.u32 + 96);
	// cmplwi r29,0
	cr0.compare<uint32_t>(r29.u32, 0, xer);
	// beq 0x824c39a0
	if (cr0.eq) goto loc_824C39A0;
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,8320
	cr6.compare<uint32_t>(r11.u32, 8320, xer);
	// bne cr6,0x824c39a0
	if (!cr6.eq) goto loc_824C39A0;
	// lwz r30,8(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 8);
loc_824C374C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824c39a0
	if (cr6.eq) goto loc_824C39A0;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// b 0x824c3810
	goto loc_824C3810;
loc_824C375C:
	// li r4,3535
	ctx.r4.s64 = 3535;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C3768:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C3774:
	// li r4,3535
	ctx.r4.s64 = 3535;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C377C:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C3788:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C3790:
	// li r4,3535
	ctx.r4.s64 = 3535;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C379C:
	// li r4,3535
	ctx.r4.s64 = 3535;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C37A8:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C37B4:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C37C0:
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm r10,r10,25,25,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r10,64
	cr6.compare<uint32_t>(ctx.r10.u32, 64, xer);
	// beq cr6,0x824c3824
	if (cr6.eq) goto loc_824C3824;
	// cmplwi cr6,r10,63
	cr6.compare<uint32_t>(ctx.r10.u32, 63, xer);
	// bne cr6,0x824c3820
	if (!cr6.eq) goto loc_824C3820;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c37f4
	if (cr0.eq) goto loc_824C37F4;
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r11,r16
	r11.u64 = r16.u64;
	// bne cr6,0x824c37f8
	if (!cr6.eq) goto loc_824C37F8;
loc_824C37F4:
	// li r11,0
	r11.s64 = 0;
loc_824C37F8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c3820
	if (!cr0.eq) goto loc_824C3820;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// beq cr6,0x824c3810
	if (cr6.eq) goto loc_824C3810;
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
loc_824C3810:
	// lwz r4,96(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 96);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne 0x824c37c0
	if (!cr0.eq) goto loc_824C37C0;
	// b 0x824c38c4
	goto loc_824C38C4;
loc_824C3820:
	// li r4,0
	ctx.r4.s64 = 0;
loc_824C3824:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x824c38c4
	if (cr6.eq) goto loc_824C38C4;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824ba210
	sub_824BA210(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,65
	cr6.compare<uint32_t>(r11.u32, 65, xer);
	// beq cr6,0x824c3850
	if (cr6.eq) goto loc_824C3850;
	// cmplwi cr6,r11,108
	cr6.compare<uint32_t>(r11.u32, 108, xer);
	// bne cr6,0x824c3a0c
	if (!cr6.eq) goto loc_824C3A0C;
loc_824C3850:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x824c38ac
	goto loc_824C38AC;
loc_824C3858:
	// lwz r11,96(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 96);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c38a8
	if (cr0.eq) goto loc_824C38A8;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,8320
	cr6.compare<uint32_t>(r11.u32, 8320, xer);
	// bne cr6,0x824c38a8
	if (!cr6.eq) goto loc_824C38A8;
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
loc_824C3878:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c38a8
	if (cr6.eq) goto loc_824C38A8;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r4,96(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 96);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x824c38a0
	if (cr0.eq) goto loc_824C38A0;
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r10,8192
	cr6.compare<uint32_t>(ctx.r10.u32, 8192, xer);
	// beq cr6,0x824c38cc
	if (cr6.eq) goto loc_824C38CC;
loc_824C38A0:
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// b 0x824c3878
	goto loc_824C3878;
loc_824C38A8:
	// rlwinm r11,r9,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
loc_824C38AC:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c38c4
	if (!cr0.eq) goto loc_824C38C4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// bne cr6,0x824c3858
	if (!cr6.eq) goto loc_824C3858;
loc_824C38C4:
	// lwz r30,12(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// b 0x824c374c
	goto loc_824C374C;
loc_824C38CC:
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824ba210
	sub_824BA210(ctx, base);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,65
	cr6.compare<uint32_t>(r11.u32, 65, xer);
	// beq cr6,0x824c38ec
	if (cr6.eq) goto loc_824C38EC;
	// cmplwi cr6,r11,108
	cr6.compare<uint32_t>(r11.u32, 108, xer);
	// bne cr6,0x824c3a18
	if (!cr6.eq) goto loc_824C3A18;
loc_824C38EC:
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r10,40(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r16,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c3a24
	if (cr0.eq) goto loc_824C3A24;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824c3944
	goto loc_824C3944;
loc_824C3940:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824C3944:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824c3940
	if (!cr6.eq) goto loc_824C3940;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// stw r3,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r3.u32);
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824c397c
	goto loc_824C397C;
loc_824C3978:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824C397C:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824c3978
	if (!cr6.eq) goto loc_824C3978;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// stw r3,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r3.u32);
loc_824C39A0:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c39bc
	if (!cr0.eq) goto loc_824C39BC;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c372c
	if (!cr6.eq) goto loc_824C372C;
loc_824C39BC:
	// lwz r11,4(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c3b34
	if (!cr0.eq) goto loc_824C3B34;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824c3b34
	if (cr0.eq) goto loc_824C3B34;
loc_824C39D0:
	// lwz r28,96(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 96);
	// cmplwi r28,0
	cr0.compare<uint32_t>(r28.u32, 0, xer);
	// beq 0x824c3b18
	if (cr0.eq) goto loc_824C3B18;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r10,r11,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r10,8192
	cr6.compare<uint32_t>(ctx.r10.u32, 8192, xer);
	// bne cr6,0x824c3b18
	if (!cr6.eq) goto loc_824C3B18;
	// rlwinm. r11,r11,15,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3b18
	if (cr0.eq) goto loc_824C3B18;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824ba210
	sub_824BA210(ctx, base);
	// lwz r29,52(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 52);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// b 0x824c3b10
	goto loc_824C3B10;
loc_824C3A0C:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C3A18:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C3A24:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C3A30:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824baf98
	sub_824BAF98(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824c3a5c
	if (cr0.eq) goto loc_824C3A5C;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm. r11,r11,15,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3a5c
	if (cr0.eq) goto loc_824C3A5C;
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// lwz r31,12(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// b 0x824c3a60
	goto loc_824C3A60;
loc_824C3A5C:
	// lwz r31,96(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 96);
loc_824C3A60:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c3aa4
	if (cr6.eq) goto loc_824C3AA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3aa4
	if (cr0.eq) goto loc_824C3AA4;
	// cmplw cr6,r31,r27
	cr6.compare<uint32_t>(r31.u32, r27.u32, xer);
	// beq cr6,0x824c3b18
	if (cr6.eq) goto loc_824C3B18;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3aa4
	if (cr0.eq) goto loc_824C3AA4;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bd870
	sub_824BD870(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_824C3AA4:
	// lwz r31,28(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c3b0c
	if (!cr0.eq) goto loc_824C3B0C;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824c3b0c
	if (cr0.eq) goto loc_824C3B0C;
loc_824C3AB8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,13824
	cr6.compare<uint32_t>(r11.u32, 13824, xer);
	// bne cr6,0x824c3ae8
	if (!cr6.eq) goto loc_824C3AE8;
	// cmplw cr6,r31,r27
	cr6.compare<uint32_t>(r31.u32, r27.u32, xer);
	// beq cr6,0x824c3b04
	if (cr6.eq) goto loc_824C3B04;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824bd870
	sub_824BD870(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_824C3AE8:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c3b0c
	if (!cr0.eq) goto loc_824C3B0C;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c3ab8
	if (!cr6.eq) goto loc_824C3AB8;
	// b 0x824c3b0c
	goto loc_824C3B0C;
loc_824C3B04:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c3b18
	if (!cr6.eq) goto loc_824C3B18;
loc_824C3B0C:
	// lwz r29,52(r29)
	r29.u64 = PPC_LOAD_U32(r29.u32 + 52);
loc_824C3B10:
	// cmplwi r29,0
	cr0.compare<uint32_t>(r29.u32, 0, xer);
	// bne 0x824c3a30
	if (!cr0.eq) goto loc_824C3A30;
loc_824C3B18:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c3b34
	if (!cr0.eq) goto loc_824C3B34;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c39d0
	if (!cr6.eq) goto loc_824C39D0;
loc_824C3B34:
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x8250fc58
	sub_8250FC58(ctx, base);
loc_824C3B3C:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// lfd f31,-144(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_824C3B48"))) PPC_WEAK_FUNC(sub_824C3B48);
PPC_FUNC_IMPL(__imp__sub_824C3B48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-416(r1)
	ea = -416 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// li r24,0
	r24.s64 = 0;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// lwz r11,36(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 36);
	// stw r23,444(r1)
	PPC_STORE_U32(ctx.r1.u32 + 444, r23.u32);
	// stw r28,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, r28.u32);
	// stb r24,104(r1)
	PPC_STORE_U8(ctx.r1.u32 + 104, r24.u8);
	// stb r24,105(r1)
	PPC_STORE_U8(ctx.r1.u32 + 105, r24.u8);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c5594
	if (!cr0.eq) goto loc_824C5594;
	// lwz r11,32(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r26,r11,-36
	xer.ca = r11.u32 > 35;
	r26.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// stw r26,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r26.u32);
	// beq 0x824c5594
	if (cr0.eq) goto loc_824C5594;
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm r11,r10,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,85
	cr6.compare<uint32_t>(r11.u32, 85, xer);
	// beq cr6,0x824c3bcc
	if (cr6.eq) goto loc_824C3BCC;
	// cmplwi cr6,r11,86
	cr6.compare<uint32_t>(r11.u32, 86, xer);
	// beq cr6,0x824c3bcc
	if (cr6.eq) goto loc_824C3BCC;
	// cmplwi cr6,r11,88
	cr6.compare<uint32_t>(r11.u32, 88, xer);
	// beq cr6,0x824c3bcc
	if (cr6.eq) goto loc_824C3BCC;
	// cmplwi cr6,r11,89
	cr6.compare<uint32_t>(r11.u32, 89, xer);
	// beq cr6,0x824c3bcc
	if (cr6.eq) goto loc_824C3BCC;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824c3bcc
	if (cr6.eq) goto loc_824C3BCC;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// mr r11,r24
	r11.u64 = r24.u64;
	// bne cr6,0x824c3bd0
	if (!cr6.eq) goto loc_824C3BD0;
loc_824C3BCC:
	// li r11,1
	r11.s64 = 1;
loc_824C3BD0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5594
	if (cr0.eq) goto loc_824C5594;
	// rlwinm. r11,r10,15,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5594
	if (cr0.eq) goto loc_824C5594;
	// li r30,1
	r30.s64 = 1;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r30,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r30.u32);
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// lwz r29,12(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// mr r31,r24
	r31.u64 = r24.u64;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// stw r29,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r29.u32);
	// rlwinm r11,r10,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,21
	cr6.compare<uint32_t>(r11.u32, 21, xer);
	// blt cr6,0x824c3c18
	if (cr6.lt) goto loc_824C3C18;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824c3c1c
	if (!cr6.gt) goto loc_824C3C1C;
loc_824C3C18:
	// mr r11,r24
	r11.u64 = r24.u64;
loc_824C3C1C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3cc0
	if (cr0.eq) goto loc_824C3CC0;
	// lwz r31,44(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 44);
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 4);
loc_824C3C2C:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824c3cd0
	if (cr6.eq) goto loc_824C3CD0;
	// lwz r11,16(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// subf r10,r24,r11
	ctx.r10.s64 = r11.s64 - r24.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c3cb8
	if (cr0.eq) goto loc_824C3CB8;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r10,r10,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c3cb8
	if (cr0.eq) goto loc_824C3CB8;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
loc_824C3C70:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c3cb8
	if (cr6.eq) goto loc_824C3CB8;
	// lwz r11,16(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// subf r8,r24,r11
	ctx.r8.s64 = r11.s64 - r24.s64;
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 ^ 1;
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq 0x824c3cb0
	if (cr0.eq) goto loc_824C3CB0;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,63
	cr6.compare<uint32_t>(r11.u32, 63, xer);
	// beq cr6,0x824c3cac
	if (cr6.eq) goto loc_824C3CAC;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// bne cr6,0x824c3cb0
	if (!cr6.eq) goto loc_824C3CB0;
loc_824C3CAC:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
loc_824C3CB0:
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// b 0x824c3c70
	goto loc_824C3C70;
loc_824C3CB8:
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// b 0x824c3c2c
	goto loc_824C3C2C;
loc_824C3CC0:
	// rlwinm. r11,r10,0,10,12
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x380000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3cd4
	if (cr0.eq) goto loc_824C3CD4;
	// lwz r31,40(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// b 0x824c3cd4
	goto loc_824C3CD4;
loc_824C3CD0:
	// stw r30,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r30.u32);
loc_824C3CD4:
	// addi r11,r1,124
	r11.s64 = ctx.r1.s64 + 124;
	// stw r24,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r24.u32);
	// addi r10,r1,164
	ctx.r10.s64 = ctx.r1.s64 + 164;
	// stw r24,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r24.u32);
	// addi r9,r1,108
	ctx.r9.s64 = ctx.r1.s64 + 108;
	// stw r24,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r24.u32);
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// addi r7,r1,98
	ctx.r7.s64 = ctx.r1.s64 + 98;
	// addi r6,r1,148
	ctx.r6.s64 = ctx.r1.s64 + 148;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824be488
	sub_824BE488(ctx, base);
	// lwz r25,108(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r30,148(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c3df8
	if (!cr0.eq) goto loc_824C3DF8;
	// cmpwi cr6,r30,3
	cr6.compare<int32_t>(r30.s32, 3, xer);
	// beq cr6,0x824c3d48
	if (cr6.eq) goto loc_824C3D48;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// bne cr6,0x824c5578
	if (!cr6.eq) goto loc_824C5578;
loc_824C3D2C:
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r29,144(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r26,116(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// lwz r28,436(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// bne cr6,0x824c5394
	if (!cr6.eq) goto loc_824C5394;
	// lwz r25,108(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
loc_824C3D48:
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824bbb90
	sub_824BBB90(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5434
	if (cr0.eq) goto loc_824C5434;
	// lbz r31,98(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + 98);
	// lwz r30,116(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r28,436(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// mr. r11,r31
	r11.u64 = r31.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3d90
	if (cr0.eq) goto loc_824C3D90;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824bb118
	sub_824BB118(ctx, base);
	// li r31,0
	r31.s64 = 0;
loc_824C3D90:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r30,-12
	r11.s64 = r30.s64 + -12;
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// beq cr6,0x824c3dd0
	if (cr6.eq) goto loc_824C3DD0;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// rlwimi r9,r10,0,31,29
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFFFFFFFFFFFD) | (ctx.r9.u64 & 0x2);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
loc_824C3DD0:
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// clrlwi r10,r31,24
	ctx.r10.u64 = r31.u32 & 0xFF;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// li r7,1
	ctx.r7.s64 = 1;
	// lbz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 96);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// rlwimi r11,r10,18,13,13
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 18) & 0x40000) | (r11.u64 & 0xFFFFFFFFFFFBFFFF);
	// lwz r29,444(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// b 0x824c541c
	goto loc_824C541C;
loc_824C3DF8:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// beq cr6,0x824c3d2c
	if (cr6.eq) goto loc_824C3D2C;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// ble cr6,0x824c3e2c
	if (!cr6.gt) goto loc_824C3E2C;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// ble cr6,0x824c3d2c
	if (!cr6.gt) goto loc_824C3D2C;
	// cmpwi cr6,r3,7
	cr6.compare<int32_t>(ctx.r3.s32, 7, xer);
	// ble cr6,0x824c3e2c
	if (!cr6.gt) goto loc_824C3E2C;
	// cmpwi cr6,r3,9
	cr6.compare<int32_t>(ctx.r3.s32, 9, xer);
	// ble cr6,0x824c3d2c
	if (!cr6.gt) goto loc_824C3D2C;
loc_824C3E2C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c3e94
	if (cr6.eq) goto loc_824C3E94;
	// lwz r31,12(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// beq cr6,0x824c3e94
	if (cr6.eq) goto loc_824C3E94;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-15744
	r11.s64 = r11.s64 + -15744;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3e94
	if (cr0.eq) goto loc_824C3E94;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r31,-8
	r11.s64 = r31.s64 + -8;
	// lwz r10,12(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// lwzx r11,r3,r11
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// clrlwi r11,r11,17
	r11.u64 = r11.u32 & 0x7FFF;
	// mulli r11,r11,40
	r11.s64 = r11.s64 * 40;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm. r11,r11,0,25,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3d2c
	if (cr0.eq) goto loc_824C3D2C;
loc_824C3E94:
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824bc1d0
	sub_824BC1D0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3d2c
	if (cr0.eq) goto loc_824C3D2C;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// bne cr6,0x824c3ecc
	if (!cr6.eq) goto loc_824C3ECC;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x824bc1d0
	sub_824BC1D0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5394
	if (cr0.eq) goto loc_824C5394;
	// lwz r23,444(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// li r24,0
	r24.s64 = 0;
loc_824C3ECC:
	// lwz r27,124(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// li r6,1
	ctx.r6.s64 = 1;
	// stw r24,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r24.u32);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c0170
	sub_824C0170(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3d2c
	if (cr0.eq) goto loc_824C3D2C;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c0170
	sub_824C0170(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3d2c
	if (cr0.eq) goto loc_824C3D2C;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c0170
	sub_824C0170(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3d2c
	if (cr0.eq) goto loc_824C3D2C;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// bne cr6,0x824c3fa0
	if (!cr6.eq) goto loc_824C3FA0;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c0170
	sub_824C0170(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5394
	if (cr0.eq) goto loc_824C5394;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c0170
	sub_824C0170(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5394
	if (cr0.eq) goto loc_824C5394;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c0170
	sub_824C0170(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5394
	if (cr0.eq) goto loc_824C5394;
	// li r24,0
	r24.s64 = 0;
loc_824C3FA0:
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// stw r10,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r10.u32);
	// stw r9,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r9.u32);
	// bne cr6,0x824c3ff0
	if (!cr6.eq) goto loc_824C3FF0;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// clrlwi. r8,r11,31
	ctx.r8.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x824c3ff0
	if (!cr0.eq) goto loc_824C3FF0;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c3ff0
	if (cr0.eq) goto loc_824C3FF0;
loc_824C3FD0:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r10,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r10.u32);
	// clrlwi. r8,r11,31
	ctx.r8.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x824c3ff0
	if (!cr0.eq) goto loc_824C3FF0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c3fd0
	if (!cr6.eq) goto loc_824C3FD0;
loc_824C3FF0:
	// lwz r11,28(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c4024
	if (!cr0.eq) goto loc_824C4024;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c4024
	if (cr0.eq) goto loc_824C4024;
loc_824C4004:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r9,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r9.u32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c4024
	if (!cr0.eq) goto loc_824C4024;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c4004
	if (!cr6.eq) goto loc_824C4004;
loc_824C4024:
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c405c
	if (!cr0.eq) goto loc_824C405C;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c405c
	if (cr0.eq) goto loc_824C405C;
loc_824C4038:
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r11,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r9,r9,0,6,4
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFBFFFFFF;
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// lwz r11,36(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c405c
	if (!cr0.eq) goto loc_824C405C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c4038
	if (!cr6.eq) goto loc_824C4038;
loc_824C405C:
	// li r5,8
	ctx.r5.s64 = 8;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824a9858
	sub_824A9858(ctx, base);
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
	// ori r10,r3,1
	ctx.r10.u64 = ctx.r3.u64 | 1;
	// stw r3,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r3.u32);
	// ori r9,r11,1
	ctx.r9.u64 = r11.u64 | 1;
	// stw r24,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r24.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c4cd0
	if (!cr0.eq) goto loc_824C4CD0;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, r30.u32);
	// beq 0x824c4cd0
	if (cr0.eq) goto loc_824C4CD0;
	// b 0x824c40b0
	goto loc_824C40B0;
loc_824C40A4:
	// lwz r25,108(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r27,124(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r28,436(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
loc_824C40B0:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r16,164(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// rlwinm r10,r11,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r10,r10,-14208
	ctx.r10.s64 = ctx.r10.s64 + -14208;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c4ca8
	if (cr0.eq) goto loc_824C4CA8;
	// rlwinm. r11,r11,6,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c4ca8
	if (!cr0.eq) goto loc_824C4CA8;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824c4ca8
	if (cr0.eq) goto loc_824C4CA8;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824c4ca8
	if (cr0.eq) goto loc_824C4CA8;
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r11,24(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// cmplw cr6,r11,r16
	cr6.compare<uint32_t>(r11.u32, r16.u32, xer);
	// beq cr6,0x824c4128
	if (cr6.eq) goto loc_824C4128;
	// li r10,0
	ctx.r10.s64 = 0;
loc_824C4128:
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r9,24(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmplw cr6,r9,r25
	cr6.compare<uint32_t>(ctx.r9.u32, r25.u32, xer);
	// beq cr6,0x824c413c
	if (cr6.eq) goto loc_824C413C;
	// li r11,0
	r11.s64 = 0;
loc_824C413C:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824c414c
	if (!cr6.eq) goto loc_824C414C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c4ca8
	if (cr6.eq) goto loc_824C4CA8;
loc_824C414C:
	// mr r31,r11
	r31.u64 = r11.u64;
	// li r22,0
	r22.s64 = 0;
	// mr r18,r25
	r18.u64 = r25.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r31,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r31.u32);
	// stb r22,97(r1)
	PPC_STORE_U8(ctx.r1.u32 + 97, r22.u8);
	// beq cr6,0x824c4188
	if (cr6.eq) goto loc_824C4188;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c41a0
	if (cr6.eq) goto loc_824C41A0;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r11,r11,0,15,17
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1C000;
	// rlwinm r9,r9,0,15,17
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1C000;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// ble cr6,0x824c41a0
	if (!cr6.gt) goto loc_824C41A0;
loc_824C4188:
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
	// li r11,1
	r11.s64 = 1;
	// mr r18,r16
	r18.u64 = r16.u64;
	// mr r16,r25
	r16.u64 = r25.u64;
	// stw r31,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r31.u32);
	// stb r11,97(r1)
	PPC_STORE_U8(ctx.r1.u32 + 97, r11.u8);
loc_824C41A0:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stb r22,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, r22.u8);
	// mr r15,r22
	r15.u64 = r22.u64;
	// mr r14,r22
	r14.u64 = r22.u64;
	// bl 0x824ba748
	sub_824BA748(ctx, base);
	// mr r17,r3
	r17.u64 = ctx.r3.u64;
	// addi r20,r31,4
	r20.s64 = r31.s64 + 4;
	// mr r21,r22
	r21.u64 = r22.u64;
loc_824C41C0:
	// lwz r19,0(r20)
	r19.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// cmplwi r19,0
	cr0.compare<uint32_t>(r19.u32, 0, xer);
	// beq 0x824c4598
	if (cr0.eq) goto loc_824C4598;
	// lwz r26,16(r19)
	r26.u64 = PPC_LOAD_U32(r19.u32 + 16);
	// li r29,0
	r29.s64 = 0;
	// subf r11,r29,r26
	r11.s64 = r26.s64 - r29.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c4584
	if (cr0.eq) goto loc_824C4584;
	// lwz r11,24(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 24);
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// beq cr6,0x824c4204
	if (cr6.eq) goto loc_824C4204;
	// li r11,1
	r11.s64 = 1;
	// stb r11,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, r11.u8);
	// b 0x824c4584
	goto loc_824C4584;
loc_824C4204:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm. r11,r11,6,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c4584
	if (!cr0.eq) goto loc_824C4584;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c422c
	if (cr0.eq) goto loc_824C422C;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x824c4230
	if (!cr6.eq) goto loc_824C4230;
loc_824C422C:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_824C4230:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c4584
	if (cr0.eq) goto loc_824C4584;
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// beq cr6,0x824c4280
	if (cr6.eq) goto loc_824C4280;
	// addi r31,r1,224
	r31.s64 = ctx.r1.s64 + 224;
loc_824C4244:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lhz r4,14(r26)
	ctx.r4.u64 = PPC_LOAD_U16(r26.u32 + 14);
	// lhz r30,14(r11)
	r30.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// cmplw cr6,r3,r24
	cr6.compare<uint32_t>(ctx.r3.u32, r24.u32, xer);
	// beq cr6,0x824c3d2c
	if (cr6.eq) goto loc_824C3D2C;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplw cr6,r29,r15
	cr6.compare<uint32_t>(r29.u32, r15.u32, xer);
	// blt cr6,0x824c4244
	if (cr6.lt) goto loc_824C4244;
loc_824C4280:
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r18
	ctx.r5.u64 = r18.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824c4584
	if (cr0.eq) goto loc_824C4584;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824ba748
	sub_824BA748(ctx, base);
	// cmplw cr6,r3,r17
	cr6.compare<uint32_t>(ctx.r3.u32, r17.u32, xer);
	// bne cr6,0x824c4584
	if (!cr6.eq) goto loc_824C4584;
	// cmplwi cr6,r22,8
	cr6.compare<uint32_t>(r22.u32, 8, xer);
	// beq cr6,0x824c4d8c
	if (cr6.eq) goto loc_824C4D8C;
	// lwz r24,12(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r30,24(r24)
	r30.u64 = PPC_LOAD_U32(r24.u32 + 24);
	// cmplw cr6,r30,r16
	cr6.compare<uint32_t>(r30.u32, r16.u32, xer);
	// beq cr6,0x824c4540
	if (cr6.eq) goto loc_824C4540;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// rlwinm r25,r11,27,30,31
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// clrlwi r7,r11,31
	ctx.r7.u64 = r11.u32 & 0x1;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// bl 0x824f0550
	sub_824F0550(ctx, base);
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// lwz r10,40(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 40);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r10,r10,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c43fc
	if (cr0.eq) goto loc_824C43FC;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824c43a4
	if (!cr6.eq) goto loc_824C43A4;
	// li r5,27
	ctx.r5.s64 = 27;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824a9858
	sub_824A9858(ctx, base);
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
	// ori r10,r3,1
	ctx.r10.u64 = ctx.r3.u64 | 1;
	// stw r3,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r3.u32);
	// ori r9,r11,1
	ctx.r9.u64 = r11.u64 | 1;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// rotlwi r10,r3,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
loc_824C43A4:
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c43d4
	if (!cr0.eq) goto loc_824C43D4;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r3,r11,-4
	xer.ca = r11.u32 > 3;
	ctx.r3.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824c43d4
	if (cr0.eq) goto loc_824C43D4;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// ble cr6,0x824c43e0
	if (!cr6.gt) goto loc_824C43E0;
loc_824C43D4:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// bl 0x824bea98
	sub_824BEA98(ctx, base);
loc_824C43E0:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stwx r31,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r31.u32);
	// b 0x824c44b4
	goto loc_824C44B4;
loc_824C43FC:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r28,8(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 8);
loc_824C4404:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824c4474
	if (cr6.eq) goto loc_824C4474;
	// lwz r29,4(r28)
	r29.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r29,r30
	cr6.compare<uint32_t>(r29.u32, r30.u32, xer);
	// beq cr6,0x824c4438
	if (cr6.eq) goto loc_824C4438;
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r3,436(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// cmplw cr6,r31,r3
	cr6.compare<uint32_t>(r31.u32, ctx.r3.u32, xer);
	// beq cr6,0x824c4440
	if (cr6.eq) goto loc_824C4440;
loc_824C4438:
	// lwz r28,12(r28)
	r28.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// b 0x824c4404
	goto loc_824C4404;
loc_824C4440:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824c4474
	if (cr6.eq) goto loc_824C4474;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r3,436(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// rlwinm r6,r11,0,27,30
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1E;
	// bl 0x824ba6e8
	sub_824BA6E8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x824c44b4
	goto loc_824C44B4;
loc_824C4474:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// b 0x824c4484
	goto loc_824C4484;
loc_824C4480:
	// addi r11,r10,8
	r11.s64 = ctx.r10.s64 + 8;
loc_824C4484:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// bne cr6,0x824c4480
	if (!cr6.eq) goto loc_824C4480;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r31,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r31.u32);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r27,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r27.u32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_824C44B4:
	// lwz r11,12(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 12);
	// rlwinm. r10,r11,0,13,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c44cc
	if (cr0.eq) goto loc_824C44CC;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// sth r11,14(r27)
	PPC_STORE_U16(r27.u32 + 14, r11.u16);
	// b 0x824c44d8
	goto loc_824C44D8;
loc_824C44CC:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824c5858
	sub_824C5858(ctx, base);
	// sth r3,14(r27)
	PPC_STORE_U16(r27.u32 + 14, ctx.r3.u16);
loc_824C44D8:
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// lwz r3,624(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 624);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c4510
	if (!cr0.eq) goto loc_824C4510;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-4
	xer.ca = r11.u32 > 3;
	r11.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c4510
	if (cr0.eq) goto loc_824C4510;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x824c451c
	if (!cr6.gt) goto loc_824C451C;
loc_824C4510:
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_824C451C:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r25,108(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// lwz r28,436(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// stwx r27,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r27.u32);
	// lwz r27,124(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
loc_824C4540:
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// addi r8,r1,224
	ctx.r8.s64 = ctx.r1.s64 + 224;
	// oris r10,r10,1024
	ctx.r10.u64 = ctx.r10.u64 | 67108864;
	// rlwinm r11,r11,27,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// addi r15,r15,1
	r15.s64 = r15.s64 + 1;
	// stwx r31,r21,r9
	PPC_STORE_U32(r21.u32 + ctx.r9.u32, r31.u32);
	// stwx r26,r21,r8
	PPC_STORE_U32(r21.u32 + ctx.r8.u32, r26.u32);
	// addi r21,r21,4
	r21.s64 = r21.s64 + 4;
	// stw r10,8(r26)
	PPC_STORE_U32(r26.u32 + 8, ctx.r10.u32);
	// li r10,3
	ctx.r10.s64 = 3;
	// slw r11,r11,r22
	r11.u64 = r22.u8 & 0x20 ? 0 : (r11.u32 << (r22.u8 & 0x3F));
	// slw r10,r10,r22
	ctx.r10.u64 = r22.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r22.u8 & 0x3F));
	// addi r22,r22,2
	r22.s64 = r22.s64 + 2;
	// andc r10,r14,r10
	ctx.r10.u64 = r14.u64 & ~ctx.r10.u64;
	// or r14,r10,r11
	r14.u64 = ctx.r10.u64 | r11.u64;
loc_824C4584:
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// cmplw cr6,r11,r19
	cr6.compare<uint32_t>(r11.u32, r19.u32, xer);
	// bne cr6,0x824c41c0
	if (!cr6.eq) goto loc_824C41C0;
	// addi r20,r19,8
	r20.s64 = r19.s64 + 8;
	// b 0x824c41c0
	goto loc_824C41C0;
loc_824C4598:
	// li r20,0
	r20.s64 = 0;
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// beq cr6,0x824c4a24
	if (cr6.eq) goto loc_824C4A24;
	// addi r21,r1,176
	r21.s64 = ctx.r1.s64 + 176;
loc_824C45A8:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// lwz r23,12(r11)
	r23.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,8(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c4a14
	if (cr0.eq) goto loc_824C4A14;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x824ba748
	sub_824BA748(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x824c49b8
	if (!cr0.eq) goto loc_824C49B8;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// lwz r5,444(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// li r29,0
	r29.s64 = 0;
	// bl 0x824be230
	sub_824BE230(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c4604
	if (cr0.eq) goto loc_824C4604;
	// li r29,1
	r29.s64 = 1;
loc_824C4604:
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c5fb0
	sub_824C5FB0(ctx, base);
	// lwz r31,20(r16)
	r31.u64 = PPC_LOAD_U32(r16.u32 + 20);
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// li r24,1
	r24.s64 = 1;
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c46ac
	if (!cr0.eq) goto loc_824C46AC;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824c46ac
	if (cr0.eq) goto loc_824C46AC;
loc_824C462C:
	// cmplw cr6,r31,r23
	cr6.compare<uint32_t>(r31.u32, r23.u32, xer);
	// beq cr6,0x824c468c
	if (cr6.eq) goto loc_824C468C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c3d2c
	if (cr0.eq) goto loc_824C3D2C;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c5fb0
	sub_824C5FB0(ctx, base);
	// cmplw cr6,r3,r22
	cr6.compare<uint32_t>(ctx.r3.u32, r22.u32, xer);
	// bne cr6,0x824c468c
	if (!cr6.eq) goto loc_824C468C;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// lwz r5,444(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824be230
	sub_824BE230(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c4688
	if (cr0.eq) goto loc_824C4688;
	// li r29,1
	r29.s64 = 1;
loc_824C4688:
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
loc_824C468C:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c46a4
	if (!cr0.eq) goto loc_824C46A4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c462c
	if (!cr6.eq) goto loc_824C462C;
loc_824C46A4:
	// cmplwi cr6,r24,4
	cr6.compare<uint32_t>(r24.u32, 4, xer);
	// bgt cr6,0x824c3d2c
	if (cr6.gt) goto loc_824C3D2C;
loc_824C46AC:
	// lwz r31,20(r16)
	r31.u64 = PPC_LOAD_U32(r16.u32 + 20);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c4708
	if (!cr0.eq) goto loc_824C4708;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824c4708
	if (cr0.eq) goto loc_824C4708;
	// addi r30,r1,240
	r30.s64 = ctx.r1.s64 + 240;
loc_824C46C4:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c5fb0
	sub_824C5FB0(ctx, base);
	// cmplw cr6,r3,r22
	cr6.compare<uint32_t>(ctx.r3.u32, r22.u32, xer);
	// bne cr6,0x824c46f0
	if (!cr6.eq) goto loc_824C46F0;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
loc_824C46F0:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c4708
	if (!cr0.eq) goto loc_824C4708;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c46c4
	if (!cr6.eq) goto loc_824C46C4;
loc_824C4708:
	// clrlwi. r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r27,0
	r27.s64 = 0;
	// beq 0x824c4848
	if (cr0.eq) goto loc_824C4848;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x824c4838
	if (cr6.eq) goto loc_824C4838;
	// addi r11,r16,24
	r11.s64 = r16.s64 + 24;
	// lwz r19,436(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// addi r28,r1,240
	r28.s64 = ctx.r1.s64 + 240;
	// addi r29,r11,4
	r29.s64 = r11.s64 + 4;
	// mr r25,r24
	r25.u64 = r24.u64;
	// ori r26,r29,1
	r26.u64 = r29.u64 | 1;
loc_824C4734:
	// lwz r30,0(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// bl 0x824bc450
	sub_824BC450(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// addi r8,r11,-32
	ctx.r8.s64 = r11.s64 + -32;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,32(r9)
	PPC_STORE_U32(ctx.r9.u32 + 32, ctx.r10.u32);
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
	// stw r8,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r8.u32);
	// bne cr6,0x824c478c
	if (!cr6.eq) goto loc_824C478C;
	// mr r27,r31
	r27.u64 = r31.u64;
loc_824C478C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r30,12(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// oris r11,r11,128
	r11.u64 = r11.u64 | 8388608;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bl 0x824c5858
	sub_824C5858(ctx, base);
	// sth r3,14(r31)
	PPC_STORE_U16(r31.u32 + 14, ctx.r3.u16);
	// lwz r3,624(r19)
	ctx.r3.u64 = PPC_LOAD_U32(r19.u32 + 624);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c47dc
	if (!cr0.eq) goto loc_824C47DC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-4
	xer.ca = r11.u32 > 3;
	r11.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c47dc
	if (cr0.eq) goto loc_824C47DC;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x824c47e8
	if (!cr6.gt) goto loc_824C47E8;
loc_824C47DC:
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_824C47E8:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r30,r23
	cr6.compare<uint32_t>(r30.u32, r23.u32, xer);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// stwx r31,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r31.u32);
	// bne cr6,0x824c480c
	if (!cr6.eq) goto loc_824C480C;
	// mr r23,r31
	r23.u64 = r31.u64;
loc_824C480C:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// addic. r25,r25,-1
	xer.ca = r25.u32 > 0;
	r25.s64 = r25.s64 + -1;
	cr0.compare<int32_t>(r25.s32, 0, xer);
	// stw r31,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r31.u32);
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// bne 0x824c4734
	if (!cr0.eq) goto loc_824C4734;
loc_824C4838:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r28,436(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
loc_824C4848:
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825536c8
	sub_825536C8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// beq cr6,0x824c48a0
	if (cr6.eq) goto loc_824C48A0;
	// rlwinm r11,r27,0,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,32(r7)
	PPC_STORE_U32(ctx.r7.u32 + 32, ctx.r9.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x824c48c4
	goto loc_824C48C4;
loc_824C48A0:
	// addi r11,r16,24
	r11.s64 = r16.s64 + 24;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// ori r6,r11,1
	ctx.r6.u64 = r11.u64 | 1;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,32(r7)
	PPC_STORE_U32(ctx.r7.u32 + 32, ctx.r9.u32);
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
loc_824C48C4:
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r11,20(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c492c
	if (!cr0.eq) goto loc_824C492C;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824c492c
	if (cr0.eq) goto loc_824C492C;
	// addi r29,r1,240
	r29.s64 = ctx.r1.s64 + 240;
	// addi r28,r31,40
	r28.s64 = r31.s64 + 40;
loc_824C48E4:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,436(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// bl 0x824c5fb0
	sub_824C5FB0(ctx, base);
	// cmplw cr6,r3,r22
	cr6.compare<uint32_t>(ctx.r3.u32, r22.u32, xer);
	// bne cr6,0x824c4910
	if (!cr6.eq) goto loc_824C4910;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
loc_824C4910:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c492c
	if (!cr0.eq) goto loc_824C492C;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c48e4
	if (!cr6.eq) goto loc_824C48E4;
loc_824C492C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// oris r11,r11,128
	r11.u64 = r11.u64 | 8388608;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bl 0x824c5858
	sub_824C5858(ctx, base);
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// sth r3,14(r31)
	PPC_STORE_U16(r31.u32 + 14, ctx.r3.u16);
	// lwz r3,624(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 624);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c497c
	if (!cr0.eq) goto loc_824C497C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-4
	xer.ca = r11.u32 > 3;
	r11.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c497c
	if (cr0.eq) goto loc_824C497C;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x824c4988
	if (!cr6.gt) goto loc_824C4988;
loc_824C497C:
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_824C4988:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// addi r8,r10,4
	ctx.r8.s64 = ctx.r10.s64 + 4;
	// lwz r25,108(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// lwz r27,124(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r28,436(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// stw r31,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r31.u32);
	// stw r7,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r7.u32);
	// stwx r31,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r31.u32);
	// stw r9,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r9.u32);
loc_824C49B8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r11,r31,40
	r11.s64 = r31.s64 + 40;
	// rlwinm r9,r10,13,29,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 13) & 0x7;
loc_824C49C8:
	// cmplw cr6,r5,r9
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r9.u32, xer);
	// bge cr6,0x824c49fc
	if (!cr6.lt) goto loc_824C49FC;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// cmplw cr6,r10,r23
	cr6.compare<uint32_t>(ctx.r10.u32, r23.u32, xer);
	// beq cr6,0x824c49ec
	if (cr6.eq) goto loc_824C49EC;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// b 0x824c49c8
	goto loc_824C49C8;
loc_824C49EC:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824ba690
	sub_824BA690(ctx, base);
	// stw r3,0(r21)
	PPC_STORE_U32(r21.u32 + 0, ctx.r3.u32);
loc_824C49FC:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824f0058
	sub_824F0058(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// oris r11,r11,256
	r11.u64 = r11.u64 | 16777216;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_824C4A14:
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// addi r21,r21,4
	r21.s64 = r21.s64 + 4;
	// cmplw cr6,r20,r15
	cr6.compare<uint32_t>(r20.u32, r15.u32, xer);
	// blt cr6,0x824c45a8
	if (cr6.lt) goto loc_824C45A8;
loc_824C4A24:
	// li r29,0
	r29.s64 = 0;
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// beq cr6,0x824c4c58
	if (cr6.eq) goto loc_824C4C58;
	// lwz r24,120(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r27,r1,176
	r27.s64 = ctx.r1.s64 + 176;
	// li r25,3
	r25.s64 = 3;
	// li r26,1
	r26.s64 = 1;
loc_824C4A44:
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// lwz r30,12(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// beq cr6,0x824c4a7c
	if (cr6.eq) goto loc_824C4A7C;
	// addi r10,r1,176
	ctx.r10.s64 = ctx.r1.s64 + 176;
loc_824C4A5C:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// beq cr6,0x824c4a7c
	if (cr6.eq) goto loc_824C4A7C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// blt cr6,0x824c4a5c
	if (cr6.lt) goto loc_824C4A5C;
loc_824C4A7C:
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// bne cr6,0x824c4c48
	if (!cr6.eq) goto loc_824C4C48;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// li r31,0
	r31.s64 = 0;
	// lbz r11,97(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 97);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r9,140(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// li r7,0
	ctx.r7.s64 = 0;
	// rlwimi r10,r11,19,12,12
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 19) & 0x80000) | (ctx.r10.u64 & 0xFFFFFFFFFFF7FFFF);
	// stw r30,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r30.u32);
	// mr r5,r15
	ctx.r5.u64 = r15.u64;
	// rlwinm r10,r10,0,0,12
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFF80000;
	// stw r9,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r9.u32);
	// stw r10,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r10.u32);
loc_824C4AB4:
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// lwz r8,196(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwzx r11,r7,r11
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bne cr6,0x824c4b54
	if (!cr6.eq) goto loc_824C4B54;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r10,17,28,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 17) & 0xE;
	// addi r4,r1,204
	ctx.r4.s64 = ctx.r1.s64 + 204;
	// rlwinm r9,r11,27,30,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// lwzx r3,r7,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// srw r11,r14,r6
	r11.u64 = ctx.r6.u8 & 0x20 ? 0 : (r14.u32 >> (ctx.r6.u8 & 0x3F));
	// slw r23,r26,r9
	r23.u64 = ctx.r9.u8 & 0x20 ? 0 : (r26.u32 << (ctx.r9.u8 & 0x3F));
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// or r31,r23,r31
	r31.u64 = r23.u64 | r31.u64;
	// slw r23,r26,r11
	r23.u64 = r11.u8 & 0x20 ? 0 : (r26.u32 << (r11.u8 & 0x3F));
	// slw r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r8.u8 & 0x3F));
	// or r29,r23,r29
	r29.u64 = r23.u64 | r29.u64;
	// slw r23,r25,r8
	r23.u64 = ctx.r8.u8 & 0x20 ? 0 : (r25.u32 << (ctx.r8.u8 & 0x3F));
	// andc r8,r10,r23
	ctx.r8.u64 = ctx.r10.u64 & ~r23.u64;
	// or r11,r8,r11
	r11.u64 = ctx.r8.u64 | r11.u64;
	// rlwimi r11,r10,0,0,23
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFF00) | (r11.u64 & 0xFFFFFFFF000000FF);
	// rlwinm r10,r11,17,28,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 17) & 0xE;
	// slw r8,r25,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (r25.u32 << (ctx.r10.u8 & 0x3F));
	// slw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// not r9,r8
	ctx.r9.u64 = ~ctx.r8.u64;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// and r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 & r11.u64;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// rlwimi r10,r11,0,24,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFFFFFFF00FF) | (ctx.r10.u64 & 0xFF00);
	// rlwinm r11,r10,18,27,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x1C;
	// stw r10,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r10.u32);
	// stwx r3,r11,r4
	PPC_STORE_U32(r11.u32 + ctx.r4.u32, ctx.r3.u32);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// rlwinm r10,r11,0,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF0000;
	// addis r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 65536;
	// rlwimi r10,r11,0,16,12
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFFFFFF8FFFF) | (ctx.r10.u64 & 0x70000);
	// stw r10,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r10.u32);
loc_824C4B54:
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// bne 0x824c4ab4
	if (!cr0.eq) goto loc_824C4AB4;
	// addi r11,r24,4
	r11.s64 = r24.s64 + 4;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c4b98
	if (!cr0.eq) goto loc_824C4B98;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-4
	xer.ca = r11.u32 > 3;
	r11.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c4b98
	if (cr0.eq) goto loc_824C4B98;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x824c4ba8
	if (!cr6.gt) goto loc_824C4BA8;
loc_824C4B98:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824bea10
	sub_824BEA10(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_824C4BA8:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// li r5,28
	ctx.r5.s64 = 28;
	// mulli r9,r10,28
	ctx.r9.s64 = ctx.r10.s64 * 28;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r3,r9,16
	ctx.r3.s64 = ctx.r9.s64 + 16;
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r10,r11,31,28,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x824c4c48
	if (!cr6.eq) goto loc_824C4C48;
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,384
	cr6.compare<uint32_t>(r11.u32, 384, xer);
	// bne cr6,0x824c4c48
	if (!cr6.eq) goto loc_824C4C48;
	// lwz r4,44(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 44);
	// lwz r3,40(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// bl 0x824a91e8
	sub_824A91E8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c4c48
	if (cr0.eq) goto loc_824C4C48;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
loc_824C4C00:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c4c3c
	if (cr6.eq) goto loc_824C4C3C;
	// lwz r11,16(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// addi r9,r11,0
	ctx.r9.s64 = r11.s64 + 0;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824c4c34
	if (cr0.eq) goto loc_824C4C34;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x824c4c48
	if (!cr6.eq) goto loc_824C4C48;
loc_824C4C34:
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// b 0x824c4c00
	goto loc_824C4C00;
loc_824C4C3C:
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
loc_824C4C48:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmplw cr6,r28,r15
	cr6.compare<uint32_t>(r28.u32, r15.u32, xer);
	// blt cr6,0x824c4a44
	if (cr6.lt) goto loc_824C4A44;
loc_824C4C58:
	// lbz r11,96(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 96);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c4ca8
	if (!cr0.eq) goto loc_824C4CA8;
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r11,31,28,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// bne cr6,0x824c4ca8
	if (!cr6.eq) goto loc_824C4CA8;
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,384
	cr6.compare<uint32_t>(r11.u32, 384, xer);
	// bne cr6,0x824c4ca8
	if (!cr6.eq) goto loc_824C4CA8;
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r4,44(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// lwz r3,40(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// bl 0x824a91e8
	sub_824A91E8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c4ca8
	if (cr0.eq) goto loc_824C4CA8;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
loc_824C4CA8:
	// lwz r11,168(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c4ccc
	if (!cr0.eq) goto loc_824C4CCC;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r30,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, r30.u32);
	// bne cr6,0x824c40a4
	if (!cr6.eq) goto loc_824C40A4;
loc_824C4CCC:
	// lwz r3,120(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
loc_824C4CD0:
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
	// stw r3,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r3.u32);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c4ce8
	if (cr0.eq) goto loc_824C4CE8;
	// li r11,0
	r11.s64 = 0;
loc_824C4CE8:
	// stw r11,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r11.u32);
loc_824C4CF4:
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c4d14
	if (cr6.eq) goto loc_824C4D14;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824c4d18
	if (!cr6.eq) goto loc_824C4D18;
loc_824C4D14:
	// li r11,1
	r11.s64 = 1;
loc_824C4D18:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c4dac
	if (!cr0.eq) goto loc_824C4DAC;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x824bc3c0
	sub_824BC3C0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r30,r31,12
	r30.s64 = r31.s64 + 12;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x824ba748
	sub_824BA748(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824c4cf4
	if (cr0.eq) goto loc_824C4CF4;
	// lhz r11,8(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 8);
	// li r7,0
	ctx.r7.s64 = 0;
	// clrlwi. r5,r11,29
	ctx.r5.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq 0x824c4cf4
	if (cr0.eq) goto loc_824C4CF4;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r8,r3,40
	ctx.r8.s64 = ctx.r3.s64 + 40;
	// rlwinm r6,r11,13,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
loc_824C4D5C:
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
loc_824C4D64:
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// bge cr6,0x824c3d2c
	if (!cr6.lt) goto loc_824C3D2C;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// cmplw cr6,r9,r4
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r4.u32, xer);
	// beq cr6,0x824c4d98
	if (cr6.eq) goto loc_824C4D98;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// b 0x824c4d64
	goto loc_824C4D64;
loc_824C4D8C:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C4D98:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r7,r5
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r5.u32, xer);
	// bge cr6,0x824c4cf4
	if (!cr6.lt) goto loc_824C4CF4;
	// b 0x824c4d5c
	goto loc_824C4D5C;
loc_824C4DAC:
	// lwz r23,436(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x824b06e8
	sub_824B06E8(ctx, base);
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// beq cr6,0x824c4e48
	if (cr6.eq) goto loc_824C4E48;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// li r11,0
	r11.s64 = 0;
	// clrlwi. r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824c4e04
	if (!cr0.eq) goto loc_824C4E04;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c4e04
	if (cr0.eq) goto loc_824C4E04;
loc_824C4DE4:
	// rlwinm r9,r10,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// clrlwi. r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824c4e04
	if (!cr0.eq) goto loc_824C4E04;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824c4de4
	if (!cr6.eq) goto loc_824C4DE4;
loc_824C4E04:
	// lwz r10,444(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// lwz r9,132(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// lwz r9,152(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r10,48(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm. r10,r10,6,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c4e3c
	if (!cr0.eq) goto loc_824C4E3C;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_824C4E3C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x824c3d2c
	if (cr6.gt) goto loc_824C3D2C;
	// lwz r23,436(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
loc_824C4E48:
	// lwz r31,124(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r22,444(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x8251aa40
	sub_8251AA40(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// beq cr6,0x824c4e8c
	if (cr6.eq) goto loc_824C4E8C;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bne cr6,0x824c3d2c
	if (!cr6.eq) goto loc_824C3D2C;
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// b 0x824c3d2c
	goto loc_824C3D2C;
loc_824C4E8C:
	// li r21,0
	r21.s64 = 0;
	// lwz r3,136(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r21,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r21.u32);
	// beq cr6,0x824c4ea4
	if (cr6.eq) goto loc_824C4EA4;
	// bl 0x824be978
	sub_824BE978(ctx, base);
loc_824C4EA4:
	// lwz r29,144(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// lbz r6,98(r1)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r1.u32 + 98);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r4,120(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r8,624(r23)
	ctx.r8.u64 = PPC_LOAD_U32(r23.u32 + 624);
	// bl 0x824c0b50
	sub_824C0B50(ctx, base);
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,21
	cr6.compare<uint32_t>(r11.u32, 21, xer);
	// blt cr6,0x824c4ef4
	if (cr6.lt) goto loc_824C4EF4;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824c4ef8
	if (!cr6.gt) goto loc_824C4EF8;
loc_824C4EF4:
	// mr r11,r21
	r11.u64 = r21.u64;
loc_824C4EF8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c4fd4
	if (cr0.eq) goto loc_824C4FD4;
loc_824C4F00:
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 4);
loc_824C4F04:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c4fe0
	if (cr6.eq) goto loc_824C4FE0;
	// lwz r11,16(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// subf r9,r21,r11
	ctx.r9.s64 = r11.s64 - r21.s64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824c4f78
	if (cr0.eq) goto loc_824C4F78;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r9,r9,0,4,6
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xE000000;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824c4f78
	if (cr0.eq) goto loc_824C4F78;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_824C4F48:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c4f78
	if (cr0.eq) goto loc_824C4F78;
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r9,r21,r9
	ctx.r9.s64 = ctx.r9.s64 - r21.s64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne 0x824c4f80
	if (!cr0.eq) goto loc_824C4F80;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// b 0x824c4f48
	goto loc_824C4F48;
loc_824C4F78:
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// b 0x824c4f04
	goto loc_824C4F04;
loc_824C4F80:
	// lwz r31,16(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,63
	cr6.compare<uint32_t>(r11.u32, 63, xer);
	// beq cr6,0x824c4f9c
	if (cr6.eq) goto loc_824C4F9C;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// bne cr6,0x824c4fc0
	if (!cr6.eq) goto loc_824C4FC0;
loc_824C4F9C:
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// lwz r3,24(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x8255e770
	sub_8255E770(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x8255a3c0
	sub_8255A3C0(ctx, base);
	// b 0x824c4f00
	goto loc_824C4F00;
loc_824C4FC0:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r4,3500
	ctx.r4.s64 = 3500;
	// addi r5,r11,28060
	ctx.r5.s64 = r11.s64 + 28060;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C4FD4:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x8255a3c0
	sub_8255A3C0(ctx, base);
loc_824C4FE0:
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// lwz r11,48(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 48);
	// rlwinm r11,r11,0,6,4
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFBFFFFFF;
	// stw r11,48(r22)
	PPC_STORE_U32(r22.u32 + 48, r11.u32);
	// bne cr6,0x824c5044
	if (!cr6.eq) goto loc_824C5044;
	// lwz r29,108(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x8255e540
	sub_8255E540(ctx, base);
	// lwz r31,100(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8255e540
	sub_8255E540(ctx, base);
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8255e6b0
	sub_8255E6B0(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c5074
	if (!cr6.eq) goto loc_824C5074;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// b 0x824c507c
	goto loc_824C507C;
loc_824C5044:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8255e540
	sub_8255E540(ctx, base);
	// lwz r29,108(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
loc_824C5050:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x824be2e0
	sub_824BE2E0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bne 0x824c50c8
	if (!cr0.eq) goto loc_824C50C8;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C5074:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
loc_824C507C:
	// cmplw cr6,r22,r5
	cr6.compare<uint32_t>(r22.u32, ctx.r5.u32, xer);
	// beq cr6,0x824c50a4
	if (cr6.eq) goto loc_824C50A4;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x824be2e0
	sub_824BE2E0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c50a4
	if (!cr0.eq) goto loc_824C50A4;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C50A4:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x824be2e0
	sub_824BE2E0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c5050
	if (!cr0.eq) goto loc_824C5050;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C50C8:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// bl 0x824be2e0
	sub_824BE2E0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c50e8
	if (!cr0.eq) goto loc_824C50E8;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C50E8:
	// lwz r10,12(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 12);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c5104
	if (cr0.eq) goto loc_824C5104;
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x824c5108
	if (!cr6.eq) goto loc_824C5108;
loc_824C5104:
	// mr r11,r21
	r11.u64 = r21.u64;
loc_824C5108:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c5158
	if (!cr0.eq) goto loc_824C5158;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c5158
	if (cr6.eq) goto loc_824C5158;
	// lwz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi r5,0
	cr0.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq 0x824c5158
	if (cr0.eq) goto loc_824C5158;
	// lwz r11,8(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c5140
	if (cr0.eq) goto loc_824C5140;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x824c5144
	if (!cr6.eq) goto loc_824C5144;
loc_824C5140:
	// mr r11,r21
	r11.u64 = r21.u64;
loc_824C5144:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c5158
	if (!cr0.eq) goto loc_824C5158;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x824be2e0
	sub_824BE2E0(ctx, base);
loc_824C5158:
	// lwz r11,28(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 28);
	// li r20,1
	r20.s64 = 1;
	// stb r20,104(r1)
	PPC_STORE_U8(ctx.r1.u32 + 104, r20.u8);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c51d0
	if (!cr0.eq) goto loc_824C51D0;
	// lwz r11,24(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 24);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r28,r11,-36
	xer.ca = r11.u32 > 35;
	r28.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824c51d0
	if (cr0.eq) goto loc_824C51D0;
	// li r24,13952
	r24.s64 = 13952;
loc_824C5180:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c5198
	if (cr0.eq) goto loc_824C5198;
	// mr r25,r21
	r25.u64 = r21.u64;
	// b 0x824c51a0
	goto loc_824C51A0;
loc_824C5198:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r25,r11,-36
	r25.s64 = r11.s64 + -36;
loc_824C51A0:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm. r10,r11,8,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c51e4
	if (!cr0.eq) goto loc_824C51E4;
	// oris r11,r11,256
	r11.u64 = r11.u64 | 16777216;
	// stw r11,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r11.u32);
loc_824C51B4:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r5,624(r23)
	ctx.r5.u64 = PPC_LOAD_U32(r23.u32 + 624);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x824f70a0
	sub_824F70A0(ctx, base);
loc_824C51C4:
	// mr r28,r25
	r28.u64 = r25.u64;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x824c5180
	if (!cr6.eq) goto loc_824C5180;
loc_824C51D0:
	// lwz r11,28(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c538c
	if (!cr0.eq) goto loc_824C538C;
	// lwz r11,24(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 24);
	// b 0x824c5380
	goto loc_824C5380;
loc_824C51E4:
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c51c4
	if (cr0.eq) goto loc_824C51C4;
	// mr r26,r21
	r26.u64 = r21.u64;
	// addi r27,r28,40
	r27.s64 = r28.s64 + 40;
loc_824C5200:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r11,r11,13,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// bge cr6,0x824c51b4
	if (!cr6.lt) goto loc_824C51B4;
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// lwz r30,12(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// oris r11,r11,256
	r11.u64 = r11.u64 | 16777216;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// lwz r5,624(r23)
	ctx.r5.u64 = PPC_LOAD_U32(r23.u32 + 624);
	// bl 0x824f70a0
	sub_824F70A0(ctx, base);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
loc_824C5238:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c5284
	if (cr6.eq) goto loc_824C5284;
	// lwz r11,16(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// subf r9,r21,r11
	ctx.r9.s64 = r11.s64 - r21.s64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824c527c
	if (cr0.eq) goto loc_824C527C;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r9,r9,0,18,24
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x3F80;
	// subf r9,r24,r9
	ctx.r9.s64 = ctx.r9.s64 - r24.s64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm. r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824c527c
	if (cr0.eq) goto loc_824C527C;
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// bne cr6,0x824c5294
	if (!cr6.eq) goto loc_824C5294;
loc_824C527C:
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// b 0x824c5238
	goto loc_824C5238;
loc_824C5284:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,0,15,17
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1C000;
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// ble cr6,0x824c5358
	if (!cr6.gt) goto loc_824C5358;
loc_824C5294:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r5,r28,0,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFC;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// rlwinm r29,r11,27,30,31
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x82556538
	sub_82556538(ctx, base);
	// lwz r31,12(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba9b8
	sub_824BA9B8(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// oris r11,r11,256
	r11.u64 = r11.u64 | 16777216;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm. r10,r10,9,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c5358
	if (cr0.eq) goto loc_824C5358;
	// oris r11,r11,128
	r11.u64 = r11.u64 | 8388608;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// rlwinm. r10,r11,0,13,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c52f4
	if (cr0.eq) goto loc_824C52F4;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// sth r11,14(r31)
	PPC_STORE_U16(r31.u32 + 14, r11.u16);
	// b 0x824c5300
	goto loc_824C5300;
loc_824C52F4:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824c5858
	sub_824C5858(ctx, base);
	// sth r3,14(r31)
	PPC_STORE_U16(r31.u32 + 14, ctx.r3.u16);
loc_824C5300:
	// lwz r3,624(r23)
	ctx.r3.u64 = PPC_LOAD_U32(r23.u32 + 624);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c5334
	if (!cr0.eq) goto loc_824C5334;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-4
	xer.ca = r11.u32 > 3;
	r11.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5334
	if (cr0.eq) goto loc_824C5334;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x824c5340
	if (!cr6.gt) goto loc_824C5340;
loc_824C5334:
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_824C5340:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// stwx r31,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r31.u32);
loc_824C5358:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// b 0x824c5200
	goto loc_824C5200;
loc_824C5364:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x824c05f8
	sub_824C05F8(ctx, base);
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c538c
	if (!cr0.eq) goto loc_824C538C;
loc_824C5380:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-36
	xer.ca = r11.u32 > 35;
	r31.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x824c5364
	if (!cr0.eq) goto loc_824C5364;
loc_824C538C:
	// stb r20,105(r1)
	PPC_STORE_U8(ctx.r1.u32 + 105, r20.u8);
	// b 0x824c5434
	goto loc_824C5434;
loc_824C5394:
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824bbb90
	sub_824BBB90(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5434
	if (cr0.eq) goto loc_824C5434;
	// addi r8,r1,97
	ctx.r8.s64 = ctx.r1.s64 + 97;
	// lwz r7,144(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r6,116(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,108(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r3,436(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// bl 0x824bbb90
	sub_824BBB90(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5434
	if (cr0.eq) goto loc_824C5434;
	// lbz r31,98(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + 98);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r29,444(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// lwz r30,436(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lbz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 96);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x824bbf38
	sub_824BBF38(ctx, base);
	// mr r11,r31
	r11.u64 = r31.u64;
	// lbz r8,97(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 97);
	// li r6,0
	ctx.r6.s64 = 0;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
loc_824C541C:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r5,108(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// bl 0x824bbf38
	sub_824BBF38(ctx, base);
	// lwz r11,48(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 48);
	// oris r11,r11,3072
	r11.u64 = r11.u64 | 201326592;
	// stw r11,48(r29)
	PPC_STORE_U32(r29.u32 + 48, r11.u32);
loc_824C5434:
	// lwz r31,120(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c5460
	if (cr6.eq) goto loc_824C5460;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824be8e0
	sub_824BE8E0(ctx, base);
	// lwz r28,436(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// addi r11,r28,924
	r11.s64 = r28.s64 + 924;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r31,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r31.u32);
	// b 0x824c5464
	goto loc_824C5464;
loc_824C5460:
	// lwz r28,436(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
loc_824C5464:
	// lwz r31,128(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// b 0x824c5484
	goto loc_824C5484;
loc_824C546C:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r31,28(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
loc_824C5484:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c546c
	if (!cr6.eq) goto loc_824C546C;
	// lwz r29,136(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824c5578
	if (cr6.eq) goto loc_824C5578;
loc_824C5498:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c54a8
	if (cr0.eq) goto loc_824C54A8;
	// li r11,0
	r11.s64 = 0;
loc_824C54A8:
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c5560
	if (!cr0.eq) goto loc_824C5560;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c54c4
	if (cr6.eq) goto loc_824C54C4;
	// li r4,0
	ctx.r4.s64 = 0;
	// b 0x824c54d0
	goto loc_824C54D0;
loc_824C54C4:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r4,r11,-4
	ctx.r4.s64 = r11.s64 + -4;
loc_824C54D0:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// addic. r10,r11,-1
	xer.ca = r11.u32 > 0;
	ctx.r10.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r31,r11,r4
	r31.u64 = PPC_LOAD_U32(r11.u32 + ctx.r4.u32);
	// stw r10,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r10.u32);
	// bne 0x824c5524
	if (!cr0.eq) goto loc_824C5524;
	// rlwinm r11,r4,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82496e38
	sub_82496E38(ctx, base);
loc_824C5524:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r30,12(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// bl 0x824bb028
	sub_824BB028(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824a98e8
	sub_824A98E8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5498
	if (cr0.eq) goto loc_824C5498;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// b 0x824c5498
	goto loc_824C5498;
loc_824C5560:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824be978
	sub_824BE978(ctx, base);
	// addi r11,r28,924
	r11.s64 = r28.s64 + 924;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r10.u32);
	// stw r29,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r29.u32);
loc_824C5578:
	// lbz r11,104(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 104);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c558c
	if (cr0.eq) goto loc_824C558C;
	// lwz r3,436(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// bl 0x8251cbe0
	sub_8251CBE0(ctx, base);
loc_824C558C:
	// lbz r3,105(r1)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r1.u32 + 105);
	// b 0x824c5598
	goto loc_824C5598;
loc_824C5594:
	// li r3,0
	ctx.r3.s64 = 0;
loc_824C5598:
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_824C55A0"))) PPC_WEAK_FUNC(sub_824C55A0);
PPC_FUNC_IMPL(__imp__sub_824C55A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcdc
	// lis r11,-28311
	r11.s64 = -1855389696;
	// lis r6,0
	ctx.r6.s64 = 0;
	// ori r11,r11,5192
	r11.u64 = r11.u64 | 5192;
	// ori r6,r6,36262
	ctx.r6.u64 = ctx.r6.u64 | 36262;
	// rlwinm r10,r3,16,24,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 16) & 0xFF;
	// rldimi r11,r6,32,0
	r11.u64 = (__builtin_rotateleft64(ctx.r6.u64, 32) & 0xFFFFFFFF00000000) | (r11.u64 & 0xFFFFFFFF);
	// rlwinm r9,r3,24,24,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 24) & 0xFF;
	// mr r28,r11
	r28.u64 = r11.u64;
	// mr r27,r11
	r27.u64 = r11.u64;
	// mr r26,r11
	r26.u64 = r11.u64;
	// mr r25,r11
	r25.u64 = r11.u64;
	// mr r24,r11
	r24.u64 = r11.u64;
	// mr r23,r11
	r23.u64 = r11.u64;
	// mr r22,r11
	r22.u64 = r11.u64;
	// mr r21,r11
	r21.u64 = r11.u64;
	// rlwinm r5,r10,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0xFFFFFFF;
	// rlwinm r11,r3,8,24,31
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 8) & 0xFF;
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// rlwinm r3,r9,28,4,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 28) & 0xFFFFFFF;
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
	// clrldi r9,r5,32
	ctx.r9.u64 = ctx.r5.u64 & 0xFFFFFFFF;
	// rlwinm r30,r8,28,4,31
	r30.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0xFFFFFFF;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// rlwinm r7,r11,28,4,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// clrldi r11,r7,32
	r11.u64 = ctx.r7.u64 & 0xFFFFFFFF;
	// clrldi r10,r6,60
	ctx.r10.u64 = ctx.r6.u64 & 0xF;
	// mr r29,r8
	r29.u64 = ctx.r8.u64;
	// clrldi r8,r4,60
	ctx.r8.u64 = ctx.r4.u64 & 0xF;
	// clrldi r5,r30,32
	ctx.r5.u64 = r30.u64 & 0xFFFFFFFF;
	// clrldi r7,r3,32
	ctx.r7.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// clrldi r6,r31,60
	ctx.r6.u64 = r31.u64 & 0xF;
	// clrldi r4,r29,60
	ctx.r4.u64 = r29.u64 & 0xF;
	// srd r30,r26,r9
	r30.u64 = ctx.r9.u8 & 0x40 ? 0 : (r26.u64 >> (ctx.r9.u8 & 0x7F));
	// srd r3,r28,r11
	ctx.r3.u64 = r11.u8 & 0x40 ? 0 : (r28.u64 >> (r11.u8 & 0x7F));
	// srd r31,r27,r10
	r31.u64 = ctx.r10.u8 & 0x40 ? 0 : (r27.u64 >> (ctx.r10.u8 & 0x7F));
	// srd r29,r25,r8
	r29.u64 = ctx.r8.u8 & 0x40 ? 0 : (r25.u64 >> (ctx.r8.u8 & 0x7F));
	// srd r26,r22,r5
	r26.u64 = ctx.r5.u8 & 0x40 ? 0 : (r22.u64 >> (ctx.r5.u8 & 0x7F));
	// srd r28,r24,r7
	r28.u64 = ctx.r7.u8 & 0x40 ? 0 : (r24.u64 >> (ctx.r7.u8 & 0x7F));
	// srd r27,r23,r6
	r27.u64 = ctx.r6.u8 & 0x40 ? 0 : (r23.u64 >> (ctx.r6.u8 & 0x7F));
	// srd r25,r21,r4
	r25.u64 = ctx.r4.u8 & 0x40 ? 0 : (r21.u64 >> (ctx.r4.u8 & 0x7F));
	// srd r30,r30,r9
	r30.u64 = ctx.r9.u8 & 0x40 ? 0 : (r30.u64 >> (ctx.r9.u8 & 0x7F));
	// srd r3,r3,r11
	ctx.r3.u64 = r11.u8 & 0x40 ? 0 : (ctx.r3.u64 >> (r11.u8 & 0x7F));
	// srd r31,r31,r10
	r31.u64 = ctx.r10.u8 & 0x40 ? 0 : (r31.u64 >> (ctx.r10.u8 & 0x7F));
	// srd r29,r29,r8
	r29.u64 = ctx.r8.u8 & 0x40 ? 0 : (r29.u64 >> (ctx.r8.u8 & 0x7F));
	// srd r26,r26,r5
	r26.u64 = ctx.r5.u8 & 0x40 ? 0 : (r26.u64 >> (ctx.r5.u8 & 0x7F));
	// srd r28,r28,r7
	r28.u64 = ctx.r7.u8 & 0x40 ? 0 : (r28.u64 >> (ctx.r7.u8 & 0x7F));
	// srd r27,r27,r6
	r27.u64 = ctx.r6.u8 & 0x40 ? 0 : (r27.u64 >> (ctx.r6.u8 & 0x7F));
	// srd r25,r25,r4
	r25.u64 = ctx.r4.u8 & 0x40 ? 0 : (r25.u64 >> (ctx.r4.u8 & 0x7F));
	// srd r9,r30,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r30.u64 >> (ctx.r9.u8 & 0x7F));
	// srd r11,r3,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r3.u64 >> (r11.u8 & 0x7F));
	// srd r10,r31,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r31.u64 >> (ctx.r10.u8 & 0x7F));
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// clrlwi r10,r9,29
	ctx.r10.u64 = ctx.r9.u32 & 0x7;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srd r9,r29,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x40 ? 0 : (r29.u64 >> (ctx.r8.u8 & 0x7F));
	// clrlwi r10,r9,29
	ctx.r10.u64 = ctx.r9.u32 & 0x7;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srd r9,r28,r7
	ctx.r9.u64 = ctx.r7.u8 & 0x40 ? 0 : (r28.u64 >> (ctx.r7.u8 & 0x7F));
	// clrlwi r10,r9,29
	ctx.r10.u64 = ctx.r9.u32 & 0x7;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srd r9,r27,r6
	ctx.r9.u64 = ctx.r6.u8 & 0x40 ? 0 : (r27.u64 >> (ctx.r6.u8 & 0x7F));
	// clrlwi r10,r9,29
	ctx.r10.u64 = ctx.r9.u32 & 0x7;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srd r9,r26,r5
	ctx.r9.u64 = ctx.r5.u8 & 0x40 ? 0 : (r26.u64 >> (ctx.r5.u8 & 0x7F));
	// clrlwi r10,r9,29
	ctx.r10.u64 = ctx.r9.u32 & 0x7;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srd r9,r25,r4
	ctx.r9.u64 = ctx.r4.u8 & 0x40 ? 0 : (r25.u64 >> (ctx.r4.u8 & 0x7F));
	// clrlwi r10,r9,29
	ctx.r10.u64 = ctx.r9.u32 & 0x7;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_824C56CC"))) PPC_WEAK_FUNC(sub_824C56CC);
PPC_FUNC_IMPL(__imp__sub_824C56CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C56D0"))) PPC_WEAK_FUNC(sub_824C56D0);
PPC_FUNC_IMPL(__imp__sub_824C56D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	// lwz r11,108(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 108);
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,10,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c570c
	if (cr0.eq) goto loc_824C570C;
	// b 0x824c56f0
	goto loc_824C56F0;
loc_824C56E4:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,0,9,9
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c56fc
	if (cr0.eq) goto loc_824C56FC;
loc_824C56F0:
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c56e4
	if (!cr0.eq) goto loc_824C56E4;
loc_824C56FC:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,10,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c56f0
	if (!cr0.eq) goto loc_824C56F0;
	// stw r11,108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 108, r11.u32);
loc_824C570C:
	// lwz r3,108(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 108);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5714"))) PPC_WEAK_FUNC(sub_824C5714);
PPC_FUNC_IMPL(__imp__sub_824C5714) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C5718"))) PPC_WEAK_FUNC(sub_824C5718);
PPC_FUNC_IMPL(__imp__sub_824C5718) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// stw r4,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r4.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stb r5,24(r31)
	PPC_STORE_U8(r31.u32 + 24, ctx.r5.u8);
	// lwz r4,8(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// bl 0x8255e590
	sub_8255E590(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5764"))) PPC_WEAK_FUNC(sub_824C5764);
PPC_FUNC_IMPL(__imp__sub_824C5764) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C5768"))) PPC_WEAK_FUNC(sub_824C5768);
PPC_FUNC_IMPL(__imp__sub_824C5768) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// b 0x824c57b4
	goto loc_824C57B4;
loc_824C5780:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r28,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r28.u32);
	// stw r28,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r28.u32);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c5800
	if (cr0.eq) goto loc_824C5800;
	// lwz r29,4(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// lbz r30,25(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 25);
	// lwz r4,12(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// bl 0x8255e590
	sub_8255E590(ctx, base);
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x824c57f8
	if (cr0.eq) goto loc_824C57F8;
loc_824C57B4:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c5780
	if (cr6.eq) goto loc_824C5780;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r28,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r28.u32);
loc_824C57CC:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// beq 0x824c5804
	if (cr0.eq) goto loc_824C5804;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,48(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm. r9,r10,10,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824c57cc
	if (!cr0.eq) goto loc_824C57CC;
	// rlwinm. r10,r10,12,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c57cc
	if (cr0.eq) goto loc_824C57CC;
	// b 0x824c5804
	goto loc_824C5804;
loc_824C57F8:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// b 0x824c5804
	goto loc_824C5804;
loc_824C5800:
	// li r3,0
	ctx.r3.s64 = 0;
loc_824C5804:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824C580C"))) PPC_WEAK_FUNC(sub_824C580C);
PPC_FUNC_IMPL(__imp__sub_824C580C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C5810"))) PPC_WEAK_FUNC(sub_824C5810);
PPC_FUNC_IMPL(__imp__sub_824C5810) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x824c583c
	if (cr6.eq) goto loc_824C583C;
loc_824C581C:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r10,0,12,12
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c5834
	if (!cr0.eq) goto loc_824C5834;
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c581c
	if (!cr0.eq) goto loc_824C581C;
loc_824C5834:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c5850
	if (!cr6.eq) goto loc_824C5850;
loc_824C583C:
	// rlwinm r11,r3,0,0,19
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFFF000;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r3,148(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 148);
	// b 0x82496e98
	sub_82496E98(ctx, base);
	return;
loc_824C5850:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5858"))) PPC_WEAK_FUNC(sub_824C5858);
PPC_FUNC_IMPL(__imp__sub_824C5858) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lhz r11,12(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 12);
	// clrlwi. r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5898
	if (cr0.eq) goto loc_824C5898;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,31,28,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// andc r11,r11,r10
	r11.u64 = r11.u64 & ~ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subf r11,r11,r3
	r11.s64 = ctx.r3.s64 - r11.s64;
	// addi r3,r11,31
	ctx.r3.s64 = r11.s64 + 31;
loc_824C5898:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C58AC"))) PPC_WEAK_FUNC(sub_824C58AC);
PPC_FUNC_IMPL(__imp__sub_824C58AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C58B0"))) PPC_WEAK_FUNC(sub_824C58B0);
PPC_FUNC_IMPL(__imp__sub_824C58B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x824c5858
	sub_824C5858(ctx, base);
	// lwz r11,672(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 672);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C58EC"))) PPC_WEAK_FUNC(sub_824C58EC);
PPC_FUNC_IMPL(__imp__sub_824C58EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C58F0"))) PPC_WEAK_FUNC(sub_824C58F0);
PPC_FUNC_IMPL(__imp__sub_824C58F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,644(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 644);
	// rlwinm r8,r4,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r8
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c5960
	if (cr0.eq) goto loc_824C5960;
	// cmplwi cr6,r11,65535
	cr6.compare<uint32_t>(r11.u32, 65535, xer);
	// bge cr6,0x824c5960
	if (!cr6.lt) goto loc_824C5960;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// cmplw cr6,r11,r4
	cr6.compare<uint32_t>(r11.u32, ctx.r4.u32, xer);
	// beq cr6,0x824c5950
	if (cr6.eq) goto loc_824C5950;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// b 0x824c5948
	goto loc_824C5948;
loc_824C5924:
	// cmplwi cr6,r11,65535
	cr6.compare<uint32_t>(r11.u32, 65535, xer);
	// bge cr6,0x824c5950
	if (!cr6.lt) goto loc_824C5950;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x824c5958
	if (cr6.eq) goto loc_824C5958;
	// stwx r11,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, r11.u32);
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,644(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 644);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lwzx r11,r7,r10
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
loc_824C5948:
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c5924
	if (!cr0.eq) goto loc_824C5924;
loc_824C5950:
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// blr 
	return;
loc_824C5958:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// b 0x82496e98
	sub_82496E98(ctx, base);
	return;
loc_824C5960:
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5968"))) PPC_WEAK_FUNC(sub_824C5968);
PPC_FUNC_IMPL(__imp__sub_824C5968) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// rlwinm r26,r11,31,28,31
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// rlwinm r28,r29,2,0,29
	r28.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
loc_824C5998:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824c59c4
	if (cr6.eq) goto loc_824C59C4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// addi r11,r26,-1
	r11.s64 = r26.s64 + -1;
	// andc r11,r26,r11
	r11.u64 = r26.u64 & ~r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r11,r11,31
	xer.ca = r11.u32 <= 31;
	r11.s64 = 31 - r11.s64;
	// add r31,r3,r11
	r31.u64 = ctx.r3.u64 + r11.u64;
	// mr r11,r31
	r11.u64 = r31.u64;
	// b 0x824c59cc
	goto loc_824C59CC;
loc_824C59C4:
	// li r31,0
	r31.s64 = 0;
	// li r11,0
	r11.s64 = 0;
loc_824C59CC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c5a2c
	if (cr6.eq) goto loc_824C5A2C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// subf r10,r3,r31
	ctx.r10.s64 = r31.s64 - ctx.r3.s64;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// and. r11,r11,r25
	r11.u64 = r11.u64 & r25.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c5a1c
	if (cr0.eq) goto loc_824C5A1C;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// cmplw cr6,r3,r29
	cr6.compare<uint32_t>(ctx.r3.u32, r29.u32, xer);
	// beq cr6,0x824c5a0c
	if (cr6.eq) goto loc_824C5A0C;
	// lwz r11,644(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 644);
	// stwx r3,r11,r28
	PPC_STORE_U32(r11.u32 + r28.u32, ctx.r3.u32);
loc_824C5A0C:
	// lwz r11,672(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 672);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// stwx r30,r11,r28
	PPC_STORE_U32(r11.u32 + r28.u32, r30.u32);
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
loc_824C5A1C:
	// addi r11,r26,-1
	r11.s64 = r26.s64 + -1;
	// andc r11,r26,r11
	r11.u64 = r26.u64 & ~r11.u64;
	// subf r26,r11,r26
	r26.s64 = r26.s64 - r11.s64;
	// b 0x824c5998
	goto loc_824C5998;
loc_824C5A2C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_824C5A34"))) PPC_WEAK_FUNC(sub_824C5A34);
PPC_FUNC_IMPL(__imp__sub_824C5A34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C5A38"))) PPC_WEAK_FUNC(sub_824C5A38);
PPC_FUNC_IMPL(__imp__sub_824C5A38) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5A48"))) PPC_WEAK_FUNC(sub_824C5A48);
PPC_FUNC_IMPL(__imp__sub_824C5A48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,0(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x824c5a74
	if (!cr0.eq) goto loc_824C5A74;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x824c5b14
	goto loc_824C5B14;
loc_824C5A74:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x824c5a84
	if (!cr6.eq) goto loc_824C5A84;
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x824c5b14
	goto loc_824C5B14;
loc_824C5A84:
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lwz r11,68(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 68);
	// lwz r10,68(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 68);
	// clrlwi r11,r11,4
	r11.u64 = r11.u32 & 0xFFFFFFF;
	// clrlwi r10,r10,4
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFFFFF;
	// subf. r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c5b10
	if (!cr0.eq) goto loc_824C5B10;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmpwi cr6,r11,109
	cr6.compare<int32_t>(r11.s32, 109, xer);
	// beq cr6,0x824c5acc
	if (cr6.eq) goto loc_824C5ACC;
	// cmpwi cr6,r11,111
	cr6.compare<int32_t>(r11.s32, 111, xer);
	// bne cr6,0x824c5ad0
	if (!cr6.eq) goto loc_824C5AD0;
	// li r9,2
	ctx.r9.s64 = 2;
	// b 0x824c5ad0
	goto loc_824C5AD0;
loc_824C5ACC:
	// li r9,1
	ctx.r9.s64 = 1;
loc_824C5AD0:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmpwi cr6,r11,109
	cr6.compare<int32_t>(r11.s32, 109, xer);
	// beq cr6,0x824c5af0
	if (cr6.eq) goto loc_824C5AF0;
	// cmpwi cr6,r11,111
	cr6.compare<int32_t>(r11.s32, 111, xer);
	// bne cr6,0x824c5af4
	if (!cr6.eq) goto loc_824C5AF4;
	// li r10,2
	ctx.r10.s64 = 2;
	// b 0x824c5af4
	goto loc_824C5AF4;
loc_824C5AF0:
	// li r10,1
	ctx.r10.s64 = 1;
loc_824C5AF4:
	// subf. r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c5b10
	if (!cr0.eq) goto loc_824C5B10;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// subf r11,r30,r3
	r11.s64 = ctx.r3.s64 - r30.s64;
loc_824C5B10:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
loc_824C5B14:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5B2C"))) PPC_WEAK_FUNC(sub_824C5B2C);
PPC_FUNC_IMPL(__imp__sub_824C5B2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C5B30"))) PPC_WEAK_FUNC(sub_824C5B30);
PPC_FUNC_IMPL(__imp__sub_824C5B30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,192(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 192);
	// lwz r8,192(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 192);
	// rlwinm r7,r9,3,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0x1;
	// rlwinm r6,r8,3,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0x1;
	// subf. r3,r7,r6
	ctx.r3.s64 = ctx.r6.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bnelr 
	if (!cr0.eq) return;
	// lwz r10,196(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 196);
	// lwz r11,196(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 196);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// bge cr6,0x824c5b6c
	if (!cr6.lt) goto loc_824C5B6C;
	// li r3,-1
	ctx.r3.s64 = -1;
	// blr 
	return;
loc_824C5B6C:
	// ble cr6,0x824c5b78
	if (!cr6.gt) goto loc_824C5B78;
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_824C5B78:
	// rlwinm r11,r9,29,21,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x7FF;
	// rlwinm r10,r8,29,21,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 29) & 0x7FF;
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5B88"))) PPC_WEAK_FUNC(sub_824C5B88);
PPC_FUNC_IMPL(__imp__sub_824C5B88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r11,0
	r11.s64 = 0;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// mr r29,r11
	r29.u64 = r11.u64;
	// stb r11,104(r1)
	PPC_STORE_U8(ctx.r1.u32 + 104, r11.u8);
	// mr r30,r11
	r30.u64 = r11.u64;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r4,8(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// bl 0x8255e590
	sub_8255E590(ctx, base);
	// b 0x824c5c4c
	goto loc_824C5C4C;
loc_824C5BE0:
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmplw cr6,r27,r3
	cr6.compare<uint32_t>(r27.u32, ctx.r3.u32, xer);
	// lwz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bne cr6,0x824c5c28
	if (!cr6.eq) goto loc_824C5C28;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x824c5c1c
	if (!cr6.eq) goto loc_824C5C1C;
	// lwz r11,16(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// b 0x824c5c24
	goto loc_824C5C24;
loc_824C5C1C:
	// lwz r11,68(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 68);
	// clrlwi r11,r11,4
	r11.u64 = r11.u32 & 0xFFFFFFF;
loc_824C5C24:
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_824C5C28:
	// cmplw cr6,r25,r3
	cr6.compare<uint32_t>(r25.u32, ctx.r3.u32, xer);
	// bne cr6,0x824c5c4c
	if (!cr6.eq) goto loc_824C5C4C;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x824c5c40
	if (!cr6.eq) goto loc_824C5C40;
	// lwz r11,16(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// b 0x824c5c48
	goto loc_824C5C48;
loc_824C5C40:
	// lwz r11,68(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 68);
	// clrlwi r11,r11,4
	r11.u64 = r11.u32 & 0xFFFFFFF;
loc_824C5C48:
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
loc_824C5C4C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824c5768
	sub_824C5768(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x824c5be0
	if (!cr0.eq) goto loc_824C5BE0;
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lis r11,4095
	r11.s64 = 268369920;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x824c5c78
	if (!cr6.gt) goto loc_824C5C78;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_824C5C78:
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
	// lwz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x824c5c90
	if (!cr6.gt) goto loc_824C5C90;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_824C5C90:
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_824C5C9C"))) PPC_WEAK_FUNC(sub_824C5C9C);
PPC_FUNC_IMPL(__imp__sub_824C5C9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C5CA0"))) PPC_WEAK_FUNC(sub_824C5CA0);
PPC_FUNC_IMPL(__imp__sub_824C5CA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// li r31,4
	r31.s64 = 4;
loc_824C5CB8:
	// ld r29,0(r30)
	r29.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// rldicl r11,r29,32,32
	r11.u64 = __builtin_rotateleft64(r29.u64, 32) & 0xFFFFFFFF;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// bl 0x824c55a0
	sub_824C55A0(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c55a0
	sub_824C55A0(ctx, base);
	// add r11,r27,r3
	r11.u64 = r27.u64 + ctx.r3.u64;
	// addic. r31,r31,-1
	xer.ca = r31.u32 > 0;
	r31.s64 = r31.s64 + -1;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bne 0x824c5cb8
	if (!cr0.eq) goto loc_824C5CB8;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_824C5CF4"))) PPC_WEAK_FUNC(sub_824C5CF4);
PPC_FUNC_IMPL(__imp__sub_824C5CF4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C5CF8"))) PPC_WEAK_FUNC(sub_824C5CF8);
PPC_FUNC_IMPL(__imp__sub_824C5CF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// rlwinm r11,r5,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// b 0x824c5d20
	goto loc_824C5D20;
loc_824C5D04:
	// ld r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// ld r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// bne cr6,0x824c5d30
	if (!cr6.eq) goto loc_824C5D30;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
loc_824C5D20:
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// blt cr6,0x824c5d04
	if (cr6.lt) goto loc_824C5D04;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_824C5D30:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5D38"))) PPC_WEAK_FUNC(sub_824C5D38);
PPC_FUNC_IMPL(__imp__sub_824C5D38) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r9,1
	ctx.r9.s64 = 1;
	// clrldi r10,r5,58
	ctx.r10.u64 = ctx.r5.u64 & 0x3F;
	// rlwinm r11,r5,29,3,28
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0x1FFFFFF8;
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// ldx r9,r11,r4
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + ctx.r4.u32);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stdx r10,r11,r4
	PPC_STORE_U64(r11.u32 + ctx.r4.u32, ctx.r10.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5D58"))) PPC_WEAK_FUNC(sub_824C5D58);
PPC_FUNC_IMPL(__imp__sub_824C5D58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,656(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 656);
	// li r7,2
	ctx.r7.s64 = 2;
	// subf r9,r11,r11
	ctx.r9.s64 = r11.s64 - r11.s64;
	// rlwinm r10,r11,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFC0;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// slw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// rlwinm r7,r10,29,3,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFF8;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// ldx r10,r7,r8
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r7.u32 + ctx.r8.u32);
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// and r3,r11,r9
	ctx.r3.u64 = r11.u64 & ctx.r9.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5D9C"))) PPC_WEAK_FUNC(sub_824C5D9C);
PPC_FUNC_IMPL(__imp__sub_824C5D9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C5DA0"))) PPC_WEAK_FUNC(sub_824C5DA0);
PPC_FUNC_IMPL(__imp__sub_824C5DA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,652(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 652);
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r11,r10
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5DB0"))) PPC_WEAK_FUNC(sub_824C5DB0);
PPC_FUNC_IMPL(__imp__sub_824C5DB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// rlwinm r30,r11,31,28,31
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// andc r10,r30,r10
	ctx.r10.u64 = r30.u64 & ~ctx.r10.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// subfic r10,r10,31
	xer.ca = ctx.r10.u32 <= 31;
	ctx.r10.s64 = 31 - ctx.r10.s64;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,652(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 652);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r11,r10
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5E1C"))) PPC_WEAK_FUNC(sub_824C5E1C);
PPC_FUNC_IMPL(__imp__sub_824C5E1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C5E20"))) PPC_WEAK_FUNC(sub_824C5E20);
PPC_FUNC_IMPL(__imp__sub_824C5E20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lis r24,0
	r24.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// ori r24,r24,65535
	r24.u64 = r24.u64 | 65535;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r10,8(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r11,652(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 652);
	// rlwinm r27,r10,31,28,31
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0xF;
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r26,r11,r10
	r26.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
loc_824C5E64:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x824c5f90
	if (cr6.eq) goto loc_824C5F90;
	// addi r10,r27,-1
	ctx.r10.s64 = r27.s64 + -1;
	// lwz r11,192(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 192);
	// li r30,0
	r30.s64 = 0;
	// andc r10,r27,r10
	ctx.r10.u64 = r27.u64 & ~ctx.r10.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// subfic r8,r10,31
	xer.ca = ctx.r10.u32 <= 31;
	ctx.r8.s64 = 31 - ctx.r10.s64;
	// clrlwi. r10,r11,29
	ctx.r10.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c5eb4
	if (cr0.eq) goto loc_824C5EB4;
	// rlwinm r9,r11,18,24,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0xFF;
	// li r11,0
	r11.s64 = 0;
loc_824C5E94:
	// srw r7,r9,r11
	ctx.r7.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// clrlwi r7,r7,30
	ctx.r7.u64 = ctx.r7.u32 & 0x3;
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// beq cr6,0x824c5eb4
	if (cr6.eq) goto loc_824C5EB4;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c5e94
	if (cr6.lt) goto loc_824C5E94;
loc_824C5EB4:
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bge cr6,0x824c5f78
	if (!cr6.lt) goto loc_824C5F78;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lhz r11,12(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 12);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// clrlwi r29,r11,29
	r29.u64 = r11.u32 & 0x7;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// add r11,r3,r29
	r11.u64 = ctx.r3.u64 + r29.u64;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bge cr6,0x824c5f4c
	if (!cr6.lt) goto loc_824C5F4C;
	// rlwinm r29,r30,2,0,29
	r29.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
loc_824C5EE8:
	// lwzx r10,r29,r26
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + r26.u32);
	// rlwinm r9,r31,29,3,28
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 29) & 0x1FFFFFF8;
	// clrldi r11,r31,58
	r11.u64 = r31.u64 & 0x3F;
	// ldx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r9.u32 + ctx.r10.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x824c5f14
	if (!cr6.eq) goto loc_824C5F14;
	// li r11,0
	r11.s64 = 0;
loc_824C5F14:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c5f40
	if (!cr0.eq) goto loc_824C5F40;
	// lhz r11,12(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 12);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// clrlwi r30,r11,29
	r30.u64 = r11.u32 & 0x7;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// add r11,r3,r30
	r11.u64 = ctx.r3.u64 + r30.u64;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x824c5ee8
	if (cr6.lt) goto loc_824C5EE8;
	// b 0x824c5f4c
	goto loc_824C5F4C;
loc_824C5F40:
	// cmplw cr6,r31,r24
	cr6.compare<uint32_t>(r31.u32, r24.u32, xer);
	// bge cr6,0x824c5f4c
	if (!cr6.lt) goto loc_824C5F4C;
	// mr r24,r31
	r24.u64 = r31.u64;
loc_824C5F4C:
	// lhz r11,12(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 12);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// clrlwi r30,r11,29
	r30.u64 = r11.u32 & 0x7;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// add r11,r3,r30
	r11.u64 = ctx.r3.u64 + r30.u64;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bge cr6,0x824c5f84
	if (!cr6.lt) goto loc_824C5F84;
	// addi r11,r27,-1
	r11.s64 = r27.s64 + -1;
	// andc r11,r27,r11
	r11.u64 = r27.u64 & ~r11.u64;
	// subf r27,r11,r27
	r27.s64 = r27.s64 - r11.s64;
	// b 0x824c5e64
	goto loc_824C5E64;
loc_824C5F78:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C5F84:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C5F90:
	// cmplwi cr6,r24,65535
	cr6.compare<uint32_t>(r24.u32, 65535, xer);
	// bne cr6,0x824c5fa4
	if (!cr6.eq) goto loc_824C5FA4;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C5FA4:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_824C5FB0"))) PPC_WEAK_FUNC(sub_824C5FB0);
PPC_FUNC_IMPL(__imp__sub_824C5FB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x824c5858
	sub_824C5858(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,652(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 652);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r11,r10
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C5FF8"))) PPC_WEAK_FUNC(sub_824C5FF8);
PPC_FUNC_IMPL(__imp__sub_824C5FF8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,40(r4)
	r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	// rlwinm r5,r4,0,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFC;
	// lwz r4,24(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// bl 0x82556538
	sub_82556538(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r30,12(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// rlwinm r5,r11,27,24,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0xFF;
	// rlwinm r4,r11,7,29,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x7;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82550ce0
	sub_82550CE0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r10,57
	ctx.r10.s64 = 57;
	// lis r9,0
	ctx.r9.s64 = 0;
	// rlwimi r11,r10,7,19,26
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 7) & 0x1FE0) | (r11.u64 & 0xFFFFFFFFFFFFE01F);
	// ori r9,r9,32783
	ctx.r9.u64 = ctx.r9.u64 | 32783;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwimi r11,r9,1,27,30
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 1) & 0x1E) | (r11.u64 & 0xFFFFFFFFFFFFFFE1);
	// rlwimi r11,r9,1,15,17
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 1) & 0x1C000) | (r11.u64 & 0xFFFFFFFFFFFE3FFF);
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// bl 0x824ba9b8
	sub_824BA9B8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C6084"))) PPC_WEAK_FUNC(sub_824C6084);
PPC_FUNC_IMPL(__imp__sub_824C6084) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C6088"))) PPC_WEAK_FUNC(sub_824C6088);
PPC_FUNC_IMPL(__imp__sub_824C6088) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lhz r11,12(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// clrlwi r28,r11,29
	r28.u64 = r11.u32 & 0x7;
	// rlwinm r22,r10,31,28,31
	r22.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0xF;
	// bl 0x824c5db0
	sub_824C5DB0(ctx, base);
	// add r24,r28,r31
	r24.u64 = r28.u64 + r31.u64;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// li r27,0
	r27.s64 = 0;
	// li r28,0
	r28.s64 = 0;
	// li r23,1
	r23.s64 = 1;
	// mr r26,r31
	r26.u64 = r31.u64;
	// cmplw cr6,r31,r24
	cr6.compare<uint32_t>(r31.u32, r24.u32, xer);
	// bge cr6,0x824c6184
	if (!cr6.lt) goto loc_824C6184;
	// li r31,0
	r31.s64 = 0;
loc_824C60E8:
	// and. r11,r23,r22
	r11.u64 = r23.u64 & r22.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c6174
	if (cr0.eq) goto loc_824C6174;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// li r10,3
	ctx.r10.s64 = 3;
	// lwz r11,192(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// li r7,2
	ctx.r7.s64 = 2;
	// lwz r8,656(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 656);
	// rlwinm r6,r11,18,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0xFF;
	// slw r11,r10,r31
	r11.u64 = r31.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r31.u8 & 0x3F));
	// andc r5,r27,r11
	ctx.r5.u64 = r27.u64 & ~r11.u64;
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r9,r11,r11
	ctx.r9.s64 = r11.s64 - r11.s64;
	// rlwinm r10,r11,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFC0;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// slw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// rlwinm r7,r10,29,3,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFF8;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// ldx r10,r7,r8
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r7.u32 + ctx.r8.u32);
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// and r11,r11,r9
	r11.u64 = r11.u64 & ctx.r9.u64;
	// li r9,1
	ctx.r9.s64 = 1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srw r11,r6,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 >> (r11.u8 & 0x3F));
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// slw r10,r11,r31
	ctx.r10.u64 = r31.u8 & 0x20 ? 0 : (r11.u32 << (r31.u8 & 0x3F));
	// slw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// or r27,r5,r10
	r27.u64 = ctx.r5.u64 | ctx.r10.u64;
	// or r28,r11,r28
	r28.u64 = r11.u64 | r28.u64;
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
loc_824C6174:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// rlwinm r23,r23,1,0,30
	r23.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r26,r24
	cr6.compare<uint32_t>(r26.u32, r24.u32, xer);
	// blt cr6,0x824c60e8
	if (cr6.lt) goto loc_824C60E8;
loc_824C6184:
	// lwz r11,192(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwimi r28,r11,14,22,27
	r28.u64 = (__builtin_rotateleft32(r11.u32, 14) & 0x3F0) | (r28.u64 & 0xFFFFFFFFFFFFFC0F);
	// clrlwi r4,r28,22
	ctx.r4.u64 = r28.u32 & 0x3FF;
	// bl 0x825518d8
	sub_825518D8(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_824C61A4"))) PPC_WEAK_FUNC(sub_824C61A4);
PPC_FUNC_IMPL(__imp__sub_824C61A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C61A8"))) PPC_WEAK_FUNC(sub_824C61A8);
PPC_FUNC_IMPL(__imp__sub_824C61A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r11,1
	r11.s64 = 1;
	// clrldi r8,r5,58
	ctx.r8.u64 = ctx.r5.u64 & 0x3F;
	// clrldi r7,r4,58
	ctx.r7.u64 = ctx.r4.u64 & 0x3F;
	// rlwinm r9,r5,29,3,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0x1FFFFFF8;
	// rlwinm r10,r4,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 29) & 0x1FFFFFF8;
	// sld r6,r11,r8
	ctx.r6.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// lwz r8,648(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 648);
	// sld r7,r11,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r7.u8 & 0x7F));
	// lwz r11,636(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 636);
	// mullw r11,r4,r11
	r11.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// ldx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + r11.u32);
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// stdx r8,r9,r11
	PPC_STORE_U64(ctx.r9.u32 + r11.u32, ctx.r8.u64);
	// lwz r11,636(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 636);
	// lwz r9,648(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 648);
	// mullw r11,r5,r11
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// ldx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r10.u32 + r11.u32);
	// or r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 | ctx.r9.u64;
	// stdx r9,r10,r11
	PPC_STORE_U64(ctx.r10.u32 + r11.u32, ctx.r9.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C6208"))) PPC_WEAK_FUNC(sub_824C6208);
PPC_FUNC_IMPL(__imp__sub_824C6208) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r31,8
	ctx.r3.s64 = r31.s64 + 8;
	// bl 0x824c5ca0
	sub_824C5CA0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,744(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 744);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c6238
	if (!cr0.eq) goto loc_824C6238;
	// li r11,32
	r11.s64 = 32;
loc_824C6238:
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// addi r3,r11,-256
	ctx.r3.s64 = r11.s64 + -256;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C6258"))) PPC_WEAK_FUNC(sub_824C6258);
PPC_FUNC_IMPL(__imp__sub_824C6258) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// lwz r10,636(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// rlwinm r25,r7,2,0,29
	r25.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r11,r11,r29
	r11.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// lwzx r9,r25,r27
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + r27.u32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// b 0x824c62ac
	goto loc_824C62AC;
loc_824C6294:
	// ld r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// ld r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// or r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 | ctx.r5.u64;
	// std r6,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r6.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C62AC:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c6294
	if (cr6.lt) goto loc_824C6294;
	// addi r11,r28,4
	r11.s64 = r28.s64 + 4;
	// lwz r10,636(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// addi r9,r7,4
	ctx.r9.s64 = ctx.r7.s64 + 4;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r11,r11,r29
	r11.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// lwzx r9,r9,r27
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r27.u32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// b 0x824c62f4
	goto loc_824C62F4;
loc_824C62DC:
	// ld r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// ld r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// or r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 | ctx.r5.u64;
	// std r6,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r6.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C62F4:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c62dc
	if (cr6.lt) goto loc_824C62DC;
	// lwz r11,192(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 192);
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c6350
	if (cr0.eq) goto loc_824C6350;
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
	// addi r11,r28,1
	r11.s64 = r28.s64 + 1;
	// mulli r10,r10,40
	ctx.r10.s64 = ctx.r10.s64 * 40;
	// mulli r11,r11,40
	r11.s64 = r11.s64 * 40;
	// add r9,r10,r27
	ctx.r9.u64 = ctx.r10.u64 + r27.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// li r10,4
	ctx.r10.s64 = 4;
loc_824C6324:
	// ld r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ld r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// std r7,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r7.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x824c6324
	if (!cr0.eq) goto loc_824C6324;
	// lwz r11,192(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// oris r11,r11,16384
	r11.u64 = r11.u64 | 1073741824;
	// stw r11,192(r29)
	PPC_STORE_U32(r29.u32 + 192, r11.u32);
loc_824C6350:
	// clrlwi. r11,r8,24
	r11.u64 = ctx.r8.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c6418
	if (!cr0.eq) goto loc_824C6418;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,636(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwzx r3,r25,r27
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + r27.u32);
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
	// lwz r11,632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x824c6418
	if (cr6.gt) goto loc_824C6418;
	// extsw r26,r28
	r26.s64 = r28.s32;
loc_824C6380:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// li r7,2
	ctx.r7.s64 = 2;
	// lwz r8,656(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 656);
	// subf r9,r11,r11
	ctx.r9.s64 = r11.s64 - r11.s64;
	// rlwinm r10,r11,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFC0;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// rlwinm r10,r10,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFF8;
	// rlwinm r6,r3,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// ldx r5,r10,r8
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r8.u32);
	// sld r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r7.u64 << (ctx.r9.u8 & 0x7F));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// and r7,r26,r9
	ctx.r7.u64 = r26.u64 & ctx.r9.u64;
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// sld r11,r7,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r7.u64 << (r11.u8 & 0x7F));
	// andc r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 & ~ctx.r9.u64;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// stdx r11,r10,r8
	PPC_STORE_U64(ctx.r10.u32 + ctx.r8.u32, r11.u64);
	// lwz r11,652(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 652);
	// stwx r29,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, r29.u32);
	// lwz r11,632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// addi r28,r11,-1
	r28.s64 = r11.s64 + -1;
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// bne cr6,0x824c63fc
	if (!cr6.eq) goto loc_824C63FC;
	// li r30,-1
	r30.s64 = -1;
	// b 0x824c6410
	goto loc_824C6410;
loc_824C63FC:
	// addi r4,r30,1
	ctx.r4.s64 = r30.s64 + 1;
	// lwz r5,636(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwzx r3,r25,r27
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + r27.u32);
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_824C6410:
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// ble cr6,0x824c6380
	if (!cr6.gt) goto loc_824C6380;
loc_824C6418:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_824C6420"))) PPC_WEAK_FUNC(sub_824C6420);
PPC_FUNC_IMPL(__imp__sub_824C6420) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// clrldi r10,r5,58
	ctx.r10.u64 = ctx.r5.u64 & 0x3F;
	// lwz r11,648(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 648);
	// li r31,1
	r31.s64 = 1;
	// rlwinm r8,r5,29,3,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0x1FFFFFF8;
	// sld r30,r31,r10
	r30.u64 = ctx.r10.u8 & 0x40 ? 0 : (r31.u64 << (ctx.r10.u8 & 0x7F));
	// lwz r10,636(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 636);
	// mullw r9,r10,r4
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// add r6,r9,r11
	ctx.r6.u64 = ctx.r9.u64 + r11.u64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + r11.u64;
	// ldx r11,r8,r6
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + ctx.r6.u32);
	// and r11,r30,r11
	r11.u64 = r30.u64 & r11.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// mr r11,r31
	r11.u64 = r31.u64;
	// bne cr6,0x824c6470
	if (!cr6.eq) goto loc_824C6470;
	// li r11,0
	r11.s64 = 0;
loc_824C6470:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c65c8
	if (!cr0.eq) goto loc_824C65C8;
	// clrldi r11,r4,58
	r11.u64 = ctx.r4.u64 & 0x3F;
	// rlwinm r10,r4,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 29) & 0x1FFFFFF8;
	// ldx r10,r10,r7
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r7.u32);
	// sld r11,r31,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (r31.u64 << (r11.u8 & 0x7F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// mr r11,r31
	r11.u64 = r31.u64;
	// bne cr6,0x824c649c
	if (!cr6.eq) goto loc_824C649C;
	// li r11,0
	r11.s64 = 0;
loc_824C649C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c65c8
	if (!cr0.eq) goto loc_824C65C8;
	// lwz r11,640(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 640);
	// mulli r10,r5,40
	ctx.r10.s64 = ctx.r5.s64 * 40;
	// mulli r9,r4,40
	ctx.r9.s64 = ctx.r4.s64 * 40;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C64C4:
	// ld r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// ld r30,0(r11)
	r30.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// nor r9,r9,r30
	ctx.r9.u64 = ~(ctx.r9.u64 | r30.u64);
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824c65b0
	if (!cr6.eq) goto loc_824C65B0;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmpwi cr6,r8,4
	cr6.compare<int32_t>(ctx.r8.s32, 4, xer);
	// blt cr6,0x824c64c4
	if (cr6.lt) goto loc_824C64C4;
	// li r11,0
	r11.s64 = 0;
loc_824C64F0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c65c8
	if (cr0.eq) goto loc_824C65C8;
	// lwz r10,644(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 644);
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c6558
	if (cr0.eq) goto loc_824C6558;
	// lwz r10,636(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 636);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x824c654c
	if (!cr6.lt) goto loc_824C654C;
	// subf r8,r11,r7
	ctx.r8.s64 = ctx.r7.s64 - r11.s64;
loc_824C652C:
	// ldx r7,r8,r11
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r8.u32 + r11.u32);
	// ld r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// and r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 & ctx.r5.u64;
	// cmpldi cr6,r7,0
	cr6.compare<uint64_t>(ctx.r7.u64, 0, xer);
	// bne cr6,0x824c65b8
	if (!cr6.eq) goto loc_824C65B8;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c652c
	if (cr6.lt) goto loc_824C652C;
loc_824C654C:
	// li r11,0
	r11.s64 = 0;
loc_824C6550:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c65c8
	if (!cr0.eq) goto loc_824C65C8;
loc_824C6558:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824c65a8
	if (cr6.eq) goto loc_824C65A8;
	// lwz r10,636(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 636);
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x824c659c
	if (!cr6.lt) goto loc_824C659C;
	// subf r9,r9,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r9.s64;
loc_824C657C:
	// ldx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + r11.u32);
	// ld r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// and r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ctx.r7.u64;
	// cmpldi cr6,r8,0
	cr6.compare<uint64_t>(ctx.r8.u64, 0, xer);
	// bne cr6,0x824c65c0
	if (!cr6.eq) goto loc_824C65C0;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c657c
	if (cr6.lt) goto loc_824C657C;
loc_824C659C:
	// li r11,0
	r11.s64 = 0;
loc_824C65A0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c65c8
	if (!cr0.eq) goto loc_824C65C8;
loc_824C65A8:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824c65cc
	goto loc_824C65CC;
loc_824C65B0:
	// mr r11,r31
	r11.u64 = r31.u64;
	// b 0x824c64f0
	goto loc_824C64F0;
loc_824C65B8:
	// mr r11,r31
	r11.u64 = r31.u64;
	// b 0x824c6550
	goto loc_824C6550;
loc_824C65C0:
	// mr r11,r31
	r11.u64 = r31.u64;
	// b 0x824c65a0
	goto loc_824C65A0;
loc_824C65C8:
	// li r3,1
	ctx.r3.s64 = 1;
loc_824C65CC:
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C65D8"))) PPC_WEAK_FUNC(sub_824C65D8);
PPC_FUNC_IMPL(__imp__sub_824C65D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r30
	r29.u64 = r30.u64;
	// li r28,4
	r28.s64 = 4;
loc_824C65F4:
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x824c6614
	if (cr0.eq) goto loc_824C6614;
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x824b1f78
	sub_824B1F78(ctx, base);
loc_824C6614:
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// bne 0x824c65f4
	if (!cr0.eq) goto loc_824C65F4;
	// lwz r11,192(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// rlwinm r11,r11,0,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF8;
	// stw r11,192(r30)
	PPC_STORE_U32(r30.u32 + 192, r11.u32);
	// lwz r11,668(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 668);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lwz r11,664(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 664);
	// stw r30,668(r31)
	PPC_STORE_U32(r31.u32 + 668, r30.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,664(r31)
	PPC_STORE_U32(r31.u32 + 664, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824C664C"))) PPC_WEAK_FUNC(sub_824C664C);
PPC_FUNC_IMPL(__imp__sub_824C664C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C6650"))) PPC_WEAK_FUNC(sub_824C6650);
PPC_FUNC_IMPL(__imp__sub_824C6650) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// lwz r11,192(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// clrlwi r10,r11,29
	ctx.r10.u64 = r11.u32 & 0x7;
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// bne cr6,0x824c66d8
	if (!cr6.eq) goto loc_824C66D8;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// clrlwi. r9,r26,24
	ctx.r9.u64 = r26.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwimi r10,r11,0,0,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFF8) | (ctx.r10.u64 & 0xFFFFFFFF00000007);
	// stw r10,192(r30)
	PPC_STORE_U32(r30.u32 + 192, ctx.r10.u32);
	// bne 0x824c66bc
	if (!cr0.eq) goto loc_824C66BC;
	// lwz r11,644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 644);
	// rlwinm r10,r28,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824c66bc
	if (cr6.eq) goto loc_824C66BC;
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r11,r10,r30
	PPC_STORE_U32(ctx.r10.u32 + r30.u32, r11.u32);
	// b 0x824c66d8
	goto loc_824C66D8;
loc_824C66BC:
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r30
	PPC_STORE_U32(r11.u32 + r30.u32, ctx.r3.u32);
loc_824C66D8:
	// clrlwi. r6,r26,24
	ctx.r6.u64 = r26.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// bne 0x824c673c
	if (!cr0.eq) goto loc_824C673C;
	// lwz r10,644(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 644);
	// rlwinm r9,r28,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r9
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c673c
	if (cr6.eq) goto loc_824C673C;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,636(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwzx r8,r10,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r9,r5,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// b 0x824c6730
	goto loc_824C6730;
loc_824C6718:
	// ld r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// ld r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// or r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 | ctx.r4.u64;
	// std r5,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r5.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
loc_824C6730:
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x824c6718
	if (cr6.lt) goto loc_824C6718;
	// b 0x824c6760
	goto loc_824C6760;
loc_824C673C:
	// clrldi r10,r27,58
	ctx.r10.u64 = r27.u64 & 0x3F;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r27,29,3,28
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 29) & 0x1FFFFFF8;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// sld r8,r7,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r7.u64 << (ctx.r10.u8 & 0x7F));
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// ldx r5,r9,r10
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r9.u32 + ctx.r10.u32);
	// or r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 | ctx.r5.u64;
	// stdx r8,r9,r10
	PPC_STORE_U64(ctx.r9.u32 + ctx.r10.u32, ctx.r8.u64);
loc_824C6760:
	// clrldi r9,r28,58
	ctx.r9.u64 = r28.u64 & 0x3F;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r28,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 29) & 0x1FFFFFF8;
	// addi r8,r29,4
	ctx.r8.s64 = r29.s64 + 4;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// ldx r5,r10,r11
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r10.u32 + r11.u32);
	// sld r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r7.u64 << (ctx.r9.u8 & 0x7F));
	// or r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 | ctx.r5.u64;
	// stdx r9,r10,r11
	PPC_STORE_U64(ctx.r10.u32 + r11.u32, ctx.r9.u64);
	// lwz r10,636(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwzx r11,r8,r30
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + r30.u32);
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r8,648(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// mullw r5,r10,r28
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(r28.s32);
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + r11.u64;
	// rlwinm r9,r5,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// b 0x824c67c0
	goto loc_824C67C0;
loc_824C67A8:
	// ld r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// ld r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// or r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 | ctx.r5.u64;
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C67C0:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c67a8
	if (cr6.lt) goto loc_824C67A8;
	// lwz r11,640(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 640);
	// mulli r9,r28,40
	ctx.r9.s64 = r28.s64 * 40;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C67DC:
	// ld r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// cmpldi cr6,r8,0
	cr6.compare<uint64_t>(ctx.r8.u64, 0, xer);
	// bne cr6,0x824c68b8
	if (!cr6.eq) goto loc_824C68B8;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// blt cr6,0x824c67dc
	if (cr6.lt) goto loc_824C67DC;
	// li r11,0
	r11.s64 = 0;
loc_824C67FC:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c684c
	if (cr0.eq) goto loc_824C684C;
	// lwz r11,192(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// addi r10,r29,1
	ctx.r10.s64 = r29.s64 + 1;
	// oris r8,r11,16384
	ctx.r8.u64 = r11.u64 | 1073741824;
	// mulli r11,r10,40
	r11.s64 = ctx.r10.s64 * 40;
	// stw r8,192(r30)
	PPC_STORE_U32(r30.u32 + 192, ctx.r8.u32);
	// lwz r10,640(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 640);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// li r9,4
	ctx.r9.s64 = 4;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
loc_824C682C:
	// ld r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ld r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x824c682c
	if (!cr0.eq) goto loc_824C682C;
loc_824C684C:
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x824c68b0
	if (!cr6.eq) goto loc_824C68B0;
	// rlwinm r11,r28,1,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,656(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 656);
	// li r7,2
	ctx.r7.s64 = 2;
	// subf r9,r11,r11
	ctx.r9.s64 = r11.s64 - r11.s64;
	// rlwinm r10,r11,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFC0;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// extsw r6,r29
	ctx.r6.s64 = r29.s32;
	// rlwinm r10,r10,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFF8;
	// rlwinm r5,r28,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// sld r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r7.u64 << (ctx.r9.u8 & 0x7F));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// and r7,r6,r9
	ctx.r7.u64 = ctx.r6.u64 & ctx.r9.u64;
	// ldx r6,r10,r8
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r8.u32);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// sld r11,r7,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r7.u64 << (r11.u8 & 0x7F));
	// andc r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 & ~ctx.r9.u64;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// stdx r11,r10,r8
	PPC_STORE_U64(ctx.r10.u32 + ctx.r8.u32, r11.u64);
	// lwz r11,652(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 652);
	// stwx r30,r11,r5
	PPC_STORE_U32(r11.u32 + ctx.r5.u32, r30.u32);
loc_824C68B0:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_824C68B8:
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// b 0x824c67fc
	goto loc_824C67FC;
}

__attribute__((alias("__imp__sub_824C68C0"))) PPC_WEAK_FUNC(sub_824C68C0);
PPC_FUNC_IMPL(__imp__sub_824C68C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mulli r10,r28,40
	ctx.r10.s64 = r28.s64 * 40;
	// lwz r11,640(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 640);
	// mulli r9,r29,40
	ctx.r9.s64 = r29.s64 * 40;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r11,r10,8
	r11.s64 = ctx.r10.s64 + 8;
	// li r8,4
	ctx.r8.s64 = 4;
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824C68F8:
	// ld r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ld r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// or r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 | ctx.r7.u64;
	// std r9,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x824c68f8
	if (!cr0.eq) goto loc_824C68F8;
	// lwz r11,648(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c6970
	if (cr6.eq) goto loc_824C6970;
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwz r10,648(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// mullw r9,r11,r28
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(r28.s32);
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// mullw r7,r11,r29
	ctx.r7.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r8,r10
	r11.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r7,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + r11.u64;
	// b 0x824c6968
	goto loc_824C6968;
loc_824C6950:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// ld r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// or r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 | ctx.r7.u64;
	// std r9,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C6968:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c6950
	if (cr6.lt) goto loc_824C6950;
loc_824C6970:
	// lwz r10,644(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 644);
	// rlwinm r30,r28,2,0,29
	r30.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// li r26,1
	r26.s64 = 1;
	// lwzx r11,r10,r30
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c69f0
	if (!cr6.eq) goto loc_824C69F0;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmplwi cr6,r10,65535
	cr6.compare<uint32_t>(ctx.r10.u32, 65535, xer);
	// ble cr6,0x824c69b4
	if (!cr6.gt) goto loc_824C69B4;
	// lwz r10,644(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 644);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwzx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stwx r8,r10,r30
	PPC_STORE_U32(ctx.r10.u32 + r30.u32, ctx.r8.u32);
	// lwz r10,644(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 644);
	// stwx r9,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, ctx.r9.u32);
	// b 0x824c69d0
	goto loc_824C69D0;
loc_824C69B4:
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r11,644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 644);
	// stwx r3,r11,r30
	PPC_STORE_U32(r11.u32 + r30.u32, ctx.r3.u32);
loc_824C69D0:
	// clrldi r9,r28,58
	ctx.r9.u64 = r28.u64 & 0x3F;
	// lwz r11,644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 644);
	// rlwinm r10,r28,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 29) & 0x1FFFFFF8;
	// lwzx r11,r11,r30
	r11.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// ldx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + r11.u32);
	// sld r9,r26,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r26.u64 << (ctx.r9.u8 & 0x7F));
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stdx r9,r10,r11
	PPC_STORE_U64(ctx.r10.u32 + r11.u32, ctx.r9.u64);
loc_824C69F0:
	// lwz r11,644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 644);
	// rlwinm r27,r29,2,0,29
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r27,r11
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + r11.u32);
	// cmplwi cr6,r10,65535
	cr6.compare<uint32_t>(ctx.r10.u32, 65535, xer);
	// ble cr6,0x824c6a60
	if (!cr6.gt) goto loc_824C6A60;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r8,636(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwzx r11,r10,r30
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// lwzx r9,r10,r27
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r27.u32);
	// rlwinm r10,r8,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// b 0x824c6a38
	goto loc_824C6A38;
loc_824C6A20:
	// ld r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// ld r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C6A38:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c6a20
	if (cr6.lt) goto loc_824C6A20;
	// lwz r11,644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 644);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r10,636(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r5,r10,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r4,r11,r27
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// bl 0x824b1f78
	sub_824B1F78(ctx, base);
	// b 0x824c6a7c
	goto loc_824C6A7C;
loc_824C6A60:
	// clrldi r9,r29,58
	ctx.r9.u64 = r29.u64 & 0x3F;
	// lwzx r11,r11,r30
	r11.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// rlwinm r10,r29,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 29) & 0x1FFFFFF8;
	// ldx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + r11.u32);
	// sld r9,r26,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r26.u64 << (ctx.r9.u8 & 0x7F));
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stdx r9,r10,r11
	PPC_STORE_U64(ctx.r10.u32 + r11.u32, ctx.r9.u64);
loc_824C6A7C:
	// lwz r11,644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 644);
	// stwx r28,r11,r27
	PPC_STORE_U32(r11.u32 + r27.u32, r28.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_824C6A8C"))) PPC_WEAK_FUNC(sub_824C6A8C);
PPC_FUNC_IMPL(__imp__sub_824C6A8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C6A90"))) PPC_WEAK_FUNC(sub_824C6A90);
PPC_FUNC_IMPL(__imp__sub_824C6A90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwz r29,668(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 668);
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// cmplwi r29,0
	cr0.compare<uint32_t>(r29.u32, 0, xer);
	// addi r30,r11,208
	r30.s64 = r11.s64 + 208;
	// beq 0x824c6ae4
	if (cr0.eq) goto loc_824C6AE4;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// li r5,200
	ctx.r5.s64 = 200;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,668(r31)
	PPC_STORE_U32(r31.u32 + 668, r11.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// addi r5,r30,-208
	ctx.r5.s64 = r30.s64 + -208;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r29,208
	ctx.r3.s64 = r29.s64 + 208;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// b 0x824c6b04
	goto loc_824C6B04;
loc_824C6AE4:
	// li r5,49
	ctx.r5.s64 = 49;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r11,660(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 660);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r11,200(r29)
	PPC_STORE_U32(r29.u32 + 200, r11.u32);
	// stw r29,660(r31)
	PPC_STORE_U32(r31.u32 + 660, r29.u32);
loc_824C6B04:
	// lwz r11,664(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 664);
	// addi r27,r29,208
	r27.s64 = r29.s64 + 208;
	// addi r30,r29,32
	r30.s64 = r29.s64 + 32;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r26,r29,16
	r26.s64 = r29.s64 + 16;
	// li r25,4
	r25.s64 = 4;
	// li r28,0
	r28.s64 = 0;
	// stw r11,664(r31)
	PPC_STORE_U32(r31.u32 + 664, r11.u32);
loc_824C6B24:
	// stw r27,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r27.u32);
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// stw r31,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r31.u32);
	// std r28,8(r30)
	PPC_STORE_U64(r30.u32 + 8, r28.u64);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// std r28,16(r30)
	PPC_STORE_U64(r30.u32 + 16, r28.u64);
	// std r28,24(r30)
	PPC_STORE_U64(r30.u32 + 24, r28.u64);
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// std r28,32(r30)
	PPC_STORE_U64(r30.u32 + 32, r28.u64);
	// lwz r11,744(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 744);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c6b58
	if (!cr0.eq) goto loc_824C6B58;
	// li r11,32
	r11.s64 = 32;
loc_824C6B58:
	// cmplwi cr6,r11,64
	cr6.compare<uint32_t>(r11.u32, 64, xer);
	// bge cr6,0x824c6b70
	if (!cr6.lt) goto loc_824C6B70;
	// subfic r5,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r5.s64 = 64 - r11.s64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824b20b0
	sub_824B20B0(ctx, base);
loc_824C6B70:
	// addic. r25,r25,-1
	xer.ca = r25.u32 > 0;
	r25.s64 = r25.s64 + -1;
	cr0.compare<int32_t>(r25.s32, 0, xer);
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// addi r30,r30,40
	r30.s64 = r30.s64 + 40;
	// bne 0x824c6b24
	if (!cr0.eq) goto loc_824C6B24;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_824C6B8C"))) PPC_WEAK_FUNC(sub_824C6B8C);
PPC_FUNC_IMPL(__imp__sub_824C6B8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C6B90"))) PPC_WEAK_FUNC(sub_824C6B90);
PPC_FUNC_IMPL(__imp__sub_824C6B90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r21,r4
	r21.u64 = ctx.r4.u64;
	// mr r17,r5
	r17.u64 = ctx.r5.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c705c
	if (!cr0.eq) goto loc_824C705C;
	// mr. r19,r11
	r19.u64 = r11.u64;
	cr0.compare<int32_t>(r19.s32, 0, xer);
	// beq 0x824c705c
	if (cr0.eq) goto loc_824C705C;
	// clrlwi r16,r6,24
	r16.u64 = ctx.r6.u32 & 0xFF;
	// li r22,1
	r22.s64 = 1;
	// li r18,-1
	r18.s64 = -1;
loc_824C6BC8:
	// cmplwi cr6,r16,0
	cr6.compare<uint32_t>(r16.u32, 0, xer);
	// bne cr6,0x824c6bd8
	if (!cr6.eq) goto loc_824C6BD8;
	// lwz r11,88(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 88);
	// stw r11,84(r19)
	PPC_STORE_U32(r19.u32 + 84, r11.u32);
loc_824C6BD8:
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// lwz r4,88(r19)
	ctx.r4.u64 = PPC_LOAD_U32(r19.u32 + 88);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// lwz r5,636(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
	// lwz r11,632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bgt cr6,0x824c6ca8
	if (cr6.gt) goto loc_824C6CA8;
loc_824C6C08:
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// lwz r8,648(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// mullw r10,r11,r3
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r3.s32);
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r9,r8
	r11.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// b 0x824c6c44
	goto loc_824C6C44;
loc_824C6C2C:
	// ld r9,0(r7)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// ld r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// std r9,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C6C44:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c6c2c
	if (cr6.lt) goto loc_824C6C2C;
	// clrldi r8,r3,58
	ctx.r8.u64 = ctx.r3.u64 & 0x3F;
	// lwz r9,648(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// rlwinm r10,r3,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// mullw r11,r11,r3
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r3.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// sld r9,r22,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x40 ? 0 : (r22.u64 << (ctx.r8.u8 & 0x7F));
	// ldx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + r11.u32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// stdx r9,r10,r11
	PPC_STORE_U64(ctx.r10.u32 + r11.u32, ctx.r9.u64);
	// lwz r11,632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// addi r30,r11,-1
	r30.s64 = r11.s64 + -1;
	// cmplw cr6,r3,r30
	cr6.compare<uint32_t>(ctx.r3.u32, r30.u32, xer);
	// bne cr6,0x824c6c90
	if (!cr6.eq) goto loc_824C6C90;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// b 0x824c6ca0
	goto loc_824C6CA0;
loc_824C6C90:
	// addi r4,r3,1
	ctx.r4.s64 = ctx.r3.s64 + 1;
	// lwz r5,636(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
loc_824C6CA0:
	// cmplw cr6,r3,r30
	cr6.compare<uint32_t>(ctx.r3.u32, r30.u32, xer);
	// ble cr6,0x824c6c08
	if (!cr6.gt) goto loc_824C6C08;
loc_824C6CA8:
	// lwz r11,28(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c7040
	if (!cr0.eq) goto loc_824C7040;
	// lwz r11,24(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 24);
	// b 0x824c7034
	goto loc_824C7034;
loc_824C6CBC:
	// lwz r11,8(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 8);
	// rlwinm. r10,r11,9,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c6e8c
	if (cr0.eq) goto loc_824C6E8C;
	// rlwinm r25,r11,31,28,31
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
loc_824C6CCC:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x824c6cf8
	if (cr6.eq) goto loc_824C6CF8;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// addi r11,r25,-1
	r11.s64 = r25.s64 + -1;
	// andc r11,r25,r11
	r11.u64 = r25.u64 & ~r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r11,r11,31
	xer.ca = r11.u32 <= 31;
	r11.s64 = 31 - r11.s64;
	// add r26,r3,r11
	r26.u64 = ctx.r3.u64 + r11.u64;
	// mr r11,r26
	r11.u64 = r26.u64;
	// b 0x824c6d00
	goto loc_824C6D00;
loc_824C6CF8:
	// li r26,0
	r26.s64 = 0;
	// li r11,0
	r11.s64 = 0;
loc_824C6D00:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c6e8c
	if (cr6.eq) goto loc_824C6E8C;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// clrldi r11,r29,58
	r11.u64 = r29.u64 & 0x3F;
	// rlwinm r30,r29,29,3,28
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 29) & 0x1FFFFFF8;
	// sld r27,r22,r11
	r27.u64 = r11.u8 & 0x40 ? 0 : (r22.u64 << (r11.u8 & 0x7F));
	// ldx r11,r30,r21
	r11.u64 = PPC_LOAD_U64(r30.u32 + r21.u32);
	// andc r11,r11,r27
	r11.u64 = r11.u64 & ~r27.u64;
	// stdx r11,r30,r21
	PPC_STORE_U64(r30.u32 + r21.u32, r11.u64);
	// lwz r11,8(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 8);
	// rlwinm r28,r11,31,28,31
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
loc_824C6D38:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824c6d64
	if (cr6.eq) goto loc_824C6D64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// andc r11,r28,r11
	r11.u64 = r28.u64 & ~r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r11,r11,31
	xer.ca = r11.u32 <= 31;
	r11.s64 = 31 - r11.s64;
	// add r4,r3,r11
	ctx.r4.u64 = ctx.r3.u64 + r11.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// b 0x824c6d6c
	goto loc_824C6D6C;
loc_824C6D64:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r11,0
	r11.s64 = 0;
loc_824C6D6C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c6de0
	if (cr6.eq) goto loc_824C6DE0;
	// cmplw cr6,r4,r26
	cr6.compare<uint32_t>(ctx.r4.u32, r26.u32, xer);
	// ble cr6,0x824c6dd0
	if (!cr6.gt) goto loc_824C6DD0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwz r9,648(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// rlwinm r10,r3,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// clrldi r9,r3,58
	ctx.r9.u64 = ctx.r3.u64 & 0x3F;
	// ldx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + r11.u32);
	// sld r9,r22,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r22.u64 << (ctx.r9.u8 & 0x7F));
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stdx r9,r10,r11
	PPC_STORE_U64(ctx.r10.u32 + r11.u32, ctx.r9.u64);
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwz r10,648(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// mullw r11,r11,r3
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r3.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// ldx r10,r30,r11
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + r11.u32);
	// or r10,r27,r10
	ctx.r10.u64 = r27.u64 | ctx.r10.u64;
	// stdx r10,r30,r11
	PPC_STORE_U64(r30.u32 + r11.u32, ctx.r10.u64);
loc_824C6DD0:
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// andc r11,r28,r11
	r11.u64 = r28.u64 & ~r11.u64;
	// subf r28,r11,r28
	r28.s64 = r28.s64 - r11.s64;
	// b 0x824c6d38
	goto loc_824C6D38;
loc_824C6DE0:
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,636(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
	// lwz r11,632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bgt cr6,0x824c6e7c
	if (cr6.gt) goto loc_824C6E7C;
loc_824C6E00:
	// clrldi r8,r3,58
	ctx.r8.u64 = ctx.r3.u64 & 0x3F;
	// lwz r9,648(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// rlwinm r10,r3,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// sld r9,r22,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x40 ? 0 : (r22.u64 << (ctx.r8.u8 & 0x7F));
	// ldx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + r11.u32);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stdx r9,r10,r11
	PPC_STORE_U64(ctx.r10.u32 + r11.u32, ctx.r9.u64);
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwz r10,648(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// mullw r11,r11,r3
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r3.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// ldx r10,r30,r11
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + r11.u32);
	// or r10,r27,r10
	ctx.r10.u64 = r27.u64 | ctx.r10.u64;
	// stdx r10,r30,r11
	PPC_STORE_U64(r30.u32 + r11.u32, ctx.r10.u64);
	// lwz r11,632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// addi r28,r11,-1
	r28.s64 = r11.s64 + -1;
	// cmplw cr6,r3,r28
	cr6.compare<uint32_t>(ctx.r3.u32, r28.u32, xer);
	// bne cr6,0x824c6e64
	if (!cr6.eq) goto loc_824C6E64;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// b 0x824c6e74
	goto loc_824C6E74;
loc_824C6E64:
	// addi r4,r3,1
	ctx.r4.s64 = ctx.r3.s64 + 1;
	// lwz r5,636(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
loc_824C6E74:
	// cmplw cr6,r3,r28
	cr6.compare<uint32_t>(ctx.r3.u32, r28.u32, xer);
	// ble cr6,0x824c6e00
	if (!cr6.gt) goto loc_824C6E00;
loc_824C6E7C:
	// addi r11,r25,-1
	r11.s64 = r25.s64 + -1;
	// andc r11,r25,r11
	r11.u64 = r25.u64 & ~r11.u64;
	// subf r25,r11,r25
	r25.s64 = r25.s64 - r11.s64;
	// b 0x824c6ccc
	goto loc_824C6CCC;
loc_824C6E8C:
	// lwz r23,0(r20)
	r23.u64 = PPC_LOAD_U32(r20.u32 + 0);
loc_824C6E90:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x824c7024
	if (cr6.eq) goto loc_824C7024;
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c701c
	if (cr0.eq) goto loc_824C701C;
	// lwz r30,12(r23)
	r30.u64 = PPC_LOAD_U32(r23.u32 + 12);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c701c
	if (cr0.eq) goto loc_824C701C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lhz r11,12(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// add r28,r11,r3
	r28.u64 = r11.u64 + ctx.r3.u64;
	// cmplw cr6,r3,r28
	cr6.compare<uint32_t>(ctx.r3.u32, r28.u32, xer);
	// bge cr6,0x824c6f08
	if (!cr6.lt) goto loc_824C6F08;
	// addi r29,r1,80
	r29.s64 = ctx.r1.s64 + 80;
loc_824C6EE8:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// blt cr6,0x824c6ee8
	if (cr6.lt) goto loc_824C6EE8;
loc_824C6F08:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm r25,r11,27,24,31
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0xFF;
	// rlwinm. r10,r11,7,29,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x7;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c701c
	if (cr0.eq) goto loc_824C701C;
	// li r26,0
	r26.s64 = 0;
	// mr r24,r10
	r24.u64 = ctx.r10.u64;
loc_824C6F20:
	// srw r10,r25,r26
	ctx.r10.u64 = r26.u8 & 0x20 ? 0 : (r25.u32 >> (r26.u8 & 0x3F));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// rlwinm r10,r10,2,28,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xC;
	// lis r12,-1024
	r12.s64 = -67108864;
	// ori r12,r12,0
	r12.u64 = r12.u64 | 0;
	// lwzx r28,r10,r11
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// rldicr r12,r12,6,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 6) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r28,r12
	r11.u64 = r28.u64 & r12.u64;
	// rlwinm r30,r28,29,3,28
	r30.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 29) & 0x1FFFFFF8;
	// sld r27,r22,r11
	r27.u64 = r11.u8 & 0x40 ? 0 : (r22.u64 << (r11.u8 & 0x7F));
	// ldx r11,r30,r21
	r11.u64 = PPC_LOAD_U64(r30.u32 + r21.u32);
	// and r11,r27,r11
	r11.u64 = r27.u64 & r11.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// mr r11,r22
	r11.u64 = r22.u64;
	// bne cr6,0x824c6f60
	if (!cr6.eq) goto loc_824C6F60;
	// li r11,0
	r11.s64 = 0;
loc_824C6F60:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c7010
	if (!cr0.eq) goto loc_824C7010;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,636(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
	// lwz r11,632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bgt cr6,0x824c7004
	if (cr6.gt) goto loc_824C7004;
loc_824C6F88:
	// clrldi r8,r3,58
	ctx.r8.u64 = ctx.r3.u64 & 0x3F;
	// lwz r9,648(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// rlwinm r10,r3,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// mullw r11,r11,r28
	r11.s64 = int64_t(r11.s32) * int64_t(r28.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// sld r9,r22,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x40 ? 0 : (r22.u64 << (ctx.r8.u8 & 0x7F));
	// ldx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + r11.u32);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stdx r9,r10,r11
	PPC_STORE_U64(ctx.r10.u32 + r11.u32, ctx.r9.u64);
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwz r10,648(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// mullw r11,r11,r3
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r3.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// ldx r10,r11,r30
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + r30.u32);
	// or r10,r27,r10
	ctx.r10.u64 = r27.u64 | ctx.r10.u64;
	// stdx r10,r11,r30
	PPC_STORE_U64(r11.u32 + r30.u32, ctx.r10.u64);
	// lwz r11,632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// addi r29,r11,-1
	r29.s64 = r11.s64 + -1;
	// cmplw cr6,r3,r29
	cr6.compare<uint32_t>(ctx.r3.u32, r29.u32, xer);
	// bne cr6,0x824c6fec
	if (!cr6.eq) goto loc_824C6FEC;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// b 0x824c6ffc
	goto loc_824C6FFC;
loc_824C6FEC:
	// addi r4,r3,1
	ctx.r4.s64 = ctx.r3.s64 + 1;
	// lwz r5,636(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
loc_824C6FFC:
	// cmplw cr6,r3,r29
	cr6.compare<uint32_t>(ctx.r3.u32, r29.u32, xer);
	// ble cr6,0x824c6f88
	if (!cr6.gt) goto loc_824C6F88;
loc_824C7004:
	// ldx r11,r30,r21
	r11.u64 = PPC_LOAD_U64(r30.u32 + r21.u32);
	// or r11,r27,r11
	r11.u64 = r27.u64 | r11.u64;
	// stdx r11,r30,r21
	PPC_STORE_U64(r30.u32 + r21.u32, r11.u64);
loc_824C7010:
	// addic. r24,r24,-1
	xer.ca = r24.u32 > 0;
	r24.s64 = r24.s64 + -1;
	cr0.compare<int32_t>(r24.s32, 0, xer);
	// addi r26,r26,2
	r26.s64 = r26.s64 + 2;
	// bne 0x824c6f20
	if (!cr0.eq) goto loc_824C6F20;
loc_824C701C:
	// lwz r23,4(r23)
	r23.u64 = PPC_LOAD_U32(r23.u32 + 4);
	// b 0x824c6e90
	goto loc_824C6E90;
loc_824C7024:
	// rlwinm r11,r20,0,0,30
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c7040
	if (!cr0.eq) goto loc_824C7040;
loc_824C7034:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r20,r11,-36
	xer.ca = r11.u32 > 35;
	r20.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r20.s32, 0, xer);
	// bne 0x824c6cbc
	if (!cr0.eq) goto loc_824C6CBC;
loc_824C7040:
	// rlwinm r11,r19,0,0,30
	r11.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c705c
	if (!cr0.eq) goto loc_824C705C;
	// mr r19,r11
	r19.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c6bc8
	if (!cr6.eq) goto loc_824C6BC8;
loc_824C705C:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_824C7064"))) PPC_WEAK_FUNC(sub_824C7064);
PPC_FUNC_IMPL(__imp__sub_824C7064) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C7068"))) PPC_WEAK_FUNC(sub_824C7068);
PPC_FUNC_IMPL(__imp__sub_824C7068) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// add r28,r4,r5
	r28.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// cmplw cr6,r4,r28
	cr6.compare<uint32_t>(ctx.r4.u32, r28.u32, xer);
	// bge cr6,0x824c70f8
	if (!cr6.lt) goto loc_824C70F8;
	// rlwinm r31,r4,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
loc_824C708C:
	// addi r10,r31,3
	ctx.r10.s64 = r31.s64 + 3;
	// li r9,2
	ctx.r9.s64 = 2;
	// clrldi r8,r10,58
	ctx.r8.u64 = ctx.r10.u64 & 0x3F;
	// rlwinm r10,r31,26,6,31
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 26) & 0x3FFFFFF;
	// clrldi r11,r31,58
	r11.u64 = r31.u64 & 0x3F;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// li r7,-1
	ctx.r7.s64 = -1;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// ldx r6,r10,r29
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r10.u32 + r29.u32);
	// sld r10,r9,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// sld r9,r7,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r7.u64 << (r11.u8 & 0x7F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 & ctx.r6.u64;
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// subfic r11,r11,15
	xer.ca = r11.u32 <= 15;
	r11.s64 = 15 - r11.s64;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c70e8
	if (cr0.eq) goto loc_824C70E8;
	// rlwimi r11,r30,4,0,27
	r11.u64 = (__builtin_rotateleft32(r30.u32, 4) & 0xFFFFFFF0) | (r11.u64 & 0xFFFFFFFF0000000F);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// bl 0x824cde38
	sub_824CDE38(ctx, base);
loc_824C70E8:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// blt cr6,0x824c708c
	if (cr6.lt) goto loc_824C708C;
loc_824C70F8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824C7100"))) PPC_WEAK_FUNC(sub_824C7100);
PPC_FUNC_IMPL(__imp__sub_824C7100) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// li r30,0
	r30.s64 = 0;
	// clrlwi. r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c7164
	if (cr0.eq) goto loc_824C7164;
loc_824C7130:
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c7868
	sub_824C7868(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c7164
	if (!cr0.eq) goto loc_824C7164;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x824c7130
	if (cr6.lt) goto loc_824C7130;
loc_824C7164:
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6650
	sub_824C6650(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_824C7188"))) PPC_WEAK_FUNC(sub_824C7188);
PPC_FUNC_IMPL(__imp__sub_824C7188) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc4
	// li r17,0
	r17.s64 = 0;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// addi r11,r1,-156
	r11.s64 = ctx.r1.s64 + -156;
	// addi r9,r1,-160
	ctx.r9.s64 = ctx.r1.s64 + -160;
	// clrlwi r16,r10,28
	r16.u64 = ctx.r10.u32 & 0xF;
	// mr r25,r17
	r25.u64 = r17.u64;
	// stw r17,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r17.u32);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stw r17,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r17.u32);
	// stw r17,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r17.u32);
	// stw r16,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, r16.u32);
	// mr r28,r16
	r28.u64 = r16.u64;
	// li r22,2
	r22.s64 = 2;
	// li r20,-1
	r20.s64 = -1;
	// beq cr6,0x824c73b4
	if (cr6.eq) goto loc_824C73B4;
	// lis r9,-28311
	ctx.r9.s64 = -1855389696;
	// lwz r21,192(r4)
	r21.u64 = PPC_LOAD_U32(ctx.r4.u32 + 192);
	// lis r6,0
	ctx.r6.s64 = 0;
	// ori r9,r9,5192
	ctx.r9.u64 = ctx.r9.u64 | 5192;
	// ori r6,r6,36262
	ctx.r6.u64 = ctx.r6.u64 | 36262;
	// clrldi r11,r28,32
	r11.u64 = r28.u64 & 0xFFFFFFFF;
	// rldimi r9,r6,32,0
	ctx.r9.u64 = (__builtin_rotateleft64(ctx.r6.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r9.u64 & 0xFFFFFFFF);
	// mr r27,r17
	r27.u64 = r17.u64;
	// mr r26,r17
	r26.u64 = r17.u64;
	// rlwinm r19,r21,2,31,31
	r19.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 2) & 0x1;
	// li r24,-1
	r24.s64 = -1;
	// srd r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// srd r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// srd r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// clrlwi r18,r11,29
	r18.u64 = r11.u32 & 0x7;
loc_824C7208:
	// mr r23,r18
	r23.u64 = r18.u64;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// beq cr6,0x824c7384
	if (cr6.eq) goto loc_824C7384;
	// clrlwi r11,r21,29
	r11.u64 = r21.u32 & 0x7;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// cmplw cr6,r25,r11
	cr6.compare<uint32_t>(r25.u32, r11.u32, xer);
	// bge cr6,0x824c7268
	if (!cr6.lt) goto loc_824C7268;
	// rlwinm r11,r10,30,2,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFC;
	// addi r9,r11,3
	ctx.r9.s64 = r11.s64 + 3;
	// clrldi r9,r9,58
	ctx.r9.u64 = ctx.r9.u64 & 0x3F;
	// sld r9,r22,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r22.u64 << (ctx.r9.u8 & 0x7F));
	// addi r6,r9,-1
	ctx.r6.s64 = ctx.r9.s64 + -1;
	// clrldi r9,r11,58
	ctx.r9.u64 = r11.u64 & 0x3F;
	// rlwinm r11,r11,26,6,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// ldx r11,r11,r4
	r11.u64 = PPC_LOAD_U64(r11.u32 + ctx.r4.u32);
	// and r11,r6,r11
	r11.u64 = ctx.r6.u64 & r11.u64;
	// sld r6,r24,r9
	ctx.r6.u64 = ctx.r9.u8 & 0x40 ? 0 : (r24.u64 << (ctx.r9.u8 & 0x7F));
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// srd r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// and r6,r11,r28
	ctx.r6.u64 = r11.u64 & r28.u64;
loc_824C7268:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x824c7330
	if (cr6.eq) goto loc_824C7330;
	// lwz r11,192(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 192);
	// mr r9,r17
	ctx.r9.u64 = r17.u64;
	// clrlwi. r29,r11,29
	r29.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// beq 0x824c72dc
	if (cr0.eq) goto loc_824C72DC;
	// mr r11,r17
	r11.u64 = r17.u64;
loc_824C7284:
	// li r31,1
	r31.s64 = 1;
	// slw r31,r31,r9
	r31.u64 = ctx.r9.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r9.u8 & 0x3F));
	// and. r31,r31,r8
	r31.u64 = r31.u64 & ctx.r8.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x824c72cc
	if (!cr0.eq) goto loc_824C72CC;
	// addi r30,r11,3
	r30.s64 = r11.s64 + 3;
	// rlwinm r16,r11,29,3,29
	r16.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r30,r30,27
	r30.u64 = r30.u32 & 0x1F;
	// addi r15,r1,44
	r15.s64 = ctx.r1.s64 + 44;
	// clrlwi r31,r11,27
	r31.u64 = r11.u32 & 0x1F;
	// lwzx r16,r16,r15
	r16.u64 = PPC_LOAD_U32(r16.u32 + r15.u32);
	// slw r30,r22,r30
	r30.u64 = r30.u8 & 0x20 ? 0 : (r22.u32 << (r30.u8 & 0x3F));
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// and r30,r30,r16
	r30.u64 = r30.u64 & r16.u64;
	// slw r16,r20,r31
	r16.u64 = r31.u8 & 0x20 ? 0 : (r20.u32 << (r31.u8 & 0x3F));
	// and r30,r30,r16
	r30.u64 = r30.u64 & r16.u64;
	// srw r31,r30,r31
	r31.u64 = r31.u8 & 0x20 ? 0 : (r30.u32 >> (r31.u8 & 0x3F));
	// cmplw cr6,r25,r31
	cr6.compare<uint32_t>(r25.u32, r31.u32, xer);
	// beq cr6,0x824c72dc
	if (cr6.eq) goto loc_824C72DC;
loc_824C72CC:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r9,r29
	cr6.compare<uint32_t>(ctx.r9.u32, r29.u32, xer);
	// blt cr6,0x824c7284
	if (cr6.lt) goto loc_824C7284;
loc_824C72DC:
	// cmplw cr6,r9,r29
	cr6.compare<uint32_t>(ctx.r9.u32, r29.u32, xer);
	// bge cr6,0x824c7330
	if (!cr6.lt) goto loc_824C7330;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r11,r10,30,2,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFC;
	// mulli r31,r9,5
	r31.s64 = ctx.r9.s64 * 5;
	// addi r9,r11,3
	ctx.r9.s64 = r11.s64 + 3;
	// clrldi r9,r9,58
	ctx.r9.u64 = ctx.r9.u64 & 0x3F;
	// sld r9,r22,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r22.u64 << (ctx.r9.u8 & 0x7F));
	// addi r30,r9,-1
	r30.s64 = ctx.r9.s64 + -1;
	// clrldi r9,r11,58
	ctx.r9.u64 = r11.u64 & 0x3F;
	// rlwinm r11,r11,26,6,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// ldx r11,r11,r5
	r11.u64 = PPC_LOAD_U64(r11.u32 + ctx.r5.u32);
	// and r11,r11,r30
	r11.u64 = r11.u64 & r30.u64;
	// sld r31,r24,r9
	r31.u64 = ctx.r9.u8 & 0x40 ? 0 : (r24.u64 << (ctx.r9.u8 & 0x7F));
	// and r11,r11,r31
	r11.u64 = r11.u64 & r31.u64;
	// srd r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// and r11,r11,r28
	r11.u64 = r11.u64 & r28.u64;
	// or r6,r11,r6
	ctx.r6.u64 = r11.u64 | ctx.r6.u64;
loc_824C7330:
	// lis r9,-28311
	ctx.r9.s64 = -1855389696;
	// lis r31,0
	r31.s64 = 0;
	// ori r9,r9,5192
	ctx.r9.u64 = ctx.r9.u64 | 5192;
	// ori r31,r31,36262
	r31.u64 = r31.u64 | 36262;
	// clrldi r11,r6,32
	r11.u64 = ctx.r6.u64 & 0xFFFFFFFF;
	// rldimi r9,r31,32,0
	ctx.r9.u64 = (__builtin_rotateleft64(r31.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r9.u64 & 0xFFFFFFFF);
	// clrlwi r29,r27,27
	r29.u64 = r27.u32 & 0x1F;
	// mr r30,r9
	r30.u64 = ctx.r9.u64;
	// rlwinm r9,r27,29,3,29
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r31,r1,-156
	r31.s64 = ctx.r1.s64 + -156;
	// slw r6,r6,r29
	ctx.r6.u64 = r29.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r29.u8 & 0x3F));
	// lwzx r29,r9,r31
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// srd r30,r30,r11
	r30.u64 = r11.u8 & 0x40 ? 0 : (r30.u64 >> (r11.u8 & 0x7F));
	// or r6,r6,r29
	ctx.r6.u64 = ctx.r6.u64 | r29.u64;
	// stwx r6,r9,r31
	PPC_STORE_U32(ctx.r9.u32 + r31.u32, ctx.r6.u32);
	// srd r9,r30,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (r30.u64 >> (r11.u8 & 0x7F));
	// srd r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// subf. r23,r11,r23
	r23.s64 = r23.s64 - r11.s64;
	cr0.compare<int32_t>(r23.s32, 0, xer);
	// beq 0x824c754c
	if (cr0.eq) goto loc_824C754C;
	// lwz r16,-152(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -152);
loc_824C7384:
	// clrlwi r6,r27,27
	ctx.r6.u64 = r27.u32 & 0x1F;
	// rlwinm r11,r27,29,3,29
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r9,r1,-160
	ctx.r9.s64 = ctx.r1.s64 + -160;
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// addi r26,r26,5
	r26.s64 = r26.s64 + 5;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmplw cr6,r25,r7
	cr6.compare<uint32_t>(r25.u32, ctx.r7.u32, xer);
	// lwzx r31,r11,r9
	r31.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// slw r6,r23,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r6.u8 & 0x3F));
	// or r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 | r31.u64;
	// stwx r6,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, ctx.r6.u32);
	// blt cr6,0x824c7208
	if (cr6.lt) goto loc_824C7208;
loc_824C73B4:
	// mr r29,r17
	r29.u64 = r17.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x824c752c
	if (cr6.eq) goto loc_824C752C;
loc_824C73C0:
	// mr r30,r17
	r30.u64 = r17.u64;
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r8,r17
	ctx.r8.u64 = r17.u64;
	// mr r11,r17
	r11.u64 = r17.u64;
loc_824C73D0:
	// addi r9,r11,3
	ctx.r9.s64 = r11.s64 + 3;
	// rlwinm r5,r11,29,3,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r9,r9,27
	ctx.r9.u64 = ctx.r9.u32 & 0x1F;
	// addi r4,r1,-160
	ctx.r4.s64 = ctx.r1.s64 + -160;
	// clrlwi r10,r11,27
	ctx.r10.u64 = r11.u32 & 0x1F;
	// lwzx r5,r5,r4
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
	// slw r9,r22,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r9.u8 & 0x3F));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// and r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 & ctx.r5.u64;
	// slw r5,r20,r10
	ctx.r5.u64 = ctx.r10.u8 & 0x20 ? 0 : (r20.u32 << (ctx.r10.u8 & 0x3F));
	// and r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 & ctx.r5.u64;
	// srw. r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r10.u8 & 0x3F));
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c754c
	if (cr0.eq) goto loc_824C754C;
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// bge cr6,0x824c7414
	if (!cr6.lt) goto loc_824C7414;
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// mr r30,r8
	r30.u64 = ctx.r8.u64;
loc_824C7414:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// blt cr6,0x824c73d0
	if (cr6.lt) goto loc_824C73D0;
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,-156
	ctx.r9.s64 = ctx.r1.s64 + -156;
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// clrlwi r10,r11,27
	ctx.r10.u64 = r11.u32 & 0x1F;
	// clrlwi r8,r8,27
	ctx.r8.u64 = ctx.r8.u32 & 0x1F;
	// rlwinm r5,r11,29,3,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// lwzx r5,r5,r9
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r9.u32);
	// slw r9,r22,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r8.u8 & 0x3F));
	// slw r8,r20,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (r20.u32 << (ctx.r10.u8 & 0x3F));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// and r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 & ctx.r5.u64;
	// and r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ctx.r8.u64;
	// srw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r10.u8 & 0x3F));
	// andc r10,r28,r10
	ctx.r10.u64 = r28.u64 & ~ctx.r10.u64;
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// andc r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r9.u64;
	// subf r28,r10,r28
	r28.s64 = r28.s64 - ctx.r10.s64;
loc_824C7470:
	// addi r4,r6,3
	ctx.r4.s64 = ctx.r6.s64 + 3;
	// clrlwi r8,r6,27
	ctx.r8.u64 = ctx.r6.u32 & 0x1F;
	// clrlwi r4,r4,27
	ctx.r4.u64 = ctx.r4.u32 & 0x1F;
	// rlwinm r9,r6,29,3,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r5,r1,-156
	ctx.r5.s64 = ctx.r1.s64 + -156;
	// slw r27,r20,r8
	r27.u64 = ctx.r8.u8 & 0x20 ? 0 : (r20.u32 << (ctx.r8.u8 & 0x3F));
	// slw r4,r22,r4
	ctx.r4.u64 = ctx.r4.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r4.u8 & 0x3F));
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// and r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 & r27.u64;
	// lwzx r27,r9,r5
	r27.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	// and r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 & r27.u64;
	// srw r4,r4,r8
	ctx.r4.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r4.u32 >> (ctx.r8.u8 & 0x3F));
	// and. r4,r4,r10
	ctx.r4.u64 = ctx.r4.u64 & ctx.r10.u64;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne 0x824c74d0
	if (!cr0.eq) goto loc_824C74D0;
	// slw r26,r10,r8
	r26.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r8.u8 & 0x3F));
	// rotlwi r27,r27,0
	r27.u64 = __builtin_rotateleft32(r27.u32, 0);
	// addi r4,r1,-160
	ctx.r4.s64 = ctx.r1.s64 + -160;
	// or r27,r26,r27
	r27.u64 = r26.u64 | r27.u64;
	// stwx r27,r9,r5
	PPC_STORE_U32(ctx.r9.u32 + ctx.r5.u32, r27.u32);
	// li r5,1
	ctx.r5.s64 = 1;
	// slw r8,r5,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r8.u8 & 0x3F));
	// lwzx r5,r9,r4
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r4.u32);
	// subf r8,r8,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r8.s64;
	// stwx r8,r9,r4
	PPC_STORE_U32(ctx.r9.u32 + ctx.r4.u32, ctx.r8.u32);
loc_824C74D0:
	// addic. r31,r31,-1
	xer.ca = r31.u32 > 0;
	r31.s64 = r31.s64 + -1;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// bne 0x824c7470
	if (!cr0.eq) goto loc_824C7470;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r9,r30,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r4,r10,31
	xer.ca = ctx.r10.u32 <= 31;
	ctx.r4.s64 = 31 - ctx.r10.s64;
	// rlwinm r6,r9,29,3,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFC;
	// rlwinm r10,r11,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// li r5,8
	ctx.r5.s64 = 8;
	// clrlwi r9,r9,27
	ctx.r9.u64 = ctx.r9.u32 & 0x1F;
	// clrlwi r11,r11,27
	r11.u64 = r11.u32 & 0x1F;
	// addi r8,r1,-160
	ctx.r8.s64 = ctx.r1.s64 + -160;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmplw cr6,r29,r7
	cr6.compare<uint32_t>(r29.u32, ctx.r7.u32, xer);
	// slw r9,r4,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r4.u32 << (ctx.r9.u8 & 0x3F));
	// lwzx r4,r10,r8
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// slw r11,r5,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r5.u32 << (r11.u8 & 0x3F));
	// lwzx r5,r6,r3
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r3.u32);
	// or r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 | ctx.r5.u64;
	// or r11,r11,r4
	r11.u64 = r11.u64 | ctx.r4.u64;
	// stwx r9,r6,r3
	PPC_STORE_U32(ctx.r6.u32 + ctx.r3.u32, ctx.r9.u32);
	// stwx r11,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, r11.u32);
	// blt cr6,0x824c73c0
	if (cr6.lt) goto loc_824C73C0;
loc_824C752C:
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r10,r28,r16
	ctx.r10.s64 = r16.s64 - r28.s64;
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// rlwinm r10,r10,10,0,21
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0xFFFFFC00;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
loc_824C754C:
	// b 0x8239bd14
	return;
}

__attribute__((alias("__imp__sub_824C7550"))) PPC_WEAK_FUNC(sub_824C7550);
PPC_FUNC_IMPL(__imp__sub_824C7550) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r24,r6
	r24.u64 = ctx.r6.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// mr r23,r8
	r23.u64 = ctx.r8.u64;
	// lwz r11,192(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 192);
	// mr r22,r9
	r22.u64 = ctx.r9.u64;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
	// li r21,0
	r21.s64 = 0;
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c759c
	if (!cr0.eq) goto loc_824C759C;
	// std r21,8(r27)
	PPC_STORE_U64(r27.u32 + 8, r21.u64);
	// std r21,16(r27)
	PPC_STORE_U64(r27.u32 + 16, r21.u64);
	// std r21,24(r27)
	PPC_STORE_U64(r27.u32 + 24, r21.u64);
	// std r21,32(r27)
	PPC_STORE_U64(r27.u32 + 32, r21.u64);
	// b 0x824c7680
	goto loc_824C7680;
loc_824C759C:
	// addi r11,r28,40
	r11.s64 = r28.s64 + 40;
	// addi r10,r27,8
	ctx.r10.s64 = r27.s64 + 8;
	// li r6,1
	ctx.r6.s64 = 1;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// std r9,8(r10)
	PPC_STORE_U64(ctx.r10.u32 + 8, ctx.r9.u64);
	// ld r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// std r9,16(r10)
	PPC_STORE_U64(ctx.r10.u32 + 16, ctx.r9.u64);
	// ld r11,24(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 24);
	// std r11,24(r10)
	PPC_STORE_U64(ctx.r10.u32 + 24, r11.u64);
	// lwz r11,192(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 192);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x824c7620
	if (!cr6.gt) goto loc_824C7620;
	// addi r7,r28,80
	ctx.r7.s64 = r28.s64 + 80;
loc_824C75DC:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// li r9,4
	ctx.r9.s64 = 4;
loc_824C75E8:
	// ld r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ld r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// and r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 & ctx.r4.u64;
	// std r5,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r5.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x824c75e8
	if (!cr0.eq) goto loc_824C75E8;
	// lwz r11,192(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 192);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r7,r7,40
	ctx.r7.s64 = ctx.r7.s64 + 40;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r6,r11
	cr6.compare<uint32_t>(ctx.r6.u32, r11.u32, xer);
	// blt cr6,0x824c75dc
	if (cr6.lt) goto loc_824C75DC;
loc_824C7620:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x824c7680
	if (cr6.eq) goto loc_824C7680;
	// lwz r11,192(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// clrlwi. r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c7680
	if (cr0.eq) goto loc_824C7680;
	// addi r7,r25,40
	ctx.r7.s64 = r25.s64 + 40;
loc_824C763C:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// li r9,4
	ctx.r9.s64 = 4;
loc_824C7648:
	// ld r5,0(r8)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ld r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// and r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 & ctx.r4.u64;
	// std r5,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r5.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x824c7648
	if (!cr0.eq) goto loc_824C7648;
	// lwz r11,192(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r7,r7,40
	ctx.r7.s64 = ctx.r7.s64 + 40;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r6,r11
	cr6.compare<uint32_t>(ctx.r6.u32, r11.u32, xer);
	// blt cr6,0x824c763c
	if (cr6.lt) goto loc_824C763C;
loc_824C7680:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r29,r21
	r29.u64 = r21.u64;
	// mr r30,r21
	r30.u64 = r21.u64;
	// b 0x824c76fc
	goto loc_824C76FC;
loc_824C7690:
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824c7188
	sub_824C7188(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm. r10,r11,0,22,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x300;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c771c
	if (!cr0.eq) goto loc_824C771C;
	// lwz r11,192(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 192);
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c76f4
	if (cr0.eq) goto loc_824C76F4;
	// li r11,1
	r11.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// slw r4,r11,r30
	ctx.r4.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// rlwimi r4,r29,4,0,27
	ctx.r4.u64 = (__builtin_rotateleft32(r29.u32, 4) & 0xFFFFFFF0) | (ctx.r4.u64 & 0xFFFFFFFF0000000F);
	// bl 0x824b2278
	sub_824B2278(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// add r11,r30,r26
	r11.u64 = r30.u64 + r26.u64;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// ble cr6,0x824c76f8
	if (!cr6.gt) goto loc_824C76F8;
	// mr r30,r21
	r30.u64 = r21.u64;
loc_824C76F4:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
loc_824C76F8:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
loc_824C76FC:
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x824b98d0
	sub_824B98D0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x824c7690
	if (!cr6.eq) goto loc_824C7690;
	// b 0x824c772c
	goto loc_824C772C;
loc_824C771C:
	// lwz r10,276(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwimi r3,r11,22,28,31
	ctx.r3.u64 = (__builtin_rotateleft32(r11.u32, 22) & 0xF) | (ctx.r3.u64 & 0xFFFFFFFFFFFFFFF0);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
loc_824C772C:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_824C7734"))) PPC_WEAK_FUNC(sub_824C7734);
PPC_FUNC_IMPL(__imp__sub_824C7734) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C7738"))) PPC_WEAK_FUNC(sub_824C7738);
PPC_FUNC_IMPL(__imp__sub_824C7738) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// lwz r11,744(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 744);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c7760
	if (!cr0.eq) goto loc_824C7760;
	// li r11,32
	r11.s64 = 32;
loc_824C7760:
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// blt cr6,0x824c7774
	if (cr6.lt) goto loc_824C7774;
	// li r4,3564
	ctx.r4.s64 = 3564;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C7774:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// oris r11,r11,16
	r11.u64 = r11.u64 | 1048576;
	// rlwinm r30,r11,16,29,31
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0x7;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r29,r11,31,28,31
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// add r11,r3,r30
	r11.u64 = ctx.r3.u64 + r30.u64;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bge cr6,0x824c785c
	if (!cr6.lt) goto loc_824C785C;
	// subfic r26,r28,63
	xer.ca = r28.u32 <= 63;
	r26.s64 = 63 - r28.s64;
	// addi r25,r28,1
	r25.s64 = r28.s64 + 1;
	// mulli r31,r3,40
	r31.s64 = ctx.r3.s64 * 40;
	// subf r24,r3,r11
	r24.s64 = r11.s64 - ctx.r3.s64;
loc_824C77B0:
	// addi r11,r29,-1
	r11.s64 = r29.s64 + -1;
	// lwz r10,640(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 640);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// andc r11,r29,r11
	r11.u64 = r29.u64 & ~r11.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// add r3,r10,r31
	ctx.r3.u64 = ctx.r10.u64 + r31.u64;
	// subfic r30,r11,31
	xer.ca = r11.u32 <= 31;
	r30.s64 = 31 - r11.s64;
	// bl 0x824c7068
	sub_824C7068(ctx, base);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824c77f4
	if (cr6.eq) goto loc_824C77F4;
	// lwz r11,640(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 640);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r11,r31
	ctx.r3.u64 = r11.u64 + r31.u64;
	// bl 0x824c7068
	sub_824C7068(ctx, base);
loc_824C77F4:
	// li r7,0
	ctx.r7.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
loc_824C77FC:
	// cmplw cr6,r7,r30
	cr6.compare<uint32_t>(ctx.r7.u32, r30.u32, xer);
	// beq cr6,0x824c7840
	if (cr6.eq) goto loc_824C7840;
	// lis r10,4369
	ctx.r10.s64 = 286326784;
	// lwz r11,640(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 640);
	// ori r10,r10,4369
	ctx.r10.u64 = ctx.r10.u64 | 4369;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// rldimi r10,r10,32,0
	ctx.r10.u64 = (__builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r10.u64 & 0xFFFFFFFF);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// li r10,4
	ctx.r10.s64 = 4;
	// sld r9,r9,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
loc_824C7828:
	// ld r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// or r6,r9,r6
	ctx.r6.u64 = ctx.r9.u64 | ctx.r6.u64;
	// std r6,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r6.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x824c7828
	if (!cr0.eq) goto loc_824C7828;
loc_824C7840:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// cmplwi cr6,r7,4
	cr6.compare<uint32_t>(ctx.r7.u32, 4, xer);
	// blt cr6,0x824c77fc
	if (cr6.lt) goto loc_824C77FC;
	// addic. r24,r24,-1
	xer.ca = r24.u32 > 0;
	r24.s64 = r24.s64 + -1;
	cr0.compare<int32_t>(r24.s32, 0, xer);
	// addi r31,r31,40
	r31.s64 = r31.s64 + 40;
	// bne 0x824c77b0
	if (!cr0.eq) goto loc_824C77B0;
loc_824C785C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_824C7864"))) PPC_WEAK_FUNC(sub_824C7864);
PPC_FUNC_IMPL(__imp__sub_824C7864) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C7868"))) PPC_WEAK_FUNC(sub_824C7868);
PPC_FUNC_IMPL(__imp__sub_824C7868) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r10,r27,4
	ctx.r10.s64 = r27.s64 + 4;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r25,29,3,28
	ctx.r9.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 29) & 0x1FFFFFF8;
	// clrldi r11,r25,58
	r11.u64 = r25.u64 & 0x3F;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// lwzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r29.u32);
	// li r26,0
	r26.s64 = 0;
	// ldx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r9.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x824c78c0
	if (!cr6.eq) goto loc_824C78C0;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_824C78C0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c78d0
	if (cr0.eq) goto loc_824C78D0;
loc_824C78C8:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824c7a34
	goto loc_824C7A34;
loc_824C78D0:
	// rlwinm r10,r27,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 636);
	// lwz r8,648(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// lwzx r7,r10,r29
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + r29.u32);
	// mullw r10,r11,r28
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(r28.s32);
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r9,r8
	r11.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x824c7920
	if (!cr6.lt) goto loc_824C7920;
	// subf r9,r11,r7
	ctx.r9.s64 = ctx.r7.s64 - r11.s64;
loc_824C7900:
	// ldx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + r11.u32);
	// ld r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// and r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ctx.r7.u64;
	// cmpldi cr6,r8,0
	cr6.compare<uint64_t>(ctx.r8.u64, 0, xer);
	// bne cr6,0x824c7a3c
	if (!cr6.eq) goto loc_824C7A3C;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c7900
	if (cr6.lt) goto loc_824C7900;
loc_824C7920:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_824C7924:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c78c8
	if (!cr0.eq) goto loc_824C78C8;
	// lwz r11,192(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c7a30
	if (cr0.eq) goto loc_824C7A30;
	// lwz r11,640(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 640);
	// mulli r10,r28,40
	ctx.r10.s64 = r28.s64 * 40;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C794C:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824c7a44
	if (!cr6.eq) goto loc_824C7A44;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// blt cr6,0x824c794c
	if (cr6.lt) goto loc_824C794C;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_824C796C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c7a30
	if (cr0.eq) goto loc_824C7A30;
	// addi r11,r1,120
	r11.s64 = ctx.r1.s64 + 120;
	// stw r31,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r31.u32);
	// lwz r10,744(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 744);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// std r26,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r26.u64);
	// std r26,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r26.u64);
	// std r26,16(r11)
	PPC_STORE_U64(r11.u32 + 16, r26.u64);
	// std r26,24(r11)
	PPC_STORE_U64(r11.u32 + 24, r26.u64);
	// bne 0x824c799c
	if (!cr0.eq) goto loc_824C799C;
	// li r10,32
	ctx.r10.s64 = 32;
loc_824C799C:
	// cmplwi cr6,r10,64
	cr6.compare<uint32_t>(ctx.r10.u32, 64, xer);
	// bge cr6,0x824c79b4
	if (!cr6.lt) goto loc_824C79B4;
	// subfic r5,r10,64
	xer.ca = ctx.r10.u32 <= 64;
	ctx.r5.s64 = 64 - ctx.r10.s64;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x824b20b0
	sub_824B20B0(ctx, base);
loc_824C79B4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c6a90
	sub_824C6A90(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x824c7100
	sub_824C7100(ctx, base);
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lwz r7,192(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// clrlwi r7,r7,29
	ctx.r7.u64 = ctx.r7.u32 & 0x7;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// or r6,r11,r27
	ctx.r6.u64 = r11.u64 | r27.u64;
	// bl 0x824c7550
	sub_824C7550(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c65d8
	sub_824C65D8(ctx, base);
	// cmpwi cr6,r29,-1
	cr6.compare<int32_t>(r29.s32, -1, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// beq cr6,0x824c7a34
	if (cr6.eq) goto loc_824C7A34;
loc_824C7A30:
	// li r3,1
	ctx.r3.s64 = 1;
loc_824C7A34:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd3c
	return;
loc_824C7A3C:
	// li r11,1
	r11.s64 = 1;
	// b 0x824c7924
	goto loc_824C7924;
loc_824C7A44:
	// li r11,1
	r11.s64 = 1;
	// b 0x824c796c
	goto loc_824C796C;
}

__attribute__((alias("__imp__sub_824C7A4C"))) PPC_WEAK_FUNC(sub_824C7A4C);
PPC_FUNC_IMPL(__imp__sub_824C7A4C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C7A50"))) PPC_WEAK_FUNC(sub_824C7A50);
PPC_FUNC_IMPL(__imp__sub_824C7A50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r27,0
	r27.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// lwz r11,192(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// mr r30,r27
	r30.u64 = r27.u64;
	// clrlwi. r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c7ab4
	if (cr0.eq) goto loc_824C7AB4;
loc_824C7A80:
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c7868
	sub_824C7868(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c7bd0
	if (!cr0.eq) goto loc_824C7BD0;
	// lwz r11,192(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x824c7a80
	if (cr6.lt) goto loc_824C7A80;
loc_824C7AB4:
	// lwz r11,192(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// clrlwi r10,r11,29
	ctx.r10.u64 = r11.u32 & 0x7;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// bne cr6,0x824c7acc
	if (!cr6.eq) goto loc_824C7ACC;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824c7bd4
	goto loc_824C7BD4;
loc_824C7ACC:
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c7bd0
	if (cr0.eq) goto loc_824C7BD0;
	// lwz r11,640(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 640);
	// mulli r10,r28,40
	ctx.r10.s64 = r28.s64 * 40;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C7AE8:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824c7bdc
	if (!cr6.eq) goto loc_824C7BDC;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// blt cr6,0x824c7ae8
	if (cr6.lt) goto loc_824C7AE8;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_824C7B08:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c7bd0
	if (cr0.eq) goto loc_824C7BD0;
	// addi r11,r1,120
	r11.s64 = ctx.r1.s64 + 120;
	// stw r31,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r31.u32);
	// lwz r10,744(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 744);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// std r27,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r27.u64);
	// std r27,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r27.u64);
	// std r27,16(r11)
	PPC_STORE_U64(r11.u32 + 16, r27.u64);
	// std r27,24(r11)
	PPC_STORE_U64(r11.u32 + 24, r27.u64);
	// bne 0x824c7b38
	if (!cr0.eq) goto loc_824C7B38;
	// li r10,32
	ctx.r10.s64 = 32;
loc_824C7B38:
	// cmplwi cr6,r10,64
	cr6.compare<uint32_t>(ctx.r10.u32, 64, xer);
	// bge cr6,0x824c7b50
	if (!cr6.lt) goto loc_824C7B50;
	// subfic r5,r10,64
	xer.ca = ctx.r10.u32 <= 64;
	ctx.r5.s64 = 64 - ctx.r10.s64;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x824b20b0
	sub_824B20B0(ctx, base);
loc_824C7B50:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c6a90
	sub_824C6A90(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x824c7100
	sub_824C7100(ctx, base);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lwz r11,192(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r27,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r27.u32);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// or r6,r11,r7
	ctx.r6.u64 = r11.u64 | ctx.r7.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// bl 0x824c7550
	sub_824C7550(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824c65d8
	sub_824C65D8(ctx, base);
	// cmpwi cr6,r29,-1
	cr6.compare<int32_t>(r29.s32, -1, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// beq cr6,0x824c7bd4
	if (cr6.eq) goto loc_824C7BD4;
loc_824C7BD0:
	// li r3,1
	ctx.r3.s64 = 1;
loc_824C7BD4:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd40
	return;
loc_824C7BDC:
	// li r11,1
	r11.s64 = 1;
	// b 0x824c7b08
	goto loc_824C7B08;
}

__attribute__((alias("__imp__sub_824C7BE4"))) PPC_WEAK_FUNC(sub_824C7BE4);
PPC_FUNC_IMPL(__imp__sub_824C7BE4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C7BE8"))) PPC_WEAK_FUNC(sub_824C7BE8);
PPC_FUNC_IMPL(__imp__sub_824C7BE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r21,0
	r21.s64 = 0;
	// stw r10,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, ctx.r10.u32);
	// mr r18,r3
	r18.u64 = ctx.r3.u64;
	// stw r7,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, ctx.r7.u32);
	// mr r19,r6
	r19.u64 = ctx.r6.u64;
	// mr r17,r5
	r17.u64 = ctx.r5.u64;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// addi r10,r1,100
	ctx.r10.s64 = ctx.r1.s64 + 100;
	// stw r21,0(r18)
	PPC_STORE_U32(r18.u32 + 0, r21.u32);
	// lwz r26,192(r19)
	r26.u64 = PPC_LOAD_U32(r19.u32 + 192);
	// mr r15,r9
	r15.u64 = ctx.r9.u64;
	// mr r14,r4
	r14.u64 = ctx.r4.u64;
	// lwz r25,192(r17)
	r25.u64 = PPC_LOAD_U32(r17.u32 + 192);
	// stw r21,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r21.u32);
	// mr r28,r21
	r28.u64 = r21.u64;
	// stw r21,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r21.u32);
	// clrlwi r16,r25,29
	r16.u64 = r25.u32 & 0x7;
	// stw r15,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, r15.u32);
	// li r22,1
	r22.s64 = 1;
	// stw r14,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, r14.u32);
	// li r27,2
	r27.s64 = 2;
	// li r9,-1
	ctx.r9.s64 = -1;
	// clrlwi. r20,r26,29
	r20.u64 = r26.u32 & 0x7;
	cr0.compare<int32_t>(r20.s32, 0, xer);
	// beq 0x824c7e30
	if (cr0.eq) goto loc_824C7E30;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// addi r24,r19,40
	r24.s64 = r19.s64 + 40;
	// subfic r23,r19,3
	xer.ca = r19.u32 <= 3;
	r23.s64 = 3 - r19.s64;
loc_824C7C60:
	// slw r11,r22,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (r22.u32 << (r28.u8 & 0x3F));
	// and. r11,r11,r15
	r11.u64 = r11.u64 & r15.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c7e1c
	if (!cr0.eq) goto loc_824C7E1C;
	// mr r29,r21
	r29.u64 = r21.u64;
	// cmplwi cr6,r16,0
	cr6.compare<uint32_t>(r16.u32, 0, xer);
	// beq cr6,0x824c7da4
	if (cr6.eq) goto loc_824C7DA4;
	// add r31,r5,r19
	r31.u64 = ctx.r5.u64 + r19.u64;
	// lwz r11,636(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 636);
	// addi r4,r17,40
	ctx.r4.s64 = r17.s64 + 40;
	// addi r6,r17,16
	ctx.r6.s64 = r17.s64 + 16;
	// rlwinm r3,r11,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r30,16(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 16);
loc_824C7C90:
	// mr r11,r30
	r11.u64 = r30.u64;
	// add r10,r3,r11
	ctx.r10.u64 = ctx.r3.u64 + r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x824c7cc8
	if (!cr6.lt) goto loc_824C7CC8;
	// lwz r7,-16(r6)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + -16);
	// subf r7,r30,r7
	ctx.r7.s64 = ctx.r7.s64 - r30.s64;
loc_824C7CA8:
	// ldx r15,r7,r11
	r15.u64 = PPC_LOAD_U64(ctx.r7.u32 + r11.u32);
	// ld r14,0(r11)
	r14.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// and r15,r15,r14
	r15.u64 = r15.u64 & r14.u64;
	// cmpldi cr6,r15,0
	cr6.compare<uint64_t>(r15.u64, 0, xer);
	// bne cr6,0x824c7f54
	if (!cr6.eq) goto loc_824C7F54;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c7ca8
	if (cr6.lt) goto loc_824C7CA8;
loc_824C7CC8:
	// mr r11,r21
	r11.u64 = r21.u64;
loc_824C7CCC:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c7d68
	if (!cr0.eq) goto loc_824C7D68;
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// add r10,r3,r11
	ctx.r10.u64 = ctx.r3.u64 + r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x824c7d0c
	if (!cr6.lt) goto loc_824C7D0C;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// subf r7,r11,r7
	ctx.r7.s64 = ctx.r7.s64 - r11.s64;
loc_824C7CEC:
	// ldx r15,r7,r11
	r15.u64 = PPC_LOAD_U64(ctx.r7.u32 + r11.u32);
	// ld r14,0(r11)
	r14.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// and r15,r15,r14
	r15.u64 = r15.u64 & r14.u64;
	// cmpldi cr6,r15,0
	cr6.compare<uint64_t>(r15.u64, 0, xer);
	// bne cr6,0x824c7f5c
	if (!cr6.eq) goto loc_824C7F5C;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c7cec
	if (cr6.lt) goto loc_824C7CEC;
loc_824C7D0C:
	// mr r11,r21
	r11.u64 = r21.u64;
loc_824C7D10:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c7d68
	if (!cr0.eq) goto loc_824C7D68;
	// rlwinm. r11,r26,0,1,1
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c7d88
	if (cr0.eq) goto loc_824C7D88;
	// rlwinm. r11,r25,0,1,1
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c7d88
	if (cr0.eq) goto loc_824C7D88;
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_824C7D34:
	// ld r15,0(r10)
	r15.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// ld r14,0(r11)
	r14.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// nor r15,r15,r14
	r15.u64 = ~(r15.u64 | r14.u64);
	// cmpldi cr6,r15,0
	cr6.compare<uint64_t>(r15.u64, 0, xer);
	// bne cr6,0x824c7f64
	if (!cr6.eq) goto loc_824C7F64;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmpwi cr6,r7,4
	cr6.compare<int32_t>(ctx.r7.s32, 4, xer);
	// blt cr6,0x824c7d34
	if (cr6.lt) goto loc_824C7D34;
	// mr r11,r21
	r11.u64 = r21.u64;
loc_824C7D60:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c7d88
	if (!cr0.eq) goto loc_824C7D88;
loc_824C7D68:
	// slw r15,r22,r29
	r15.u64 = r29.u8 & 0x20 ? 0 : (r22.u32 << (r29.u8 & 0x3F));
	// clrlwi r7,r5,27
	ctx.r7.u64 = ctx.r5.u32 & 0x1F;
	// rlwinm r11,r5,29,3,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// slw r7,r15,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r15.u32 << (ctx.r7.u8 & 0x3F));
	// lwzx r15,r11,r10
	r15.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// or r7,r7,r15
	ctx.r7.u64 = ctx.r7.u64 | r15.u64;
	// stwx r7,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r7.u32);
loc_824C7D88:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// addi r4,r4,40
	ctx.r4.s64 = ctx.r4.s64 + 40;
	// cmplw cr6,r29,r16
	cr6.compare<uint32_t>(r29.u32, r16.u32, xer);
	// blt cr6,0x824c7c90
	if (cr6.lt) goto loc_824C7C90;
	// lwz r15,372(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// lwz r14,332(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
loc_824C7DA4:
	// add r7,r23,r5
	ctx.r7.u64 = r23.u64 + ctx.r5.u64;
	// clrlwi r10,r5,27
	ctx.r10.u64 = ctx.r5.u32 & 0x1F;
	// add r7,r7,r19
	ctx.r7.u64 = ctx.r7.u64 + r19.u64;
	// rlwinm r11,r5,29,3,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r7,r7,27
	ctx.r7.u64 = ctx.r7.u32 & 0x1F;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lis r6,-28311
	ctx.r6.s64 = -1855389696;
	// lis r3,0
	ctx.r3.s64 = 0;
	// ori r6,r6,5192
	ctx.r6.u64 = ctx.r6.u64 | 5192;
	// ori r3,r3,36262
	ctx.r3.u64 = ctx.r3.u64 | 36262;
	// lwzx r4,r11,r4
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + ctx.r4.u32);
	// rldimi r6,r3,32,0
	ctx.r6.u64 = (__builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r6.u64 & 0xFFFFFFFF);
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// addi r6,r1,100
	ctx.r6.s64 = ctx.r1.s64 + 100;
	// slw r31,r9,r10
	r31.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// slw r7,r27,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r27.u32 << (ctx.r7.u8 & 0x3F));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// and r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 & ctx.r4.u64;
	// lwzx r4,r11,r6
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// and r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 & r31.u64;
	// srw r7,r7,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r10.u8 & 0x3F));
	// clrldi r7,r7,32
	ctx.r7.u64 = ctx.r7.u64 & 0xFFFFFFFF;
	// srd r3,r3,r7
	ctx.r3.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r3.u64 >> (ctx.r7.u8 & 0x7F));
	// srd r3,r3,r7
	ctx.r3.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r3.u64 >> (ctx.r7.u8 & 0x7F));
	// srd r7,r3,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r3.u64 >> (ctx.r7.u8 & 0x7F));
	// clrlwi r7,r7,29
	ctx.r7.u64 = ctx.r7.u32 & 0x7;
	// subfic r7,r7,4
	xer.ca = ctx.r7.u32 <= 4;
	ctx.r7.s64 = 4 - ctx.r7.s64;
	// slw r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r10.u8 & 0x3F));
	// or r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 | ctx.r4.u64;
	// stwx r10,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r10.u32);
loc_824C7E1C:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r24,r24,40
	r24.s64 = r24.s64 + 40;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// cmplw cr6,r28,r20
	cr6.compare<uint32_t>(r28.u32, r20.u32, xer);
	// blt cr6,0x824c7c60
	if (cr6.lt) goto loc_824C7C60;
loc_824C7E30:
	// mr r28,r21
	r28.u64 = r21.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x824c7fdc
	if (cr6.eq) goto loc_824C7FDC;
	// li r4,4
	ctx.r4.s64 = 4;
loc_824C7E40:
	// rlwinm r6,r4,29,3,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r5,r1,356
	ctx.r5.s64 = ctx.r1.s64 + 356;
	// addi r11,r4,-1
	r11.s64 = ctx.r4.s64 + -1;
	// addi r7,r4,-4
	ctx.r7.s64 = ctx.r4.s64 + -4;
	// clrlwi r26,r11,27
	r26.u64 = r11.u32 & 0x1F;
	// clrlwi r10,r4,27
	ctx.r10.u64 = ctx.r4.u32 & 0x1F;
	// lwzx r5,r6,r5
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r5.u32);
	// addi r6,r11,4
	ctx.r6.s64 = r11.s64 + 4;
	// rlwinm r11,r7,29,3,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r29,r6,27
	r29.u64 = ctx.r6.u32 & 0x1F;
	// addi r3,r1,356
	ctx.r3.s64 = ctx.r1.s64 + 356;
	// clrlwi r6,r7,27
	ctx.r6.u64 = ctx.r7.u32 & 0x1F;
	// addi r31,r1,96
	r31.s64 = ctx.r1.s64 + 96;
	// lwzx r7,r11,r3
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r3.u32);
	// slw r30,r9,r10
	r30.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// slw r11,r27,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (r27.u32 << (r29.u8 & 0x3F));
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// and r5,r3,r5
	ctx.r5.u64 = ctx.r3.u64 & ctx.r5.u64;
	// and r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 & r30.u64;
	// slw r11,r27,r26
	r11.u64 = r26.u8 & 0x20 ? 0 : (r27.u32 << (r26.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// srw r30,r5,r10
	r30.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r10.u8 & 0x3F));
	// slw r10,r9,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r6.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// srw r10,r11,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r6.u8 & 0x3F));
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r11,3
	ctx.r7.s64 = r11.s64 + 3;
	// clrlwi r7,r7,27
	ctx.r7.u64 = ctx.r7.u32 & 0x1F;
	// slw r5,r22,r10
	ctx.r5.u64 = ctx.r10.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r10.u8 & 0x3F));
	// clrlwi r10,r11,27
	ctx.r10.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,29,3,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// lwzx r6,r11,r31
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// slw r11,r27,r7
	r11.u64 = ctx.r7.u8 & 0x20 ? 0 : (r27.u32 << (ctx.r7.u8 & 0x3F));
	// slw r7,r9,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// srw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// and. r11,r11,r5
	r11.u64 = r11.u64 & ctx.r5.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c826c
	if (!cr0.eq) goto loc_824C826C;
	// mr r29,r21
	r29.u64 = r21.u64;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x824c7fcc
	if (cr6.eq) goto loc_824C7FCC;
	// mr r11,r21
	r11.u64 = r21.u64;
loc_824C7EF4:
	// cmplw cr6,r29,r30
	cr6.compare<uint32_t>(r29.u32, r30.u32, xer);
	// clrlwi r7,r11,27
	ctx.r7.u64 = r11.u32 & 0x1F;
	// rlwinm r10,r11,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r6,r11,3
	ctx.r6.s64 = r11.s64 + 3;
	// bne cr6,0x824c7f6c
	if (!cr6.eq) goto loc_824C7F6C;
	// clrlwi r6,r6,27
	ctx.r6.u64 = ctx.r6.u32 & 0x1F;
	// subfic r26,r5,15
	xer.ca = ctx.r5.u32 <= 15;
	r26.s64 = 15 - ctx.r5.s64;
	// slw r25,r9,r7
	r25.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r7.u8 & 0x3F));
	// addi r31,r1,100
	r31.s64 = ctx.r1.s64 + 100;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwzx r24,r10,r3
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// slw r6,r27,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (r27.u32 << (ctx.r6.u8 & 0x3F));
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// nand r6,r6,r25
	ctx.r6.u64 = ~(ctx.r6.u64 & r25.u64);
	// slw r25,r22,r7
	r25.u64 = ctx.r7.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r7.u8 & 0x3F));
	// slw r7,r26,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r26.u32 << (ctx.r7.u8 & 0x3F));
	// lwzx r26,r10,r31
	r26.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// and r26,r6,r26
	r26.u64 = ctx.r6.u64 & r26.u64;
	// and r6,r6,r24
	ctx.r6.u64 = ctx.r6.u64 & r24.u64;
	// or r26,r26,r25
	r26.u64 = r26.u64 | r25.u64;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// stwx r26,r10,r31
	PPC_STORE_U32(ctx.r10.u32 + r31.u32, r26.u32);
	// stwx r7,r10,r3
	PPC_STORE_U32(ctx.r10.u32 + ctx.r3.u32, ctx.r7.u32);
	// b 0x824c7fbc
	goto loc_824C7FBC;
loc_824C7F54:
	// mr r11,r22
	r11.u64 = r22.u64;
	// b 0x824c7ccc
	goto loc_824C7CCC;
loc_824C7F5C:
	// mr r11,r22
	r11.u64 = r22.u64;
	// b 0x824c7d10
	goto loc_824C7D10;
loc_824C7F64:
	// mr r11,r22
	r11.u64 = r22.u64;
	// b 0x824c7d60
	goto loc_824C7D60;
loc_824C7F6C:
	// clrlwi r3,r6,27
	ctx.r3.u64 = ctx.r6.u32 & 0x1F;
	// addi r6,r1,100
	ctx.r6.s64 = ctx.r1.s64 + 100;
	// slw r26,r9,r7
	r26.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r7.u8 & 0x3F));
	// addi r31,r1,96
	r31.s64 = ctx.r1.s64 + 96;
	// lwzx r25,r10,r6
	r25.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// lwzx r24,r10,r31
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// slw r3,r27,r3
	ctx.r3.u64 = ctx.r3.u8 & 0x20 ? 0 : (r27.u32 << (ctx.r3.u8 & 0x3F));
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// and r3,r3,r26
	ctx.r3.u64 = ctx.r3.u64 & r26.u64;
	// slw r26,r5,r7
	r26.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r7.u8 & 0x3F));
	// and r25,r3,r25
	r25.u64 = ctx.r3.u64 & r25.u64;
	// or r26,r26,r24
	r26.u64 = r26.u64 | r24.u64;
	// stwx r26,r10,r31
	PPC_STORE_U32(ctx.r10.u32 + r31.u32, r26.u32);
	// srw r31,r25,r7
	r31.u64 = ctx.r7.u8 & 0x20 ? 0 : (r25.u32 >> (ctx.r7.u8 & 0x3F));
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// slw r7,r31,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r7.u8 & 0x3F));
	// lwzx r31,r10,r6
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// andc r3,r31,r3
	ctx.r3.u64 = r31.u64 & ~ctx.r3.u64;
	// or r7,r3,r7
	ctx.r7.u64 = ctx.r3.u64 | ctx.r7.u64;
	// stwx r7,r10,r6
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, ctx.r7.u32);
loc_824C7FBC:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r29,r20
	cr6.compare<uint32_t>(r29.u32, r20.u32, xer);
	// blt cr6,0x824c7ef4
	if (cr6.lt) goto loc_824C7EF4;
loc_824C7FCC:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// cmplw cr6,r28,r8
	cr6.compare<uint32_t>(r28.u32, ctx.r8.u32, xer);
	// blt cr6,0x824c7e40
	if (cr6.lt) goto loc_824C7E40;
loc_824C7FDC:
	// li r30,15
	r30.s64 = 15;
	// mr r29,r21
	r29.u64 = r21.u64;
	// mr r28,r21
	r28.u64 = r21.u64;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x824c8194
	if (cr6.eq) goto loc_824C8194;
loc_824C7FF0:
	// slw r11,r22,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (r22.u32 << (r28.u8 & 0x3F));
	// and. r11,r11,r15
	r11.u64 = r11.u64 & r15.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c8188
	if (!cr0.eq) goto loc_824C8188;
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// li r5,15
	ctx.r5.s64 = 15;
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// mr r11,r21
	r11.u64 = r21.u64;
loc_824C800C:
	// slw r10,r22,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r8.u8 & 0x3F));
	// and. r10,r10,r15
	ctx.r10.u64 = ctx.r10.u64 & r15.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8068
	if (!cr0.eq) goto loc_824C8068;
	// addi r7,r11,3
	ctx.r7.s64 = r11.s64 + 3;
	// rlwinm r4,r11,29,3,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r7,r7,27
	ctx.r7.u64 = ctx.r7.u32 & 0x1F;
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// clrlwi r10,r11,27
	ctx.r10.u64 = r11.u32 & 0x1F;
	// lwzx r4,r4,r3
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r3.u32);
	// slw r7,r27,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r27.u32 << (ctx.r7.u8 & 0x3F));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// and r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 & ctx.r4.u64;
	// slw r4,r9,r10
	ctx.r4.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// and r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 & ctx.r4.u64;
	// srw r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r10.u8 & 0x3F));
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// bgt cr6,0x824c8068
	if (cr6.gt) goto loc_824C8068;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c826c
	if (cr6.eq) goto loc_824C826C;
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// bge cr6,0x824c8068
	if (!cr6.lt) goto loc_824C8068;
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
loc_824C8068:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r8,r20
	cr6.compare<uint32_t>(ctx.r8.u32, r20.u32, xer);
	// blt cr6,0x824c800c
	if (cr6.lt) goto loc_824C800C;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r8,r5,3
	ctx.r8.s64 = ctx.r5.s64 + 3;
	// clrlwi r11,r5,27
	r11.u64 = ctx.r5.u32 & 0x1F;
	// clrlwi r8,r8,27
	ctx.r8.u64 = ctx.r8.u32 & 0x1F;
	// rlwinm r7,r5,29,3,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0x1FFFFFFC;
	// lwzx r7,r7,r10
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	// slw r10,r27,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (r27.u32 << (ctx.r8.u8 & 0x3F));
	// slw r8,r9,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 & ctx.r7.u64;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
	// srw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// beq cr6,0x824c826c
	if (cr6.eq) goto loc_824C826C;
	// andc r11,r30,r11
	r11.u64 = r30.u64 & ~r11.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// andc r11,r11,r10
	r11.u64 = r11.u64 & ~ctx.r10.u64;
	// cntlzw r10,r11
	ctx.r10.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// or r29,r11,r29
	r29.u64 = r11.u64 | r29.u64;
	// subfic r31,r10,31
	xer.ca = ctx.r10.u32 <= 31;
	r31.s64 = 31 - ctx.r10.s64;
loc_824C80D8:
	// slw r10,r22,r3
	ctx.r10.u64 = ctx.r3.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r3.u8 & 0x3F));
	// and. r10,r10,r15
	ctx.r10.u64 = ctx.r10.u64 & r15.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8140
	if (!cr0.eq) goto loc_824C8140;
	// addi r4,r7,3
	ctx.r4.s64 = ctx.r7.s64 + 3;
	// clrlwi r8,r7,27
	ctx.r8.u64 = ctx.r7.u32 & 0x1F;
	// clrlwi r4,r4,27
	ctx.r4.u64 = ctx.r4.u32 & 0x1F;
	// rlwinm r10,r7,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// slw r26,r9,r8
	r26.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// slw r4,r27,r4
	ctx.r4.u64 = ctx.r4.u8 & 0x20 ? 0 : (r27.u32 << (ctx.r4.u8 & 0x3F));
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// and r4,r4,r26
	ctx.r4.u64 = ctx.r4.u64 & r26.u64;
	// lwzx r26,r10,r6
	r26.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// and r4,r4,r26
	ctx.r4.u64 = ctx.r4.u64 & r26.u64;
	// srw r4,r4,r8
	ctx.r4.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r4.u32 >> (ctx.r8.u8 & 0x3F));
	// and. r4,r4,r11
	ctx.r4.u64 = ctx.r4.u64 & r11.u64;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne 0x824c8140
	if (!cr0.eq) goto loc_824C8140;
	// slw r25,r11,r8
	r25.u64 = ctx.r8.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r8.u8 & 0x3F));
	// rotlwi r26,r26,0
	r26.u64 = __builtin_rotateleft32(r26.u32, 0);
	// addi r4,r1,100
	ctx.r4.s64 = ctx.r1.s64 + 100;
	// or r26,r25,r26
	r26.u64 = r25.u64 | r26.u64;
	// slw r8,r22,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r8.u8 & 0x3F));
	// stwx r26,r10,r6
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, r26.u32);
	// lwzx r6,r10,r4
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r4.u32);
	// subf r8,r8,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r8.s64;
	// stwx r8,r10,r4
	PPC_STORE_U32(ctx.r10.u32 + ctx.r4.u32, ctx.r8.u32);
loc_824C8140:
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmplw cr6,r3,r20
	cr6.compare<uint32_t>(ctx.r3.u32, r20.u32, xer);
	// blt cr6,0x824c80d8
	if (cr6.lt) goto loc_824C80D8;
	// rlwinm r11,r5,29,3,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0x1FFFFFFC;
	// rlwinm r10,r5,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r7,r5,27
	ctx.r7.u64 = ctx.r5.u32 & 0x1F;
	// clrlwi r6,r5,27
	ctx.r6.u64 = ctx.r5.u32 & 0x1F;
	// li r5,15
	ctx.r5.s64 = 15;
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// lwzx r4,r11,r18
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + r18.u32);
	// slw r6,r31,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r6.u8 & 0x3F));
	// slw r7,r5,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r7.u8 & 0x3F));
	// lwzx r5,r10,r8
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// or r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 | ctx.r4.u64;
	// or r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 | ctx.r5.u64;
	// stwx r6,r11,r18
	PPC_STORE_U32(r11.u32 + r18.u32, ctx.r6.u32);
	// stwx r7,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r7.u32);
loc_824C8188:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// cmplw cr6,r28,r20
	cr6.compare<uint32_t>(r28.u32, r20.u32, xer);
	// blt cr6,0x824c7ff0
	if (cr6.lt) goto loc_824C7FF0;
loc_824C8194:
	// cntlzw r11,r29
	r11.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
	// subfic r11,r11,32
	xer.ca = r11.u32 <= 32;
	r11.s64 = 32 - r11.s64;
	// cmplw cr6,r16,r11
	cr6.compare<uint32_t>(r16.u32, r11.u32, xer);
	// bgt cr6,0x824c81ac
	if (cr6.gt) goto loc_824C81AC;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_824C81AC:
	// cmplw cr6,r10,r20
	cr6.compare<uint32_t>(ctx.r10.u32, r20.u32, xer);
	// ble cr6,0x824c81cc
	if (!cr6.gt) goto loc_824C81CC;
	// cmplw cr6,r16,r11
	cr6.compare<uint32_t>(r16.u32, r11.u32, xer);
	// ble cr6,0x824c81c4
	if (!cr6.gt) goto loc_824C81C4;
	// mr r31,r16
	r31.u64 = r16.u64;
	// b 0x824c81d0
	goto loc_824C81D0;
loc_824C81C4:
	// mr r31,r11
	r31.u64 = r11.u64;
	// b 0x824c81d0
	goto loc_824C81D0;
loc_824C81CC:
	// mr r31,r20
	r31.u64 = r20.u64;
loc_824C81D0:
	// lwz r11,192(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 192);
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c825c
	if (cr0.eq) goto loc_824C825C;
	// lwz r11,192(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 192);
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c825c
	if (cr0.eq) goto loc_824C825C;
	// addi r11,r1,120
	r11.s64 = ctx.r1.s64 + 120;
	// stw r14,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r14.u32);
	// lwz r10,744(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 744);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// std r21,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r21.u64);
	// std r21,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r21.u64);
	// std r21,16(r11)
	PPC_STORE_U64(r11.u32 + 16, r21.u64);
	// std r21,24(r11)
	PPC_STORE_U64(r11.u32 + 24, r21.u64);
	// bne 0x824c8210
	if (!cr0.eq) goto loc_824C8210;
	// li r10,32
	ctx.r10.s64 = 32;
loc_824C8210:
	// cmplwi cr6,r10,64
	cr6.compare<uint32_t>(ctx.r10.u32, 64, xer);
	// bge cr6,0x824c8228
	if (!cr6.lt) goto loc_824C8228;
	// subfic r5,r10,64
	xer.ca = ctx.r10.u32 <= 64;
	ctx.r5.s64 = 64 - ctx.r10.s64;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x824b20b0
	sub_824B20B0(ctx, base);
loc_824C8228:
	// addi r11,r1,100
	r11.s64 = ctx.r1.s64 + 100;
	// lwz r6,0(r18)
	ctx.r6.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// lwz r9,380(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r3,r14
	ctx.r3.u64 = r14.u64;
	// bl 0x824c7550
	sub_824C7550(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824c826c
	if (cr6.eq) goto loc_824C826C;
loc_824C825C:
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// rlwinm r10,r31,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 16) & 0xFFFF0000;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r11,0(r18)
	PPC_STORE_U32(r18.u32 + 0, r11.u32);
loc_824C826C:
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_824C8278"))) PPC_WEAK_FUNC(sub_824C8278);
PPC_FUNC_IMPL(__imp__sub_824C8278) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// stw r6,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r6.u32);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824c82e0
	if (!cr6.eq) goto loc_824C82E0;
	// lwz r11,192(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// lwz r10,192(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 192);
	// clrlwi r8,r11,29
	ctx.r8.u64 = r11.u32 & 0x7;
	// clrlwi r9,r10,29
	ctx.r9.u64 = ctx.r10.u32 & 0x7;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - r31.s64;
	// cmplwi cr6,r9,4
	cr6.compare<uint32_t>(ctx.r9.u32, 4, xer);
	// bgt cr6,0x824c82e0
	if (cr6.gt) goto loc_824C82E0;
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c82e0
	if (!cr0.eq) goto loc_824C82E0;
	// rlwinm. r11,r10,0,1,1
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c82e0
	if (!cr0.eq) goto loc_824C82E0;
loc_824C82D8:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x824c841c
	goto loc_824C841C;
loc_824C82E0:
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824c7be8
	sub_824C7BE8(ctx, base);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm. r11,r11,0,12,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xF0000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c82d8
	if (!cr0.eq) goto loc_824C82D8;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c8320
	if (cr6.eq) goto loc_824C8320;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824c841c
	goto loc_824C841C;
loc_824C8320:
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// beq cr6,0x824c83dc
	if (cr6.eq) goto loc_824C83DC;
	// li r11,4
	r11.s64 = 4;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r31,2
	r31.s64 = 2;
	// li r6,-1
	ctx.r6.s64 = -1;
loc_824C8344:
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// clrlwi r8,r8,27
	ctx.r8.u64 = ctx.r8.u32 & 0x1F;
	// clrlwi r24,r3,27
	r24.u64 = ctx.r3.u32 & 0x1F;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r10,r11,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r23,r1,236
	r23.s64 = ctx.r1.s64 + 236;
	// addi r7,r11,-4
	ctx.r7.s64 = r11.s64 + -4;
	// addi r22,r1,236
	r22.s64 = ctx.r1.s64 + 236;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwzx r23,r10,r23
	r23.u64 = PPC_LOAD_U32(ctx.r10.u32 + r23.u32);
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// slw r8,r31,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r8.u8 & 0x3F));
	// addi r21,r8,-1
	r21.s64 = ctx.r8.s64 + -1;
	// and r23,r21,r23
	r23.u64 = r21.u64 & r23.u64;
	// slw r8,r31,r24
	ctx.r8.u64 = r24.u8 & 0x20 ? 0 : (r31.u32 << (r24.u8 & 0x3F));
	// addi r24,r8,-1
	r24.s64 = ctx.r8.s64 + -1;
	// rlwinm r8,r7,29,3,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 29) & 0x1FFFFFFC;
	// slw r21,r6,r9
	r21.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r9.u8 & 0x3F));
	// clrlwi r7,r7,27
	ctx.r7.u64 = ctx.r7.u32 & 0x1F;
	// and r23,r23,r21
	r23.u64 = r23.u64 & r21.u64;
	// lwzx r22,r8,r22
	r22.u64 = PPC_LOAD_U32(ctx.r8.u32 + r22.u32);
	// and r24,r24,r22
	r24.u64 = r24.u64 & r22.u64;
	// slw r21,r6,r7
	r21.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r7.u8 & 0x3F));
	// srw r23,r23,r9
	r23.u64 = ctx.r9.u8 & 0x20 ? 0 : (r23.u32 >> (ctx.r9.u8 & 0x3F));
	// and r24,r24,r21
	r24.u64 = r24.u64 & r21.u64;
	// slw r23,r23,r7
	r23.u64 = ctx.r7.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r7.u8 & 0x3F));
	// srw r7,r24,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r24.u32 >> (ctx.r7.u8 & 0x3F));
	// slw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// lwzx r7,r8,r4
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r4.u32);
	// or r7,r23,r7
	ctx.r7.u64 = r23.u64 | ctx.r7.u64;
	// stwx r7,r8,r4
	PPC_STORE_U32(ctx.r8.u32 + ctx.r4.u32, ctx.r7.u32);
	// lwzx r8,r10,r3
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stwx r9,r10,r3
	PPC_STORE_U32(ctx.r10.u32 + ctx.r3.u32, ctx.r9.u32);
	// bne 0x824c8344
	if (!cr0.eq) goto loc_824C8344;
loc_824C83DC:
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824c7be8
	sub_824C7BE8(ctx, base);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r11,0,12,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xF0000;
	// subf r11,r27,r11
	r11.s64 = r11.s64 - r27.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// xori r3,r11,1
	ctx.r3.u64 = r11.u64 ^ 1;
loc_824C841C:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_824C8424"))) PPC_WEAK_FUNC(sub_824C8424);
PPC_FUNC_IMPL(__imp__sub_824C8424) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C8428"))) PPC_WEAK_FUNC(sub_824C8428);
PPC_FUNC_IMPL(__imp__sub_824C8428) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// stw r6,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r6.u32);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824c7be8
	sub_824C7BE8(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r25,2
	r25.s64 = 2;
	// li r29,-1
	r29.s64 = -1;
	// rlwinm. r10,r11,0,12,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xF0000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8588
	if (!cr0.eq) goto loc_824C8588;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824c84a0
	if (cr6.eq) goto loc_824C84A0;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824C84A0:
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r31,r26
	r31.u64 = r26.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// beq cr6,0x824c8560
	if (cr6.eq) goto loc_824C8560;
	// li r11,4
	r11.s64 = 4;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
loc_824C84C8:
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// clrlwi r8,r8,27
	ctx.r8.u64 = ctx.r8.u32 & 0x1F;
	// clrlwi r28,r3,27
	r28.u64 = ctx.r3.u32 & 0x1F;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r10,r11,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r22,r1,236
	r22.s64 = ctx.r1.s64 + 236;
	// addi r7,r11,-4
	ctx.r7.s64 = r11.s64 + -4;
	// addi r21,r1,236
	r21.s64 = ctx.r1.s64 + 236;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwzx r22,r10,r22
	r22.u64 = PPC_LOAD_U32(ctx.r10.u32 + r22.u32);
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// slw r8,r25,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r25.u32 << (ctx.r8.u8 & 0x3F));
	// addi r20,r8,-1
	r20.s64 = ctx.r8.s64 + -1;
	// and r22,r20,r22
	r22.u64 = r20.u64 & r22.u64;
	// slw r8,r25,r28
	ctx.r8.u64 = r28.u8 & 0x20 ? 0 : (r25.u32 << (r28.u8 & 0x3F));
	// addi r28,r8,-1
	r28.s64 = ctx.r8.s64 + -1;
	// rlwinm r8,r7,29,3,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 29) & 0x1FFFFFFC;
	// slw r20,r29,r9
	r20.u64 = ctx.r9.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r9.u8 & 0x3F));
	// clrlwi r7,r7,27
	ctx.r7.u64 = ctx.r7.u32 & 0x1F;
	// and r22,r22,r20
	r22.u64 = r22.u64 & r20.u64;
	// lwzx r21,r8,r21
	r21.u64 = PPC_LOAD_U32(ctx.r8.u32 + r21.u32);
	// and r28,r28,r21
	r28.u64 = r28.u64 & r21.u64;
	// slw r20,r29,r7
	r20.u64 = ctx.r7.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r7.u8 & 0x3F));
	// srw r22,r22,r9
	r22.u64 = ctx.r9.u8 & 0x20 ? 0 : (r22.u32 >> (ctx.r9.u8 & 0x3F));
	// and r28,r28,r20
	r28.u64 = r28.u64 & r20.u64;
	// slw r22,r22,r7
	r22.u64 = ctx.r7.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r7.u8 & 0x3F));
	// srw r7,r28,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r28.u32 >> (ctx.r7.u8 & 0x3F));
	// slw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// lwzx r7,r8,r4
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r4.u32);
	// or r7,r22,r7
	ctx.r7.u64 = r22.u64 | ctx.r7.u64;
	// stwx r7,r8,r4
	PPC_STORE_U32(ctx.r8.u32 + ctx.r4.u32, ctx.r7.u32);
	// lwzx r8,r10,r3
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stwx r9,r10,r3
	PPC_STORE_U32(ctx.r10.u32 + ctx.r3.u32, ctx.r9.u32);
	// bne 0x824c84c8
	if (!cr0.eq) goto loc_824C84C8;
loc_824C8560:
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824c7be8
	sub_824C7BE8(ctx, base);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_824C8588:
	// rlwinm r30,r11,16,28,31
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xF;
	// b 0x824c85c0
	goto loc_824C85C0;
loc_824C8590:
	// lwz r11,636(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 636);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// rlwinm r11,r11,2,27,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0x1C;
	// stwx r3,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r3.u32);
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// rlwimi r10,r11,0,0,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFF8) | (ctx.r10.u64 & 0xFFFFFFFF00000007);
	// stw r10,192(r31)
	PPC_STORE_U32(r31.u32 + 192, ctx.r10.u32);
loc_824C85C0:
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// blt cr6,0x824c8590
	if (cr6.lt) goto loc_824C8590;
	// lwz r11,192(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 192);
	// li r28,0
	r28.s64 = 0;
	// clrlwi. r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c8654
	if (cr0.eq) goto loc_824C8654;
	// li r30,0
	r30.s64 = 0;
loc_824C85E4:
	// li r11,1
	r11.s64 = 1;
	// slw r11,r11,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (r11.u32 << (r28.u8 & 0x3F));
	// and. r11,r11,r24
	r11.u64 = r11.u64 & r24.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c863c
	if (!cr0.eq) goto loc_824C863C;
	// addi r10,r30,3
	ctx.r10.s64 = r30.s64 + 3;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// clrlwi r10,r10,27
	ctx.r10.u64 = ctx.r10.u32 & 0x1F;
	// rlwinm r9,r30,29,3,29
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r11,r30,27
	r11.u64 = r30.u32 & 0x1F;
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// lwzx r9,r9,r5
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// slw r10,r25,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r25.u32 << (ctx.r10.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// slw r9,r29,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 << (r11.u8 & 0x3F));
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// srw r5,r10,r11
	ctx.r5.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// bl 0x824c6258
	sub_824C6258(ctx, base);
loc_824C863C:
	// lwz r11,192(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 192);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// blt cr6,0x824c85e4
	if (cr6.lt) goto loc_824C85E4;
loc_824C8654:
	// clrlwi. r11,r23,24
	r11.u64 = r23.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c8668
	if (!cr0.eq) goto loc_824C8668;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x824c65d8
	sub_824C65D8(ctx, base);
loc_824C8668:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_824C8674"))) PPC_WEAK_FUNC(sub_824C8674);
PPC_FUNC_IMPL(__imp__sub_824C8674) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824C8678"))) PPC_WEAK_FUNC(sub_824C8678);
PPC_FUNC_IMPL(__imp__sub_824C8678) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x824c8428
	sub_824C8428(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824C86B8"))) PPC_WEAK_FUNC(sub_824C86B8);
PPC_FUNC_IMPL(__imp__sub_824C86B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// stw r5,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r5.u32);
	// li r14,1
	r14.s64 = 1;
	// stw r6,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, ctx.r6.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r6,1
	cr6.compare<uint32_t>(ctx.r6.u32, 1, xer);
	// stb r11,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, r11.u8);
	// stw r14,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r14.u32);
	// blt cr6,0x824cc9e0
	if (cr6.lt) goto loc_824CC9E0;
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// b 0x824c86f8
	goto loc_824C86F8;
loc_824C86F4:
	// li r14,1
	r14.s64 = 1;
loc_824C86F8:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// li r20,0
	r20.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c89c4
	if (!cr6.eq) goto loc_824C89C4;
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// rlwinm. r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c887c
	if (cr0.eq) goto loc_824C887C;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c887c
	if (!cr0.eq) goto loc_824C887C;
	// mr. r28,r11
	r28.u64 = r11.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824c887c
	if (cr0.eq) goto loc_824C887C;
loc_824C8728:
	// lwz r11,28(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8860
	if (!cr0.eq) goto loc_824C8860;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824c8860
	if (cr0.eq) goto loc_824C8860;
loc_824C873C:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r10,r11,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r10,13184
	cr6.compare<uint32_t>(ctx.r10.u32, 13184, xer);
	// bne cr6,0x824c8844
	if (!cr6.eq) goto loc_824C8844;
	// rlwinm. r6,r11,31,28,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq 0x824c8844
	if (cr0.eq) goto loc_824C8844;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x824c8770
	if (!cr6.eq) goto loc_824C8770;
	// rlwinm r11,r11,0,31,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFE1;
	// rlwinm r11,r11,0,18,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFE3FFF;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// b 0x824c8844
	goto loc_824C8844;
loc_824C8770:
	// addi r11,r6,-1
	r11.s64 = ctx.r6.s64 + -1;
	// lwz r7,40(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// mr r5,r14
	ctx.r5.u64 = r14.u64;
	// andc r10,r6,r11
	ctx.r10.u64 = ctx.r6.u64 & ~r11.u64;
	// li r11,2
	r11.s64 = 2;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// lwz r9,0(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// subfic r10,r10,31
	xer.ca = ctx.r10.u32 <= 31;
	ctx.r10.s64 = 31 - ctx.r10.s64;
	// rlwinm r8,r9,27,24,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0xFF;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// srw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
	// clrlwi r10,r10,30
	ctx.r10.u64 = ctx.r10.u32 & 0x3;
	// ori r10,r10,228
	ctx.r10.u64 = ctx.r10.u64 | 228;
loc_824C87A4:
	// slw r4,r14,r5
	ctx.r4.u64 = ctx.r5.u8 & 0x20 ? 0 : (r14.u32 << (ctx.r5.u8 & 0x3F));
	// and. r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 & ctx.r6.u64;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq 0x824c87b8
	if (cr0.eq) goto loc_824C87B8;
	// srw r4,r8,r11
	ctx.r4.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (r11.u8 & 0x3F));
	// b 0x824c87c0
	goto loc_824C87C0;
loc_824C87B8:
	// addi r4,r11,-2
	ctx.r4.s64 = r11.s64 + -2;
	// srw r4,r10,r4
	ctx.r4.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r4.u8 & 0x3F));
loc_824C87C0:
	// li r3,3
	ctx.r3.s64 = 3;
	// clrlwi r4,r4,30
	ctx.r4.u64 = ctx.r4.u32 & 0x3;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// slw r3,r3,r11
	ctx.r3.u64 = r11.u8 & 0x20 ? 0 : (ctx.r3.u32 << (r11.u8 & 0x3F));
	// slw r4,r4,r11
	ctx.r4.u64 = r11.u8 & 0x20 ? 0 : (ctx.r4.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// andc r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r3.u64;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// or r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 | ctx.r4.u64;
	// blt cr6,0x824c87a4
	if (cr6.lt) goto loc_824C87A4;
	// lwz r31,12(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// cmplw cr6,r8,r10
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, xer);
	// beq cr6,0x824c882c
	if (cr6.eq) goto loc_824C882C;
	// rlwimi r9,r10,5,19,26
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 5) & 0x1FE0) | (ctx.r9.u64 & 0xFFFFFFFFFFFFE01F);
	// stw r9,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r9.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824c8818
	if (cr6.lt) goto loc_824C8818;
	// cmplwi cr6,r11,101
	cr6.compare<uint32_t>(r11.u32, 101, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// ble cr6,0x824c881c
	if (!cr6.gt) goto loc_824C881C;
loc_824C8818:
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824C881C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c882c
	if (cr0.eq) goto loc_824C882C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba9b8
	sub_824BA9B8(ctx, base);
loc_824C882C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c8844
	if (!cr0.eq) goto loc_824C8844;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824f0710
	sub_824F0710(ctx, base);
loc_824C8844:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8860
	if (!cr0.eq) goto loc_824C8860;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c873c
	if (!cr6.eq) goto loc_824C873C;
loc_824C8860:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c887c
	if (!cr0.eq) goto loc_824C887C;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c8728
	if (!cr6.eq) goto loc_824C8728;
loc_824C887C:
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// rlwinm. r10,r11,0,27,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c8894
	if (cr0.eq) goto loc_824C8894;
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// beq 0x824c8898
	if (cr0.eq) goto loc_824C8898;
loc_824C8894:
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824C8898:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c89c4
	if (cr0.eq) goto loc_824C89C4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b3250
	sub_824B3250(ctx, base);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c89c4
	if (!cr0.eq) goto loc_824C89C4;
	// mr. r27,r11
	r27.u64 = r11.u64;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// beq 0x824c89c4
	if (cr0.eq) goto loc_824C89C4;
loc_824C88BC:
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c88e4
	if (cr0.eq) goto loc_824C88E4;
	// lwz r11,28(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c88dc
	if (cr0.eq) goto loc_824C88DC;
	// addi r30,r27,32
	r30.s64 = r27.s64 + 32;
	// b 0x824c88e8
	goto loc_824C88E8;
loc_824C88DC:
	// addi r30,r27,24
	r30.s64 = r27.s64 + 24;
	// b 0x824c88e8
	goto loc_824C88E8;
loc_824C88E4:
	// addi r30,r27,16
	r30.s64 = r27.s64 + 16;
loc_824C88E8:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c89a0
	if (cr0.eq) goto loc_824C89A0;
	// mr r31,r20
	r31.u64 = r20.u64;
	// b 0x824c89a0
	goto loc_824C89A0;
loc_824C88FC:
	// lwz r28,8(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r11,r28,6,31,31
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c8950
	if (cr0.eq) goto loc_824C8950;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b0570
	sub_824B0570(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c8944
	if (cr0.eq) goto loc_824C8944;
	// rlwinm. r11,r28,0,27,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x1E;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c8944
	if (cr0.eq) goto loc_824C8944;
	// rlwinm. r11,r28,9,31,31
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c8944
	if (!cr0.eq) goto loc_824C8944;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b3a28
	sub_824B3A28(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c8944
	if (!cr0.eq) goto loc_824C8944;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824f0710
	sub_824F0710(ctx, base);
loc_824C8944:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,6,4
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFBFFFFFF;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_824C8950:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c8968
	if (!cr0.eq) goto loc_824C8968;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c88fc
	if (!cr6.eq) goto loc_824C88FC;
loc_824C8968:
	// addi r10,r27,32
	ctx.r10.s64 = r27.s64 + 32;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x824c89a8
	if (cr6.eq) goto loc_824C89A8;
	// addi r11,r27,24
	r11.s64 = r27.s64 + 24;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x824c8990
	if (cr6.eq) goto loc_824C8990;
	// lwz r9,28(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 28);
	// mr r30,r11
	r30.u64 = r11.u64;
	// clrlwi. r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824c8994
	if (cr0.eq) goto loc_824C8994;
loc_824C8990:
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
loc_824C8994:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c89a8
	if (!cr0.eq) goto loc_824C89A8;
loc_824C89A0:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c88fc
	if (!cr6.eq) goto loc_824C88FC;
loc_824C89A8:
	// rlwinm r11,r27,0,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c89c4
	if (!cr0.eq) goto loc_824C89C4;
	// mr r27,r11
	r27.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c88bc
	if (!cr6.eq) goto loc_824C88BC;
loc_824C89C4:
	// lwz r11,112(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 112);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,632(r29)
	PPC_STORE_U32(r29.u32 + 632, r11.u32);
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r3,672(r29)
	PPC_STORE_U32(r29.u32 + 672, ctx.r3.u32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8b0c
	if (!cr0.eq) goto loc_824C8B0C;
	// mr. r28,r11
	r28.u64 = r11.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824c8b0c
	if (cr0.eq) goto loc_824C8B0C;
loc_824C89F8:
	// lwz r11,20(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 20);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c8a20
	if (cr0.eq) goto loc_824C8A20;
	// lwz r11,28(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c8a18
	if (cr0.eq) goto loc_824C8A18;
	// addi r11,r28,32
	r11.s64 = r28.s64 + 32;
	// b 0x824c8a24
	goto loc_824C8A24;
loc_824C8A18:
	// addi r11,r28,24
	r11.s64 = r28.s64 + 24;
	// b 0x824c8a24
	goto loc_824C8A24;
loc_824C8A20:
	// addi r11,r28,16
	r11.s64 = r28.s64 + 16;
loc_824C8A24:
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r31,31
	ctx.r10.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c8a34
	if (cr0.eq) goto loc_824C8A34;
	// mr r31,r20
	r31.u64 = r20.u64;
loc_824C8A34:
	// mr r30,r11
	r30.u64 = r11.u64;
	// b 0x824c8ae8
	goto loc_824C8AE8;
loc_824C8A3C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c8a98
	if (cr0.eq) goto loc_824C8A98;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lhz r11,12(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 12);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r9,r14
	ctx.r9.u64 = r14.u64;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// rlwinm r8,r10,31,28,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0xF;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bge cr6,0x824c8a98
	if (!cr6.lt) goto loc_824C8A98;
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r3,r11
	r11.s64 = r11.s64 - ctx.r3.s64;
loc_824C8A78:
	// and. r7,r9,r8
	ctx.r7.u64 = ctx.r9.u64 & ctx.r8.u64;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x824c8a88
	if (cr0.eq) goto loc_824C8A88;
	// lwz r7,672(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// stwx r31,r10,r7
	PPC_STORE_U32(ctx.r10.u32 + ctx.r7.u32, r31.u32);
loc_824C8A88:
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// bne 0x824c8a78
	if (!cr0.eq) goto loc_824C8A78;
loc_824C8A98:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c8ab0
	if (!cr0.eq) goto loc_824C8AB0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c8a3c
	if (!cr6.eq) goto loc_824C8A3C;
loc_824C8AB0:
	// addi r10,r28,32
	ctx.r10.s64 = r28.s64 + 32;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x824c8af0
	if (cr6.eq) goto loc_824C8AF0;
	// addi r11,r28,24
	r11.s64 = r28.s64 + 24;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x824c8ad8
	if (cr6.eq) goto loc_824C8AD8;
	// lwz r9,28(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// mr r30,r11
	r30.u64 = r11.u64;
	// clrlwi. r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824c8adc
	if (cr0.eq) goto loc_824C8ADC;
loc_824C8AD8:
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
loc_824C8ADC:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c8af0
	if (!cr0.eq) goto loc_824C8AF0;
loc_824C8AE8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c8a3c
	if (!cr6.eq) goto loc_824C8A3C;
loc_824C8AF0:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8b0c
	if (!cr0.eq) goto loc_824C8B0C;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c89f8
	if (!cr6.eq) goto loc_824C89F8;
loc_824C8B0C:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// mr r15,r3
	r15.u64 = ctx.r3.u64;
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// lwz r4,672(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r15,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r15.u32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// lis r10,-32180
	ctx.r10.s64 = -2108948480;
	// addi r18,r15,4
	r18.s64 = r15.s64 + 4;
	// li r5,4
	ctx.r5.s64 = 4;
	// addi r6,r10,23112
	ctx.r6.s64 = ctx.r10.s64 + 23112;
	// addi r4,r11,-1
	ctx.r4.s64 = r11.s64 + -1;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// bl 0x823a0760
	sub_823A0760(ctx, base);
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// li r5,0
	ctx.r5.s64 = 0;
	// mulli r4,r11,40
	ctx.r4.s64 = r11.s64 * 40;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// mr r30,r14
	r30.u64 = r14.u64;
	// stw r3,640(r29)
	PPC_STORE_U32(r29.u32 + 640, ctx.r3.u32);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x824c8bd8
	if (!cr6.gt) goto loc_824C8BD8;
	// li r31,40
	r31.s64 = 40;
loc_824C8B84:
	// lwz r11,640(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 640);
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r29.u32);
	// std r20,8(r3)
	PPC_STORE_U64(ctx.r3.u32 + 8, r20.u64);
	// std r20,16(r3)
	PPC_STORE_U64(ctx.r3.u32 + 16, r20.u64);
	// std r20,24(r3)
	PPC_STORE_U64(ctx.r3.u32 + 24, r20.u64);
	// std r20,32(r3)
	PPC_STORE_U64(ctx.r3.u32 + 32, r20.u64);
	// lwz r11,744(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 744);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c8bb0
	if (!cr0.eq) goto loc_824C8BB0;
	// li r11,32
	r11.s64 = 32;
loc_824C8BB0:
	// cmplwi cr6,r11,64
	cr6.compare<uint32_t>(r11.u32, 64, xer);
	// bge cr6,0x824c8bc4
	if (!cr6.lt) goto loc_824C8BC4;
	// subfic r5,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r5.s64 = 64 - r11.s64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// bl 0x824b20b0
	sub_824B20B0(ctx, base);
loc_824C8BC4:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,40
	r31.s64 = r31.s64 + 40;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x824c8b84
	if (cr6.lt) goto loc_824C8B84;
loc_824C8BD8:
	// lwz r11,44(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 44);
	// lis r10,-1
	ctx.r10.s64 = -65536;
	// rlwinm r11,r11,0,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF0000;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c8cf8
	if (cr0.eq) goto loc_824C8CF8;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r31,r20
	r31.u64 = r20.u64;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c8c14
	if (cr0.eq) goto loc_824C8C14;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824C8C14:
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8dd0
	if (!cr0.eq) goto loc_824C8DD0;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c8dd0
	if (cr0.eq) goto loc_824C8DD0;
loc_824C8C28:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// addi r10,r10,-14336
	ctx.r10.s64 = ctx.r10.s64 + -14336;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c8c54
	if (cr0.eq) goto loc_824C8C54;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x824c8c50
	if (cr6.eq) goto loc_824C8C50;
	// mr r31,r11
	r31.u64 = r11.u64;
	// b 0x824c8c54
	goto loc_824C8C54;
loc_824C8C50:
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
loc_824C8C54:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8c6c
	if (!cr0.eq) goto loc_824C8C6C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c8c28
	if (!cr6.eq) goto loc_824C8C28;
loc_824C8C6C:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x824c8dd0
	if (cr6.eq) goto loc_824C8DD0;
	// lwz r10,532(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 532);
	// mr r11,r20
	r11.u64 = r20.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// ble cr6,0x824c8cd0
	if (!cr6.gt) goto loc_824C8CD0;
	// addi r10,r29,344
	ctx.r10.s64 = r29.s64 + 344;
loc_824C8C88:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// clrlwi r9,r9,27
	ctx.r9.u64 = ctx.r9.u32 & 0x1F;
	// cmplwi cr6,r9,14
	cr6.compare<uint32_t>(ctx.r9.u32, 14, xer);
	// beq cr6,0x824c8cb0
	if (cr6.eq) goto loc_824C8CB0;
	// lwz r9,532(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 532);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x824c8c88
	if (cr6.lt) goto loc_824C8C88;
	// b 0x824c8cd0
	goto loc_824C8CD0;
loc_824C8CB0:
	// addi r11,r11,29
	r11.s64 = r11.s64 + 29;
	// mulli r11,r11,12
	r11.s64 = r11.s64 * 12;
	// lwzx r11,r11,r29
	r11.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x824c8cd0
	if (!cr6.eq) goto loc_824C8CD0;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r31,r11
	r31.u64 = r11.u64;
loc_824C8CD0:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c7738
	sub_824C7738(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c8dd0
	if (cr6.eq) goto loc_824C8DD0;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c7738
	sub_824C7738(ctx, base);
	// b 0x824c8dd0
	goto loc_824C8DD0;
loc_824C8CF8:
	// mr r30,r20
	r30.u64 = r20.u64;
	// mr r28,r14
	r28.u64 = r14.u64;
loc_824C8D00:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c8d10
	if (cr0.eq) goto loc_824C8D10;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824C8D10:
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8dc4
	if (!cr0.eq) goto loc_824C8DC4;
	// mr. r31,r11
	r31.u64 = r11.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824c8dc4
	if (cr0.eq) goto loc_824C8DC4;
loc_824C8D24:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,14336
	cr6.compare<uint32_t>(r11.u32, 14336, xer);
	// bne cr6,0x824c8da8
	if (!cr6.eq) goto loc_824C8DA8;
	// lwz r9,532(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 532);
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824c8d74
	if (cr0.eq) goto loc_824C8D74;
	// addi r11,r29,348
	r11.s64 = r29.s64 + 348;
loc_824C8D48:
	// lwz r8,-4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// rlwinm. r8,r8,0,18,18
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x2000;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824c8d60
	if (cr0.eq) goto loc_824C8D60;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r31,r8
	cr6.compare<uint32_t>(r31.u32, ctx.r8.u32, xer);
	// beq cr6,0x824c8d74
	if (cr6.eq) goto loc_824C8D74;
loc_824C8D60:
	// lwz r8,532(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 532);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// blt cr6,0x824c8d48
	if (cr6.lt) goto loc_824C8D48;
loc_824C8D74:
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// bne cr6,0x824c8d84
	if (!cr6.eq) goto loc_824C8D84;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x824c8d94
	if (cr6.eq) goto loc_824C8D94;
loc_824C8D84:
	// cmpwi cr6,r28,2
	cr6.compare<int32_t>(r28.s32, 2, xer);
	// bne cr6,0x824c8da8
	if (!cr6.eq) goto loc_824C8DA8;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bge cr6,0x824c8da8
	if (!cr6.lt) goto loc_824C8DA8;
loc_824C8D94:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c7738
	sub_824C7738(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
loc_824C8DA8:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c8dc4
	if (!cr0.eq) goto loc_824C8DC4;
	// mr r31,r11
	r31.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c8d24
	if (!cr6.eq) goto loc_824C8D24;
loc_824C8DC4:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// cmpwi cr6,r28,2
	cr6.compare<int32_t>(r28.s32, 2, xer);
	// ble cr6,0x824c8d00
	if (!cr6.gt) goto loc_824C8D00;
loc_824C8DD0:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// mr r26,r14
	r26.u64 = r14.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x824c8f90
	if (!cr6.gt) goto loc_824C8F90;
	// mr r27,r18
	r27.u64 = r18.u64;
loc_824C8DE4:
	// lwz r28,0(r27)
	r28.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplwi r28,0
	cr0.compare<uint32_t>(r28.u32, 0, xer);
	// beq 0x824c8f90
	if (cr0.eq) goto loc_824C8F90;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmpwi cr6,r11,19
	cr6.compare<int32_t>(r11.s32, 19, xer);
	// beq cr6,0x824c8e08
	if (cr6.eq) goto loc_824C8E08;
	// cmpwi cr6,r11,29
	cr6.compare<int32_t>(r11.s32, 29, xer);
	// bne cr6,0x824c8ea4
	if (!cr6.eq) goto loc_824C8EA4;
loc_824C8E08:
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// oris r11,r11,16
	r11.u64 = r11.u64 | 1048576;
	// rlwinm r31,r11,16,29,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0x7;
	// stw r11,12(r28)
	PPC_STORE_U32(r28.u32 + 12, r11.u32);
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// add r11,r3,r31
	r11.u64 = ctx.r3.u64 + r31.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bge cr6,0x824c8ea4
	if (!cr6.lt) goto loc_824C8EA4;
	// mulli r7,r3,40
	ctx.r7.s64 = ctx.r3.s64 * 40;
	// subf r5,r3,r11
	ctx.r5.s64 = r11.s64 - ctx.r3.s64;
loc_824C8E38:
	// mr r6,r20
	ctx.r6.u64 = r20.u64;
	// mr r8,r20
	ctx.r8.u64 = r20.u64;
loc_824C8E40:
	// cmplw cr6,r6,r4
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r4.u32, xer);
	// beq cr6,0x824c8e84
	if (cr6.eq) goto loc_824C8E84;
	// lis r10,4369
	ctx.r10.s64 = 286326784;
	// lwz r11,640(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 640);
	// ori r10,r10,4369
	ctx.r10.u64 = ctx.r10.u64 | 4369;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldimi r10,r10,32,0
	ctx.r10.u64 = (__builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r10.u64 & 0xFFFFFFFF);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// li r10,4
	ctx.r10.s64 = 4;
	// sld r9,r9,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
loc_824C8E6C:
	// ld r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// or r3,r3,r9
	ctx.r3.u64 = ctx.r3.u64 | ctx.r9.u64;
	// std r3,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r3.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x824c8e6c
	if (!cr0.eq) goto loc_824C8E6C;
loc_824C8E84:
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// cmplwi cr6,r6,4
	cr6.compare<uint32_t>(ctx.r6.u32, 4, xer);
	// blt cr6,0x824c8e40
	if (cr6.lt) goto loc_824C8E40;
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r7,r7,40
	ctx.r7.s64 = ctx.r7.s64 + 40;
	// bne 0x824c8e38
	if (!cr0.eq) goto loc_824C8E38;
loc_824C8EA4:
	// lwz r31,4(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 4);
loc_824C8EA8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c8f7c
	if (cr6.eq) goto loc_824C8F7C;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r10,r20,r11
	ctx.r10.s64 = r11.s64 - r20.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c8f74
	if (cr0.eq) goto loc_824C8F74;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r10,r10,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c8f74
	if (cr0.eq) goto loc_824C8F74;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,25,25,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// addi r10,r10,-73
	ctx.r10.s64 = ctx.r10.s64 + -73;
	// cmplwi cr6,r10,5
	cr6.compare<uint32_t>(ctx.r10.u32, 5, xer);
	// bgt cr6,0x824c8f74
	if (cr6.gt) goto loc_824C8F74;
	// lwz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// lwz r30,40(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x824c8f74
	if (!cr6.eq) goto loc_824C8F74;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lis r10,4369
	ctx.r10.s64 = 286326784;
	// lwz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r9,4
	ctx.r9.s64 = 4;
	// ori r10,r10,4369
	ctx.r10.u64 = ctx.r10.u64 | 4369;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rldicl r7,r7,59,62
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u64, 59) & 0x3;
	// rldimi r10,r10,32,0
	ctx.r10.u64 = (__builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r10.u64 & 0xFFFFFFFF);
	// rlwinm r11,r11,27,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// lwz r10,640(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 640);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// mulli r11,r11,40
	r11.s64 = r11.s64 * 40;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// sld r8,r8,r7
	ctx.r8.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r7.u8 & 0x7F));
loc_824C8F50:
	// ld r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// or r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 | ctx.r8.u64;
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x824c8f50
	if (!cr0.eq) goto loc_824C8F50;
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// oris r11,r11,16
	r11.u64 = r11.u64 | 1048576;
	// stw r11,12(r28)
	PPC_STORE_U32(r28.u32 + 12, r11.u32);
loc_824C8F74:
	// lwz r31,8(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// b 0x824c8ea8
	goto loc_824C8EA8;
loc_824C8F7C:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// blt cr6,0x824c8de4
	if (cr6.lt) goto loc_824C8DE4;
loc_824C8F90:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r10,r11,63
	ctx.r10.s64 = r11.s64 + 63;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r16,r10,29,3,28
	r16.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFF8;
	// rlwinm r11,r16,29,3,31
	r11.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 29) & 0x1FFFFFFF;
	// stw r16,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r16.u32);
	// stw r11,636(r29)
	PPC_STORE_U32(r29.u32 + 636, r11.u32);
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// mr r23,r14
	r23.u64 = r14.u64;
	// stw r3,644(r29)
	PPC_STORE_U32(r29.u32 + 644, ctx.r3.u32);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x824c936c
	if (!cr6.gt) goto loc_824C936C;
	// mr r22,r18
	r22.u64 = r18.u64;
loc_824C8FD0:
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824c936c
	if (cr0.eq) goto loc_824C936C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9358
	if (cr0.eq) goto loc_824C9358;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c9358
	if (!cr0.eq) goto loc_824C9358;
	// li r5,37
	ctx.r5.s64 = 37;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824a9858
	sub_824A9858(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// ori r11,r25,1
	r11.u64 = r25.u64 | 1;
	// addi r24,r25,4
	r24.s64 = r25.s64 + 4;
	// ori r10,r24,1
	ctx.r10.u64 = r24.u64 | 1;
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// stw r10,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r10.u32);
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824c9048
	if (!cr0.eq) goto loc_824C9048;
	// rlwinm r11,r10,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r3,r11,-4
	xer.ca = r11.u32 > 3;
	ctx.r3.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824c9048
	if (cr0.eq) goto loc_824C9048;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x824c9054
	if (!cr6.gt) goto loc_824C9054;
loc_824C9048:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
loc_824C9054:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stwx r31,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r31.u32);
loc_824C906C:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c907c
	if (cr0.eq) goto loc_824C907C;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824C907C:
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c9348
	if (!cr0.eq) goto loc_824C9348;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c9098
	if (cr6.eq) goto loc_824C9098;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// b 0x824c90a4
	goto loc_824C90A4;
loc_824C9098:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r4,r11,-4
	ctx.r4.s64 = r11.s64 + -4;
loc_824C90A4:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// addic. r10,r11,-1
	xer.ca = r11.u32 > 0;
	ctx.r10.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r26,r11,r4
	r26.u64 = PPC_LOAD_U32(r11.u32 + ctx.r4.u32);
	// stw r10,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r10.u32);
	// bne 0x824c90f8
	if (!cr0.eq) goto loc_824C90F8;
	// rlwinm r11,r4,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82496e38
	sub_82496E38(ctx, base);
loc_824C90F8:
	// lwz r30,8(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm r11,r30,0,18,24
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c92e8
	if (cr0.eq) goto loc_824C92E8;
	// lwz r31,0(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x824c906c
	if (!cr6.eq) goto loc_824C906C;
loc_824C9120:
	// rlwinm. r11,r30,0,27,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x1E;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c906c
	if (cr0.eq) goto loc_824C906C;
	// lwz r27,12(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9248
	if (cr0.eq) goto loc_824C9248;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// beq cr6,0x824c9248
	if (cr6.eq) goto loc_824C9248;
	// lwz r11,640(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 640);
	// mulli r10,r30,40
	ctx.r10.s64 = r30.s64 * 40;
	// mulli r9,r31,40
	ctx.r9.s64 = r31.s64 * 40;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// mr r8,r20
	ctx.r8.u64 = r20.u64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C91A0:
	// ld r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// ld r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// nor r9,r9,r7
	ctx.r9.u64 = ~(ctx.r9.u64 | ctx.r7.u64);
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824c9340
	if (!cr6.eq) goto loc_824C9340;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmpwi cr6,r8,4
	cr6.compare<int32_t>(ctx.r8.s32, 4, xer);
	// blt cr6,0x824c91a0
	if (cr6.lt) goto loc_824C91A0;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824C91CC:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// beq 0x824cca20
	if (cr0.eq) goto loc_824CCA20;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x824c68c0
	sub_824C68C0(ctx, base);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c68c0
	sub_824C68C0(ctx, base);
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c9224
	if (!cr0.eq) goto loc_824C9224;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r3,r11,-4
	xer.ca = r11.u32 > 3;
	ctx.r3.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824c9224
	if (cr0.eq) goto loc_824C9224;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x824c9230
	if (!cr6.gt) goto loc_824C9230;
loc_824C9224:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
loc_824C9230:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stwx r27,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r27.u32);
loc_824C9248:
	// lwz r30,4(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 4);
loc_824C924C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824c906c
	if (cr6.eq) goto loc_824C906C;
	// lwz r31,16(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// subf r11,r20,r31
	r11.s64 = r31.s64 - r20.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c92e0
	if (cr0.eq) goto loc_824C92E0;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c92e0
	if (cr0.eq) goto loc_824C92E0;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c92bc
	if (!cr0.eq) goto loc_824C92BC;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r3,r11,-4
	xer.ca = r11.u32 > 3;
	ctx.r3.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824c92bc
	if (cr0.eq) goto loc_824C92BC;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x824c92c8
	if (!cr6.gt) goto loc_824C92C8;
loc_824C92BC:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824b4e50
	sub_824B4E50(ctx, base);
loc_824C92C8:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stwx r31,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r31.u32);
loc_824C92E0:
	// lwz r30,8(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// b 0x824c924c
	goto loc_824C924C;
loc_824C92E8:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c906c
	if (cr0.eq) goto loc_824C906C;
	// rlwinm. r11,r30,0,0,0
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c906c
	if (!cr0.eq) goto loc_824C906C;
	// rlwinm. r11,r30,0,10,12
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x380000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9310
	if (cr0.eq) goto loc_824C9310;
	// lwz r31,40(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// b 0x824c9120
	goto loc_824C9120;
loc_824C9310:
	// lwz r31,0(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 0);
loc_824C9314:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824cca14
	if (cr6.eq) goto loc_824CCA14;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824c9120
	if (!cr0.eq) goto loc_824C9120;
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x824c9314
	goto loc_824C9314;
loc_824C9340:
	// mr r11,r14
	r11.u64 = r14.u64;
	// b 0x824c91cc
	goto loc_824C91CC;
loc_824C9348:
	// addi r11,r29,924
	r11.s64 = r29.s64 + 924;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r10.u32);
	// stw r25,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r25.u32);
loc_824C9358:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r22,r22,4
	r22.s64 = r22.s64 + 4;
	// cmplw cr6,r23,r11
	cr6.compare<uint32_t>(r23.u32, r11.u32, xer);
	// blt cr6,0x824c8fd0
	if (cr6.lt) goto loc_824C8FD0;
loc_824C936C:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// mr r26,r14
	r26.u64 = r14.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x824c94b4
	if (!cr6.gt) goto loc_824C94B4;
	// mr r27,r18
	r27.u64 = r18.u64;
loc_824C9380:
	// lwz r3,0(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824c94b4
	if (cr0.eq) goto loc_824C94B4;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c94a0
	if (cr0.eq) goto loc_824C94A0;
	// lwz r30,0(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x824c93c0
	if (cr0.eq) goto loc_824C93C0;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// bne cr6,0x824c93c4
	if (!cr6.eq) goto loc_824C93C4;
loc_824C93C0:
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824C93C4:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c94a0
	if (!cr0.eq) goto loc_824C94A0;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r10,r11,11,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 11) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c94a0
	if (!cr0.eq) goto loc_824C94A0;
	// rlwinm. r11,r11,12,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c94a0
	if (!cr0.eq) goto loc_824C94A0;
	// lwz r31,12(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// beq cr6,0x824c94a0
	if (cr6.eq) goto loc_824C94A0;
	// lwz r11,640(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 640);
	// mulli r10,r30,40
	ctx.r10.s64 = r30.s64 * 40;
	// mulli r9,r31,40
	ctx.r9.s64 = r31.s64 * 40;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// mr r8,r20
	ctx.r8.u64 = r20.u64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C944C:
	// ld r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// ld r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// nor r9,r9,r7
	ctx.r9.u64 = ~(ctx.r9.u64 | ctx.r7.u64);
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824c962c
	if (!cr6.eq) goto loc_824C962C;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmpwi cr6,r8,4
	cr6.compare<int32_t>(ctx.r8.s32, 4, xer);
	// blt cr6,0x824c944c
	if (cr6.lt) goto loc_824C944C;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824C9478:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c94a0
	if (cr0.eq) goto loc_824C94A0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c68c0
	sub_824C68C0(ctx, base);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c68c0
	sub_824C68C0(ctx, base);
loc_824C94A0:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// blt cr6,0x824c9380
	if (cr6.lt) goto loc_824C9380;
loc_824C94B4:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// mr r17,r14
	r17.u64 = r14.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x824c97e4
	if (!cr6.gt) goto loc_824C97E4;
loc_824C94C4:
	// lwz r19,0(r18)
	r19.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// cmplwi r19,0
	cr0.compare<uint32_t>(r19.u32, 0, xer);
	// beq 0x824c97e4
	if (cr0.eq) goto loc_824C97E4;
	// lwz r11,8(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c97cc
	if (cr0.eq) goto loc_824C97CC;
	// lwz r11,0(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c9504
	if (cr0.eq) goto loc_824C9504;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// bne cr6,0x824c9508
	if (!cr6.eq) goto loc_824C9508;
loc_824C9504:
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824C9508:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c97cc
	if (cr0.eq) goto loc_824C97CC;
	// lwz r11,24(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 24);
	// mr r22,r20
	r22.u64 = r20.u64;
	// mr r25,r20
	r25.u64 = r20.u64;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c97cc
	if (!cr0.eq) goto loc_824C97CC;
	// mr. r21,r11
	r21.u64 = r11.u64;
	cr0.compare<int32_t>(r21.s32, 0, xer);
	// beq 0x824c97cc
	if (cr0.eq) goto loc_824C97CC;
loc_824C9530:
	// cmplw cr6,r19,r21
	cr6.compare<uint32_t>(r19.u32, r21.u32, xer);
	// beq cr6,0x824c9794
	if (cr6.eq) goto loc_824C9794;
	// lwz r11,8(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9794
	if (cr0.eq) goto loc_824C9794;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c956c
	if (cr0.eq) goto loc_824C956C;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// bne cr6,0x824c9570
	if (!cr6.eq) goto loc_824C9570;
loc_824C956C:
	// li r11,0
	r11.s64 = 0;
loc_824C9570:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9794
	if (cr0.eq) goto loc_824C9794;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// cmplw cr6,r23,r24
	cr6.compare<uint32_t>(r23.u32, r24.u32, xer);
	// beq cr6,0x824c9794
	if (cr6.eq) goto loc_824C9794;
	// lwz r11,640(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 640);
	// mulli r10,r23,40
	ctx.r10.s64 = r23.s64 * 40;
	// mulli r9,r24,40
	ctx.r9.s64 = r24.s64 * 40;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824C95D4:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// ld r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// nor r9,r9,r7
	ctx.r9.u64 = ~(ctx.r9.u64 | ctx.r7.u64);
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824c9634
	if (!cr6.eq) goto loc_824C9634;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmpwi cr6,r8,4
	cr6.compare<int32_t>(ctx.r8.s32, 4, xer);
	// blt cr6,0x824c95d4
	if (cr6.lt) goto loc_824C95D4;
	// li r11,0
	r11.s64 = 0;
loc_824C9600:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9794
	if (cr0.eq) goto loc_824C9794;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x824c9640
	if (!cr6.eq) goto loc_824C9640;
	// lwz r11,0(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
loc_824C9618:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c963c
	if (cr6.eq) goto loc_824C963C;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// b 0x824c9618
	goto loc_824C9618;
loc_824C962C:
	// mr r11,r14
	r11.u64 = r14.u64;
	// b 0x824c9478
	goto loc_824C9478;
loc_824C9634:
	// mr r11,r14
	r11.u64 = r14.u64;
	// b 0x824c9600
	goto loc_824C9600;
loc_824C963C:
	// mr r25,r10
	r25.u64 = ctx.r10.u64;
loc_824C9640:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
loc_824C9648:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824c965c
	if (cr6.eq) goto loc_824C965C;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// b 0x824c9648
	goto loc_824C9648;
loc_824C965C:
	// cmplw cr6,r10,r25
	cr6.compare<uint32_t>(ctx.r10.u32, r25.u32, xer);
	// bne cr6,0x824c9794
	if (!cr6.eq) goto loc_824C9794;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x824c96d8
	if (!cr6.eq) goto loc_824C96D8;
	// li r5,0
	ctx.r5.s64 = 0;
	// rlwinm r4,r25,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// lwz r31,0(r19)
	r31.u64 = PPC_LOAD_U32(r19.u32 + 0);
	// mr r30,r22
	r30.u64 = r22.u64;
loc_824C9688:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c96c0
	if (cr6.eq) goto loc_824C96C0;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// rlwinm r28,r11,27,30,31
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r28
	ctx.r4.u64 = r11.u64 + r28.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// b 0x824c9688
	goto loc_824C9688;
loc_824C96C0:
	// lis r11,-32180
	r11.s64 = -2108948480;
	// li r5,4
	ctx.r5.s64 = 4;
	// addi r6,r11,23096
	ctx.r6.s64 = r11.s64 + 23096;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x823a0760
	sub_823A0760(ctx, base);
loc_824C96D8:
	// rlwinm r27,r25,2,0,29
	r27.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,0(r21)
	r31.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// add r30,r22,r27
	r30.u64 = r22.u64 + r27.u64;
	// mr r28,r30
	r28.u64 = r30.u64;
loc_824C96E8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c9720
	if (cr6.eq) goto loc_824C9720;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// rlwinm r26,r11,27,30,31
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r26
	ctx.r4.u64 = r11.u64 + r26.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// b 0x824c96e8
	goto loc_824C96E8;
loc_824C9720:
	// lis r11,-32180
	r11.s64 = -2108948480;
	// li r5,4
	ctx.r5.s64 = 4;
	// addi r6,r11,23096
	ctx.r6.s64 = r11.s64 + 23096;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x823a0760
	sub_823A0760(ctx, base);
	// mr r11,r22
	r11.u64 = r22.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi r27,0
	cr0.compare<uint32_t>(r27.u32, 0, xer);
	// beq 0x824c976c
	if (cr0.eq) goto loc_824C976C;
	// add r9,r11,r27
	ctx.r9.u64 = r11.u64 + r27.u64;
loc_824C974C:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// subf. r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c976c
	if (!cr0.eq) goto loc_824C976C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bne cr6,0x824c974c
	if (!cr6.eq) goto loc_824C974C;
loc_824C976C:
	// cmpwi r10,0
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c9794
	if (!cr0.eq) goto loc_824C9794;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c68c0
	sub_824C68C0(ctx, base);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c68c0
	sub_824C68C0(ctx, base);
loc_824C9794:
	// rlwinm r11,r21,0,0,30
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c97b0
	if (!cr0.eq) goto loc_824C97B0;
	// mr r21,r11
	r21.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c9530
	if (!cr6.eq) goto loc_824C9530;
loc_824C97B0:
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x824c97cc
	if (cr6.eq) goto loc_824C97CC;
	// li r6,0
	ctx.r6.s64 = 0;
	// rlwinm r5,r25,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b1f78
	sub_824B1F78(ctx, base);
loc_824C97CC:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// addi r17,r17,1
	r17.s64 = r17.s64 + 1;
	// addi r18,r18,4
	r18.s64 = r18.s64 + 4;
	// cmplw cr6,r17,r11
	cr6.compare<uint32_t>(r17.u32, r11.u32, xer);
	// li r20,0
	r20.s64 = 0;
	// blt cr6,0x824c94c4
	if (cr6.lt) goto loc_824C94C4;
loc_824C97E4:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r18,r20
	r18.u64 = r20.u64;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c99dc
	if (!cr0.eq) goto loc_824C99DC;
	// mr. r24,r11
	r24.u64 = r11.u64;
	cr0.compare<int32_t>(r24.s32, 0, xer);
	// beq 0x824c99dc
	if (cr0.eq) goto loc_824C99DC;
loc_824C97FC:
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// stw r3,88(r24)
	PPC_STORE_U32(r24.u32 + 88, ctx.r3.u32);
	// lwz r28,20(r24)
	r28.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// clrlwi. r11,r28,31
	r11.u64 = r28.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c98b4
	if (!cr0.eq) goto loc_824C98B4;
	// cmplwi r28,0
	cr0.compare<uint32_t>(r28.u32, 0, xer);
	// beq 0x824c98b4
	if (cr0.eq) goto loc_824C98B4;
loc_824C9840:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c989c
	if (cr0.eq) goto loc_824C989C;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lhz r11,12(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 12);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
	// cmplw cr6,r3,r30
	cr6.compare<uint32_t>(ctx.r3.u32, r30.u32, xer);
	// bge cr6,0x824c989c
	if (!cr6.lt) goto loc_824C989C;
loc_824C986C:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// clrldi r10,r3,58
	ctx.r10.u64 = ctx.r3.u64 & 0x3F;
	// rlwinm r11,r3,29,3,28
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// ldx r9,r11,r23
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + r23.u32);
	// sld r10,r14,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r14.u64 << (ctx.r10.u8 & 0x7F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stdx r10,r11,r23
	PPC_STORE_U64(r11.u32 + r23.u32, ctx.r10.u64);
	// blt cr6,0x824c986c
	if (cr6.lt) goto loc_824C986C;
loc_824C989C:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r28,36(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r28,31
	r11.u64 = r28.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c98b4
	if (!cr0.eq) goto loc_824C98B4;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x824c9840
	if (!cr6.eq) goto loc_824C9840;
loc_824C98B4:
	// lwz r11,28(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r11,r24,32
	r11.s64 = r24.s64 + 32;
	// bne 0x824c98c8
	if (!cr0.eq) goto loc_824C98C8;
	// addi r11,r24,24
	r11.s64 = r24.s64 + 24;
loc_824C98C8:
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r31,31
	ctx.r10.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824c98d8
	if (cr0.eq) goto loc_824C98D8;
	// mr r31,r20
	r31.u64 = r20.u64;
loc_824C98D8:
	// mr r25,r11
	r25.u64 = r11.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c9994
	if (cr6.eq) goto loc_824C9994;
loc_824C98E4:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9958
	if (cr0.eq) goto loc_824C9958;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lhz r11,12(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 12);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r27,r14
	r27.u64 = r14.u64;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// rlwinm r26,r10,31,28,31
	r26.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0xF;
	// add r28,r11,r3
	r28.u64 = r11.u64 + ctx.r3.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplw cr6,r3,r28
	cr6.compare<uint32_t>(ctx.r3.u32, r28.u32, xer);
	// bge cr6,0x824c9958
	if (!cr6.lt) goto loc_824C9958;
loc_824C991C:
	// and. r11,r27,r26
	r11.u64 = r27.u64 & r26.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9948
	if (cr0.eq) goto loc_824C9948;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// clrldi r10,r3,58
	ctx.r10.u64 = ctx.r3.u64 & 0x3F;
	// rlwinm r11,r3,29,3,28
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// ldx r9,r11,r23
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + r23.u32);
	// sld r10,r14,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r14.u64 << (ctx.r10.u8 & 0x7F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stdx r10,r11,r23
	PPC_STORE_U64(r11.u32 + r23.u32, ctx.r10.u64);
loc_824C9948:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// rlwinm r27,r27,1,0,30
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// blt cr6,0x824c991c
	if (cr6.lt) goto loc_824C991C;
loc_824C9958:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c9970
	if (!cr0.eq) goto loc_824C9970;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824c98e4
	if (!cr6.eq) goto loc_824C98E4;
loc_824C9970:
	// addi r11,r24,32
	r11.s64 = r24.s64 + 32;
	// cmplw cr6,r25,r11
	cr6.compare<uint32_t>(r25.u32, r11.u32, xer);
	// beq cr6,0x824c9994
	if (cr6.eq) goto loc_824C9994;
	// mr r25,r11
	r25.u64 = r11.u64;
	// lwz r31,4(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c9994
	if (!cr0.eq) goto loc_824C9994;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x824c98e4
	if (!cr0.eq) goto loc_824C98E4;
loc_824C9994:
	// stw r23,84(r24)
	PPC_STORE_U32(r24.u32 + 84, r23.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// rlwinm r11,r24,0,0,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,48(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 48);
	// clrlwi r9,r18,24
	ctx.r9.u64 = r18.u32 & 0xFF;
	// stw r3,80(r24)
	PPC_STORE_U32(r24.u32 + 80, ctx.r3.u32);
	// rlwinm r10,r10,13,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 13) & 0x1;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// or r18,r10,r9
	r18.u64 = ctx.r10.u64 | ctx.r9.u64;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c99dc
	if (!cr0.eq) goto loc_824C99DC;
	// mr r24,r11
	r24.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c97fc
	if (!cr6.eq) goto loc_824C97FC;
loc_824C99DC:
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
loc_824C99F4:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r19,r20
	r19.u64 = r20.u64;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c9eb4
	if (!cr0.eq) goto loc_824C9EB4;
	// mr. r21,r11
	r21.u64 = r11.u64;
	cr0.compare<int32_t>(r21.s32, 0, xer);
	// beq 0x824c9eb4
	if (cr0.eq) goto loc_824C9EB4;
loc_824C9A0C:
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,48(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 48);
	// rlwinm. r11,r11,12,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c9c8c
	if (!cr0.eq) goto loc_824C9C8C;
	// lwz r20,12(r21)
	r20.u64 = PPC_LOAD_U32(r21.u32 + 12);
loc_824C9A2C:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x824c9db8
	if (cr6.eq) goto loc_824C9DB8;
	// lwz r10,636(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// mr r11,r23
	r11.u64 = r23.u64;
	// lwz r22,0(r20)
	r22.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// lwz r9,80(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 80);
	// cmplw cr6,r23,r10
	cr6.compare<uint32_t>(r23.u32, ctx.r10.u32, xer);
	// bge cr6,0x824c9a74
	if (!cr6.lt) goto loc_824C9A74;
loc_824C9A54:
	// ld r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// ld r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c9a54
	if (cr6.lt) goto loc_824C9A54;
loc_824C9A74:
	// lwz r11,20(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c9c84
	if (!cr0.eq) goto loc_824C9C84;
	// mr. r24,r11
	r24.u64 = r11.u64;
	cr0.compare<int32_t>(r24.s32, 0, xer);
	// beq 0x824c9c84
	if (cr0.eq) goto loc_824C9C84;
loc_824C9A88:
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9c68
	if (cr0.eq) goto loc_824C9C68;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// mr. r25,r3
	r25.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r25.s32, 0, xer);
	// beq 0x824c9c68
	if (cr0.eq) goto loc_824C9C68;
	// lwz r26,12(r25)
	r26.u64 = PPC_LOAD_U32(r25.u32 + 12);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// clrldi r10,r3,58
	ctx.r10.u64 = ctx.r3.u64 & 0x3F;
	// rlwinm r11,r3,29,3,28
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// ldx r9,r11,r23
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + r23.u32);
	// sld r10,r14,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r14.u64 << (ctx.r10.u8 & 0x7F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stdx r10,r11,r23
	PPC_STORE_U64(r11.u32 + r23.u32, ctx.r10.u64);
	// lwz r11,48(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 48);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9c68
	if (cr0.eq) goto loc_824C9C68;
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r28,8(r22)
	r28.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
loc_824C9B24:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824c9bac
	if (cr6.eq) goto loc_824C9BAC;
	// lwz r31,4(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r30,r11,27,30,31
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r30
	ctx.r4.u64 = r11.u64 + r30.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// clrldi r10,r3,58
	ctx.r10.u64 = ctx.r3.u64 & 0x3F;
	// rlwinm r11,r3,29,3,28
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// ldx r9,r11,r27
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + r27.u32);
	// sld r10,r14,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r14.u64 << (ctx.r10.u8 & 0x7F));
	// and r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 & ctx.r10.u64;
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// mr r9,r14
	ctx.r9.u64 = r14.u64;
	// bne cr6,0x824c9b90
	if (!cr6.eq) goto loc_824C9B90;
	// li r9,0
	ctx.r9.s64 = 0;
loc_824C9B90:
	// clrlwi. r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824c9bb0
	if (!cr0.eq) goto loc_824C9BB0;
	// ldx r9,r11,r27
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + r27.u32);
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// stdx r10,r11,r27
	PPC_STORE_U64(r11.u32 + r27.u32, ctx.r10.u64);
	// lwz r28,12(r28)
	r28.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// b 0x824c9b24
	goto loc_824C9B24;
loc_824C9BAC:
	// li r31,0
	r31.s64 = 0;
loc_824C9BB0:
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b1f78
	sub_824B1F78(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c9c68
	if (cr6.eq) goto loc_824C9C68;
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
loc_824C9BD4:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824c9c68
	if (cr6.eq) goto loc_824C9C68;
	// cmplw cr6,r31,r25
	cr6.compare<uint32_t>(r31.u32, r25.u32, xer);
	// beq cr6,0x824c9c60
	if (cr6.eq) goto loc_824C9C60;
	// lwz r11,24(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 24);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r14,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r14.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824c9c60
	if (cr0.eq) goto loc_824C9C60;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r30,r11,27,30,31
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r30
	ctx.r4.u64 = r11.u64 + r30.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// clrldi r10,r3,58
	ctx.r10.u64 = ctx.r3.u64 & 0x3F;
	// rlwinm r11,r3,29,3,28
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// ldx r9,r11,r23
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + r23.u32);
	// sld r10,r14,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r14.u64 << (ctx.r10.u8 & 0x7F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stdx r10,r11,r23
	PPC_STORE_U64(r11.u32 + r23.u32, ctx.r10.u64);
loc_824C9C60:
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x824c9bd4
	goto loc_824C9BD4;
loc_824C9C68:
	// rlwinm r11,r24,0,0,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c9c84
	if (!cr0.eq) goto loc_824C9C84;
	// mr r24,r11
	r24.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c9a88
	if (!cr6.eq) goto loc_824C9A88;
loc_824C9C84:
	// lwz r20,8(r20)
	r20.u64 = PPC_LOAD_U32(r20.u32 + 8);
	// b 0x824c9a2c
	goto loc_824C9A2C;
loc_824C9C8C:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824c5810
	sub_824C5810(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// lwz r27,8(r28)
	r27.u64 = PPC_LOAD_U32(r28.u32 + 8);
loc_824C9C9C:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x824c9db8
	if (cr6.eq) goto loc_824C9DB8;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
loc_824C9CAC:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c9ccc
	if (cr6.eq) goto loc_824C9CCC;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r9,r9,11,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 11) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824c9cd0
	if (!cr0.eq) goto loc_824C9CD0;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// b 0x824c9cac
	goto loc_824C9CAC;
loc_824C9CCC:
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824C9CD0:
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c9db0
	if (!cr0.eq) goto loc_824C9DB0;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824c9db0
	if (cr0.eq) goto loc_824C9DB0;
loc_824C9CE4:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9d94
	if (cr0.eq) goto loc_824C9D94;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// mr. r11,r3
	r11.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824c9d94
	if (cr0.eq) goto loc_824C9D94;
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,48(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 48);
	// clrlwi r10,r10,13
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFF;
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// clrlwi r8,r10,27
	ctx.r8.u64 = ctx.r10.u32 & 0x1F;
	// rlwinm r10,r10,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,40(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// slw r9,r14,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (r14.u32 << (ctx.r8.u8 & 0x3F));
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824c9d94
	if (cr0.eq) goto loc_824C9D94;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// clrldi r10,r3,58
	ctx.r10.u64 = ctx.r3.u64 & 0x3F;
	// rlwinm r11,r3,29,3,28
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// ldx r9,r11,r23
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + r23.u32);
	// sld r10,r14,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r14.u64 << (ctx.r10.u8 & 0x7F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stdx r10,r11,r23
	PPC_STORE_U64(r11.u32 + r23.u32, ctx.r10.u64);
loc_824C9D94:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c9db0
	if (!cr0.eq) goto loc_824C9DB0;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c9ce4
	if (!cr6.eq) goto loc_824C9CE4;
loc_824C9DB0:
	// lwz r27,12(r27)
	r27.u64 = PPC_LOAD_U32(r27.u32 + 12);
	// b 0x824c9c9c
	goto loc_824C9C9C;
loc_824C9DB8:
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// lwz r3,88(r21)
	ctx.r3.u64 = PPC_LOAD_U32(r21.u32 + 88);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,636(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// lwz r9,84(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 84);
	// mr r11,r23
	r11.u64 = r23.u64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// cmplw cr6,r23,r10
	cr6.compare<uint32_t>(r23.u32, ctx.r10.u32, xer);
	// bge cr6,0x824c9e04
	if (!cr6.lt) goto loc_824C9E04;
loc_824C9DE4:
	// ld r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// ld r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// andc r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 & ~ctx.r8.u64;
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c9de4
	if (cr6.lt) goto loc_824C9DE4;
loc_824C9E04:
	// lwz r10,636(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// mr r11,r23
	r11.u64 = r23.u64;
	// lwz r9,80(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 80);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// cmplw cr6,r23,r10
	cr6.compare<uint32_t>(r23.u32, ctx.r10.u32, xer);
	// bge cr6,0x824c9e40
	if (!cr6.lt) goto loc_824C9E40;
loc_824C9E20:
	// ld r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// ld r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824c9e20
	if (cr6.lt) goto loc_824C9E20;
loc_824C9E40:
	// lwz r3,80(r21)
	ctx.r3.u64 = PPC_LOAD_U32(r21.u32 + 80);
	// li r20,0
	r20.s64 = 0;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi r16,0
	cr0.compare<uint32_t>(r16.u32, 0, xer);
	// beq 0x824c9e80
	if (cr0.eq) goto loc_824C9E80;
	// add r8,r11,r16
	ctx.r8.u64 = r11.u64 + r16.u64;
loc_824C9E60:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824c9e80
	if (!cr0.eq) goto loc_824C9E80;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x824c9e60
	if (!cr6.eq) goto loc_824C9E60;
loc_824C9E80:
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824c9e98
	if (cr0.eq) goto loc_824C9E98;
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r19,r14
	r19.u64 = r14.u64;
loc_824C9E98:
	// rlwinm r11,r21,0,0,30
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c9eb4
	if (!cr0.eq) goto loc_824C9EB4;
	// mr r21,r11
	r21.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c9a0c
	if (!cr6.eq) goto loc_824C9A0C;
loc_824C9EB4:
	// clrlwi. r11,r19,24
	r11.u64 = r19.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824c99f4
	if (!cr0.eq) goto loc_824C99F4;
	// lwz r11,36(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 36);
	// rlwinm. r11,r11,0,23,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x100;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca044
	if (cr0.eq) goto loc_824CA044;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca044
	if (!cr0.eq) goto loc_824CA044;
	// mr. r27,r11
	r27.u64 = r11.u64;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// beq 0x824ca044
	if (cr0.eq) goto loc_824CA044;
loc_824C9EDC:
	// lwz r11,40(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 40);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r11,r11,13,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca028
	if (cr0.eq) goto loc_824CA028;
	// lwz r11,28(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 28);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca028
	if (!cr0.eq) goto loc_824CA028;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824ca028
	if (cr0.eq) goto loc_824CA028;
loc_824C9F00:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca00c
	if (cr0.eq) goto loc_824CA00C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca00c
	if (cr0.eq) goto loc_824CA00C;
	// rlwinm r31,r27,0,0,30
	r31.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r28,r20
	r28.u64 = r20.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca00c
	if (!cr0.eq) goto loc_824CA00C;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824ca00c
	if (cr6.eq) goto loc_824CA00C;
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 4);
loc_824C9F40:
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
loc_824C9F44:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824c9f80
	if (cr6.eq) goto loc_824C9F80;
	// lwz r11,16(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// subf r7,r20,r11
	ctx.r7.s64 = r11.s64 - r20.s64;
	// cntlzw r7,r7
	ctx.r7.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// rlwinm r7,r7,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// xori r7,r7,1
	ctx.r7.u64 = ctx.r7.u64 ^ 1;
	// cmplwi r7,0
	cr0.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq 0x824c9f74
	if (cr0.eq) goto loc_824C9F74;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x824c9f7c
	if (cr6.eq) goto loc_824C9F7C;
loc_824C9F74:
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// b 0x824c9f44
	goto loc_824C9F44;
loc_824C9F7C:
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
loc_824C9F80:
	// rlwinm r11,r9,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824c9f9c
	if (!cr0.eq) goto loc_824C9F9C;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c9f40
	if (!cr6.eq) goto loc_824C9F40;
loc_824C9F9C:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824ca00c
	if (cr6.eq) goto loc_824CA00C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x824c9ffc
	goto loc_824C9FFC;
loc_824C9FC0:
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// beq cr6,0x824ca00c
	if (cr6.eq) goto loc_824CA00C;
	// clrldi r8,r3,58
	ctx.r8.u64 = ctx.r3.u64 & 0x3F;
	// lwz r9,88(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// rlwinm r11,r3,29,3,28
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// rlwinm r7,r10,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// ldx r6,r11,r9
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + ctx.r9.u32);
	// sld r8,r14,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r14.u64 << (ctx.r8.u8 & 0x7F));
	// or r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 | ctx.r6.u64;
	// stdx r6,r11,r9
	PPC_STORE_U64(r11.u32 + ctx.r9.u32, ctx.r6.u64);
	// lwz r10,80(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	// ldx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + ctx.r10.u32);
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// stdx r9,r11,r10
	PPC_STORE_U64(r11.u32 + ctx.r10.u32, ctx.r9.u64);
	// lwz r10,4(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
loc_824C9FFC:
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824ca00c
	if (!cr0.eq) goto loc_824CA00C;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824c9fc0
	if (!cr6.eq) goto loc_824C9FC0;
loc_824CA00C:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca028
	if (!cr0.eq) goto loc_824CA028;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c9f00
	if (!cr6.eq) goto loc_824C9F00;
loc_824CA028:
	// rlwinm r11,r27,0,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca044
	if (!cr0.eq) goto loc_824CA044;
	// mr r27,r11
	r27.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824c9edc
	if (!cr6.eq) goto loc_824C9EDC;
loc_824CA044:
	// clrlwi. r11,r18,24
	r11.u64 = r18.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca2e8
	if (cr0.eq) goto loc_824CA2E8;
	// lwz r31,4(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824ca088
	if (!cr0.eq) goto loc_824CA088;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824ca088
	if (cr0.eq) goto loc_824CA088;
loc_824CA060:
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824ca088
	if (!cr0.eq) goto loc_824CA088;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824ca060
	if (!cr6.eq) goto loc_824CA060;
loc_824CA088:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca2e8
	if (!cr0.eq) goto loc_824CA2E8;
	// mr. r27,r11
	r27.u64 = r11.u64;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// beq 0x824ca2e8
	if (cr0.eq) goto loc_824CA2E8;
loc_824CA09C:
	// lwz r10,12(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 12);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_824CA0A4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824ca2cc
	if (cr6.eq) goto loc_824CA2CC;
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,48(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 48);
	// rlwinm. r9,r9,13,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 13) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824ca0c4
	if (!cr0.eq) goto loc_824CA0C4;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824ca0a4
	goto loc_824CA0A4;
loc_824CA0C4:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_824CA0C8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824ca0e8
	if (cr6.eq) goto loc_824CA0E8;
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwinm. r10,r10,11,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 11) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca0ec
	if (!cr0.eq) goto loc_824CA0EC;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824ca0c8
	goto loc_824CA0C8;
loc_824CA0E8:
	// mr r31,r20
	r31.u64 = r20.u64;
loc_824CA0EC:
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// lwz r4,80(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca19c
	if (!cr0.eq) goto loc_824CA19C;
	// mr. r30,r11
	r30.u64 = r11.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824ca19c
	if (cr0.eq) goto loc_824CA19C;
loc_824CA110:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca180
	if (cr0.eq) goto loc_824CA180;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// mr. r11,r3
	r11.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca180
	if (cr0.eq) goto loc_824CA180;
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// clrldi r10,r3,58
	ctx.r10.u64 = ctx.r3.u64 & 0x3F;
	// rlwinm r11,r3,29,3,28
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFF8;
	// ldx r9,r11,r23
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + r23.u32);
	// sld r10,r14,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r14.u64 << (ctx.r10.u8 & 0x7F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stdx r10,r11,r23
	PPC_STORE_U64(r11.u32 + r23.u32, ctx.r10.u64);
loc_824CA180:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca19c
	if (!cr0.eq) goto loc_824CA19C;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824ca110
	if (!cr6.eq) goto loc_824CA110;
loc_824CA19C:
	// lwz r11,116(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 116);
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x824ca214
	if (cr6.eq) goto loc_824CA214;
loc_824CA1B0:
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x824ca214
	if (cr6.eq) goto loc_824CA214;
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm. r9,r10,10,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824ca20c
	if (!cr0.eq) goto loc_824CA20C;
	// rlwinm. r10,r10,12,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824ca20c
	if (cr0.eq) goto loc_824CA20C;
	// lwz r10,636(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// lwz r9,88(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 88);
	// mr r11,r23
	r11.u64 = r23.u64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// cmplw cr6,r23,r10
	cr6.compare<uint32_t>(r23.u32, ctx.r10.u32, xer);
	// bge cr6,0x824ca20c
	if (!cr6.lt) goto loc_824CA20C;
loc_824CA1EC:
	// ld r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// ld r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// andc r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 & ~ctx.r7.u64;
	// std r7,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r7.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824ca1ec
	if (cr6.lt) goto loc_824CA1EC;
loc_824CA20C:
	// lwz r8,4(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x824ca1b0
	goto loc_824CA1B0;
loc_824CA214:
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
loc_824CA218:
	// lwz r9,636(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// lwz r11,80(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// b 0x824ca248
	goto loc_824CA248;
loc_824CA230:
	// ld r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// ld r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// std r7,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r7.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824CA248:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x824ca230
	if (cr6.lt) goto loc_824CA230;
	// lwz r9,636(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// lwz r11,88(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// b 0x824ca280
	goto loc_824CA280;
loc_824CA268:
	// ld r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// ld r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// std r7,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r7.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824CA280:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x824ca268
	if (cr6.lt) goto loc_824CA268;
	// lwz r11,56(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca29c
	if (cr0.eq) goto loc_824CA29C;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// b 0x824ca2c4
	goto loc_824CA2C4;
loc_824CA29C:
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// beq cr6,0x824ca2cc
	if (cr6.eq) goto loc_824CA2CC;
loc_824CA2A4:
	// lwz r11,60(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x824ca2c0
	if (!cr6.eq) goto loc_824CA2C0;
	// lwz r10,52(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 52);
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// bne cr6,0x824ca2a4
	if (!cr6.eq) goto loc_824CA2A4;
	// b 0x824ca2c4
	goto loc_824CA2C4;
loc_824CA2C0:
	// lwz r10,60(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 60);
loc_824CA2C4:
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// bne cr6,0x824ca218
	if (!cr6.eq) goto loc_824CA218;
loc_824CA2CC:
	// rlwinm r11,r27,0,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca2e8
	if (!cr0.eq) goto loc_824CA2E8;
	// mr r27,r11
	r27.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824ca09c
	if (!cr6.eq) goto loc_824CA09C;
loc_824CA2E8:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mullw r4,r11,r16
	ctx.r4.s64 = int64_t(r11.s32) * int64_t(r16.s32);
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r3,648(r29)
	PPC_STORE_U32(r29.u32 + 648, ctx.r3.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// beq cr6,0x824ca31c
	if (cr6.eq) goto loc_824CA31C;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// bl 0x824deb08
	sub_824DEB08(ctx, base);
	// b 0x824ca32c
	goto loc_824CA32C;
loc_824CA31C:
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x824c6b90
	sub_824C6B90(ctx, base);
loc_824CA32C:
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b1f78
	sub_824B1F78(ctx, base);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824ca354
	if (cr0.eq) goto loc_824CA354;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824CA354:
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca4e8
	if (!cr0.eq) goto loc_824CA4E8;
	// mr. r22,r11
	r22.u64 = r11.u64;
	cr0.compare<int32_t>(r22.s32, 0, xer);
	// beq 0x824ca4e8
	if (cr0.eq) goto loc_824CA4E8;
	// li r21,14336
	r21.s64 = 14336;
loc_824CA36C:
	// lwz r11,8(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r21,r11
	r11.s64 = r11.s64 - r21.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca4cc
	if (cr0.eq) goto loc_824CA4CC;
	// lhz r11,12(r22)
	r11.u64 = PPC_LOAD_U16(r22.u32 + 12);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// clrlwi r31,r11,29
	r31.u64 = r11.u32 & 0x7;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// add r24,r23,r31
	r24.u64 = r23.u64 + r31.u64;
	// mr r31,r23
	r31.u64 = r23.u64;
	// cmplw cr6,r23,r24
	cr6.compare<uint32_t>(r23.u32, r24.u32, xer);
	// bge cr6,0x824ca3fc
	if (!cr6.lt) goto loc_824CA3FC;
loc_824CA3A8:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// addi r30,r31,1
	r30.s64 = r31.s64 + 1;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r31,r30
	r31.u64 = r30.u64;
	// cmplw cr6,r30,r24
	cr6.compare<uint32_t>(r30.u32, r24.u32, xer);
	// bge cr6,0x824ca3f0
	if (!cr6.lt) goto loc_824CA3F0;
loc_824CA3C8:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c61a8
	sub_824C61A8(ctx, base);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmplw cr6,r31,r24
	cr6.compare<uint32_t>(r31.u32, r24.u32, xer);
	// blt cr6,0x824ca3c8
	if (cr6.lt) goto loc_824CA3C8;
loc_824CA3F0:
	// mr r31,r30
	r31.u64 = r30.u64;
	// cmplw cr6,r31,r24
	cr6.compare<uint32_t>(r31.u32, r24.u32, xer);
	// blt cr6,0x824ca3a8
	if (cr6.lt) goto loc_824CA3A8;
loc_824CA3FC:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824ca40c
	if (cr0.eq) goto loc_824CA40C;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824CA40C:
	// lwz r25,20(r11)
	r25.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r11,r25,31
	r11.u64 = r25.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824ca4cc
	if (!cr0.eq) goto loc_824CA4CC;
	// cmplwi r25,0
	cr0.compare<uint32_t>(r25.u32, 0, xer);
	// beq 0x824ca4cc
	if (cr0.eq) goto loc_824CA4CC;
loc_824CA420:
	// cmplw cr6,r25,r22
	cr6.compare<uint32_t>(r25.u32, r22.u32, xer);
	// beq cr6,0x824ca4b4
	if (cr6.eq) goto loc_824CA4B4;
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r21,r11
	r11.s64 = r11.s64 - r21.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca4b4
	if (cr0.eq) goto loc_824CA4B4;
	// lhz r11,12(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 12);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// clrlwi r31,r11,29
	r31.u64 = r11.u32 & 0x7;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r26,r23
	r26.u64 = r23.u64;
	// cmplw cr6,r23,r24
	cr6.compare<uint32_t>(r23.u32, r24.u32, xer);
	// bge cr6,0x824ca4b4
	if (!cr6.lt) goto loc_824CA4B4;
	// add r28,r30,r31
	r28.u64 = r30.u64 + r31.u64;
loc_824CA464:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r31,r30
	r31.u64 = r30.u64;
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// bge cr6,0x824ca4a8
	if (!cr6.lt) goto loc_824CA4A8;
loc_824CA480:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c61a8
	sub_824C61A8(ctx, base);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmplw cr6,r31,r28
	cr6.compare<uint32_t>(r31.u32, r28.u32, xer);
	// blt cr6,0x824ca480
	if (cr6.lt) goto loc_824CA480;
loc_824CA4A8:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// cmplw cr6,r26,r24
	cr6.compare<uint32_t>(r26.u32, r24.u32, xer);
	// blt cr6,0x824ca464
	if (cr6.lt) goto loc_824CA464;
loc_824CA4B4:
	// rlwinm r11,r25,0,0,30
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r25,36(r11)
	r25.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r25,31
	r11.u64 = r25.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824ca4cc
	if (!cr0.eq) goto loc_824CA4CC;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x824ca420
	if (!cr6.eq) goto loc_824CA420;
loc_824CA4CC:
	// rlwinm r11,r22,0,0,30
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca4e8
	if (!cr0.eq) goto loc_824CA4E8;
	// mr r22,r11
	r22.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824ca36c
	if (!cr6.eq) goto loc_824CA36C;
loc_824CA4E8:
	// mr r11,r14
	r11.u64 = r14.u64;
	// lwz r10,632(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// stw r20,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r20.u32);
	// stb r20,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r20.u8);
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// ble cr6,0x824cb4e8
	if (!cr6.gt) goto loc_824CB4E8;
loc_824CA508:
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r16,r10,r15
	r16.u64 = PPC_LOAD_U32(ctx.r10.u32 + r15.u32);
	// cmplwi r16,0
	cr0.compare<uint32_t>(r16.u32, 0, xer);
	// beq 0x824cb4e8
	if (cr0.eq) goto loc_824CB4E8;
	// lwz r10,8(r16)
	ctx.r10.u64 = PPC_LOAD_U32(r16.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// addi r10,r10,-14208
	ctx.r10.s64 = ctx.r10.s64 + -14208;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824cb4d4
	if (cr0.eq) goto loc_824CB4D4;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r15,r3
	r15.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r10,0(r16)
	ctx.r10.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824CA554:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824ca568
	if (cr6.eq) goto loc_824CA568;
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// b 0x824ca554
	goto loc_824CA554;
loc_824CA568:
	// rlwinm r31,r11,3,0,28
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r31,8
	ctx.r4.s64 = r31.s64 + 8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r20,r11
	r20.u64 = r11.u64;
	// stw r4,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r4.u32);
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lis r11,4095
	r11.s64 = 268369920;
	// mr r14,r3
	r14.u64 = ctx.r3.u64;
	// ori r17,r11,65535
	r17.u64 = r11.u64 | 65535;
	// lis r11,4095
	r11.s64 = 268369920;
	// add r22,r14,r31
	r22.u64 = r14.u64 + r31.u64;
	// ori r18,r11,65534
	r18.u64 = r11.u64 | 65534;
loc_824CA59C:
	// stw r20,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r20.u32);
	// li r21,0
	r21.s64 = 0;
	// lwz r27,0(r16)
	r27.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// mr r23,r14
	r23.u64 = r14.u64;
loc_824CA5AC:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x824ca958
	if (cr6.eq) goto loc_824CA958;
	// lwz r24,12(r27)
	r24.u64 = PPC_LOAD_U32(r27.u32 + 12);
	// stw r21,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r21.u32);
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca904
	if (cr0.eq) goto loc_824CA904;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,24(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 24);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ca650
	if (!cr0.eq) goto loc_824CA650;
	// mr. r31,r11
	r31.u64 = r11.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824ca650
	if (cr0.eq) goto loc_824CA650;
loc_824CA604:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca62c
	if (cr0.eq) goto loc_824CA62C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// cmplw cr6,r25,r3
	cr6.compare<uint32_t>(r25.u32, ctx.r3.u32, xer);
	// beq cr6,0x824ca648
	if (cr6.eq) goto loc_824CA648;
loc_824CA62C:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824ca650
	if (!cr0.eq) goto loc_824CA650;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824ca604
	if (!cr6.eq) goto loc_824CA604;
	// b 0x824ca650
	goto loc_824CA650;
loc_824CA648:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824ca914
	if (!cr6.eq) goto loc_824CA914;
loc_824CA650:
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6420
	sub_824C6420(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca700
	if (cr0.eq) goto loc_824CA700;
	// lwz r11,644(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 644);
	// rlwinm r10,r25,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,24(r16)
	ctx.r9.u64 = PPC_LOAD_U32(r16.u32 + 24);
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r4,80(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 80);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824ca694
	if (cr0.eq) goto loc_824CA694;
	// lwz r5,636(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// bl 0x824c5cf8
	sub_824C5CF8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// b 0x824ca6bc
	goto loc_824CA6BC;
loc_824CA694:
	// rlwinm r11,r25,29,3,28
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 29) & 0x1FFFFFF8;
	// clrldi r10,r25,58
	ctx.r10.u64 = r25.u64 & 0x3F;
	// ldx r9,r11,r4
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + ctx.r4.u32);
	// li r11,1
	r11.s64 = 1;
	// sld r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// bne cr6,0x824ca6b8
	if (!cr6.eq) goto loc_824CA6B8;
	// li r11,0
	r11.s64 = 0;
loc_824CA6B8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
loc_824CA6BC:
	// bne 0x824ca938
	if (!cr0.eq) goto loc_824CA938;
	// addi r8,r1,136
	ctx.r8.s64 = ctx.r1.s64 + 136;
	// addi r7,r22,4
	ctx.r7.s64 = r22.s64 + 4;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c5b88
	sub_824C5B88(ctx, base);
	// lwz r11,24(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 24);
	// lwz r10,4(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 4);
	// lwz r11,68(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 68);
	// clrlwi r11,r11,4
	r11.u64 = r11.u32 & 0xFFFFFFF;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r11,r17
	cr6.compare<uint32_t>(r11.u32, r17.u32, xer);
	// ble cr6,0x824ca6fc
	if (!cr6.gt) goto loc_824CA6FC;
	// mr r11,r17
	r11.u64 = r17.u64;
loc_824CA6FC:
	// stw r11,4(r23)
	PPC_STORE_U32(r23.u32 + 4, r11.u32);
loc_824CA700:
	// lwz r28,0(r16)
	r28.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// addi r26,r14,4
	r26.s64 = r14.s64 + 4;
loc_824CA708:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824ca904
	if (cr6.eq) goto loc_824CA904;
	// cmplw cr6,r28,r27
	cr6.compare<uint32_t>(r28.u32, r27.u32, xer);
	// beq cr6,0x824ca904
	if (cr6.eq) goto loc_824CA904;
	// lwz r30,12(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca8f8
	if (cr0.eq) goto loc_824CA8F8;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x824c6420
	sub_824C6420(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca8f8
	if (cr0.eq) goto loc_824CA8F8;
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rlwinm r10,r11,25,25,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r10,115
	cr6.compare<uint32_t>(ctx.r10.u32, 115, xer);
	// beq cr6,0x824ca784
	if (cr6.eq) goto loc_824CA784;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,14720
	cr6.compare<uint32_t>(r11.u32, 14720, xer);
	// bne cr6,0x824ca860
	if (!cr6.eq) goto loc_824CA860;
loc_824CA784:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,14720
	cr6.compare<uint32_t>(r11.u32, 14720, xer);
	// bne cr6,0x824ca80c
	if (!cr6.eq) goto loc_824CA80C;
	// lwz r11,24(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 24);
	// lwz r9,24(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r9,40(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r8,r11,27
	ctx.r8.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// slw r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824ca810
	if (cr0.eq) goto loc_824CA810;
loc_824CA7DC:
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// cmplwi cr6,r11,14720
	cr6.compare<uint32_t>(r11.u32, 14720, xer);
	// beq cr6,0x824ca7fc
	if (cr6.eq) goto loc_824CA7FC;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
loc_824CA7FC:
	// bl 0x824bb028
	sub_824BB028(ctx, base);
	// addi r20,r20,-1
	r20.s64 = r20.s64 + -1;
	// addi r22,r22,-8
	r22.s64 = r22.s64 + -8;
	// b 0x824ca59c
	goto loc_824CA59C;
loc_824CA80C:
	// li r9,1
	ctx.r9.s64 = 1;
loc_824CA810:
	// cmplwi cr6,r10,115
	cr6.compare<uint32_t>(ctx.r10.u32, 115, xer);
	// bne cr6,0x824ca864
	if (!cr6.eq) goto loc_824CA864;
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// lwz r10,24(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 24);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r10,40(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r8,r11,27
	ctx.r8.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r9,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824ca7dc
	if (!cr0.eq) goto loc_824CA7DC;
	// b 0x824ca864
	goto loc_824CA864;
loc_824CA860:
	// li r9,1
	ctx.r9.s64 = 1;
loc_824CA864:
	// clrldi r11,r31,58
	r11.u64 = r31.u64 & 0x3F;
	// lwz r10,24(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 24);
	// rlwinm r8,r31,29,3,28
	ctx.r8.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 29) & 0x1FFFFFF8;
	// lwz r10,88(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// ldx r10,r8,r10
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r8.u32 + ctx.r10.u32);
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// bne cr6,0x824ca890
	if (!cr6.eq) goto loc_824CA890;
	// li r11,0
	r11.s64 = 0;
loc_824CA890:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca8a0
	if (cr0.eq) goto loc_824CA8A0;
	// stw r18,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r18.u32);
	// b 0x824ca8f8
	goto loc_824CA8F8;
loc_824CA8A0:
	// clrldi r11,r25,58
	r11.u64 = r25.u64 & 0x3F;
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// rlwinm r8,r25,29,3,28
	ctx.r8.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 29) & 0x1FFFFFF8;
	// lwz r10,88(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// ldx r10,r8,r10
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r8.u32 + ctx.r10.u32);
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// bne cr6,0x824ca8cc
	if (!cr6.eq) goto loc_824CA8CC;
	// li r11,0
	r11.s64 = 0;
loc_824CA8CC:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ca8dc
	if (cr0.eq) goto loc_824CA8DC;
	// stw r18,4(r23)
	PPC_STORE_U32(r23.u32 + 4, r18.u32);
	// b 0x824ca8f8
	goto loc_824CA8F8;
loc_824CA8DC:
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// addi r7,r23,4
	ctx.r7.s64 = r23.s64 + 4;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c5b88
	sub_824C5B88(ctx, base);
loc_824CA8F8:
	// lwz r28,4(r28)
	r28.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// addi r26,r26,8
	r26.s64 = r26.s64 + 8;
	// b 0x824ca708
	goto loc_824CA708;
loc_824CA904:
	// lwz r27,4(r27)
	r27.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// addi r23,r23,8
	r23.s64 = r23.s64 + 8;
	// b 0x824ca5ac
	goto loc_824CA5AC;
loc_824CA914:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x824ca958
	if (cr6.eq) goto loc_824CA958;
	// addi r10,r14,4
	ctx.r10.s64 = r14.s64 + 4;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824CA924:
	// stw r17,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r17.u32);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne 0x824ca924
	if (!cr0.eq) goto loc_824CA924;
	// b 0x824ca958
	goto loc_824CA958;
loc_824CA938:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x824ca958
	if (cr6.eq) goto loc_824CA958;
	// addi r10,r14,4
	ctx.r10.s64 = r14.s64 + 4;
	// mr r11,r20
	r11.u64 = r20.u64;
loc_824CA948:
	// stw r17,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r17.u32);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne 0x824ca948
	if (!cr0.eq) goto loc_824CA948;
loc_824CA958:
	// addic. r11,r20,1
	xer.ca = r20.u32 > 4294967294;
	r11.s64 = r20.s64 + 1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r7,-1
	ctx.r7.s64 = -1;
	// mr r8,r17
	ctx.r8.u64 = r17.u64;
	// beq 0x824cca28
	if (cr0.eq) goto loc_824CCA28;
	// addi r10,r14,4
	ctx.r10.s64 = r14.s64 + 4;
loc_824CA96C:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bge cr6,0x824ca980
	if (!cr6.lt) goto loc_824CA980;
	// lwz r7,-4(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
loc_824CA980:
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne 0x824ca96c
	if (!cr0.eq) goto loc_824CA96C;
	// cmplw cr6,r8,r17
	cr6.compare<uint32_t>(ctx.r8.u32, r17.u32, xer);
	// beq cr6,0x824cca28
	if (cr6.eq) goto loc_824CCA28;
	// cmplw cr6,r7,r20
	cr6.compare<uint32_t>(ctx.r7.u32, r20.u32, xer);
	// beq cr6,0x824ca9e4
	if (cr6.eq) goto loc_824CA9E4;
	// lwz r11,0(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
loc_824CA9A4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824ca9c0
	if (cr6.eq) goto loc_824CA9C0;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// beq cr6,0x824ca9c0
	if (cr6.eq) goto loc_824CA9C0;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// b 0x824ca9a4
	goto loc_824CA9A4;
loc_824CA9C0:
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
loc_824CA9E4:
	// li r17,0
	r17.s64 = 0;
loc_824CA9E8:
	// lwz r20,0(r16)
	r20.u64 = PPC_LOAD_U32(r16.u32 + 0);
loc_824CA9EC:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x824cb334
	if (cr6.eq) goto loc_824CB334;
	// lwz r23,12(r20)
	r23.u64 = PPC_LOAD_U32(r20.u32 + 12);
	// lwz r11,8(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824caa7c
	if (cr0.eq) goto loc_824CAA7C;
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r30,r11,r31
	r30.u64 = r11.u64 + r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// clrlwi. r11,r17,24
	r11.u64 = r17.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824caa3c
	if (!cr0.eq) goto loc_824CAA3C;
	// cmplw cr6,r31,r19
	cr6.compare<uint32_t>(r31.u32, r19.u32, xer);
	// beq cr6,0x824caa7c
	if (cr6.eq) goto loc_824CAA7C;
loc_824CAA3C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824caa84
	if (!cr6.eq) goto loc_824CAA84;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6420
	sub_824C6420(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824caa84
	if (!cr0.eq) goto loc_824CAA84;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c68c0
	sub_824C68C0(ctx, base);
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c68c0
	sub_824C68C0(ctx, base);
loc_824CAA7C:
	// lwz r20,4(r20)
	r20.u64 = PPC_LOAD_U32(r20.u32 + 4);
	// b 0x824ca9ec
	goto loc_824CA9EC;
loc_824CAA84:
	// lwz r21,24(r16)
	r21.u64 = PPC_LOAD_U32(r16.u32 + 24);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// li r22,0
	r22.s64 = 0;
	// bl 0x824c5718
	sub_824C5718(ctx, base);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x824c5768
	sub_824C5768(ctx, base);
	// mr. r25,r3
	r25.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r25.s32, 0, xer);
	// beq 0x824cb304
	if (cr0.eq) goto loc_824CB304;
	// clrlwi r18,r17,24
	r18.u64 = r17.u32 & 0xFF;
	// li r27,1
	r27.s64 = 1;
loc_824CAAB4:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r20
	ctx.r6.u64 = r20.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82519da8
	sub_82519DA8(ctx, base);
	// cmplw cr6,r20,r3
	cr6.compare<uint32_t>(r20.u32, ctx.r3.u32, xer);
	// bne cr6,0x824cb2f4
	if (!cr6.eq) goto loc_824CB2F4;
	// lwz r11,160(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824cab28
	if (cr6.eq) goto loc_824CAB28;
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r10,24(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// lwz r10,40(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// slw r10,r27,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r27.u32 << (ctx.r9.u8 & 0x3F));
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824cb104
	if (!cr0.eq) goto loc_824CB104;
	// lwz r25,164(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// b 0x824cb104
	goto loc_824CB104;
loc_824CAB28:
	// lwz r11,48(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 48);
	// rlwinm. r10,r11,13,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cb104
	if (!cr0.eq) goto loc_824CB104;
	// rlwinm. r11,r11,11,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 11) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cb104
	if (!cr0.eq) goto loc_824CB104;
	// lwz r11,12(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 12);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824cab58
	if (cr0.eq) goto loc_824CAB58;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r11,r27
	r11.u64 = r27.u64;
	// bne cr6,0x824cab5c
	if (!cr6.eq) goto loc_824CAB5C;
loc_824CAB58:
	// li r11,0
	r11.s64 = 0;
loc_824CAB5C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb104
	if (cr0.eq) goto loc_824CB104;
	// lwz r11,28(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cab90
	if (!cr0.eq) goto loc_824CAB90;
	// lwz r11,24(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 24);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r24,r11,-36
	xer.ca = r11.u32 > 35;
	r24.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r24.s32, 0, xer);
	// beq 0x824cab90
	if (cr0.eq) goto loc_824CAB90;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824b05c0
	sub_824B05C0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cabbc
	if (!cr0.eq) goto loc_824CABBC;
loc_824CAB90:
	// lwz r11,36(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 36);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cca4c
	if (!cr0.eq) goto loc_824CCA4C;
	// lwz r11,32(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r24,r11,-36
	xer.ca = r11.u32 > 35;
	r24.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r24.s32, 0, xer);
	// beq 0x824cca4c
	if (cr0.eq) goto loc_824CCA4C;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824b05c0
	sub_824B05C0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cca4c
	if (cr0.eq) goto loc_824CCA4C;
loc_824CABBC:
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,84
	cr6.compare<uint32_t>(r11.u32, 84, xer);
	// beq cr6,0x824cb0fc
	if (cr6.eq) goto loc_824CB0FC;
	// cmplwi cr6,r11,83
	cr6.compare<uint32_t>(r11.u32, 83, xer);
	// beq cr6,0x824cb0fc
	if (cr6.eq) goto loc_824CB0FC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b09b8
	sub_824B09B8(ctx, base);
	// lwz r11,156(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r10,68(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// rlwimi r11,r10,0,0,3
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xF0000000) | (r11.u64 & 0xFFFFFFFF0FFFFFFF);
	// stw r11,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r11.u32);
	// lwz r11,48(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 48);
	// rlwinm. r10,r11,6,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824cafa8
	if (cr0.eq) goto loc_824CAFA8;
	// lwz r30,8(r24)
	r30.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r6,r30,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 18) & 0x7;
	// rlwinm r5,r30,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 13) & 0x7;
	// rlwinm r4,r30,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r24,-12
	r11.s64 = r24.s64 + -12;
	// rlwinm r26,r30,14,31,31
	r26.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 14) & 0x1;
	// lwzx r28,r3,r11
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// cmplw cr6,r28,r21
	cr6.compare<uint32_t>(r28.u32, r21.u32, xer);
	// beq cr6,0x824cac3c
	if (cr6.eq) goto loc_824CAC3C;
	// clrlwi r11,r26,24
	r11.u64 = r26.u32 & 0xFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// clrlwi r26,r11,24
	r26.u64 = r11.u32 & 0xFF;
loc_824CAC3C:
	// rlwinm. r11,r30,15,31,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r27,r25
	r27.u64 = r25.u64;
	// beq 0x824cae74
	if (cr0.eq) goto loc_824CAE74;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,21
	cr6.compare<uint32_t>(r11.u32, 21, xer);
	// blt cr6,0x824cac70
	if (cr6.lt) goto loc_824CAC70;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824cac74
	if (!cr6.gt) goto loc_824CAC74;
loc_824CAC70:
	// li r11,0
	r11.s64 = 0;
loc_824CAC74:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cae10
	if (cr0.eq) goto loc_824CAE10;
	// cmplw cr6,r21,r28
	cr6.compare<uint32_t>(r21.u32, r28.u32, xer);
	// bne cr6,0x824cae10
	if (!cr6.eq) goto loc_824CAE10;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r7,4(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// li r26,1
	r26.s64 = 1;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
loc_824CAC98:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x824cad2c
	if (cr6.eq) goto loc_824CAD2C;
	// lwz r10,16(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// subf r8,r6,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r6.s64;
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 ^ 1;
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq 0x824cad24
	if (cr0.eq) goto loc_824CAD24;
	// lwz r8,8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r8,r8,0,18,24
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x3F80;
	// addi r8,r8,-14208
	ctx.r8.s64 = ctx.r8.s64 + -14208;
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm. r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824cad24
	if (cr0.eq) goto loc_824CAD24;
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
loc_824CACD8:
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x824cad1c
	if (cr6.eq) goto loc_824CAD1C;
	// lwz r10,16(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// subf r5,r6,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r6.s64;
	// cntlzw r5,r5
	ctx.r5.u64 = ctx.r5.u32 == 0 ? 32 : __builtin_clz(ctx.r5.u32);
	// rlwinm r5,r5,27,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x1;
	// xori r5,r5,1
	ctx.r5.u64 = ctx.r5.u64 ^ 1;
	// cmplwi r5,0
	cr0.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq 0x824cad10
	if (cr0.eq) goto loc_824CAD10;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r10,8064
	cr6.compare<uint32_t>(ctx.r10.u32, 8064, xer);
	// beq cr6,0x824cad18
	if (cr6.eq) goto loc_824CAD18;
loc_824CAD10:
	// lwz r8,8(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// b 0x824cacd8
	goto loc_824CACD8;
loc_824CAD18:
	// lwz r9,24(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 24);
loc_824CAD1C:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x824cad34
	if (!cr6.eq) goto loc_824CAD34;
loc_824CAD24:
	// lwz r7,8(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// b 0x824cac98
	goto loc_824CAC98;
loc_824CAD2C:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824cca34
	if (cr6.eq) goto loc_824CCA34;
loc_824CAD34:
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// li r10,31
	ctx.r10.s64 = 31;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// rlwimi r9,r10,8,18,24
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 8) & 0x3F80) | (ctx.r9.u64 & 0xFFFFFFFFFFFFC07F);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// lwz r28,12(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// bl 0x82553608
	sub_82553608(ctx, base);
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// rlwimi r11,r10,16,13,15
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0x70000) | (r11.u64 & 0xFFFFFFFFFFF8FFFF);
	// stw r11,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r11.u32);
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r8,1
	ctx.r8.s64 = 1;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,63
	ctx.r6.s64 = 63;
	// addi r5,r31,24
	ctx.r5.s64 = r31.s64 + 24;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r10,28(r30)
	PPC_STORE_U32(r30.u32 + 28, ctx.r10.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// sth r11,14(r30)
	PPC_STORE_U16(r30.u32 + 14, r11.u16);
	// bl 0x82553560
	sub_82553560(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824b3148
	sub_824B3148(ctx, base);
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// ori r11,r11,64
	r11.u64 = r11.u64 | 64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r11.u32);
	// bl 0x824b0608
	sub_824B0608(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824ba9b8
	sub_824BA9B8(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// oris r11,r11,256
	r11.u64 = r11.u64 | 16777216;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// oris r11,r11,256
	r11.u64 = r11.u64 | 16777216;
	// stw r11,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r11.u32);
loc_824CAE10:
	// lwz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// clrlwi r11,r26,24
	r11.u64 = r26.u32 & 0xFF;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwimi r10,r11,23,8,8
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 23) & 0x800000) | (ctx.r10.u64 & 0xFFFFFFFFFF7FFFFF);
	// stw r25,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r25.u32);
	// oris r11,r9,128
	r11.u64 = ctx.r9.u64 | 8388608;
	// oris r10,r10,64
	ctx.r10.u64 = ctx.r10.u64 | 4194304;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// stw r10,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r10.u32);
	// lwz r11,76(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 76);
	// rlwinm. r10,r11,8,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824caf44
	if (cr0.eq) goto loc_824CAF44;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r30,72(r25)
	r30.u64 = PPC_LOAD_U32(r25.u32 + 72);
	// mr r28,r11
	r28.u64 = r11.u64;
	// bl 0x824c56d0
	sub_824C56D0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// rlwinm r10,r28,7,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 7) & 0x1;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// stw r30,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r30.u32);
	// rlwimi r11,r10,25,6,6
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 25) & 0x2000000) | (r11.u64 & 0xFFFFFFFFFDFFFFFF);
	// stw r3,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r3.u32);
	// oris r10,r9,384
	ctx.r10.u64 = ctx.r9.u64 | 25165824;
	// oris r11,r11,256
	r11.u64 = r11.u64 | 16777216;
	// b 0x824caf3c
	goto loc_824CAF3C;
loc_824CAE74:
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r30,40(r24)
	r30.u64 = PPC_LOAD_U32(r24.u32 + 40);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,116
	ctx.r6.s64 = 116;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// addi r11,r31,24
	r11.s64 = r31.s64 + 24;
	// rlwinm r10,r3,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// ori r7,r11,1
	ctx.r7.u64 = r11.u64 | 1;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,32(r8)
	PPC_STORE_U32(ctx.r8.u32 + 32, ctx.r9.u32);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// oris r11,r11,768
	r11.u64 = r11.u64 | 50331648;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// clrlwi r11,r26,24
	r11.u64 = r26.u32 & 0xFF;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwimi r10,r11,25,6,6
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 25) & 0x2000000) | (ctx.r10.u64 & 0xFFFFFFFFFDFFFFFF);
	// stw r25,108(r31)
	PPC_STORE_U32(r31.u32 + 108, r25.u32);
	// oris r11,r9,384
	r11.u64 = ctx.r9.u64 | 25165824;
	// stw r3,72(r31)
	PPC_STORE_U32(r31.u32 + 72, ctx.r3.u32);
	// oris r10,r10,256
	ctx.r10.u64 = ctx.r10.u64 | 16777216;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// stw r10,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r10.u32);
	// lwz r11,76(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 76);
	// rlwinm. r10,r11,10,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824caf44
	if (cr0.eq) goto loc_824CAF44;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mr r30,r11
	r30.u64 = r11.u64;
	// bl 0x824a92b0
	sub_824A92B0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// rlwinm r10,r30,9,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 9) & 0x1;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// stw r3,112(r31)
	PPC_STORE_U32(r31.u32 + 112, ctx.r3.u32);
	// rlwimi r11,r10,23,8,8
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 23) & 0x800000) | (r11.u64 & 0xFFFFFFFFFF7FFFFF);
	// oris r10,r9,128
	ctx.r10.u64 = ctx.r9.u64 | 8388608;
	// oris r11,r11,64
	r11.u64 = r11.u64 | 4194304;
loc_824CAF3C:
	// stw r11,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r11.u32);
	// stw r10,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r10.u32);
loc_824CAF44:
	// rlwinm r11,r27,0,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,48(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 48);
	// rlwinm. r11,r11,5,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824caf88
	if (cr0.eq) goto loc_824CAF88;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// oris r11,r11,2048
	r11.u64 = r11.u64 | 134217728;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
loc_824CAF88:
	// lwz r11,48(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 48);
	// oris r11,r11,2048
	r11.u64 = r11.u64 | 134217728;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// stw r11,48(r27)
	PPC_STORE_U32(r27.u32 + 48, r11.u32);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwimi r10,r11,0,7,5
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFFFFDFFFFFF) | (ctx.r10.u64 & 0x2000000);
	// stw r10,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r10.u32);
	// b 0x824caffc
	goto loc_824CAFFC;
loc_824CAFA8:
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cca40
	if (!cr0.eq) goto loc_824CCA40;
	// rlwinm r11,r21,0,0,30
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824cafc8
	if (cr0.eq) goto loc_824CAFC8;
	// li r27,0
	r27.s64 = 0;
	// b 0x824cafd0
	goto loc_824CAFD0;
loc_824CAFC8:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r27,r11,-4
	r27.s64 = r11.s64 + -4;
loc_824CAFD0:
	// rlwinm r11,r27,0,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_824CAFFC:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8255e6b0
	sub_8255E6B0(ctx, base);
	// lwz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r4,156(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// bl 0x8255e4c0
	sub_8255E4C0(ctx, base);
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r24,-12
	r11.s64 = r24.s64 + -12;
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r10,r21
	cr6.compare<uint32_t>(ctx.r10.u32, r21.u32, xer);
	// bne cr6,0x824cb0d8
	if (!cr6.eq) goto loc_824CB0D8;
	// cmplw cr6,r27,r25
	cr6.compare<uint32_t>(r27.u32, r25.u32, xer);
	// stw r31,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r31.u32);
	// bne cr6,0x824cb0d8
	if (!cr6.eq) goto loc_824CB0D8;
	// lwz r8,8(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rlwinm r10,r8,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r10,11392
	cr6.compare<uint32_t>(ctx.r10.u32, 11392, xer);
	// bne cr6,0x824cb0d8
	if (!cr6.eq) goto loc_824CB0D8;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r9,r10,30
	ctx.r9.u64 = ctx.r10.u32 & 0x3;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824cb094
	if (!cr0.eq) goto loc_824CB094;
	// rlwinm r10,r10,30,28,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0xF;
	// cmpwi cr6,r10,9
	cr6.compare<int32_t>(ctx.r10.s32, 9, xer);
	// beq cr6,0x824cb0d8
	if (cr6.eq) goto loc_824CB0D8;
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// beq cr6,0x824cb0d8
	if (cr6.eq) goto loc_824CB0D8;
loc_824CB094:
	// lwz r9,12(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 12);
loc_824CB098:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824cb0b4
	if (cr6.eq) goto loc_824CB0B4;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// bne cr6,0x824cb0b8
	if (!cr6.eq) goto loc_824CB0B8;
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// b 0x824cb098
	goto loc_824CB098;
loc_824CB0B4:
	// li r10,0
	ctx.r10.s64 = 0;
loc_824CB0B8:
	// not r9,r8
	ctx.r9.u64 = ~ctx.r8.u64;
	// rlwimi r9,r8,0,14,12
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r8.u32, 0) & 0xFFFFFFFFFFFBFFFF) | (ctx.r9.u64 & 0x40000);
	// stw r9,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r9.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// rlwimi r9,r10,0,31,29
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFFFFFFFFFFFD) | (ctx.r9.u64 & 0x2);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
loc_824CB0D8:
	// lwz r11,88(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 88);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r25,r31
	r25.u64 = r31.u64;
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// bl 0x8251cbe0
	sub_8251CBE0(ctx, base);
	// li r27,1
	r27.s64 = 1;
	// b 0x824cb104
	goto loc_824CB104;
loc_824CB0FC:
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// beq cr6,0x824cb32c
	if (cr6.eq) goto loc_824CB32C;
loc_824CB104:
	// clrlwi. r11,r22,24
	r11.u64 = r22.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb13c
	if (cr0.eq) goto loc_824CB13C;
	// lwz r10,0(r16)
	ctx.r10.u64 = PPC_LOAD_U32(r16.u32 + 0);
loc_824CB110:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824cb13c
	if (cr6.eq) goto loc_824CB13C;
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm. r9,r9,9,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824cb134
	if (!cr0.eq) goto loc_824CB134;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmplw cr6,r11,r25
	cr6.compare<uint32_t>(r11.u32, r25.u32, xer);
	// beq cr6,0x824cb2f4
	if (cr6.eq) goto loc_824CB2F4;
loc_824CB134:
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// b 0x824cb110
	goto loc_824CB110;
loc_824CB13C:
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lwz r11,24(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmplw cr6,r11,r25
	cr6.compare<uint32_t>(r11.u32, r25.u32, xer);
	// bne cr6,0x824cb204
	if (!cr6.eq) goto loc_824CB204;
	// lwz r10,8(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 8);
	// rlwinm r11,r10,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,31
	cr6.compare<uint32_t>(r11.u32, 31, xer);
	// blt cr6,0x824cb174
	if (cr6.lt) goto loc_824CB174;
	// cmplwi cr6,r11,81
	cr6.compare<uint32_t>(r11.u32, 81, xer);
	// mr r11,r27
	r11.u64 = r27.u64;
	// ble cr6,0x824cb178
	if (!cr6.gt) goto loc_824CB178;
loc_824CB174:
	// li r11,0
	r11.s64 = 0;
loc_824CB178:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb204
	if (cr0.eq) goto loc_824CB204;
	// rlwinm r11,r10,31,28,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0xF;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// beq cr6,0x824cb204
	if (cr6.eq) goto loc_824CB204;
	// not r11,r11
	r11.u64 = ~r11.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// andc r31,r11,r10
	r31.u64 = r11.u64 & ~ctx.r10.u64;
	// addi r11,r31,-1
	r11.s64 = r31.s64 + -1;
	// andc r11,r31,r11
	r11.u64 = r31.u64 & ~r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r30,r11,31
	xer.ca = r11.u32 <= 31;
	r30.s64 = 31 - r11.s64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// add r10,r3,r30
	ctx.r10.u64 = ctx.r3.u64 + r30.u64;
	// lwz r11,672(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// rlwinm r9,r31,1,27,30
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0x1E;
	// stb r27,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r27.u8);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r30,5,22,26
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 5) & 0x3E0;
	// rlwinm r8,r8,0,25,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFFF7F;
	// stwx r23,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r23.u32);
	// ori r8,r8,7296
	ctx.r8.u64 = ctx.r8.u64 | 7296;
	// lwz r11,8(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 8);
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// rlwinm r10,r11,0,0,17
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFC000;
	// addi r10,r10,16384
	ctx.r10.s64 = ctx.r10.s64 + 16384;
	// rlwimi r10,r11,0,18,14
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFFFFFFE3FFF) | (ctx.r10.u64 & 0x1C000);
	// stw r10,8(r23)
	PPC_STORE_U32(r23.u32 + 8, ctx.r10.u32);
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// rlwinm r11,r11,0,27,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFF9F;
	// rlwinm r11,r11,0,24,21
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFCFF;
	// or r11,r8,r11
	r11.u64 = ctx.r8.u64 | r11.u64;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// b 0x824cb2f0
	goto loc_824CB2F0;
loc_824CB204:
	// addi r30,r25,24
	r30.s64 = r25.s64 + 24;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cb264
	if (!cr0.eq) goto loc_824CB264;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// b 0x824cb258
	goto loc_824CB258;
loc_824CB21C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b37b8
	sub_824B37B8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb248
	if (cr0.eq) goto loc_824CB248;
	// lwz r11,48(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 48);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb318
	if (cr0.eq) goto loc_824CB318;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a90a0
	sub_824A90A0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb318
	if (cr0.eq) goto loc_824CB318;
loc_824CB248:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cb264
	if (!cr0.eq) goto loc_824CB264;
loc_824CB258:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-36
	xer.ca = r11.u32 > 35;
	r31.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x824cb21c
	if (!cr0.eq) goto loc_824CB21C;
loc_824CB264:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// ori r5,r11,2
	ctx.r5.u64 = r11.u64 | 2;
loc_824CB26C:
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82556538
	sub_82556538(ctx, base);
	// lwz r11,24(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// lwz r31,12(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// cmplw cr6,r11,r25
	cr6.compare<uint32_t>(r11.u32, r25.u32, xer);
	// beq cr6,0x824cb2d4
	if (cr6.eq) goto loc_824CB2D4;
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,40(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824f12a8
	sub_824F12A8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,44(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824f12a8
	sub_824F12A8(ctx, base);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// clrlwi. r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cb2d4
	if (!cr0.eq) goto loc_824CB2D4;
	// rlwimi r11,r27,16,13,15
	r11.u64 = (__builtin_rotateleft32(r27.u32, 16) & 0x70000) | (r11.u64 & 0xFFFFFFFFFFF8FFFF);
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r11,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r11.u32);
	// stw r10,28(r30)
	PPC_STORE_U32(r30.u32 + 28, ctx.r10.u32);
	// sth r19,14(r30)
	PPC_STORE_U16(r30.u32 + 14, r19.u16);
loc_824CB2D4:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// rlwimi r11,r27,16,13,15
	r11.u64 = (__builtin_rotateleft32(r27.u32, 16) & 0x70000) | (r11.u64 & 0xFFFFFFFFFFF8FFFF);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// sth r19,14(r31)
	PPC_STORE_U16(r31.u32 + 14, r19.u16);
loc_824CB2F0:
	// mr r22,r27
	r22.u64 = r27.u64;
loc_824CB2F4:
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x824c5768
	sub_824C5768(ctx, base);
	// mr. r25,r3
	r25.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r25.s32, 0, xer);
	// bne 0x824caab4
	if (!cr0.eq) goto loc_824CAAB4;
loc_824CB304:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// bl 0x824bb028
	sub_824BB028(ctx, base);
	// b 0x824ca9e8
	goto loc_824CA9E8;
loc_824CB318:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824cb264
	if (cr6.eq) goto loc_824CB264;
	// rlwimi r31,r27,0,30,31
	r31.u64 = (__builtin_rotateleft32(r27.u32, 0) & 0x3) | (r31.u64 & 0xFFFFFFFFFFFFFFFC);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// b 0x824cb26c
	goto loc_824CB26C;
loc_824CB32C:
	// mr r17,r27
	r17.u64 = r27.u64;
	// b 0x824ca9e8
	goto loc_824CA9E8;
loc_824CB334:
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// clrlwi. r11,r17,24
	r11.u64 = r17.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cb354
	if (!cr0.eq) goto loc_824CB354;
	// cmplw cr6,r28,r19
	cr6.compare<uint32_t>(r28.u32, r19.u32, xer);
	// beq cr6,0x824cb4b0
	if (cr6.eq) goto loc_824CB4B0;
loc_824CB354:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824cb398
	if (!cr6.eq) goto loc_824CB398;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6420
	sub_824C6420(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cb398
	if (!cr0.eq) goto loc_824CB398;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c68c0
	sub_824C68C0(ctx, base);
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c68c0
	sub_824C68C0(ctx, base);
	// b 0x824cb4b0
	goto loc_824CB4B0;
loc_824CB398:
	// lwz r30,24(r16)
	r30.u64 = PPC_LOAD_U32(r16.u32 + 24);
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r11,r30,24
	r11.s64 = r30.s64 + 24;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// ori r5,r11,2
	ctx.r5.u64 = r11.u64 | 2;
	// bl 0x82557d10
	sub_82557D10(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r25,1
	r25.s64 = 1;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwimi r11,r25,16,13,15
	r11.u64 = (__builtin_rotateleft32(r25.u32, 16) & 0x70000) | (r11.u64 & 0xFFFFFFFFFFF8FFFF);
	// oris r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 8388608;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x824ba1a8
	sub_824BA1A8(ctx, base);
	// lwz r11,672(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// rlwinm r10,r15,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r31,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r31.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r10,108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r31,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, r31.u32);
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// lwz r10,648(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 648);
	// mullw r11,r28,r11
	r11.s64 = int64_t(r28.s32) * int64_t(r11.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r27,r11,r10
	r27.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cb4a4
	if (!cr0.eq) goto loc_824CB4A4;
	// mr. r26,r11
	r26.u64 = r11.u64;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// beq 0x824cb4a4
	if (cr0.eq) goto loc_824CB4A4;
loc_824CB424:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb488
	if (cr0.eq) goto loc_824CB488;
	// lwz r30,0(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 0);
loc_824CB440:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824cb488
	if (cr6.eq) goto loc_824CB488;
	// lwz r3,12(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm. r11,r11,9,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb480
	if (cr0.eq) goto loc_824CB480;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm r28,r11,27,30,31
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// add r11,r3,r28
	r11.u64 = ctx.r3.u64 + r28.u64;
	// rlwinm r10,r11,29,3,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFF8;
	// clrldi r11,r11,58
	r11.u64 = r11.u64 & 0x3F;
	// ldx r9,r10,r27
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r10.u32 + r27.u32);
	// sld r11,r25,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (r25.u64 << (r11.u8 & 0x7F));
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stdx r11,r10,r27
	PPC_STORE_U64(ctx.r10.u32 + r27.u32, r11.u64);
loc_824CB480:
	// lwz r30,4(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// b 0x824cb440
	goto loc_824CB440;
loc_824CB488:
	// rlwinm r11,r26,0,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cb4a4
	if (!cr0.eq) goto loc_824CB4A4;
	// mr r26,r11
	r26.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824cb424
	if (!cr6.eq) goto loc_824CB424;
loc_824CB4A4:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
loc_824CB4B0:
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r5,116(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b1f78
	sub_824B1F78(ctx, base);
	// lwz r15,108(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r14,1
	r14.s64 = 1;
	// li r20,0
	r20.s64 = 0;
loc_824CB4D4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,632(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// blt cr6,0x824ca508
	if (cr6.lt) goto loc_824CA508;
loc_824CB4E8:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,63
	r11.s64 = r11.s64 + 63;
	// rlwinm r4,r11,29,3,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFF8;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,632(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r4,r10,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,656(r29)
	PPC_STORE_U32(r29.u32 + 656, r11.u32);
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// stw r20,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r20.u32);
	// stw r14,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r14.u32);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// stw r3,652(r29)
	PPC_STORE_U32(r29.u32 + 652, ctx.r3.u32);
	// ble cr6,0x824cc5d8
	if (!cr6.gt) goto loc_824CC5D8;
loc_824CB538:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r10,108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r16,r11,r10
	r16.u64 = r11.u64 + ctx.r10.u64;
	// lwz r15,0(r16)
	r15.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// cmplwi r15,0
	cr0.compare<uint32_t>(r15.u32, 0, xer);
	// beq 0x824cc5d8
	if (cr0.eq) goto loc_824CC5D8;
	// lwz r11,8(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb5c4
	if (cr0.eq) goto loc_824CB5C4;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,652(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x824cc5c0
	if (!cr6.eq) goto loc_824CC5C0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6a90
	sub_824C6A90(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6650
	sub_824C6650(ctx, base);
	// b 0x824cc5c0
	goto loc_824CC5C0;
loc_824CB5C4:
	// lhz r11,12(r15)
	r11.u64 = PPC_LOAD_U16(r15.u32 + 12);
	// clrlwi r19,r11,29
	r19.u64 = r11.u32 & 0x7;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// lwz r11,8(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// mr r14,r3
	r14.u64 = ctx.r3.u64;
	// li r18,0
	r18.s64 = 0;
	// rlwinm r20,r11,31,28,31
	r20.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// add r17,r14,r19
	r17.u64 = r14.u64 + r19.u64;
loc_824CB5E4:
	// addi r11,r1,100
	r11.s64 = ctx.r1.s64 + 100;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r21,1
	r21.s64 = 1;
	// mr r26,r14
	r26.u64 = r14.u64;
	// cmplw cr6,r14,r17
	cr6.compare<uint32_t>(r14.u32, r17.u32, xer);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bge cr6,0x824cb76c
	if (!cr6.lt) goto loc_824CB76C;
	// li r22,0
	r22.s64 = 0;
	// li r23,0
	r23.s64 = 0;
loc_824CB608:
	// and. r11,r21,r20
	r11.u64 = r21.u64 & r20.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb754
	if (cr0.eq) goto loc_824CB754;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r11,652(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// li r24,1
	r24.s64 = 1;
	// rlwinm r10,r25,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r31,r14
	r31.u64 = r14.u64;
	// li r27,0
	r27.s64 = 0;
	// li r30,0
	r30.s64 = 0;
	// lwzx r28,r10,r11
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
loc_824CB63C:
	// and. r11,r24,r20
	r11.u64 = r24.u64 & r20.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb73c
	if (cr0.eq) goto loc_824CB73C;
	// cmplw cr6,r26,r31
	cr6.compare<uint32_t>(r26.u32, r31.u32, xer);
	// beq cr6,0x824cb73c
	if (cr6.eq) goto loc_824CB73C;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// lwz r10,652(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// li r11,0
	r11.s64 = 0;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// lwzx r4,r9,r10
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// beq cr6,0x824cb6c0
	if (cr6.eq) goto loc_824CB6C0;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x824cb6b4
	if (cr6.eq) goto loc_824CB6B4;
	// cmplw cr6,r28,r4
	cr6.compare<uint32_t>(r28.u32, ctx.r4.u32, xer);
	// beq cr6,0x824cb6f0
	if (cr6.eq) goto loc_824CB6F0;
	// addi r11,r1,124
	r11.s64 = ctx.r1.s64 + 124;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r6,124(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// bl 0x824c8278
	sub_824C8278(ctx, base);
	// b 0x824cb6d8
	goto loc_824CB6D8;
loc_824CB6B4:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// b 0x824cb6d0
	goto loc_824CB6D0;
loc_824CB6C0:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x824cb6ec
	if (cr6.eq) goto loc_824CB6EC;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_824CB6D0:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c7a50
	sub_824C7A50(ctx, base);
loc_824CB6D8:
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// b 0x824cb6f0
	goto loc_824CB6F0;
loc_824CB6EC:
	// li r11,0
	r11.s64 = 0;
loc_824CB6F0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb73c
	if (cr0.eq) goto loc_824CB73C;
	// li r7,1
	ctx.r7.s64 = 1;
	// clrlwi r5,r30,27
	ctx.r5.u64 = r30.u32 & 0x1F;
	// clrlwi r6,r23,27
	ctx.r6.u64 = r23.u32 & 0x1F;
	// rlwinm r9,r23,29,3,29
	ctx.r9.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// rlwinm r11,r30,29,3,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r10,r1,100
	ctx.r10.s64 = ctx.r1.s64 + 100;
	// slw r4,r7,r27
	ctx.r4.u64 = r27.u8 & 0x20 ? 0 : (ctx.r7.u32 << (r27.u8 & 0x3F));
	// slw r7,r7,r22
	ctx.r7.u64 = r22.u8 & 0x20 ? 0 : (ctx.r7.u32 << (r22.u8 & 0x3F));
	// slw r6,r4,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r4.u32 << (ctx.r6.u8 & 0x3F));
	// slw r7,r7,r5
	ctx.r7.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r5.u8 & 0x3F));
	// lwzx r5,r9,r8
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	// or r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 | ctx.r5.u64;
	// stwx r6,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, ctx.r6.u32);
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// or r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 | ctx.r9.u64;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
loc_824CB73C:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// rlwinm r24,r24,1,0,30
	r24.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r31,r17
	cr6.compare<uint32_t>(r31.u32, r17.u32, xer);
	// blt cr6,0x824cb63c
	if (cr6.lt) goto loc_824CB63C;
loc_824CB754:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// addi r23,r23,4
	r23.s64 = r23.s64 + 4;
	// rlwinm r21,r21,1,0,30
	r21.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r26,r17
	cr6.compare<uint32_t>(r26.u32, r17.u32, xer);
	// blt cr6,0x824cb608
	if (cr6.lt) goto loc_824CB608;
loc_824CB76C:
	// li r5,-1
	ctx.r5.s64 = -1;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// beq cr6,0x824cb814
	if (cr6.eq) goto loc_824CB814;
	// li r10,0
	ctx.r10.s64 = 0;
loc_824CB78C:
	// and. r11,r6,r20
	r11.u64 = ctx.r6.u64 & r20.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb800
	if (cr0.eq) goto loc_824CB800;
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// rlwinm r4,r10,29,3,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r9,r10,3
	ctx.r9.s64 = ctx.r10.s64 + 3;
	// clrlwi r11,r10,27
	r11.u64 = ctx.r10.u32 & 0x1F;
	// clrlwi r9,r9,27
	ctx.r9.u64 = ctx.r9.u32 & 0x1F;
	// lis r31,-28311
	r31.s64 = -1855389696;
	// lwzx r4,r4,r3
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r3.u32);
	// li r3,2
	ctx.r3.s64 = 2;
	// lis r30,0
	r30.s64 = 0;
	// ori r31,r31,5192
	r31.u64 = r31.u64 | 5192;
	// ori r30,r30,36262
	r30.u64 = r30.u64 | 36262;
	// rldimi r31,r30,32,0
	r31.u64 = (__builtin_rotateleft64(r30.u64, 32) & 0xFFFFFFFF00000000) | (r31.u64 & 0xFFFFFFFF);
	// slw r9,r3,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r3.u32 << (ctx.r9.u8 & 0x3F));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// and r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 & ctx.r4.u64;
	// slw r4,r5,r11
	ctx.r4.u64 = r11.u8 & 0x20 ? 0 : (ctx.r5.u32 << (r11.u8 & 0x3F));
	// and r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 & ctx.r4.u64;
	// srw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r9,r31,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (r31.u64 >> (r11.u8 & 0x7F));
	// srd r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// srd r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x824cb800
	if (!cr6.lt) goto loc_824CB800;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
loc_824CB800:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r7,r19
	cr6.compare<uint32_t>(ctx.r7.u32, r19.u32, xer);
	// blt cr6,0x824cb78c
	if (cr6.lt) goto loc_824CB78C;
loc_824CB814:
	// add r30,r28,r14
	r30.u64 = r28.u64 + r14.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,652(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r31,r10,r11
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x824cb86c
	if (!cr0.eq) goto loc_824CB86C;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6a90
	sub_824C6A90(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// bl 0x824c6650
	sub_824C6650(ctx, base);
loc_824CB86C:
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// clrlwi r10,r11,27
	ctx.r10.u64 = r11.u32 & 0x1F;
	// addi r7,r11,3
	ctx.r7.s64 = r11.s64 + 3;
	// rlwinm r11,r11,29,3,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r7,r7,27
	ctx.r7.u64 = ctx.r7.u32 & 0x1F;
	// li r9,-1
	ctx.r9.s64 = -1;
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// li r11,2
	r11.s64 = 2;
	// slw r9,r9,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// slw r11,r11,r7
	r11.u64 = ctx.r7.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r7.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// and r11,r11,r9
	r11.u64 = r11.u64 & ctx.r9.u64;
	// srw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// andc. r28,r20,r11
	r28.u64 = r20.u64 & ~r11.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824cb978
	if (cr0.eq) goto loc_824CB978;
loc_824CB8B0:
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// andc r30,r28,r11
	r30.u64 = r28.u64 & ~r11.u64;
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// subf r28,r30,r28
	r28.s64 = r28.s64 - r30.s64;
	// subf r11,r11,r14
	r11.s64 = r14.s64 - r11.s64;
	// addi r26,r11,31
	r26.s64 = r11.s64 + 31;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r11,652(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// rlwinm r10,r25,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r27,r10,r11
	r27.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmplw cr6,r27,r31
	cr6.compare<uint32_t>(r27.u32, r31.u32, xer);
	// beq cr6,0x824cb96c
	if (cr6.eq) goto loc_824CB96C;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// beq cr6,0x824cb940
	if (cr6.eq) goto loc_824CB940;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r6,128(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// bl 0x824c8278
	sub_824C8278(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb970
	if (cr0.eq) goto loc_824CB970;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c8678
	sub_824C8678(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x824cb96c
	goto loc_824CB96C;
loc_824CB940:
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// bl 0x824c7a50
	sub_824C7A50(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb970
	if (cr0.eq) goto loc_824CB970;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c7100
	sub_824C7100(ctx, base);
loc_824CB96C:
	// subf r20,r30,r20
	r20.s64 = r20.s64 - r30.s64;
loc_824CB970:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x824cb8b0
	if (!cr6.eq) goto loc_824CB8B0;
loc_824CB978:
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x824cb5e4
	if (!cr6.eq) goto loc_824CB5E4;
	// lwz r11,8(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// cmplwi cr6,r18,1
	cr6.compare<uint32_t>(r18.u32, 1, xer);
	// ble cr6,0x824cbcac
	if (!cr6.gt) goto loc_824CBCAC;
	// lwz r22,88(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rlwinm r25,r11,31,28,31
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// li r24,1
	r24.s64 = 1;
	// li r23,0
	r23.s64 = 0;
loc_824CB9A0:
	// mr r27,r23
	r27.u64 = r23.u64;
	// mr r30,r23
	r30.u64 = r23.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// mr r31,r14
	r31.u64 = r14.u64;
	// cmplw cr6,r14,r17
	cr6.compare<uint32_t>(r14.u32, r17.u32, xer);
	// bge cr6,0x824cba00
	if (!cr6.lt) goto loc_824CBA00;
loc_824CB9B8:
	// and. r11,r28,r25
	r11.u64 = r28.u64 & r25.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cb9f0
	if (cr0.eq) goto loc_824CB9F0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,652(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// beq cr6,0x824cb9e8
	if (cr6.eq) goto loc_824CB9E8;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bne cr6,0x824cb9f0
	if (!cr6.eq) goto loc_824CB9F0;
loc_824CB9E8:
	// mr r30,r11
	r30.u64 = r11.u64;
	// or r27,r28,r27
	r27.u64 = r28.u64 | r27.u64;
loc_824CB9F0:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// rlwinm r28,r28,1,0,30
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r31,r17
	cr6.compare<uint32_t>(r31.u32, r17.u32, xer);
	// blt cr6,0x824cb9b8
	if (cr6.lt) goto loc_824CB9B8;
loc_824CBA00:
	// subf r25,r27,r25
	r25.s64 = r25.s64 - r27.s64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x824cbc84
	if (cr6.eq) goto loc_824CBC84;
	// addi r26,r15,4
	r26.s64 = r15.s64 + 4;
loc_824CBA10:
	// addi r11,r27,-1
	r11.s64 = r27.s64 + -1;
	// lwz r7,0(r26)
	ctx.r7.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// andc r28,r27,r11
	r28.u64 = r27.u64 & ~r11.u64;
	// cntlzw r11,r28
	r11.u64 = r28.u32 == 0 ? 32 : __builtin_clz(r28.u32);
	// subfic r30,r11,31
	xer.ca = r11.u32 <= 31;
	r30.s64 = 31 - r11.s64;
loc_824CBA24:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x824cbc7c
	if (cr6.eq) goto loc_824CBC7C;
	// lwz r9,16(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// subf r11,r23,r9
	r11.s64 = ctx.r9.s64 - r23.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824cbaa8
	if (cr0.eq) goto loc_824CBAA8;
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// rlwinm r11,r10,7,29,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x7;
	// cntlzw r8,r11
	ctx.r8.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 ^ 1;
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq 0x824cbaa8
	if (cr0.eq) goto loc_824CBAA8;
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// rlwinm. r9,r9,0,12,12
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824cbaa8
	if (!cr0.eq) goto loc_824CBAA8;
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824cbaa0
	if (cr6.eq) goto loc_824CBAA0;
	// rlwinm r9,r10,27,24,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
loc_824CBA84:
	// srw r6,r9,r10
	ctx.r6.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r10.u8 & 0x3F));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// clrlwi r6,r6,30
	ctx.r6.u64 = ctx.r6.u32 & 0x3;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// slw r6,r24,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (r24.u32 << (ctx.r6.u8 & 0x3F));
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// bne 0x824cba84
	if (!cr0.eq) goto loc_824CBA84;
loc_824CBAA0:
	// cmplw cr6,r8,r28
	cr6.compare<uint32_t>(ctx.r8.u32, r28.u32, xer);
	// beq cr6,0x824cbab0
	if (cr6.eq) goto loc_824CBAB0;
loc_824CBAA8:
	// lwz r7,8(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// b 0x824cba24
	goto loc_824CBA24;
loc_824CBAB0:
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824b3878
	sub_824B3878(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cbacc
	if (cr0.eq) goto loc_824CBACC;
	// mr r5,r15
	ctx.r5.u64 = r15.u64;
	// rlwimi r5,r24,0,30,31
	ctx.r5.u64 = (__builtin_rotateleft32(r24.u32, 0) & 0x3) | (ctx.r5.u64 & 0xFFFFFFFFFFFFFFFC);
	// b 0x824cbadc
	goto loc_824CBADC;
loc_824CBACC:
	// lwz r11,24(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 24);
	// addi r11,r11,24
	r11.s64 = r11.s64 + 24;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// ori r5,r11,2
	ctx.r5.u64 = r11.u64 | 2;
loc_824CBADC:
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// lwz r4,24(r15)
	ctx.r4.u64 = PPC_LOAD_U32(r15.u32 + 24);
	// mr r6,r15
	ctx.r6.u64 = r15.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82557fa8
	sub_82557FA8(ctx, base);
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// rlwinm. r11,r11,0,11,11
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x100000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cbb0c
	if (cr0.eq) goto loc_824CBB0C;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// b 0x824cbb14
	goto loc_824CBB14;
loc_824CBB0C:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
loc_824CBB14:
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// add r30,r30,r14
	r30.u64 = r30.u64 + r14.u64;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwimi r11,r24,16,13,15
	r11.u64 = (__builtin_rotateleft32(r24.u32, 16) & 0x70000) | (r11.u64 & 0xFFFFFFFFFFF8FFFF);
	// oris r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 8388608;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x824ba1a8
	sub_824BA1A8(ctx, base);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// oris r9,r11,8
	ctx.r9.u64 = r11.u64 | 524288;
	// mr r11,r22
	r11.u64 = r22.u64;
	// stw r9,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r9.u32);
	// lwz r9,672(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// stwx r31,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, r31.u32);
	// lwz r10,632(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// cmplw cr6,r22,r10
	cr6.compare<uint32_t>(r22.u32, ctx.r10.u32, xer);
	// bge cr6,0x824cbb90
	if (!cr6.lt) goto loc_824CBB90;
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
loc_824CBB6C:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r15
	cr6.compare<uint32_t>(ctx.r9.u32, r15.u32, xer);
	// bne cr6,0x824cbb90
	if (!cr6.eq) goto loc_824CBB90;
	// stw r31,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r31.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,632(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x824cbb6c
	if (cr6.lt) goto loc_824CBB6C;
loc_824CBB90:
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
loc_824CBB94:
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824cbc7c
	if (cr0.eq) goto loc_824CBC7C;
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r23,r8
	ctx.r10.s64 = ctx.r8.s64 - r23.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824cbc68
	if (cr0.eq) goto loc_824CBC68;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r10,7,29,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x7;
	// cntlzw r7,r9
	ctx.r7.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r7,r7,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// xori r7,r7,1
	ctx.r7.u64 = ctx.r7.u64 ^ 1;
	// cmplwi r7,0
	cr0.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq 0x824cbc68
	if (cr0.eq) goto loc_824CBC68;
	// lwz r8,12(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// rlwinm. r8,r8,0,12,12
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x824cbc68
	if (!cr0.eq) goto loc_824CBC68;
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824cbc1c
	if (cr6.eq) goto loc_824CBC1C;
	// rotlwi r5,r10,0
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// rlwinm r8,r10,27,24,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// rlwinm r10,r5,7,29,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 7) & 0x7;
loc_824CBC00:
	// srw r5,r8,r9
	ctx.r5.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r9.u8 & 0x3F));
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrlwi r5,r5,30
	ctx.r5.u64 = ctx.r5.u32 & 0x3;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// slw r5,r24,r5
	ctx.r5.u64 = ctx.r5.u8 & 0x20 ? 0 : (r24.u32 << (ctx.r5.u8 & 0x3F));
	// or r7,r5,r7
	ctx.r7.u64 = ctx.r5.u64 | ctx.r7.u64;
	// bne 0x824cbc00
	if (!cr0.eq) goto loc_824CBC00;
loc_824CBC1C:
	// cmplw cr6,r7,r28
	cr6.compare<uint32_t>(ctx.r7.u32, r28.u32, xer);
	// bne cr6,0x824cbc68
	if (!cr6.eq) goto loc_824CBC68;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824cbc34
	goto loc_824CBC34;
loc_824CBC30:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824CBC34:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824cbc30
	if (!cr6.eq) goto loc_824CBC30;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r31,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r31.u32);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r10,r10,0,27,18
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFE01F;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_824CBC68:
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824cbb94
	if (!cr6.eq) goto loc_824CBB94;
	// addi r6,r11,8
	ctx.r6.s64 = r11.s64 + 8;
	// b 0x824cbb94
	goto loc_824CBB94;
loc_824CBC7C:
	// subf. r27,r28,r27
	r27.s64 = r27.s64 - r28.s64;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// bne 0x824cba10
	if (!cr0.eq) goto loc_824CBA10;
loc_824CBC84:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x824cb9a0
	if (!cr6.eq) goto loc_824CB9A0;
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// rlwinm. r11,r11,0,11,11
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x100000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cc5c0
	if (!cr0.eq) goto loc_824CC5C0;
	// lwz r11,8(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// sth r23,14(r15)
	PPC_STORE_U16(r15.u32 + 14, r23.u16);
	// rlwinm r11,r11,0,9,7
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFF7FFFFF;
	// stw r11,8(r15)
	PPC_STORE_U32(r15.u32 + 8, r11.u32);
	// b 0x824cc5c0
	goto loc_824CC5C0;
loc_824CBCAC:
	// rlwinm r10,r11,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r10,r10,-13952
	ctx.r10.s64 = ctx.r10.s64 + -13952;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824cc5c0
	if (cr0.eq) goto loc_824CC5C0;
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r20,r11,31,28,31
	r20.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,652(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r19,r15,40
	r19.s64 = r15.s64 + 40;
	// li r23,0
	r23.s64 = 0;
	// mr r22,r19
	r22.u64 = r19.u64;
	// li r21,1
	r21.s64 = 1;
	// lwzx r18,r10,r11
	r18.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
loc_824CBCEC:
	// lwz r11,8(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// rlwinm r11,r11,13,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// cmplw cr6,r23,r11
	cr6.compare<uint32_t>(r23.u32, r11.u32, xer);
	// bge cr6,0x824cbf84
	if (!cr6.lt) goto loc_824CBF84;
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// slw r28,r21,r23
	r28.u64 = r23.u8 & 0x20 ? 0 : (r21.u32 << (r23.u8 & 0x3F));
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm. r10,r10,9,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cbd1c
	if (!cr0.eq) goto loc_824CBD1C;
loc_824CBD14:
	// subf r20,r28,r20
	r20.s64 = r20.s64 - r28.s64;
	// b 0x824cbf78
	goto loc_824CBF78;
loc_824CBD1C:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r24,r11,r31
	r24.u64 = r11.u64 + r31.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r11,652(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// rlwinm r10,r27,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r30,r10,r11
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmplw cr6,r30,r18
	cr6.compare<uint32_t>(r30.u32, r18.u32, xer);
	// beq cr6,0x824cbd14
	if (cr6.eq) goto loc_824CBD14;
	// rlwinm r11,r15,0,0,30
	r11.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x824cbe10
	goto loc_824CBE10;
loc_824CBD5C:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm. r10,r11,9,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824cbe0c
	if (cr0.eq) goto loc_824CBE0C;
	// li r28,0
	r28.s64 = 0;
	// rlwinm r31,r11,31,28,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
loc_824CBD70:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824cbd9c
	if (cr6.eq) goto loc_824CBD9C;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// addi r11,r31,-1
	r11.s64 = r31.s64 + -1;
	// andc r11,r31,r11
	r11.u64 = r31.u64 & ~r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r11,r11,31
	xer.ca = r11.u32 <= 31;
	r11.s64 = 31 - r11.s64;
	// add r4,r3,r11
	ctx.r4.u64 = ctx.r3.u64 + r11.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// b 0x824cbda4
	goto loc_824CBDA4;
loc_824CBD9C:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r11,0
	r11.s64 = 0;
loc_824CBDA4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824cbe04
	if (cr6.eq) goto loc_824CBE04;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824cbe2c
	if (cr6.eq) goto loc_824CBE2C;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c5d58
	sub_824C5D58(ctx, base);
	// clrldi r11,r6,58
	r11.u64 = ctx.r6.u64 & 0x3F;
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r6,29,3,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFF8;
	// lwzx r10,r10,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// ldx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r9.u32);
	// sld r11,r21,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (r21.u64 << (r11.u8 & 0x7F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// mr r11,r21
	r11.u64 = r21.u64;
	// bne cr6,0x824cbdf8
	if (!cr6.eq) goto loc_824CBDF8;
	// li r11,0
	r11.s64 = 0;
loc_824CBDF8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cbe34
	if (cr0.eq) goto loc_824CBE34;
loc_824CBE00:
	// mr r28,r21
	r28.u64 = r21.u64;
loc_824CBE04:
	// clrlwi. r11,r28,24
	r11.u64 = r28.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cbe44
	if (!cr0.eq) goto loc_824CBE44;
loc_824CBE0C:
	// rlwinm r11,r26,0,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xFFFFFFFE;
loc_824CBE10:
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cbe4c
	if (!cr0.eq) goto loc_824CBE4C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r26,r11
	r26.u64 = r11.u64;
	// bne cr6,0x824cbd5c
	if (!cr6.eq) goto loc_824CBD5C;
	// b 0x824cbe4c
	goto loc_824CBE4C;
loc_824CBE2C:
	// cmplw cr6,r6,r27
	cr6.compare<uint32_t>(ctx.r6.u32, r27.u32, xer);
	// beq cr6,0x824cbe00
	if (cr6.eq) goto loc_824CBE00;
loc_824CBE34:
	// addi r11,r31,-1
	r11.s64 = r31.s64 + -1;
	// andc r11,r31,r11
	r11.u64 = r31.u64 & ~r11.u64;
	// subf r31,r11,r31
	r31.s64 = r31.s64 - r11.s64;
	// b 0x824cbd70
	goto loc_824CBD70;
loc_824CBE44:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// bne cr6,0x824cbf78
	if (!cr6.eq) goto loc_824CBF78;
loc_824CBE4C:
	// add r4,r23,r14
	ctx.r4.u64 = r23.u64 + r14.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x824c5d58
	sub_824C5D58(ctx, base);
	// clrldi r9,r31,58
	ctx.r9.u64 = r31.u64 & 0x3F;
	// lwz r10,648(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 648);
	// rlwinm r28,r31,29,3,28
	r28.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 29) & 0x1FFFFFF8;
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// mullw r11,r11,r27
	r11.s64 = int64_t(r11.s32) * int64_t(r27.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sld r10,r21,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x40 ? 0 : (r21.u64 << (ctx.r9.u8 & 0x7F));
	// not r25,r10
	r25.u64 = ~ctx.r10.u64;
	// ldx r10,r11,r28
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + r28.u32);
	// and r10,r25,r10
	ctx.r10.u64 = r25.u64 & ctx.r10.u64;
	// stdx r10,r11,r28
	PPC_STORE_U64(r11.u32 + r28.u32, ctx.r10.u64);
	// bne cr6,0x824cbecc
	if (!cr6.eq) goto loc_824CBECC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6a90
	sub_824C6A90(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x824c6650
	sub_824C6650(ctx, base);
loc_824CBECC:
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c5d58
	sub_824C5D58(ctx, base);
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// lwz r7,648(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 648);
	// mullw r10,r31,r11
	ctx.r10.s64 = int64_t(r31.s32) * int64_t(r11.s32);
	// rlwinm r8,r10,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r8,r7
	r11.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwzx r8,r10,r30
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// b 0x824cbf18
	goto loc_824CBF18;
loc_824CBF00:
	// ld r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// ld r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// andc r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 & ~ctx.r7.u64;
	// std r7,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r7.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824CBF18:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x824cbf00
	if (cr6.lt) goto loc_824CBF00;
	// addi r11,r26,4
	r11.s64 = r26.s64 + 4;
	// lwz r8,636(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// lwzx r9,r10,r30
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r8,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r11,r11,r18
	r11.u64 = PPC_LOAD_U32(r11.u32 + r18.u32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// b 0x824cbf58
	goto loc_824CBF58;
loc_824CBF40:
	// ld r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// ld r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// andc r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 & ~ctx.r8.u64;
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_824CBF58:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824cbf40
	if (cr6.lt) goto loc_824CBF40;
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r30
	r11.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// ldx r10,r11,r28
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + r28.u32);
	// and r10,r25,r10
	ctx.r10.u64 = r25.u64 & ctx.r10.u64;
	// stdx r10,r11,r28
	PPC_STORE_U64(r11.u32 + r28.u32, ctx.r10.u64);
loc_824CBF78:
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r22,r22,4
	r22.s64 = r22.s64 + 4;
	// b 0x824cbcec
	goto loc_824CBCEC;
loc_824CBF84:
	// lwz r11,8(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// li r17,1
	r17.s64 = 1;
	// lwz r16,84(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rlwinm r11,r11,31,28,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// subf r22,r20,r11
	r22.s64 = r11.s64 - r20.s64;
loc_824CBF98:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6a90
	sub_824C6A90(ctx, base);
	// addi r11,r1,104
	r11.s64 = ctx.r1.s64 + 104;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r7,8(r15)
	ctx.r7.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// mr r21,r3
	r21.u64 = ctx.r3.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// rlwinm r7,r7,13,29,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 13) & 0x7;
	// mr r5,r18
	ctx.r5.u64 = r18.u64;
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// ori r11,r11,4096
	r11.u64 = r11.u64 | 4096;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// oris r11,r11,32
	r11.u64 = r11.u64 | 2097152;
	// oris r11,r11,768
	r11.u64 = r11.u64 | 50331648;
	// oris r6,r11,12288
	ctx.r6.u64 = r11.u64 | 805306368;
	// stw r6,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r6.u32);
	// bl 0x824c8428
	sub_824C8428(ctx, base);
	// mr r28,r20
	r28.u64 = r20.u64;
loc_824CBFF8:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824cc270
	if (cr6.eq) goto loc_824CC270;
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// andc r11,r28,r11
	r11.u64 = r28.u64 & ~r11.u64;
	// subf r28,r11,r28
	r28.s64 = r28.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r26,r11,31
	xer.ca = r11.u32 <= 31;
	r26.s64 = 31 - r11.s64;
	// addi r11,r26,10
	r11.s64 = r26.s64 + 10;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r15
	r11.u64 = PPC_LOAD_U32(r11.u32 + r15.u32);
	// lwz r23,12(r11)
	r23.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r30,r11,r31
	r30.u64 = r11.u64 + r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,652(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// rlwinm r10,r31,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r27,r10,r11
	r27.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmplwi r27,0
	cr0.compare<uint32_t>(r27.u32, 0, xer);
	// bne 0x824cc088
	if (!cr0.eq) goto loc_824CC088;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6a90
	sub_824C6A90(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x824c6650
	sub_824C6650(ctx, base);
loc_824CC088:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c5d58
	sub_824C5D58(ctx, base);
	// rlwinm r25,r3,2,0,29
	r25.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,636(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// lwzx r3,r25,r27
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + r27.u32);
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bgt cr6,0x824cbff8
	if (cr6.gt) goto loc_824CBFF8;
	// rlwinm r24,r26,2,0,29
	r24.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
loc_824CC0C0:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwzx r11,r24,r21
	r11.u64 = PPC_LOAD_U32(r24.u32 + r21.u32);
	// clrldi r10,r30,58
	ctx.r10.u64 = r30.u64 & 0x3F;
	// rlwinm r8,r30,29,3,28
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 29) & 0x1FFFFFF8;
	// ldx r11,r11,r8
	r11.u64 = PPC_LOAD_U64(r11.u32 + ctx.r8.u32);
	// sld r7,r17,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x40 ? 0 : (r17.u64 << (ctx.r10.u8 & 0x7F));
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// mr r11,r17
	r11.u64 = r17.u64;
	// bne cr6,0x824cc0f8
	if (!cr6.eq) goto loc_824CC0F8;
	// li r11,0
	r11.s64 = 0;
loc_824CC0F8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cc19c
	if (!cr0.eq) goto loc_824CC19C;
	// lwz r10,8(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// li r11,0
	r11.s64 = 0;
	// rlwinm. r6,r10,13,29,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 13) & 0x7;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq 0x824cc158
	if (cr0.eq) goto loc_824CC158;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
loc_824CC114:
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// beq cr6,0x824cc140
	if (cr6.eq) goto loc_824CC140;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// ldx r10,r10,r8
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r8.u32);
	// and r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 & ctx.r7.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// bne cr6,0x824cc138
	if (!cr6.eq) goto loc_824CC138;
	// li r10,0
	ctx.r10.s64 = 0;
loc_824CC138:
	// clrlwi. r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cc158
	if (!cr0.eq) goto loc_824CC158;
loc_824CC140:
	// lwz r10,8(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// rlwinm r10,r10,13,29,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 13) & 0x7;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824cc114
	if (cr6.lt) goto loc_824CC114;
loc_824CC158:
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// bne cr6,0x824cc1dc
	if (!cr6.eq) goto loc_824CC1DC;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c7868
	sub_824C7868(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cc1dc
	if (cr0.eq) goto loc_824CC1DC;
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6650
	sub_824C6650(ctx, base);
loc_824CC19C:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x824cc1b4
	if (!cr6.eq) goto loc_824CC1B4;
	// li r31,-1
	r31.s64 = -1;
	// b 0x824cc1c8
	goto loc_824CC1C8;
loc_824CC1B4:
	// addi r4,r31,1
	ctx.r4.s64 = r31.s64 + 1;
	// lwz r5,636(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// lwzx r3,r25,r27
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + r27.u32);
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
loc_824CC1C8:
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// ble cr6,0x824cc0c0
	if (!cr6.gt) goto loc_824CC0C0;
	// b 0x824cbff8
	goto loc_824CBFF8;
loc_824CC1DC:
	// li r28,0
	r28.s64 = 0;
	// mr r27,r19
	r27.u64 = r19.u64;
loc_824CC1E4:
	// lwz r11,8(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// rlwinm r11,r11,13,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bge cr6,0x824cc25c
	if (!cr6.lt) goto loc_824CC25C;
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplw cr6,r23,r11
	cr6.compare<uint32_t>(r23.u32, r11.u32, xer);
	// bne cr6,0x824cc250
	if (!cr6.eq) goto loc_824CC250;
	// rlwinm r5,r15,0,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 0) & 0xFFFFFFFC;
	// lwz r4,24(r15)
	ctx.r4.u64 = PPC_LOAD_U32(r15.u32 + 24);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// slw r30,r17,r28
	r30.u64 = r28.u8 & 0x20 ? 0 : (r17.u32 << (r28.u8 & 0x3F));
	// bl 0x82556538
	sub_82556538(ctx, base);
	// lwz r31,12(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// add r4,r28,r14
	ctx.r4.u64 = r28.u64 + r14.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// rlwimi r11,r17,16,13,15
	r11.u64 = (__builtin_rotateleft32(r17.u32, 16) & 0x70000) | (r11.u64 & 0xFFFFFFFFFFF8FFFF);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r11,r16
	r11.u64 = r16.u64;
	// sth r3,14(r31)
	PPC_STORE_U16(r31.u32 + 14, ctx.r3.u16);
	// mr r16,r31
	r16.u64 = r31.u64;
	// or r22,r30,r22
	r22.u64 = r30.u64 | r22.u64;
	// subf r20,r30,r20
	r20.s64 = r20.s64 - r30.s64;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
loc_824CC250:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// b 0x824cc1e4
	goto loc_824CC1E4;
loc_824CC25C:
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// stw r16,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r16.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c65d8
	sub_824C65D8(ctx, base);
	// b 0x824cbf98
	goto loc_824CBF98;
loc_824CC270:
	// addi r11,r1,104
	r11.s64 = ctx.r1.s64 + 104;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// beq cr6,0x824cc450
	if (cr6.eq) goto loc_824CC450;
	// lwz r24,104(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
loc_824CC288:
	// li r30,0
	r30.s64 = 0;
	// li r25,0
	r25.s64 = 0;
	// mr r31,r20
	r31.u64 = r20.u64;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x824cc304
	if (cr6.eq) goto loc_824CC304;
loc_824CC29C:
	// addi r11,r31,-1
	r11.s64 = r31.s64 + -1;
	// andc r28,r31,r11
	r28.u64 = r31.u64 & ~r11.u64;
	// cntlzw r11,r28
	r11.u64 = r28.u32 == 0 ? 32 : __builtin_clz(r28.u32);
	// subf r31,r28,r31
	r31.s64 = r31.s64 - r28.s64;
	// subfic r11,r11,41
	xer.ca = r11.u32 <= 41;
	r11.s64 = 41 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r15
	r11.u64 = PPC_LOAD_U32(r11.u32 + r15.u32);
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r27,r11,27,30,31
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r27
	ctx.r4.u64 = r11.u64 + r27.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,652(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// beq cr6,0x824cc2f4
	if (cr6.eq) goto loc_824CC2F4;
	// cmplw cr6,r25,r11
	cr6.compare<uint32_t>(r25.u32, r11.u32, xer);
	// bne cr6,0x824cc2fc
	if (!cr6.eq) goto loc_824CC2FC;
loc_824CC2F4:
	// or r30,r28,r30
	r30.u64 = r28.u64 | r30.u64;
	// mr r25,r11
	r25.u64 = r11.u64;
loc_824CC2FC:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824cc29c
	if (!cr6.eq) goto loc_824CC29C;
loc_824CC304:
	// subf r20,r30,r20
	r20.s64 = r20.s64 - r30.s64;
	// li r28,0
	r28.s64 = 0;
	// li r27,0
	r27.s64 = 0;
	// mr r31,r30
	r31.u64 = r30.u64;
loc_824CC314:
	// addi r11,r31,-1
	r11.s64 = r31.s64 + -1;
	// andc r11,r31,r11
	r11.u64 = r31.u64 & ~r11.u64;
	// subf r31,r11,r31
	r31.s64 = r31.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r11,r11,41
	xer.ca = r11.u32 <= 41;
	r11.s64 = 41 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r15
	r11.u64 = PPC_LOAD_U32(r11.u32 + r15.u32);
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r26,r11,27,30,31
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r26
	ctx.r4.u64 = r11.u64 + r26.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c5d58
	sub_824C5D58(ctx, base);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// slw r11,r17,r3
	r11.u64 = ctx.r3.u8 & 0x20 ? 0 : (r17.u32 << (ctx.r3.u8 & 0x3F));
	// or r27,r11,r27
	r27.u64 = r11.u64 | r27.u64;
	// bne cr6,0x824cc314
	if (!cr6.eq) goto loc_824CC314;
	// lwz r11,192(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bge cr6,0x824cc444
	if (!cr6.lt) goto loc_824CC444;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c8278
	sub_824C8278(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cc3d0
	if (cr0.eq) goto loc_824CC3D0;
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c8428
	sub_824C8428(ctx, base);
	// b 0x824cc444
	goto loc_824CC444;
loc_824CC3D0:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824cc444
	if (cr6.eq) goto loc_824CC444;
	// rlwinm r26,r15,0,0,29
	r26.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 0) & 0xFFFFFFFC;
loc_824CC3DC:
	// addi r11,r30,-1
	r11.s64 = r30.s64 + -1;
	// lwz r4,24(r15)
	ctx.r4.u64 = PPC_LOAD_U32(r15.u32 + 24);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// andc r28,r30,r11
	r28.u64 = r30.u64 & ~r11.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// cntlzw r11,r28
	r11.u64 = r28.u32 == 0 ? 32 : __builtin_clz(r28.u32);
	// subfic r27,r11,31
	xer.ca = r11.u32 <= 31;
	r27.s64 = 31 - r11.s64;
	// addi r11,r27,10
	r11.s64 = r27.s64 + 10;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r31,r11,r15
	r31.u64 = PPC_LOAD_U32(r11.u32 + r15.u32);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// bl 0x82556538
	sub_82556538(ctx, base);
	// lwz r31,12(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// add r4,r27,r14
	ctx.r4.u64 = r27.u64 + r14.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// rlwimi r11,r17,16,13,15
	r11.u64 = (__builtin_rotateleft32(r17.u32, 16) & 0x70000) | (r11.u64 & 0xFFFFFFFFFFF8FFFF);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r11,r16
	r11.u64 = r16.u64;
	// sth r3,14(r31)
	PPC_STORE_U16(r31.u32 + 14, ctx.r3.u16);
	// subf. r30,r28,r30
	r30.s64 = r30.s64 - r28.s64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// mr r16,r31
	r16.u64 = r31.u64;
	// or r22,r28,r22
	r22.u64 = r28.u64 | r22.u64;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// bne 0x824cc3dc
	if (!cr0.eq) goto loc_824CC3DC;
loc_824CC444:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x824cc288
	if (!cr6.eq) goto loc_824CC288;
	// stw r16,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r16.u32);
loc_824CC450:
	// lwz r11,192(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 192);
	// lwz r10,192(r18)
	ctx.r10.u64 = PPC_LOAD_U32(r18.u32 + 192);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x824cc4b0
	if (!cr6.lt) goto loc_824CC4B0;
loc_824CC468:
	// lwz r11,636(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r11,192(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 192);
	// rlwinm r11,r11,2,27,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0x1C;
	// stwx r3,r11,r18
	PPC_STORE_U32(r11.u32 + r18.u32, ctx.r3.u32);
	// lwz r11,192(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 192);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// rlwimi r10,r11,0,0,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFF8) | (ctx.r10.u64 & 0xFFFFFFFF00000007);
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// stw r10,192(r18)
	PPC_STORE_U32(r18.u32 + 192, ctx.r10.u32);
	// lwz r10,192(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 192);
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x824cc468
	if (cr6.lt) goto loc_824CC468;
loc_824CC4B0:
	// lwz r11,8(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 8);
	// rlwinm r11,r11,31,28,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0xF;
	// subf. r30,r22,r11
	r30.s64 = r11.s64 - r22.s64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x824cc5b4
	if (cr0.eq) goto loc_824CC5B4;
loc_824CC4C0:
	// addi r11,r30,-1
	r11.s64 = r30.s64 + -1;
	// andc r11,r30,r11
	r11.u64 = r30.u64 & ~r11.u64;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r11,r11,41
	xer.ca = r11.u32 <= 41;
	r11.s64 = 41 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r15
	r11.u64 = PPC_LOAD_U32(r11.u32 + r15.u32);
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r31,r11,27,30,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,652(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 652);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r31,r10,r11
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmplw cr6,r31,r18
	cr6.compare<uint32_t>(r31.u32, r18.u32, xer);
	// beq cr6,0x824cc5ac
	if (cr6.eq) goto loc_824CC5AC;
	// lwz r11,192(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 192);
	// li r23,0
	r23.s64 = 0;
	// clrlwi. r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cc5a0
	if (cr0.eq) goto loc_824CC5A0;
	// mr r22,r21
	r22.u64 = r21.u64;
loc_824CC524:
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// li r27,0
	r27.s64 = 0;
	// clrlwi. r24,r11,29
	r24.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r24.s32, 0, xer);
	// beq 0x824cc588
	if (cr0.eq) goto loc_824CC588;
	// lwz r26,636(r29)
	r26.u64 = PPC_LOAD_U32(r29.u32 + 636);
	// mr r28,r31
	r28.u64 = r31.u64;
	// lwz r25,0(r22)
	r25.u64 = PPC_LOAD_U32(r22.u32 + 0);
loc_824CC540:
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x824c5cf8
	sub_824C5CF8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cc56c
	if (!cr0.eq) goto loc_824CC56C;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmplw cr6,r27,r24
	cr6.compare<uint32_t>(r27.u32, r24.u32, xer);
	// blt cr6,0x824cc540
	if (cr6.lt) goto loc_824CC540;
	// b 0x824cc588
	goto loc_824CC588;
loc_824CC56C:
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c6258
	sub_824C6258(ctx, base);
loc_824CC588:
	// lwz r11,192(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 192);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r22,r22,4
	r22.s64 = r22.s64 + 4;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r23,r11
	cr6.compare<uint32_t>(r23.u32, r11.u32, xer);
	// blt cr6,0x824cc524
	if (cr6.lt) goto loc_824CC524;
loc_824CC5A0:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c65d8
	sub_824C65D8(ctx, base);
loc_824CC5AC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824cc4c0
	if (!cr6.eq) goto loc_824CC4C0;
loc_824CC5B4:
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c65d8
	sub_824C65D8(ctx, base);
loc_824CC5C0:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r10,632(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// blt cr6,0x824cb538
	if (cr6.lt) goto loc_824CB538;
loc_824CC5D8:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824cc8c4
	if (!cr6.eq) goto loc_824CC8C4;
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// rlwinm. r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cc8c4
	if (cr0.eq) goto loc_824CC8C4;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cc8c4
	if (!cr0.eq) goto loc_824CC8C4;
	// mr. r18,r11
	r18.u64 = r11.u64;
	cr0.compare<int32_t>(r18.s32, 0, xer);
	// beq 0x824cc8c4
	if (cr0.eq) goto loc_824CC8C4;
	// lwz r15,84(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r14,2
	r14.s64 = 2;
	// li r16,-1
	r16.s64 = -1;
	// li r17,0
	r17.s64 = 0;
loc_824CC614:
	// lwz r21,28(r18)
	r21.u64 = PPC_LOAD_U32(r18.u32 + 28);
	// clrlwi. r11,r21,31
	r11.u64 = r21.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cc8a4
	if (!cr0.eq) goto loc_824CC8A4;
	// cmplwi r21,0
	cr0.compare<uint32_t>(r21.u32, 0, xer);
	// beq 0x824cc8a4
	if (cr0.eq) goto loc_824CC8A4;
loc_824CC628:
	// lwz r11,8(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 8);
	// rlwinm r10,r11,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r10,13184
	cr6.compare<uint32_t>(ctx.r10.u32, 13184, xer);
	// bne cr6,0x824cc88c
	if (!cr6.eq) goto loc_824CC88C;
	// rlwinm. r11,r11,0,27,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1E;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cc88c
	if (cr0.eq) goto loc_824CC88C;
	// lwz r23,40(r21)
	r23.u64 = PPC_LOAD_U32(r21.u32 + 40);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r22,12(r23)
	r22.u64 = PPC_LOAD_U32(r23.u32 + 12);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// bl 0x824c5db0
	sub_824C5DB0(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c5db0
	sub_824C5DB0(ctx, base);
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// lwz r10,8(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 8);
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// mr r26,r17
	r26.u64 = r17.u64;
	// rlwinm r30,r10,31,28,31
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0xF;
	// mr r31,r17
	r31.u64 = r17.u64;
	// stw r17,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r17.u32);
loc_824CC680:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824cc6ac
	if (cr6.eq) goto loc_824CC6AC;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// addi r11,r30,-1
	r11.s64 = r30.s64 + -1;
	// andc r11,r30,r11
	r11.u64 = r30.u64 & ~r11.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r11,r11,31
	xer.ca = r11.u32 <= 31;
	r11.s64 = 31 - r11.s64;
	// add r28,r3,r11
	r28.u64 = ctx.r3.u64 + r11.u64;
	// mr r11,r28
	r11.u64 = r28.u64;
	// b 0x824cc6b4
	goto loc_824CC6B4;
loc_824CC6AC:
	// mr r28,r17
	r28.u64 = r17.u64;
	// mr r11,r17
	r11.u64 = r17.u64;
loc_824CC6B4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824cc77c
	if (cr6.eq) goto loc_824CC77C;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	// rlwinm r10,r10,27,24,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// clrlwi r28,r11,30
	r28.u64 = r11.u32 & 0x3;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r4,r11,r28
	ctx.r4.u64 = r11.u64 + r28.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r5,r31,29,3,29
	ctx.r5.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r27,r1,96
	r27.s64 = ctx.r1.s64 + 96;
	// bl 0x824c5d58
	sub_824C5D58(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwzx r10,r5,r27
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + r27.u32);
	// clrlwi r9,r31,27
	ctx.r9.u64 = r31.u32 & 0x1F;
	// addi r6,r31,4
	ctx.r6.s64 = r31.s64 + 4;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r28,r1,96
	r28.s64 = ctx.r1.s64 + 96;
	// slw r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r9.u8 & 0x3F));
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwx r11,r5,r27
	PPC_STORE_U32(ctx.r5.u32 + r27.u32, r11.u32);
	// rlwinm r5,r6,29,3,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFC;
	// bl 0x824c5d58
	sub_824C5D58(ctx, base);
	// clrlwi r10,r6,27
	ctx.r10.u64 = ctx.r6.u32 & 0x1F;
	// lwzx r11,r5,r28
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + r28.u32);
	// addi r9,r30,-1
	ctx.r9.s64 = r30.s64 + -1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// andc r9,r30,r9
	ctx.r9.u64 = r30.u64 & ~ctx.r9.u64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// subf r30,r9,r30
	r30.s64 = r30.s64 - ctx.r9.s64;
	// slw r10,r3,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r3.u32 << (ctx.r10.u8 & 0x3F));
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stwx r11,r5,r28
	PPC_STORE_U32(ctx.r5.u32 + r28.u32, r11.u32);
	// b 0x824cc680
	goto loc_824CC680;
loc_824CC77C:
	// mr r30,r17
	r30.u64 = r17.u64;
	// cmplw cr6,r20,r19
	cr6.compare<uint32_t>(r20.u32, r19.u32, xer);
	// bne cr6,0x824cc818
	if (!cr6.eq) goto loc_824CC818;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824cc870
	if (cr6.eq) goto loc_824CC870;
	// li r11,4
	r11.s64 = 4;
loc_824CC798:
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// clrlwi r8,r8,27
	ctx.r8.u64 = ctx.r8.u32 & 0x1F;
	// clrlwi r5,r7,27
	ctx.r5.u64 = ctx.r7.u32 & 0x1F;
	// rlwinm r7,r11,29,3,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// clrlwi r9,r11,27
	ctx.r9.u64 = r11.u32 & 0x1F;
	// addi r10,r11,-4
	ctx.r10.s64 = r11.s64 + -4;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwzx r4,r7,r4
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r4.u32);
	// slw r7,r14,r8
	ctx.r7.u64 = ctx.r8.u8 & 0x20 ? 0 : (r14.u32 << (ctx.r8.u8 & 0x3F));
	// slw r8,r14,r5
	ctx.r8.u64 = ctx.r5.u8 & 0x20 ? 0 : (r14.u32 << (ctx.r5.u8 & 0x3F));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r5,r8,-1
	ctx.r5.s64 = ctx.r8.s64 + -1;
	// and r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 & ctx.r4.u64;
	// clrlwi r8,r10,27
	ctx.r8.u64 = ctx.r10.u32 & 0x1F;
	// slw r4,r16,r9
	ctx.r4.u64 = ctx.r9.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r9.u8 & 0x3F));
	// rlwinm r10,r10,29,3,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFC;
	// and r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 & ctx.r4.u64;
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// and r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 & ctx.r10.u64;
	// srw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r9.u8 & 0x3F));
	// slw r7,r16,r8
	ctx.r7.u64 = ctx.r8.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r8.u8 & 0x3F));
	// and r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 & ctx.r7.u64;
	// srw r10,r10,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r8.u8 & 0x3F));
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x824cc86c
	if (!cr6.eq) goto loc_824CC86C;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r6,r26
	cr6.compare<uint32_t>(ctx.r6.u32, r26.u32, xer);
	// blt cr6,0x824cc798
	if (cr6.lt) goto loc_824CC798;
	// b 0x824cc870
	goto loc_824CC870;
loc_824CC818:
	// lwz r31,96(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c8278
	sub_824C8278(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cc86c
	if (cr0.eq) goto loc_824CC86C;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c8428
	sub_824C8428(ctx, base);
	// b 0x824cc870
	goto loc_824CC870;
loc_824CC86C:
	// li r30,1
	r30.s64 = 1;
loc_824CC870:
	// clrlwi. r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cc88c
	if (cr0.eq) goto loc_824CC88C;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824c5ff8
	sub_824C5FF8(ctx, base);
	// stw r15,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r15.u32);
	// mr r15,r3
	r15.u64 = ctx.r3.u64;
loc_824CC88C:
	// rlwinm r11,r21,0,0,30
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r21,36(r11)
	r21.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r21,31
	r11.u64 = r21.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cc8a4
	if (!cr0.eq) goto loc_824CC8A4;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// bne cr6,0x824cc628
	if (!cr6.eq) goto loc_824CC628;
loc_824CC8A4:
	// rlwinm r11,r18,0,0,30
	r11.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cc8cc
	if (!cr0.eq) goto loc_824CC8CC;
	// mr r18,r11
	r18.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824cc614
	if (!cr6.eq) goto loc_824CC614;
	// b 0x824cc8cc
	goto loc_824CC8CC;
loc_824CC8C4:
	// lwz r15,84(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r17,0
	r17.s64 = 0;
loc_824CC8CC:
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// bne cr6,0x824cc94c
	if (!cr6.eq) goto loc_824CC94C;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824cc94c
	if (!cr6.eq) goto loc_824CC94C;
	// lwz r31,92(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824cc950
	if (!cr6.eq) goto loc_824CC950;
	// lbz r11,80(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824cc950
	if (!cr0.eq) goto loc_824CC950;
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,640(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 640);
	// mulli r5,r11,40
	ctx.r5.s64 = r11.s64 * 40;
	// bl 0x824b1f78
	sub_824B1F78(ctx, base);
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r4,648(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 648);
	// stw r17,640(r29)
	PPC_STORE_U32(r29.u32 + 640, r17.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b1f78
	sub_824B1F78(ctx, base);
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r10,372(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// stw r17,648(r29)
	PPC_STORE_U32(r29.u32 + 648, r17.u32);
	// lbz r3,81(r1)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd10
	return;
loc_824CC94C:
	// lwz r31,92(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_824CC950:
	// li r11,1
	r11.s64 = 1;
	// stw r17,644(r29)
	PPC_STORE_U32(r29.u32 + 644, r17.u32);
	// stw r17,648(r29)
	PPC_STORE_U32(r29.u32 + 648, r17.u32);
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// stw r17,660(r29)
	PPC_STORE_U32(r29.u32 + 660, r17.u32);
	// stw r17,664(r29)
	PPC_STORE_U32(r29.u32 + 664, r17.u32);
	// stw r17,668(r29)
	PPC_STORE_U32(r29.u32 + 668, r17.u32);
	// stb r11,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, r11.u8);
	// stw r17,672(r29)
	PPC_STORE_U32(r29.u32 + 672, r17.u32);
	// beq cr6,0x824cc99c
	if (cr6.eq) goto loc_824CC99C;
loc_824CC978:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// oris r11,r11,8
	r11.u64 = r11.u64 | 524288;
	// stw r11,12(r15)
	PPC_STORE_U32(r15.u32 + 12, r11.u32);
	// bl 0x824f0118
	sub_824F0118(ctx, base);
	// lwz r15,28(r15)
	r15.u64 = PPC_LOAD_U32(r15.u32 + 28);
	// cmplwi r15,0
	cr0.compare<uint32_t>(r15.u32, 0, xer);
	// bne 0x824cc978
	if (!cr0.eq) goto loc_824CC978;
loc_824CC99C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824cc9c0
	if (cr6.eq) goto loc_824CC9C0;
loc_824CC9A4:
	// lwz r11,112(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 112);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,112(r29)
	PPC_STORE_U32(r29.u32 + 112, r11.u32);
	// sth r11,14(r31)
	PPC_STORE_U16(r31.u32 + 14, r11.u16);
	// lwz r31,28(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x824cc9a4
	if (!cr0.eq) goto loc_824CC9A4;
loc_824CC9C0:
	// addi r3,r29,924
	ctx.r3.s64 = r29.s64 + 924;
	// bl 0x82497058
	sub_82497058(ctx, base);
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r10,380(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// ble cr6,0x824c86f4
	if (!cr6.gt) goto loc_824C86F4;
loc_824CC9E0:
	// lwz r11,36(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 36);
	// rlwinm. r11,r11,0,21,21
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x400;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cca08
	if (cr0.eq) goto loc_824CCA08;
	// lwz r11,48(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 48);
	// rlwinm. r10,r11,0,23,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x100;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cca08
	if (!cr0.eq) goto loc_824CCA08;
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,48(r29)
	PPC_STORE_U32(r29.u32 + 48, r11.u32);
	// bl 0x82496b70
	sub_82496B70(ctx, base);
loc_824CCA08:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CCA14:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CCA20:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CCA28:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CCA34:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CCA40:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CCA4C:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
}

__attribute__((alias("__imp__sub_824CCA58"))) PPC_WEAK_FUNC(sub_824CCA58);
PPC_FUNC_IMPL(__imp__sub_824CCA58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bccc
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r6,100
	ctx.r6.s64 = 100;
	// addi r5,r1,100
	ctx.r5.s64 = ctx.r1.s64 + 100;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// bl 0x824c86b8
	sub_824C86B8(ctx, base);
	// lwz r11,664(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 664);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824cd018
	if (cr6.eq) goto loc_824CD018;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824bf048
	sub_824BF048(ctx, base);
	// li r17,0
	r17.s64 = 0;
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccae0
	if (cr0.eq) goto loc_824CCAE0;
	// addi r3,r28,924
	ctx.r3.s64 = r28.s64 + 924;
	// stw r17,644(r28)
	PPC_STORE_U32(r28.u32 + 644, r17.u32);
	// stw r17,648(r28)
	PPC_STORE_U32(r28.u32 + 648, r17.u32);
	// stw r17,660(r28)
	PPC_STORE_U32(r28.u32 + 660, r17.u32);
	// stw r17,664(r28)
	PPC_STORE_U32(r28.u32 + 664, r17.u32);
	// stw r17,668(r28)
	PPC_STORE_U32(r28.u32 + 668, r17.u32);
	// stw r17,672(r28)
	PPC_STORE_U32(r28.u32 + 672, r17.u32);
	// bl 0x82497058
	sub_82497058(ctx, base);
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r1,100
	ctx.r5.s64 = ctx.r1.s64 + 100;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c86b8
	sub_824C86B8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccae0
	if (cr0.eq) goto loc_824CCAE0;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CCAE0:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824ccaf0
	if (cr0.eq) goto loc_824CCAF0;
	// mr r11,r17
	r11.u64 = r17.u64;
loc_824CCAF0:
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ccbb0
	if (!cr0.eq) goto loc_824CCBB0;
	// mr. r31,r11
	r31.u64 = r11.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824ccbb0
	if (cr0.eq) goto loc_824CCBB0;
loc_824CCB04:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14336
	r11.s64 = r11.s64 + -14336;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccb94
	if (cr0.eq) goto loc_824CCB94;
	// lwz r9,532(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 532);
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824ccb60
	if (cr0.eq) goto loc_824CCB60;
	// addi r11,r28,348
	r11.s64 = r28.s64 + 348;
loc_824CCB30:
	// lwz r8,-4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// clrlwi r8,r8,27
	ctx.r8.u64 = ctx.r8.u32 & 0x1F;
	// cmplwi cr6,r8,17
	cr6.compare<uint32_t>(ctx.r8.u32, 17, xer);
	// bne cr6,0x824ccb4c
	if (!cr6.eq) goto loc_824CCB4C;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r31,r8
	cr6.compare<uint32_t>(r31.u32, ctx.r8.u32, xer);
	// beq cr6,0x824ccb60
	if (cr6.eq) goto loc_824CCB60;
loc_824CCB4C:
	// lwz r8,532(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 532);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// blt cr6,0x824ccb30
	if (cr6.lt) goto loc_824CCB30;
loc_824CCB60:
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x824ccb94
	if (cr6.lt) goto loc_824CCB94;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba1a0
	sub_824BA1A0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,652(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 652);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r10,192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 192);
	// oris r10,r10,8192
	ctx.r10.u64 = ctx.r10.u64 | 536870912;
	// stw r10,192(r11)
	PPC_STORE_U32(r11.u32 + 192, ctx.r10.u32);
loc_824CCB94:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ccbb0
	if (!cr0.eq) goto loc_824CCBB0;
	// mr r31,r11
	r31.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824ccb04
	if (!cr6.eq) goto loc_824CCB04;
loc_824CCBB0:
	// lwz r11,40(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// rlwinm. r10,r11,0,27,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824ccbc8
	if (cr0.eq) goto loc_824CCBC8;
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// beq 0x824ccbcc
	if (cr0.eq) goto loc_824CCBCC;
loc_824CCBC8:
	// mr r11,r17
	r11.u64 = r17.u64;
loc_824CCBCC:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccce4
	if (cr0.eq) goto loc_824CCCE4;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824b3250
	sub_824B3250(ctx, base);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ccce4
	if (!cr0.eq) goto loc_824CCCE4;
	// mr. r29,r11
	r29.u64 = r11.u64;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// beq 0x824ccce4
	if (cr0.eq) goto loc_824CCCE4;
loc_824CCBF0:
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccc18
	if (cr0.eq) goto loc_824CCC18;
	// lwz r11,28(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccc10
	if (cr0.eq) goto loc_824CCC10;
	// addi r30,r29,32
	r30.s64 = r29.s64 + 32;
	// b 0x824ccc1c
	goto loc_824CCC1C;
loc_824CCC10:
	// addi r30,r29,24
	r30.s64 = r29.s64 + 24;
	// b 0x824ccc1c
	goto loc_824CCC1C;
loc_824CCC18:
	// addi r30,r29,16
	r30.s64 = r29.s64 + 16;
loc_824CCC1C:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cccc0
	if (cr0.eq) goto loc_824CCCC0;
	// mr r31,r17
	r31.u64 = r17.u64;
	// b 0x824cccc0
	goto loc_824CCCC0;
loc_824CCC30:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm. r10,r11,6,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824ccc70
	if (cr0.eq) goto loc_824CCC70;
	// rlwinm. r10,r11,9,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824ccc64
	if (cr0.eq) goto loc_824CCC64;
	// rlwinm. r11,r11,0,27,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1E;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccc64
	if (cr0.eq) goto loc_824CCC64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c5db0
	sub_824C5DB0(ctx, base);
	// lis r11,4095
	r11.s64 = 268369920;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// stw r11,196(r3)
	PPC_STORE_U32(ctx.r3.u32 + 196, r11.u32);
loc_824CCC64:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,6,4
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFBFFFFFF;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_824CCC70:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824ccc88
	if (!cr0.eq) goto loc_824CCC88;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824ccc30
	if (!cr6.eq) goto loc_824CCC30;
loc_824CCC88:
	// addi r10,r29,32
	ctx.r10.s64 = r29.s64 + 32;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x824cccc8
	if (cr6.eq) goto loc_824CCCC8;
	// addi r11,r29,24
	r11.s64 = r29.s64 + 24;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x824cccb0
	if (cr6.eq) goto loc_824CCCB0;
	// lwz r9,28(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// mr r30,r11
	r30.u64 = r11.u64;
	// clrlwi. r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824cccb4
	if (cr0.eq) goto loc_824CCCB4;
loc_824CCCB0:
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
loc_824CCCB4:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cccc8
	if (!cr0.eq) goto loc_824CCCC8;
loc_824CCCC0:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824ccc30
	if (!cr6.eq) goto loc_824CCC30;
loc_824CCCC8:
	// rlwinm r11,r29,0,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824ccce4
	if (!cr0.eq) goto loc_824CCCE4;
	// mr r29,r11
	r29.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824ccbf0
	if (!cr6.eq) goto loc_824CCBF0;
loc_824CCCE4:
	// lwz r11,664(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 664);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// lwz r31,660(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 660);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824ccd88
	if (cr0.eq) goto loc_824CCD88;
	// mr r25,r24
	r25.u64 = r24.u64;
loc_824CCD0C:
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// clrlwi. r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccd7c
	if (cr0.eq) goto loc_824CCD7C;
	// stw r31,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r31.u32);
	// mr r29,r17
	r29.u64 = r17.u64;
	// lwz r30,192(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// rlwinm. r11,r30,0,1,1
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccd5c
	if (cr0.eq) goto loc_824CCD5C;
	// clrlwi. r11,r30,29
	r11.u64 = r30.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccd5c
	if (cr0.eq) goto loc_824CCD5C;
	// rotlwi r11,r30,0
	r11.u64 = __builtin_rotateleft32(r30.u32, 0);
	// addi r26,r31,32
	r26.s64 = r31.s64 + 32;
	// clrlwi r27,r11,29
	r27.u64 = r11.u32 & 0x7;
loc_824CCD44:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x824c6208
	sub_824C6208(ctx, base);
	// addic. r27,r27,-1
	xer.ca = r27.u32 > 0;
	r27.s64 = r27.s64 + -1;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// addi r26,r26,40
	r26.s64 = r26.s64 + 40;
	// add r29,r3,r29
	r29.u64 = ctx.r3.u64 + r29.u64;
	// bne 0x824ccd44
	if (!cr0.eq) goto loc_824CCD44;
loc_824CCD5C:
	// clrlwi r11,r30,29
	r11.u64 = r30.u32 & 0x7;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x824ccd74
	if (!cr6.lt) goto loc_824CCD74;
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// twllei r11,0
	// divwu r29,r10,r11
	r29.u32 = ctx.r10.u32 / r11.u32;
loc_824CCD74:
	// rlwimi r30,r29,3,18,28
	r30.u64 = (__builtin_rotateleft32(r29.u32, 3) & 0x3FF8) | (r30.u64 & 0xFFFFFFFFFFFFC007);
	// stw r30,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r30.u32);
loc_824CCD7C:
	// lwz r31,200(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x824ccd0c
	if (!cr0.eq) goto loc_824CCD0C;
loc_824CCD88:
	// lis r11,-32180
	r11.s64 = -2108948480;
	// lwz r4,664(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 664);
	// li r5,4
	ctx.r5.s64 = 4;
	// addi r6,r11,23344
	ctx.r6.s64 = r11.s64 + 23344;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x823a0760
	sub_823A0760(ctx, base);
	// addi r11,r1,120
	r11.s64 = ctx.r1.s64 + 120;
	// stw r28,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r28.u32);
	// lwz r10,744(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 744);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// std r17,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r17.u64);
	// std r17,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r17.u64);
	// std r17,16(r11)
	PPC_STORE_U64(r11.u32 + 16, r17.u64);
	// std r17,24(r11)
	PPC_STORE_U64(r11.u32 + 24, r17.u64);
	// bne 0x824ccdc8
	if (!cr0.eq) goto loc_824CCDC8;
	// li r10,32
	ctx.r10.s64 = 32;
loc_824CCDC8:
	// cmplwi cr6,r10,64
	cr6.compare<uint32_t>(ctx.r10.u32, 64, xer);
	// bge cr6,0x824ccde0
	if (!cr6.lt) goto loc_824CCDE0;
	// subfic r5,r10,64
	xer.ca = ctx.r10.u32 <= 64;
	ctx.r5.s64 = 64 - ctx.r10.s64;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x824b20b0
	sub_824B20B0(ctx, base);
loc_824CCDE0:
	// lwz r11,664(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 664);
	// mr r18,r17
	r18.u64 = r17.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x824ccfc4
	if (!cr6.gt) goto loc_824CCFC4;
	// mr r19,r24
	r19.u64 = r24.u64;
	// li r23,2
	r23.s64 = 2;
	// li r21,-1
	r21.s64 = -1;
loc_824CCDFC:
	// addi r11,r1,104
	r11.s64 = ctx.r1.s64 + 104;
	// lwz r26,0(r19)
	r26.u64 = PPC_LOAD_U32(r19.u32 + 0);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r17,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r17.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r11,192(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 192);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// clrlwi r7,r11,29
	ctx.r7.u64 = r11.u32 & 0x7;
	// lwz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// bl 0x824c7550
	sub_824C7550(ctx, base);
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// cmpwi cr6,r22,-1
	cr6.compare<int32_t>(r22.s32, -1, xer);
	// beq cr6,0x824cd020
	if (cr6.eq) goto loc_824CD020;
	// lwz r11,192(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 192);
	// mr r20,r17
	r20.u64 = r17.u64;
	// clrlwi. r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ccf88
	if (cr0.eq) goto loc_824CCF88;
	// rlwinm r24,r22,0,0,27
	r24.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0xFFFFFFF0;
	// mr r29,r17
	r29.u64 = r17.u64;
	// addi r25,r26,16
	r25.s64 = r26.s64 + 16;
loc_824CCE60:
	// addi r10,r29,1
	ctx.r10.s64 = r29.s64 + 1;
	// rlwinm r9,r29,29,3,29
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r10,r10,27
	ctx.r10.u64 = ctx.r10.u32 & 0x1F;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// clrlwi r11,r29,27
	r11.u64 = r29.u32 & 0x1F;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwzx r9,r9,r8
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	// slw r10,r23,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r10.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// slw r9,r21,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (r21.u32 << (r11.u8 & 0x3F));
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// srw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// or r27,r11,r24
	r27.u64 = r11.u64 | r24.u64;
	// b 0x824ccf4c
	goto loc_824CCF4C;
loc_824CCEA8:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c58f0
	sub_824C58F0(ctx, base);
	// lwz r11,652(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 652);
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r31,r10,r11
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// rlwinm. r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824ccf30
	if (!cr0.eq) goto loc_824CCF30;
	// cmplw cr6,r31,r26
	cr6.compare<uint32_t>(r31.u32, r26.u32, xer);
	// beq cr6,0x824ccf30
	if (cr6.eq) goto loc_824CCF30;
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,656(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 656);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r10,r11,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFC0;
	// subf r9,r11,r11
	ctx.r9.s64 = r11.s64 - r11.s64;
	// rlwinm r7,r10,29,3,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFF8;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// ldx r10,r7,r8
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r7.u32 + ctx.r8.u32);
	// slw r9,r23,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r9.u8 & 0x3F));
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// and r11,r11,r9
	r11.u64 = r11.u64 & ctx.r9.u64;
	// mulli r11,r11,40
	r11.s64 = r11.s64 * 40;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// addi r3,r11,32
	ctx.r3.s64 = r11.s64 + 32;
	// bl 0x824b2278
	sub_824B2278(ctx, base);
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// oris r11,r11,16384
	r11.u64 = r11.u64 | 1073741824;
	// stw r11,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r11.u32);
loc_824CCF30:
	// lwz r11,632(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 632);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x824ccf48
	if (!cr6.eq) goto loc_824CCF48;
	// mr r30,r21
	r30.u64 = r21.u64;
	// b 0x824ccf5c
	goto loc_824CCF5C;
loc_824CCF48:
	// addi r4,r30,1
	ctx.r4.s64 = r30.s64 + 1;
loc_824CCF4C:
	// lwz r5,636(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 636);
	// lwz r3,0(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// bl 0x824b1bd8
	sub_824B1BD8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_824CCF5C:
	// lwz r11,632(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 632);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x824ccea8
	if (!cr6.gt) goto loc_824CCEA8;
	// lwz r11,192(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 192);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// cmplw cr6,r20,r11
	cr6.compare<uint32_t>(r20.u32, r11.u32, xer);
	// blt cr6,0x824cce60
	if (cr6.lt) goto loc_824CCE60;
loc_824CCF88:
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// li r10,16384
	ctx.r10.s64 = 16384;
	// lwz r11,192(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 192);
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// rlwimi r10,r22,4,18,23
	ctx.r10.u64 = (__builtin_rotateleft32(r22.u32, 4) & 0x3F00) | (ctx.r10.u64 & 0xFFFFFFFFFFFFC0FF);
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// rlwinm r11,r11,0,18,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFF0003FFF;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// addi r19,r19,4
	r19.s64 = r19.s64 + 4;
	// rlwinm r10,r10,14,0,17
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 14) & 0xFFFFC000;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r11,192(r26)
	PPC_STORE_U32(r26.u32 + 192, r11.u32);
	// lwz r11,664(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 664);
	// cmplw cr6,r18,r11
	cr6.compare<uint32_t>(r18.u32, r11.u32, xer);
	// blt cr6,0x824ccdfc
	if (cr6.lt) goto loc_824CCDFC;
loc_824CCFC4:
	// lwz r10,632(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 632);
	// mr r11,r17
	r11.u64 = r17.u64;
	// li r30,1
	r30.s64 = 1;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x824cd018
	if (!cr6.gt) goto loc_824CD018;
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r29,r10,4
	r29.s64 = ctx.r10.s64 + 4;
loc_824CCFE0:
	// lwz r31,0(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824cd018
	if (cr0.eq) goto loc_824CD018;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x824cd004
	if (cr6.eq) goto loc_824CD004;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824c6088
	sub_824C6088(ctx, base);
	// mr r11,r31
	r11.u64 = r31.u64;
loc_824CD004:
	// lwz r10,632(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 632);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// blt cr6,0x824ccfe0
	if (cr6.lt) goto loc_824CCFE0;
loc_824CD018:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd1c
	return;
loc_824CD020:
	// lwz r11,36(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// rlwinm. r11,r11,0,21,21
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x400;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cd048
	if (cr0.eq) goto loc_824CD048;
	// lwz r11,48(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 48);
	// rlwinm. r10,r11,0,23,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x100;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cd048
	if (!cr0.eq) goto loc_824CD048;
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r11,48(r28)
	PPC_STORE_U32(r28.u32 + 48, r11.u32);
	// bl 0x82496b70
	sub_82496B70(ctx, base);
loc_824CD048:
	// li r4,3562
	ctx.r4.s64 = 3562;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
}

__attribute__((alias("__imp__sub_824CD054"))) PPC_WEAK_FUNC(sub_824CD054);
PPC_FUNC_IMPL(__imp__sub_824CD054) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CD058"))) PPC_WEAK_FUNC(sub_824CD058);
PPC_FUNC_IMPL(__imp__sub_824CD058) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r10,r11,25,25,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// blt cr6,0x824cd074
	if (cr6.lt) goto loc_824CD074;
	// cmplwi cr6,r10,101
	cr6.compare<uint32_t>(ctx.r10.u32, 101, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824cd078
	if (!cr6.gt) goto loc_824CD078;
loc_824CD074:
	// li r11,0
	r11.s64 = 0;
loc_824CD078:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cd088
	if (!cr0.eq) goto loc_824CD088;
	// li r3,7
	ctx.r3.s64 = 7;
	// blr 
	return;
loc_824CD088:
	// cmplwi cr6,r10,95
	cr6.compare<uint32_t>(ctx.r10.u32, 95, xer);
	// blt cr6,0x824cd09c
	if (cr6.lt) goto loc_824CD09C;
	// cmplwi cr6,r10,101
	cr6.compare<uint32_t>(ctx.r10.u32, 101, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824cd0a0
	if (!cr6.gt) goto loc_824CD0A0;
loc_824CD09C:
	// li r11,0
	r11.s64 = 0;
loc_824CD0A0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cd0c8
	if (cr0.eq) goto loc_824CD0C8;
	// addi r11,r10,-95
	r11.s64 = ctx.r10.s64 + -95;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// addi r3,r11,3
	ctx.r3.s64 = r11.s64 + 3;
	// blr 
	return;
loc_824CD0C8:
	// cmplwi cr6,r10,82
	cr6.compare<uint32_t>(ctx.r10.u32, 82, xer);
	// blt cr6,0x824cd0dc
	if (cr6.lt) goto loc_824CD0DC;
	// cmplwi cr6,r10,94
	cr6.compare<uint32_t>(ctx.r10.u32, 94, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824cd0e0
	if (!cr6.gt) goto loc_824CD0E0;
loc_824CD0DC:
	// li r11,0
	r11.s64 = 0;
loc_824CD0E0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cd0f8
	if (cr0.eq) goto loc_824CD0F8;
	// addi r11,r10,-83
	r11.s64 = ctx.r10.s64 + -83;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// blr 
	return;
loc_824CD0F8:
	// li r3,2
	ctx.r3.s64 = 2;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD100"))) PPC_WEAK_FUNC(sub_824CD100);
PPC_FUNC_IMPL(__imp__sub_824CD100) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// lwz r11,756(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 756);
	// addi r10,r11,0
	ctx.r10.s64 = r11.s64 + 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824cd164
	if (cr0.eq) goto loc_824CD164;
	// bne cr6,0x824cd158
	if (!cr6.eq) goto loc_824CD158;
	// lwz r11,44(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// lis r10,-1
	ctx.r10.s64 = -65536;
	// rlwinm r11,r11,0,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF0000;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,28,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xC;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_824CD158:
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// li r4,3567
	ctx.r4.s64 = 3567;
	// b 0x82496e98
	sub_82496E98(ctx, base);
	return;
loc_824CD164:
	// bne cr6,0x824cd19c
	if (!cr6.eq) goto loc_824CD19C;
	// lwz r11,44(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// lis r10,-1
	ctx.r10.s64 = -65536;
	// rlwinm r11,r11,0,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF0000;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,28,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xC;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_824CD19C:
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// li r4,3568
	ctx.r4.s64 = 3568;
	// b 0x82496e98
	sub_82496E98(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_824CD1A8"))) PPC_WEAK_FUNC(sub_824CD1A8);
PPC_FUNC_IMPL(__imp__sub_824CD1A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// rlwinm r10,r4,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwzx r11,r10,r3
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// rlwimi r11,r5,4,14,27
	r11.u64 = (__builtin_rotateleft32(ctx.r5.u32, 4) & 0x3FFF0) | (r11.u64 & 0xFFFFFFFFFFFC000F);
	// rlwimi r11,r6,0,28,31
	r11.u64 = (__builtin_rotateleft32(ctx.r6.u32, 0) & 0xF) | (r11.u64 & 0xFFFFFFFFFFFFFFF0);
	// stwx r11,r10,r3
	PPC_STORE_U32(ctx.r10.u32 + ctx.r3.u32, r11.u32);
	// bne cr6,0x824cd1d0
	if (!cr6.eq) goto loc_824CD1D0;
	// li r9,1
	ctx.r9.s64 = 1;
	// rlwimi r11,r9,18,12,13
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 18) & 0xC0000) | (r11.u64 & 0xFFFFFFFFFFF3FFFF);
	// b 0x824cd21c
	goto loc_824CD21C;
loc_824CD1D0:
	// ble cr6,0x824cd1e0
	if (!cr6.gt) goto loc_824CD1E0;
	// cmpwi cr6,r7,9
	cr6.compare<int32_t>(ctx.r7.s32, 9, xer);
	// li r9,1
	ctx.r9.s64 = 1;
	// ble cr6,0x824cd1e4
	if (!cr6.gt) goto loc_824CD1E4;
loc_824CD1E0:
	// li r9,0
	ctx.r9.s64 = 0;
loc_824CD1E4:
	// clrlwi. r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824cd220
	if (cr0.eq) goto loc_824CD220;
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// blt cr6,0x824cd200
	if (cr6.lt) goto loc_824CD200;
	// cmpwi cr6,r7,5
	cr6.compare<int32_t>(ctx.r7.s32, 5, xer);
	// li r9,1
	ctx.r9.s64 = 1;
	// ble cr6,0x824cd204
	if (!cr6.gt) goto loc_824CD204;
loc_824CD200:
	// li r9,0
	ctx.r9.s64 = 0;
loc_824CD204:
	// clrlwi. r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824cd218
	if (cr0.eq) goto loc_824CD218;
	// li r9,1
	ctx.r9.s64 = 1;
	// rlwimi r11,r9,19,12,13
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 19) & 0xC0000) | (r11.u64 & 0xFFFFFFFFFFF3FFFF);
	// b 0x824cd21c
	goto loc_824CD21C;
loc_824CD218:
	// oris r11,r11,12
	r11.u64 = r11.u64 | 786432;
loc_824CD21C:
	// stwx r11,r10,r3
	PPC_STORE_U32(ctx.r10.u32 + ctx.r3.u32, r11.u32);
loc_824CD220:
	// rlwinm r11,r8,11,13,20
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 11) & 0x7F800;
	// lwzx r9,r10,r3
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// clrlwi r8,r7,26
	ctx.r8.u64 = ctx.r7.u32 & 0x3F;
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// rlwinm r9,r9,0,12,5
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFC0FFFFF;
	// rlwinm r11,r11,20,0,11
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 20) & 0xFFF00000;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwx r11,r10,r3
	PPC_STORE_U32(ctx.r10.u32 + ctx.r3.u32, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD248"))) PPC_WEAK_FUNC(sub_824CD248);
PPC_FUNC_IMPL(__imp__sub_824CD248) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r9,24(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r11,98
	cr6.compare<int32_t>(r11.s32, 98, xer);
	// bgt cr6,0x824cd2e8
	if (cr6.gt) goto loc_824CD2E8;
	// beq cr6,0x824cd2a0
	if (cr6.eq) goto loc_824CD2A0;
	// cmpwi cr6,r11,66
	cr6.compare<int32_t>(r11.s32, 66, xer);
	// beq cr6,0x824cd2a0
	if (cr6.eq) goto loc_824CD2A0;
	// cmpwi cr6,r11,67
	cr6.compare<int32_t>(r11.s32, 67, xer);
	// beq cr6,0x824cd418
	if (cr6.eq) goto loc_824CD418;
	// cmpwi cr6,r11,73
	cr6.compare<int32_t>(r11.s32, 73, xer);
	// beq cr6,0x824cd3cc
	if (cr6.eq) goto loc_824CD3CC;
	// cmpwi cr6,r11,83
	cr6.compare<int32_t>(r11.s32, 83, xer);
	// beq cr6,0x824cd370
	if (cr6.eq) goto loc_824CD370;
	// cmpwi cr6,r11,86
	cr6.compare<int32_t>(r11.s32, 86, xer);
	// b 0x824cd304
	goto loc_824CD304;
loc_824CD2A0:
	// cmpwi cr6,r29,7
	cr6.compare<int32_t>(r29.s32, 7, xer);
	// beq cr6,0x824cd2bc
	if (cr6.eq) goto loc_824CD2BC;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x824cd2bc
	if (cr6.eq) goto loc_824CD2BC;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// li r4,4509
	ctx.r4.s64 = 4509;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CD2BC:
	// lwz r11,44(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// lis r10,-1
	ctx.r10.s64 = -65536;
	// li r29,0
	r29.s64 = 0;
	// rlwinm r11,r11,0,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF0000;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r31,r11,2,24,24
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0x80;
	// b 0x824cd43c
	goto loc_824CD43C;
loc_824CD2E8:
	// cmpwi cr6,r11,99
	cr6.compare<int32_t>(r11.s32, 99, xer);
	// beq cr6,0x824cd418
	if (cr6.eq) goto loc_824CD418;
	// cmpwi cr6,r11,105
	cr6.compare<int32_t>(r11.s32, 105, xer);
	// beq cr6,0x824cd3cc
	if (cr6.eq) goto loc_824CD3CC;
	// cmpwi cr6,r11,115
	cr6.compare<int32_t>(r11.s32, 115, xer);
	// beq cr6,0x824cd370
	if (cr6.eq) goto loc_824CD370;
	// cmpwi cr6,r11,118
	cr6.compare<int32_t>(r11.s32, 118, xer);
loc_824CD304:
	// beq cr6,0x824cd328
	if (cr6.eq) goto loc_824CD328;
	// cmpwi cr6,r29,7
	cr6.compare<int32_t>(r29.s32, 7, xer);
	// beq cr6,0x824cd31c
	if (cr6.eq) goto loc_824CD31C;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// li r4,4509
	ctx.r4.s64 = 4509;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CD31C:
	// li r31,256
	r31.s64 = 256;
	// li r30,-1
	r30.s64 = -1;
	// b 0x824cd440
	goto loc_824CD440;
loc_824CD328:
	// cmpwi cr6,r29,3
	cr6.compare<int32_t>(r29.s32, 3, xer);
	// bne cr6,0x824cd364
	if (!cr6.eq) goto loc_824CD364;
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// cmpwi cr6,r10,102
	cr6.compare<int32_t>(ctx.r10.s32, 102, xer);
	// beq cr6,0x824cd34c
	if (cr6.eq) goto loc_824CD34C;
	// cmpwi cr6,r10,70
	cr6.compare<int32_t>(ctx.r10.s32, 70, xer);
	// bne cr6,0x824cd364
	if (!cr6.eq) goto loc_824CD364;
loc_824CD34C:
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// li r31,0
	r31.s64 = 0;
	// li r30,96
	r30.s64 = 96;
	// stb r10,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r10.u8);
	// b 0x824cd440
	goto loc_824CD440;
loc_824CD364:
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// li r4,4509
	ctx.r4.s64 = 4509;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CD370:
	// cmpwi cr6,r29,3
	cr6.compare<int32_t>(r29.s32, 3, xer);
	// beq cr6,0x824cd384
	if (cr6.eq) goto loc_824CD384;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// li r4,4509
	ctx.r4.s64 = 4509;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CD384:
	// lwz r11,756(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 756);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824cd398
	if (cr6.eq) goto loc_824CD398;
	// lwz r31,752(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 752);
	// b 0x824cd3c4
	goto loc_824CD3C4;
loc_824CD398:
	// lwz r11,44(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// lis r10,-1
	ctx.r10.s64 = -65536;
	// rlwinm r11,r11,0,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF0000;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r31,r11,31,27,27
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x10;
loc_824CD3C4:
	// li r30,32
	r30.s64 = 32;
	// b 0x824cd440
	goto loc_824CD440;
loc_824CD3CC:
	// cmpwi cr6,r29,7
	cr6.compare<int32_t>(r29.s32, 7, xer);
	// beq cr6,0x824cd3e8
	if (cr6.eq) goto loc_824CD3E8;
	// cmpwi cr6,r29,1
	cr6.compare<int32_t>(r29.s32, 1, xer);
	// beq cr6,0x824cd3e8
	if (cr6.eq) goto loc_824CD3E8;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// li r4,4509
	ctx.r4.s64 = 4509;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CD3E8:
	// lwz r11,44(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// lis r10,-1
	ctx.r10.s64 = -65536;
	// li r29,1
	r29.s64 = 1;
	// rlwinm r11,r11,0,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF0000;
	// li r30,32
	r30.s64 = 32;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r31,r11,31,27,27
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x10;
	// b 0x824cd440
	goto loc_824CD440;
loc_824CD418:
	// cmpwi cr6,r29,7
	cr6.compare<int32_t>(r29.s32, 7, xer);
	// beq cr6,0x824cd434
	if (cr6.eq) goto loc_824CD434;
	// cmpwi cr6,r29,2
	cr6.compare<int32_t>(r29.s32, 2, xer);
	// beq cr6,0x824cd434
	if (cr6.eq) goto loc_824CD434;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// li r4,4509
	ctx.r4.s64 = 4509;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CD434:
	// li r29,2
	r29.s64 = 2;
	// li r31,0
	r31.s64 = 0;
loc_824CD43C:
	// li r30,256
	r30.s64 = 256;
loc_824CD440:
	// li r5,10
	ctx.r5.s64 = 10;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r9,1
	ctx.r3.s64 = ctx.r9.s64 + 1;
	// bl 0x823a1778
	sub_823A1778(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824cd464
	if (cr6.eq) goto loc_824CD464;
	// li r31,256
	r31.s64 = 256;
loc_824CD464:
	// add r11,r3,r31
	r11.u64 = ctx.r3.u64 + r31.u64;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// blt cr6,0x824cd474
	if (cr6.lt) goto loc_824CD474;
	// li r31,256
	r31.s64 = 256;
loc_824CD474:
	// add r11,r3,r31
	r11.u64 = ctx.r3.u64 + r31.u64;
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// ble cr6,0x824cd484
	if (!cr6.gt) goto loc_824CD484;
	// li r11,256
	r11.s64 = 256;
loc_824CD484:
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// stw r29,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r29.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_824CD494"))) PPC_WEAK_FUNC(sub_824CD494);
PPC_FUNC_IMPL(__imp__sub_824CD494) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CD498"))) PPC_WEAK_FUNC(sub_824CD498);
PPC_FUNC_IMPL(__imp__sub_824CD498) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r11,r11,18,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r10,r10,18,29,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x7;
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD4B8"))) PPC_WEAK_FUNC(sub_824CD4B8);
PPC_FUNC_IMPL(__imp__sub_824CD4B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// clrldi r10,r4,58
	ctx.r10.u64 = ctx.r4.u64 & 0x3F;
	// li r9,-1
	ctx.r9.s64 = -1;
	// rlwinm r11,r4,26,6,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 26) & 0x3FFFFFF;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// sld r9,r9,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// bge cr6,0x824cd4fc
	if (!cr6.lt) goto loc_824CD4FC;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
loc_824CD4D8:
	// ld r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// and r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824cd504
	if (!cr6.eq) goto loc_824CD504;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// li r9,-1
	ctx.r9.s64 = -1;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// blt cr6,0x824cd4d8
	if (cr6.lt) goto loc_824CD4D8;
loc_824CD4FC:
	// li r3,-1
	ctx.r3.s64 = -1;
	// blr 
	return;
loc_824CD504:
	// addi r10,r9,-1
	ctx.r10.s64 = ctx.r9.s64 + -1;
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// andc r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ~ctx.r10.u64;
	// cntlzd r10,r10
	ctx.r10.u64 = ctx.r10.u64 == 0 ? 64 : __builtin_clzll(ctx.r10.u64);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r3,r11,63
	ctx.r3.s64 = r11.s64 + 63;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD520"))) PPC_WEAK_FUNC(sub_824CD520);
PPC_FUNC_IMPL(__imp__sub_824CD520) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// clrldi r10,r4,58
	ctx.r10.u64 = ctx.r4.u64 & 0x3F;
	// li r9,-1
	ctx.r9.s64 = -1;
	// rlwinm r11,r4,26,6,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 26) & 0x3FFFFFF;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// sld r9,r9,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// bge cr6,0x824cd564
	if (!cr6.lt) goto loc_824CD564;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
loc_824CD540:
	// ld r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// andc r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ~ctx.r8.u64;
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824cd56c
	if (!cr6.eq) goto loc_824CD56C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// li r9,-1
	ctx.r9.s64 = -1;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// blt cr6,0x824cd540
	if (cr6.lt) goto loc_824CD540;
loc_824CD564:
	// li r3,-1
	ctx.r3.s64 = -1;
	// blr 
	return;
loc_824CD56C:
	// addi r10,r9,-1
	ctx.r10.s64 = ctx.r9.s64 + -1;
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// andc r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ~ctx.r10.u64;
	// cntlzd r10,r10
	ctx.r10.u64 = ctx.r10.u64 == 0 ? 64 : __builtin_clzll(ctx.r10.u64);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r3,r11,63
	ctx.r3.s64 = r11.s64 + 63;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD588"))) PPC_WEAK_FUNC(sub_824CD588);
PPC_FUNC_IMPL(__imp__sub_824CD588) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// clrldi r10,r4,58
	ctx.r10.u64 = ctx.r4.u64 & 0x3F;
	// li r9,-1
	ctx.r9.s64 = -1;
	// rlwinm r11,r4,26,6,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 26) & 0x3FFFFFF;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// bge cr6,0x824cd5cc
	if (!cr6.lt) goto loc_824CD5CC;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
loc_824CD5A8:
	// ld r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// andc r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r8.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// bne cr6,0x824cd5d4
	if (!cr6.eq) goto loc_824CD5D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// li r10,-1
	ctx.r10.s64 = -1;
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// blt cr6,0x824cd5a8
	if (cr6.lt) goto loc_824CD5A8;
loc_824CD5CC:
	// li r3,-1
	ctx.r3.s64 = -1;
	// blr 
	return;
loc_824CD5D4:
	// addi r7,r10,-1
	ctx.r7.s64 = ctx.r10.s64 + -1;
	// rlwinm r9,r11,6,0,25
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// andc r11,r10,r7
	r11.u64 = ctx.r10.u64 & ~ctx.r7.u64;
	// cntlzd r11,r11
	r11.u64 = r11.u64 == 0 ? 64 : __builtin_clzll(r11.u64);
	// subfic r11,r11,63
	xer.ca = r11.u32 <= 63;
	r11.s64 = 63 - r11.s64;
	// rlwinm r10,r11,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + r11.u64;
	// srd r11,r8,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r8.u64 >> (ctx.r10.u8 & 0x7F));
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD600"))) PPC_WEAK_FUNC(sub_824CD600);
PPC_FUNC_IMPL(__imp__sub_824CD600) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// clrldi r10,r4,58
	ctx.r10.u64 = ctx.r4.u64 & 0x3F;
	// li r9,2
	ctx.r9.s64 = 2;
	// rlwinm. r11,r4,26,6,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 26) & 0x3FFFFFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// addi r8,r10,-1
	ctx.r8.s64 = ctx.r10.s64 + -1;
	// blt 0x824cd640
	if (cr0.lt) goto loc_824CD640;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
loc_824CD620:
	// ld r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// andc r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 & ~ctx.r9.u64;
	// cmpldi cr6,r8,0
	cr6.compare<uint64_t>(ctx.r8.u64, 0, xer);
	// bne cr6,0x824cd648
	if (!cr6.eq) goto loc_824CD648;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r8,-1
	ctx.r8.s64 = -1;
	// addi r10,r10,-8
	ctx.r10.s64 = ctx.r10.s64 + -8;
	// bge 0x824cd620
	if (!cr0.lt) goto loc_824CD620;
loc_824CD640:
	// li r3,-1
	ctx.r3.s64 = -1;
	// blr 
	return;
loc_824CD648:
	// rlwinm r10,r11,6,0,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// cntlzd r11,r8
	r11.u64 = ctx.r8.u64 == 0 ? 64 : __builtin_clzll(ctx.r8.u64);
	// subfic r11,r11,63
	xer.ca = r11.u32 <= 63;
	r11.s64 = 63 - r11.s64;
	// rlwinm r8,r11,0,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
	// srd r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD66C"))) PPC_WEAK_FUNC(sub_824CD66C);
PPC_FUNC_IMPL(__imp__sub_824CD66C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CD670"))) PPC_WEAK_FUNC(sub_824CD670);
PPC_FUNC_IMPL(__imp__sub_824CD670) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// clrldi r10,r4,58
	ctx.r10.u64 = ctx.r4.u64 & 0x3F;
	// li r9,-1
	ctx.r9.s64 = -1;
	// rlwinm r11,r4,26,6,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 26) & 0x3FFFFFF;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// sld r9,r9,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// bge cr6,0x824cd6b4
	if (!cr6.lt) goto loc_824CD6B4;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
loc_824CD690:
	// ld r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// andc r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ~ctx.r8.u64;
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824cd6bc
	if (!cr6.eq) goto loc_824CD6BC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// li r9,-1
	ctx.r9.s64 = -1;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// blt cr6,0x824cd690
	if (cr6.lt) goto loc_824CD690;
loc_824CD6B4:
	// li r3,-1
	ctx.r3.s64 = -1;
	// blr 
	return;
loc_824CD6BC:
	// addi r10,r9,-1
	ctx.r10.s64 = ctx.r9.s64 + -1;
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// andc r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ~ctx.r10.u64;
	// cntlzd r10,r10
	ctx.r10.u64 = ctx.r10.u64 == 0 ? 64 : __builtin_clzll(ctx.r10.u64);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r3,r11,63
	ctx.r3.s64 = r11.s64 + 63;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD6D8"))) PPC_WEAK_FUNC(sub_824CD6D8);
PPC_FUNC_IMPL(__imp__sub_824CD6D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// clrlwi r9,r4,27
	ctx.r9.u64 = ctx.r4.u32 & 0x1F;
	// li r3,-1
	ctx.r3.s64 = -1;
	// rlwinm r11,r4,27,5,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x7FFFFFF;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// slw r9,r3,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r3.u32 << (ctx.r9.u8 & 0x3F));
	// bgelr cr6
	if (!cr6.lt) return;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
loc_824CD6FC:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// andc. r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ~ctx.r8.u64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824cd720
	if (!cr0.eq) goto loc_824CD720;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824cd6fc
	if (cr6.lt) goto loc_824CD6FC;
	// blr 
	return;
loc_824CD720:
	// addi r10,r9,-1
	ctx.r10.s64 = ctx.r9.s64 + -1;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// andc r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ~ctx.r10.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r3,r11,31
	ctx.r3.s64 = r11.s64 + 31;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD73C"))) PPC_WEAK_FUNC(sub_824CD73C);
PPC_FUNC_IMPL(__imp__sub_824CD73C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CD740"))) PPC_WEAK_FUNC(sub_824CD740);
PPC_FUNC_IMPL(__imp__sub_824CD740) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// clrlwi r10,r4,27
	ctx.r10.u64 = ctx.r4.u32 & 0x1F;
	// li r3,-1
	ctx.r3.s64 = -1;
	// rlwinm r11,r4,27,5,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x7FFFFFF;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// slw r10,r3,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r3.u32 << (ctx.r10.u8 & 0x3F));
	// bgelr cr6
	if (!cr6.lt) return;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
loc_824CD764:
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// andc. r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r8.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cd788
	if (!cr0.eq) goto loc_824CD788;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824cd764
	if (cr6.lt) goto loc_824CD764;
	// blr 
	return;
loc_824CD788:
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// andc r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 & ~ctx.r9.u64;
	// rlwinm r10,r11,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// cntlzw r11,r9
	r11.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// subfic r11,r11,31
	xer.ca = r11.u32 <= 31;
	r11.s64 = 31 - r11.s64;
	// rlwinm r9,r11,0,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
	// srw r11,r8,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r9.u8 & 0x3F));
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD7B4"))) PPC_WEAK_FUNC(sub_824CD7B4);
PPC_FUNC_IMPL(__imp__sub_824CD7B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CD7B8"))) PPC_WEAK_FUNC(sub_824CD7B8);
PPC_FUNC_IMPL(__imp__sub_824CD7B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	// li r8,2
	ctx.r8.s64 = 2;
	// clrlwi r10,r4,27
	ctx.r10.u64 = ctx.r4.u32 & 0x1F;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// rlwinm. r11,r4,27,5,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x7FFFFFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r3,-1
	ctx.r3.s64 = -1;
	// slw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
	// addi r8,r10,-1
	ctx.r8.s64 = ctx.r10.s64 + -1;
	// bltlr 
	if (cr0.lt) return;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_824CD7E0:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// andc. r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 & ~ctx.r9.u64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x824cd800
	if (!cr0.eq) goto loc_824CD800;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// bge 0x824cd7e0
	if (!cr0.lt) goto loc_824CD7E0;
	// blr 
	return;
loc_824CD800:
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r10,r11,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// subfic r11,r8,31
	xer.ca = ctx.r8.u32 <= 31;
	r11.s64 = 31 - ctx.r8.s64;
	// rlwinm r8,r11,0,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
	// srw r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r8.u8 & 0x3F));
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD824"))) PPC_WEAK_FUNC(sub_824CD824);
PPC_FUNC_IMPL(__imp__sub_824CD824) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CD828"))) PPC_WEAK_FUNC(sub_824CD828);
PPC_FUNC_IMPL(__imp__sub_824CD828) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r31,-8
	r11.s64 = r31.s64 + -8;
	// lwzx r11,r3,r11
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// rlwinm r3,r11,17,18,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 17) & 0x3FFF;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD874"))) PPC_WEAK_FUNC(sub_824CD874);
PPC_FUNC_IMPL(__imp__sub_824CD874) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CD878"))) PPC_WEAK_FUNC(sub_824CD878);
PPC_FUNC_IMPL(__imp__sub_824CD878) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
loc_824CD894:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r3,r30,8
	ctx.r3.s64 = r30.s64 + 8;
	// ble cr6,0x824cd8ac
	if (!cr6.gt) goto loc_824CD8AC;
	// bl 0x824cd588
	sub_824CD588(ctx, base);
	// b 0x824cd8b4
	goto loc_824CD8B4;
loc_824CD8AC:
	// addi r4,r4,3
	ctx.r4.s64 = ctx.r4.s64 + 3;
	// bl 0x824cd600
	sub_824CD600(ctx, base);
loc_824CD8B4:
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824cd908
	if (cr6.eq) goto loc_824CD908;
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// beq cr6,0x824cd914
	if (cr6.eq) goto loc_824CD914;
	// lis r9,-27863
	ctx.r9.s64 = -1826029568;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r8,1186
	ctx.r8.s64 = 1186;
	// ori r9,r9,13532
	ctx.r9.u64 = ctx.r9.u64 | 13532;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// rldimi r9,r8,32,0
	ctx.r9.u64 = (__builtin_rotateleft64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r9.u64 & 0xFFFFFFFF);
	// srd r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// srd r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// srd r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bge cr6,0x824cd924
	if (!cr6.lt) goto loc_824CD924;
	// add r11,r3,r29
	r11.u64 = ctx.r3.u64 + r29.u64;
	// rlwinm r4,r11,0,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// add r11,r4,r31
	r11.u64 = ctx.r4.u64 + r31.u64;
	// cmplwi cr6,r11,1024
	cr6.compare<uint32_t>(r11.u32, 1024, xer);
	// ble cr6,0x824cd894
	if (!cr6.gt) goto loc_824CD894;
loc_824CD908:
	// li r3,-1
	ctx.r3.s64 = -1;
loc_824CD90C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
loc_824CD914:
	// clrlwi r11,r3,30
	r11.u64 = ctx.r3.u32 & 0x3;
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// b 0x824cd928
	goto loc_824CD928;
loc_824CD924:
	// not r11,r10
	r11.u64 = ~ctx.r10.u64;
loc_824CD928:
	// rlwimi r11,r3,2,0,27
	r11.u64 = (__builtin_rotateleft32(ctx.r3.u32, 2) & 0xFFFFFFF0) | (r11.u64 & 0xFFFFFFFF0000000F);
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// b 0x824cd90c
	goto loc_824CD90C;
}

__attribute__((alias("__imp__sub_824CD934"))) PPC_WEAK_FUNC(sub_824CD934);
PPC_FUNC_IMPL(__imp__sub_824CD934) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CD938"))) PPC_WEAK_FUNC(sub_824CD938);
PPC_FUNC_IMPL(__imp__sub_824CD938) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r8,2
	ctx.r8.s64 = 2;
	// clrlwi r9,r5,27
	ctx.r9.u64 = ctx.r5.u32 & 0x1F;
	// li r6,-1
	ctx.r6.s64 = -1;
	// clrlwi r7,r4,27
	ctx.r7.u64 = ctx.r4.u32 & 0x1F;
	// rlwinm r11,r4,27,5,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x7FFFFFF;
	// rlwinm r10,r5,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x7FFFFFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// slw r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r9.u8 & 0x3F));
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// slw r7,r6,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r7.u8 & 0x3F));
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// addi r6,r9,-1
	ctx.r6.s64 = ctx.r9.s64 + -1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// bne cr6,0x824cd998
	if (!cr6.eq) goto loc_824CD998;
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// addi r11,r11,0
	r11.s64 = r11.s64 + 0;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// xori r3,r11,1
	ctx.r3.u64 = r11.u64 ^ 1;
	// blr 
	return;
loc_824CD998:
	// and. r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cd9a8
	if (cr0.eq) goto loc_824CD9A8;
loc_824CD9A0:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_824CD9A8:
	// addi r11,r8,4
	r11.s64 = ctx.r8.s64 + 4;
	// b 0x824cd9c0
	goto loc_824CD9C0;
loc_824CD9B0:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824cd9a0
	if (!cr6.eq) goto loc_824CD9A0;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_824CD9C0:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x824cd9b0
	if (cr6.lt) goto loc_824CD9B0;
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r3,r11,1
	ctx.r3.u64 = r11.u64 ^ 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CD9E0"))) PPC_WEAK_FUNC(sub_824CD9E0);
PPC_FUNC_IMPL(__imp__sub_824CD9E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// clrlwi r9,r5,27
	ctx.r9.u64 = ctx.r5.u32 & 0x1F;
	// li r7,2
	ctx.r7.s64 = 2;
	// clrlwi r6,r4,27
	ctx.r6.u64 = ctx.r4.u32 & 0x1F;
	// li r8,-1
	ctx.r8.s64 = -1;
	// rlwinm r10,r5,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x7FFFFFF;
	// rlwinm r11,r4,27,5,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x7FFFFFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// slw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r9,-1
	ctx.r5.s64 = ctx.r9.s64 + -1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// slw r6,r8,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r6.u8 & 0x3F));
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r7,r7,r3
	ctx.r7.u64 = ctx.r7.u64 + ctx.r3.u64;
	// bne cr6,0x824cda30
	if (!cr6.eq) goto loc_824CDA30;
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// and r10,r5,r6
	ctx.r10.u64 = ctx.r5.u64 & ctx.r6.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// blr 
	return;
loc_824CDA30:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r11,r9,4
	r11.s64 = ctx.r9.s64 + 4;
	// or r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 | ctx.r6.u64;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// bge cr6,0x824cda6c
	if (!cr6.lt) goto loc_824CDA6C;
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addic. r10,r10,1
	xer.ca = ctx.r10.u32 > 4294967294;
	ctx.r10.s64 = ctx.r10.s64 + 1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824cda6c
	if (cr0.eq) goto loc_824CDA6C;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_824CDA60:
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x824cda60
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_824CDA60;
loc_824CDA6C:
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// or r11,r11,r5
	r11.u64 = r11.u64 | ctx.r5.u64;
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CDA7C"))) PPC_WEAK_FUNC(sub_824CDA7C);
PPC_FUNC_IMPL(__imp__sub_824CDA7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CDA80"))) PPC_WEAK_FUNC(sub_824CDA80);
PPC_FUNC_IMPL(__imp__sub_824CDA80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// li r8,2
	ctx.r8.s64 = 2;
	// clrldi r9,r5,58
	ctx.r9.u64 = ctx.r5.u64 & 0x3F;
	// clrldi r6,r4,58
	ctx.r6.u64 = ctx.r4.u64 & 0x3F;
	// li r31,-1
	r31.s64 = -1;
	// rlwinm r11,r4,26,6,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 26) & 0x3FFFFFF;
	// rlwinm r10,r5,26,6,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 26) & 0x3FFFFFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// sld r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r9.u8 & 0x7F));
	// rlwinm r8,r11,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r7,r9,-1
	ctx.r7.s64 = ctx.r9.s64 + -1;
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// sld r6,r31,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x40 ? 0 : (r31.u64 << (ctx.r6.u8 & 0x7F));
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// bne cr6,0x824cdac8
	if (!cr6.eq) goto loc_824CDAC8;
	// and r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 & ctx.r7.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
loc_824CDAC8:
	// ld r11,0(r9)
	r11.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x824cdaec
	if (cr6.eq) goto loc_824CDAEC;
	// rlwinm r10,r5,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFC0;
loc_824CDADC:
	// cntlzd r11,r11
	r11.u64 = r11.u64 == 0 ? 64 : __builtin_clzll(r11.u64);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
loc_824CDAE4:
	// addi r3,r11,63
	ctx.r3.s64 = r11.s64 + 63;
	// b 0x824cdb48
	goto loc_824CDB48;
loc_824CDAEC:
	// addi r11,r9,-8
	r11.s64 = ctx.r9.s64 + -8;
	// b 0x824cdb04
	goto loc_824CDB04;
loc_824CDAF4:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824cdb24
	if (!cr6.eq) goto loc_824CDB24;
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
loc_824CDB04:
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bgt cr6,0x824cdaf4
	if (cr6.gt) goto loc_824CDAF4;
	// ld r11,0(r8)
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x824cdb44
	if (cr6.eq) goto loc_824CDB44;
	// rlwinm r10,r4,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFC0;
	// b 0x824cdadc
	goto loc_824CDADC;
loc_824CDB24:
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// rlwinm r10,r4,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFC0;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// cntlzd r9,r9
	ctx.r9.u64 = ctx.r9.u64 == 0 ? 64 : __builtin_clzll(ctx.r9.u64);
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// b 0x824cdae4
	goto loc_824CDAE4;
loc_824CDB44:
	// li r3,-1
	ctx.r3.s64 = -1;
loc_824CDB48:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CDB50"))) PPC_WEAK_FUNC(sub_824CDB50);
PPC_FUNC_IMPL(__imp__sub_824CDB50) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r8,2
	ctx.r8.s64 = 2;
	// clrldi r9,r5,58
	ctx.r9.u64 = ctx.r5.u64 & 0x3F;
	// clrldi r7,r4,58
	ctx.r7.u64 = ctx.r4.u64 & 0x3F;
	// rlwinm r11,r4,26,6,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 26) & 0x3FFFFFF;
	// rlwinm r10,r5,26,6,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 26) & 0x3FFFFFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// sld r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r9.u8 & 0x7F));
	// li r8,-1
	ctx.r8.s64 = -1;
	// addi r6,r9,-1
	ctx.r6.s64 = ctx.r9.s64 + -1;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// sld r7,r8,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r7.u8 & 0x7F));
	// rlwinm r8,r10,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// bne cr6,0x824cdba0
	if (!cr6.eq) goto loc_824CDBA0;
	// ld r11,0(r9)
	r11.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// and r10,r6,r7
	ctx.r10.u64 = ctx.r6.u64 & ctx.r7.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// std r11,0(r9)
	PPC_STORE_U64(ctx.r9.u32 + 0, r11.u64);
	// blr 
	return;
loc_824CDBA0:
	// ld r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r11,r9,8
	r11.s64 = ctx.r9.s64 + 8;
	// or r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 | ctx.r7.u64;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// std r10,0(r9)
	PPC_STORE_U64(ctx.r9.u32 + 0, ctx.r10.u64);
	// bge cr6,0x824cdbe0
	if (!cr6.lt) goto loc_824CDBE0;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - r11.s64;
	// li r9,-1
	ctx.r9.s64 = -1;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// rlwinm r10,r10,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
	// addic. r10,r10,1
	xer.ca = ctx.r10.u32 > 4294967294;
	ctx.r10.s64 = ctx.r10.s64 + 1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824cdbe0
	if (cr0.eq) goto loc_824CDBE0;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_824CDBD4:
	// std r9,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bdnz 0x824cdbd4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_824CDBD4;
loc_824CDBE0:
	// ld r11,0(r8)
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// or r11,r11,r6
	r11.u64 = r11.u64 | ctx.r6.u64;
	// std r11,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, r11.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CDBF0"))) PPC_WEAK_FUNC(sub_824CDBF0);
PPC_FUNC_IMPL(__imp__sub_824CDBF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
loc_824CDC08:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// ble cr6,0x824cdc20
	if (!cr6.gt) goto loc_824CDC20;
	// bl 0x824cd740
	sub_824CD740(ctx, base);
	// b 0x824cdc24
	goto loc_824CDC24;
loc_824CDC20:
	// bl 0x824cd7b8
	sub_824CD7B8(ctx, base);
loc_824CDC24:
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824cdc74
	if (cr6.eq) goto loc_824CDC74;
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// beq cr6,0x824cdc80
	if (cr6.eq) goto loc_824CDC80;
	// lis r9,-27863
	ctx.r9.s64 = -1826029568;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r8,1186
	ctx.r8.s64 = 1186;
	// ori r9,r9,13532
	ctx.r9.u64 = ctx.r9.u64 | 13532;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// rldimi r9,r8,32,0
	ctx.r9.u64 = (__builtin_rotateleft64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r9.u64 & 0xFFFFFFFF);
	// srd r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// srd r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// srd r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bge cr6,0x824cdc8c
	if (!cr6.lt) goto loc_824CDC8C;
	// add r4,r3,r29
	ctx.r4.u64 = ctx.r3.u64 + r29.u64;
	// add r11,r4,r31
	r11.u64 = ctx.r4.u64 + r31.u64;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// ble cr6,0x824cdc08
	if (!cr6.gt) goto loc_824CDC08;
loc_824CDC74:
	// li r3,-1
	ctx.r3.s64 = -1;
loc_824CDC78:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
loc_824CDC80:
	// rlwinm r11,r3,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 4) & 0xFFFFFFF0;
	// ori r3,r11,1
	ctx.r3.u64 = r11.u64 | 1;
	// b 0x824cdc78
	goto loc_824CDC78;
loc_824CDC8C:
	// not r11,r10
	r11.u64 = ~ctx.r10.u64;
	// rlwinm r10,r3,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 4) & 0xFFFFFFF0;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// or r3,r11,r10
	ctx.r3.u64 = r11.u64 | ctx.r10.u64;
	// b 0x824cdc78
	goto loc_824CDC78;
}

__attribute__((alias("__imp__sub_824CDCA0"))) PPC_WEAK_FUNC(sub_824CDCA0);
PPC_FUNC_IMPL(__imp__sub_824CDCA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// li r8,2
	ctx.r8.s64 = 2;
	// clrldi r9,r5,58
	ctx.r9.u64 = ctx.r5.u64 & 0x3F;
	// clrldi r6,r4,58
	ctx.r6.u64 = ctx.r4.u64 & 0x3F;
	// li r31,-1
	r31.s64 = -1;
	// rlwinm r11,r4,26,6,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 26) & 0x3FFFFFF;
	// rlwinm r10,r5,26,6,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 26) & 0x3FFFFFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// sld r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r9.u8 & 0x7F));
	// rlwinm r8,r11,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r7,r9,-1
	ctx.r7.s64 = ctx.r9.s64 + -1;
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// sld r6,r31,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x40 ? 0 : (r31.u64 << (ctx.r6.u8 & 0x7F));
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// bne cr6,0x824cdce8
	if (!cr6.eq) goto loc_824CDCE8;
	// and r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 & ctx.r7.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
loc_824CDCE8:
	// ld r11,0(r9)
	r11.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x824cdd0c
	if (cr6.eq) goto loc_824CDD0C;
	// rlwinm r10,r5,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFC0;
loc_824CDCFC:
	// cntlzd r11,r11
	r11.u64 = r11.u64 == 0 ? 64 : __builtin_clzll(r11.u64);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
loc_824CDD04:
	// addi r3,r11,63
	ctx.r3.s64 = r11.s64 + 63;
	// b 0x824cdd68
	goto loc_824CDD68;
loc_824CDD0C:
	// addi r11,r9,-8
	r11.s64 = ctx.r9.s64 + -8;
	// b 0x824cdd24
	goto loc_824CDD24;
loc_824CDD14:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// bne cr6,0x824cdd44
	if (!cr6.eq) goto loc_824CDD44;
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
loc_824CDD24:
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bgt cr6,0x824cdd14
	if (cr6.gt) goto loc_824CDD14;
	// ld r11,0(r8)
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x824cdd64
	if (cr6.eq) goto loc_824CDD64;
	// rlwinm r10,r4,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFC0;
	// b 0x824cdcfc
	goto loc_824CDCFC;
loc_824CDD44:
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// rlwinm r10,r4,0,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFC0;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// cntlzd r9,r9
	ctx.r9.u64 = ctx.r9.u64 == 0 ? 64 : __builtin_clzll(ctx.r9.u64);
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// b 0x824cdd04
	goto loc_824CDD04;
loc_824CDD64:
	// li r3,-1
	ctx.r3.s64 = -1;
loc_824CDD68:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CDD70"))) PPC_WEAK_FUNC(sub_824CDD70);
PPC_FUNC_IMPL(__imp__sub_824CDD70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// li r7,2
	ctx.r7.s64 = 2;
	// clrlwi r8,r5,27
	ctx.r8.u64 = ctx.r5.u32 & 0x1F;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// clrlwi r31,r4,27
	r31.u64 = ctx.r4.u32 & 0x1F;
	// li r3,-1
	ctx.r3.s64 = -1;
	// rlwinm r10,r4,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x7FFFFFF;
	// rlwinm r9,r5,27,5,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x7FFFFFF;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// slw r8,r7,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r8.u8 & 0x3F));
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r8,-1
	ctx.r6.s64 = ctx.r8.s64 + -1;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// slw r31,r3,r31
	r31.u64 = r31.u8 & 0x20 ? 0 : (ctx.r3.u32 << (r31.u8 & 0x3F));
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// bne cr6,0x824cddbc
	if (!cr6.eq) goto loc_824CDDBC;
	// and r31,r31,r6
	r31.u64 = r31.u64 & ctx.r6.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
loc_824CDDBC:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// and. r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 & ctx.r6.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824cdde4
	if (cr0.eq) goto loc_824CDDE4;
	// rlwinm r11,r5,0,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFE0;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// b 0x824cde08
	goto loc_824CDE08;
loc_824CDDD8:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne 0x824cde14
	if (!cr0.eq) goto loc_824CDE14;
loc_824CDDE4:
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bgt cr6,0x824cddd8
	if (cr6.gt) goto loc_824CDDD8;
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// and. r11,r11,r31
	r11.u64 = r11.u64 & r31.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cde0c
	if (cr0.eq) goto loc_824CDE0C;
	// rlwinm r10,r4,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFE0;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
loc_824CDE08:
	// addi r3,r11,31
	ctx.r3.s64 = r11.s64 + 31;
loc_824CDE0C:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_824CDE14:
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// rlwinm r10,r4,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFE0;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// b 0x824cde08
	goto loc_824CDE08;
}

__attribute__((alias("__imp__sub_824CDE34"))) PPC_WEAK_FUNC(sub_824CDE34);
PPC_FUNC_IMPL(__imp__sub_824CDE34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CDE38"))) PPC_WEAK_FUNC(sub_824CDE38);
PPC_FUNC_IMPL(__imp__sub_824CDE38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// rlwinm r5,r4,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 28) & 0xFFFFFFF;
	// clrldi r9,r4,60
	ctx.r9.u64 = ctx.r4.u64 & 0xF;
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// li r7,2
	ctx.r7.s64 = 2;
	// rlwinm r8,r11,26,6,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// addi r4,r11,3
	ctx.r4.s64 = r11.s64 + 3;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// clrldi r10,r11,58
	ctx.r10.u64 = r11.u64 & 0x3F;
	// rlwinm r11,r8,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// clrldi r8,r4,58
	ctx.r8.u64 = ctx.r4.u64 & 0x3F;
	// li r6,-1
	ctx.r6.s64 = -1;
	// sld r8,r7,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r7.u64 << (ctx.r8.u8 & 0x7F));
	// sld r7,r6,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r6.u64 << (ctx.r10.u8 & 0x7F));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// and r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ctx.r7.u64;
	// ldx r7,r11,r3
	ctx.r7.u64 = PPC_LOAD_U64(r11.u32 + ctx.r3.u32);
	// and r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ctx.r7.u64;
	// srd r8,r8,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r8.u64 >> (ctx.r10.u8 & 0x7F));
	// and r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmpldi cr6,r8,0
	cr6.compare<uint64_t>(ctx.r8.u64, 0, xer);
	// beq cr6,0x824cde98
	if (cr6.eq) goto loc_824CDE98;
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,3526
	ctx.r4.s64 = 3526;
	// b 0x82496e98
	sub_82496E98(ctx, base);
	return;
loc_824CDE98:
	// ldx r8,r11,r3
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + ctx.r3.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// or r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 | ctx.r8.u64;
	// stdx r10,r11,r3
	PPC_STORE_U64(r11.u32 + ctx.r3.u32, ctx.r10.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CDEAC"))) PPC_WEAK_FUNC(sub_824CDEAC);
PPC_FUNC_IMPL(__imp__sub_824CDEAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CDEB0"))) PPC_WEAK_FUNC(sub_824CDEB0);
PPC_FUNC_IMPL(__imp__sub_824CDEB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r31,r27,2,0,29
	r31.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// addi r30,r28,8
	r30.s64 = r28.s64 + 8;
	// addi r29,r11,-1
	r29.s64 = r11.s64 + -1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x824b1ec8
	sub_824B1EC8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cdf00
	if (cr0.eq) goto loc_824CDF00;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r4,3526
	ctx.r4.s64 = 3526;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CDF00:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824cdb50
	sub_824CDB50(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_824CDF18"))) PPC_WEAK_FUNC(sub_824CDF18);
PPC_FUNC_IMPL(__imp__sub_824CDF18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r6,-4
	ctx.r6.s64 = -4;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,255
	ctx.r4.s64 = 255;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x824cd878
	sub_824CD878(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x824cdf54
	if (!cr6.eq) goto loc_824CDF54;
	// li r4,3566
	ctx.r4.s64 = 3566;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CDF54:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// clrlwi r3,r31,28
	ctx.r3.u64 = r31.u32 & 0xF;
	// rlwinm r30,r31,30,2,29
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 30) & 0x3FFFFFFC;
	// bl 0x824b3760
	sub_824B3760(ctx, base);
	// clrldi r9,r3,32
	ctx.r9.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// clrldi r10,r30,58
	ctx.r10.u64 = r30.u64 & 0x3F;
	// rlwinm r11,r30,26,6,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 26) & 0x3FFFFFF;
	// rlwimi r3,r31,0,0,27
	ctx.r3.u64 = (__builtin_rotateleft32(r31.u32, 0) & 0xFFFFFFF0) | (ctx.r3.u64 & 0xFFFFFFFF0000000F);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// ldx r8,r11,r29
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + r29.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// or r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 | ctx.r8.u64;
	// stdx r10,r11,r29
	PPC_STORE_U64(r11.u32 + r29.u32, ctx.r10.u64);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CDF94"))) PPC_WEAK_FUNC(sub_824CDF94);
PPC_FUNC_IMPL(__imp__sub_824CDF94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CDF98"))) PPC_WEAK_FUNC(sub_824CDF98);
PPC_FUNC_IMPL(__imp__sub_824CDF98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r29,r3,8
	r29.s64 = ctx.r3.s64 + 8;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// bl 0x824cd520
	sub_824CD520(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x824cdfcc
	if (!cr6.eq) goto loc_824CDFCC;
loc_824CDFC4:
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x824ce014
	goto loc_824CE014;
loc_824CDFCC:
	// rlwinm r30,r30,2,0,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x824ce000
	goto loc_824CE000;
loc_824CDFD4:
	// addi r5,r11,-1
	ctx.r5.s64 = r11.s64 + -1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824cda80
	sub_824CDA80(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// bne cr6,0x824cdff8
	if (!cr6.eq) goto loc_824CDFF8;
	// clrlwi. r11,r31,30
	r11.u64 = r31.u32 & 0x3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ce010
	if (cr0.eq) goto loc_824CE010;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_824CDFF8:
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
	// rlwinm r31,r11,0,0,29
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
loc_824CE000:
	// add r11,r30,r31
	r11.u64 = r30.u64 + r31.u64;
	// cmplwi cr6,r11,1024
	cr6.compare<uint32_t>(r11.u32, 1024, xer);
	// ble cr6,0x824cdfd4
	if (!cr6.gt) goto loc_824CDFD4;
	// b 0x824cdfc4
	goto loc_824CDFC4;
loc_824CE010:
	// rlwinm r3,r31,30,2,31
	ctx.r3.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 30) & 0x3FFFFFFF;
loc_824CE014:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CE01C"))) PPC_WEAK_FUNC(sub_824CE01C);
PPC_FUNC_IMPL(__imp__sub_824CE01C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CE020"))) PPC_WEAK_FUNC(sub_824CE020);
PPC_FUNC_IMPL(__imp__sub_824CE020) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	// rlwinm r5,r4,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 28) & 0xFFFFFFF;
	// li r8,2
	ctx.r8.s64 = 2;
	// clrlwi r11,r5,27
	r11.u64 = ctx.r5.u32 & 0x1F;
	// li r7,-1
	ctx.r7.s64 = -1;
	// rlwinm r10,r5,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r9,r4,28
	ctx.r9.u64 = ctx.r4.u32 & 0xF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// slw r8,r8,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// slw r7,r7,r11
	ctx.r7.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (r11.u8 & 0x3F));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// and r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ctx.r7.u64;
	// lwzx r7,r10,r3
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// and r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ctx.r7.u64;
	// srw r8,r8,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (r11.u8 & 0x3F));
	// and. r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 & ctx.r9.u64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824ce070
	if (cr0.eq) goto loc_824CE070;
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,3526
	ctx.r4.s64 = 3526;
	// b 0x82496e98
	sub_82496E98(ctx, base);
	return;
loc_824CE070:
	// lwzx r8,r10,r3
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// slw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// stwx r11,r10,r3
	PPC_STORE_U32(ctx.r10.u32 + ctx.r3.u32, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CE084"))) PPC_WEAK_FUNC(sub_824CE084);
PPC_FUNC_IMPL(__imp__sub_824CE084) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CE088"))) PPC_WEAK_FUNC(sub_824CE088);
PPC_FUNC_IMPL(__imp__sub_824CE088) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// add r11,r31,r5
	r11.u64 = r31.u64 + ctx.r5.u64;
	// addi r30,r28,4
	r30.s64 = r28.s64 + 4;
	// addi r29,r11,-1
	r29.s64 = r11.s64 + -1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x824cd938
	sub_824CD938(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ce0cc
	if (cr0.eq) goto loc_824CE0CC;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r4,3526
	ctx.r4.s64 = 3526;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CE0CC:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824cd9e0
	sub_824CD9E0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824CE0E4"))) PPC_WEAK_FUNC(sub_824CE0E4);
PPC_FUNC_IMPL(__imp__sub_824CE0E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CE0E8"))) PPC_WEAK_FUNC(sub_824CE0E8);
PPC_FUNC_IMPL(__imp__sub_824CE0E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// rlwinm r5,r4,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 28) & 0xFFFFFFF;
	// li r8,2
	ctx.r8.s64 = 2;
	// clrldi r11,r5,58
	r11.u64 = ctx.r5.u64 & 0x3F;
	// li r7,-1
	ctx.r7.s64 = -1;
	// rlwinm r10,r5,26,6,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 26) & 0x3FFFFFF;
	// clrldi r9,r4,60
	ctx.r9.u64 = ctx.r4.u64 & 0xF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// sld r8,r8,r11
	ctx.r8.u64 = r11.u8 & 0x40 ? 0 : (ctx.r8.u64 << (r11.u8 & 0x7F));
	// sld r7,r7,r11
	ctx.r7.u64 = r11.u8 & 0x40 ? 0 : (ctx.r7.u64 << (r11.u8 & 0x7F));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// and r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ctx.r7.u64;
	// ldx r7,r10,r3
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r3.u32);
	// and r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ctx.r7.u64;
	// srd r8,r8,r11
	ctx.r8.u64 = r11.u8 & 0x40 ? 0 : (ctx.r8.u64 >> (r11.u8 & 0x7F));
	// and r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmpldi cr6,r8,0
	cr6.compare<uint64_t>(ctx.r8.u64, 0, xer);
	// beq cr6,0x824ce13c
	if (cr6.eq) goto loc_824CE13C;
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,3526
	ctx.r4.s64 = 3526;
	// b 0x82496e98
	sub_82496E98(ctx, base);
	return;
loc_824CE13C:
	// ldx r8,r10,r3
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r3.u32);
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// stdx r11,r10,r3
	PPC_STORE_U64(ctx.r10.u32 + ctx.r3.u32, r11.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CE150"))) PPC_WEAK_FUNC(sub_824CE150);
PPC_FUNC_IMPL(__imp__sub_824CE150) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// add r11,r31,r5
	r11.u64 = r31.u64 + ctx.r5.u64;
	// addi r30,r28,8
	r30.s64 = r28.s64 + 8;
	// addi r29,r11,-1
	r29.s64 = r11.s64 + -1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x824b1ec8
	sub_824B1EC8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ce194
	if (cr0.eq) goto loc_824CE194;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r4,3526
	ctx.r4.s64 = 3526;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CE194:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824cdb50
	sub_824CDB50(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824CE1AC"))) PPC_WEAK_FUNC(sub_824CE1AC);
PPC_FUNC_IMPL(__imp__sub_824CE1AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CE1B0"))) PPC_WEAK_FUNC(sub_824CE1B0);
PPC_FUNC_IMPL(__imp__sub_824CE1B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// add r11,r31,r5
	r11.u64 = r31.u64 + ctx.r5.u64;
	// addi r30,r28,4
	r30.s64 = r28.s64 + 4;
	// addi r29,r11,-1
	r29.s64 = r11.s64 + -1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x824cd938
	sub_824CD938(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ce1f4
	if (cr0.eq) goto loc_824CE1F4;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r4,3526
	ctx.r4.s64 = 3526;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CE1F4:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824cd9e0
	sub_824CD9E0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824CE20C"))) PPC_WEAK_FUNC(sub_824CE20C);
PPC_FUNC_IMPL(__imp__sub_824CE20C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CE210"))) PPC_WEAK_FUNC(sub_824CE210);
PPC_FUNC_IMPL(__imp__sub_824CE210) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r6,-1
	ctx.r6.s64 = -1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,31
	ctx.r4.s64 = 31;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x824cdbf0
	sub_824CDBF0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x824ce24c
	if (!cr6.eq) goto loc_824CE24C;
	// li r4,3570
	ctx.r4.s64 = 3570;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CE24C:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// clrlwi r3,r31,28
	ctx.r3.u64 = r31.u32 & 0xF;
	// rlwinm r30,r31,28,4,31
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 28) & 0xFFFFFFF;
	// bl 0x824b3760
	sub_824B3760(ctx, base);
	// clrlwi r10,r30,27
	ctx.r10.u64 = r30.u32 & 0x1F;
	// rlwinm r11,r30,27,5,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r11,r29
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// slw r10,r3,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r3.u32 << (ctx.r10.u8 & 0x3F));
	// rlwimi r3,r31,0,0,27
	ctx.r3.u64 = (__builtin_rotateleft32(r31.u32, 0) & 0xFFFFFFF0) | (ctx.r3.u64 & 0xFFFFFFFF0000000F);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stwx r10,r11,r29
	PPC_STORE_U32(r11.u32 + r29.u32, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CE288"))) PPC_WEAK_FUNC(sub_824CE288);
PPC_FUNC_IMPL(__imp__sub_824CE288) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r29,r3,4
	r29.s64 = ctx.r3.s64 + 4;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824cd6d8
	sub_824CD6D8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x824ce2d4
	if (!cr6.eq) goto loc_824CE2D4;
loc_824CE2B0:
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x824ce2e8
	goto loc_824CE2E8;
loc_824CE2B8:
	// addi r5,r11,-1
	ctx.r5.s64 = r11.s64 + -1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824cdd70
	sub_824CDD70(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824ce2e4
	if (cr6.eq) goto loc_824CE2E4;
	// addi r31,r3,1
	r31.s64 = ctx.r3.s64 + 1;
loc_824CE2D4:
	// add r11,r31,r30
	r11.u64 = r31.u64 + r30.u64;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// ble cr6,0x824ce2b8
	if (!cr6.gt) goto loc_824CE2B8;
	// b 0x824ce2b0
	goto loc_824CE2B0;
loc_824CE2E4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_824CE2E8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CE2F0"))) PPC_WEAK_FUNC(sub_824CE2F0);
PPC_FUNC_IMPL(__imp__sub_824CE2F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r29,r3,8
	r29.s64 = ctx.r3.s64 + 8;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824cd670
	sub_824CD670(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x824ce33c
	if (!cr6.eq) goto loc_824CE33C;
loc_824CE318:
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x824ce350
	goto loc_824CE350;
loc_824CE320:
	// addi r5,r11,-1
	ctx.r5.s64 = r11.s64 + -1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824cdca0
	sub_824CDCA0(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824ce34c
	if (cr6.eq) goto loc_824CE34C;
	// addi r31,r3,1
	r31.s64 = ctx.r3.s64 + 1;
loc_824CE33C:
	// add r11,r31,r30
	r11.u64 = r31.u64 + r30.u64;
	// cmplwi cr6,r11,256
	cr6.compare<uint32_t>(r11.u32, 256, xer);
	// ble cr6,0x824ce320
	if (!cr6.gt) goto loc_824CE320;
	// b 0x824ce318
	goto loc_824CE318;
loc_824CE34C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_824CE350:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CE358"))) PPC_WEAK_FUNC(sub_824CE358);
PPC_FUNC_IMPL(__imp__sub_824CE358) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r29,r3,4
	r29.s64 = ctx.r3.s64 + 4;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824cd6d8
	sub_824CD6D8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x824ce3a4
	if (!cr6.eq) goto loc_824CE3A4;
loc_824CE380:
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x824ce3b8
	goto loc_824CE3B8;
loc_824CE388:
	// addi r5,r11,-1
	ctx.r5.s64 = r11.s64 + -1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824cdd70
	sub_824CDD70(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824ce3b4
	if (cr6.eq) goto loc_824CE3B4;
	// addi r31,r3,1
	r31.s64 = ctx.r3.s64 + 1;
loc_824CE3A4:
	// add r11,r31,r30
	r11.u64 = r31.u64 + r30.u64;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// ble cr6,0x824ce388
	if (!cr6.gt) goto loc_824CE388;
	// b 0x824ce380
	goto loc_824CE380;
loc_824CE3B4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_824CE3B8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CE3C0"))) PPC_WEAK_FUNC(sub_824CE3C0);
PPC_FUNC_IMPL(__imp__sub_824CE3C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r3,764
	r11.s64 = ctx.r3.s64 + 764;
	// cmplwi cr6,r4,132
	cr6.compare<uint32_t>(ctx.r4.u32, 132, xer);
	// ble cr6,0x824ce3e8
	if (!cr6.gt) goto loc_824CE3E8;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// bl 0x824a9e10
	sub_824A9E10(ctx, base);
	// b 0x824ce454
	goto loc_824CE454;
loc_824CE3E8:
	// lwz r9,140(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// lwz r10,144(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 144);
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// addi r9,r9,4096
	ctx.r9.s64 = ctx.r9.s64 + 4096;
	// cmplw cr6,r9,r4
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r4.u32, xer);
	// blt cr6,0x824ce40c
	if (cr6.lt) goto loc_824CE40C;
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// stw r9,144(r11)
	PPC_STORE_U32(r11.u32 + 144, ctx.r9.u32);
	// b 0x824ce450
	goto loc_824CE450;
loc_824CE40C:
	// rlwinm r10,r4,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r31,r10,r11
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824ce444
	if (cr0.eq) goto loc_824CE444;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwx r9,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, ctx.r9.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// b 0x824ce450
	goto loc_824CE450;
loc_824CE444:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// bl 0x824a9530
	sub_824A9530(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_824CE450:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
loc_824CE454:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CE468"))) PPC_WEAK_FUNC(sub_824CE468);
PPC_FUNC_IMPL(__imp__sub_824CE468) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r11,84(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 84);
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x824a9f38
	sub_824A9F38(ctx, base);
	// li r22,0
	r22.s64 = 0;
	// mr r17,r3
	r17.u64 = ctx.r3.u64;
	// mr r18,r22
	r18.u64 = r22.u64;
	// addi r27,r19,56
	r27.s64 = r19.s64 + 56;
	// li r26,7
	r26.s64 = 7;
	// li r16,1
	r16.s64 = 1;
	// li r15,3
	r15.s64 = 3;
loc_824CE4A4:
	// lwz r30,0(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x824ce6f8
	if (cr0.eq) goto loc_824CE6F8;
	// rlwinm r11,r18,2,0,29
	r11.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r11,r17
	r28.u64 = r11.u64 + r17.u64;
loc_824CE4B8:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,14592
	cr6.compare<uint32_t>(r11.u32, 14592, xer);
	// beq cr6,0x824ce6ec
	if (cr6.eq) goto loc_824CE6EC;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824d8728
	sub_824D8728(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ce6d8
	if (cr0.eq) goto loc_824CE6D8;
	// mr r31,r22
	r31.u64 = r22.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
loc_824CE4F0:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x824ce554
	if (cr6.eq) goto loc_824CE554;
	// lwz r11,16(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// subf r11,r22,r11
	r11.s64 = r11.s64 - r22.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824ce54c
	if (cr0.eq) goto loc_824CE54C;
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// rlwinm. r11,r10,7,29,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ce548
	if (cr0.eq) goto loc_824CE548;
	// rlwinm r8,r10,27,24,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
loc_824CE52C:
	// srw r6,r8,r10
	ctx.r6.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// clrlwi r6,r6,30
	ctx.r6.u64 = ctx.r6.u32 & 0x3;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// slw r6,r16,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (r16.u32 << (ctx.r6.u8 & 0x3F));
	// or r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 | ctx.r9.u64;
	// bne 0x824ce52c
	if (!cr0.eq) goto loc_824CE52C;
loc_824CE548:
	// or r5,r9,r5
	ctx.r5.u64 = ctx.r9.u64 | ctx.r5.u64;
loc_824CE54C:
	// lwz r7,8(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// b 0x824ce4f0
	goto loc_824CE4F0;
loc_824CE554:
	// addi r3,r30,40
	ctx.r3.s64 = r30.s64 + 40;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
loc_824CE568:
	// and. r11,r7,r5
	r11.u64 = ctx.r7.u64 & ctx.r5.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ce5c4
	if (cr0.eq) goto loc_824CE5C4;
	// lfs f0,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	f0.f64 = double(temp.f32);
	// mr r11,r22
	r11.u64 = r22.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824ce5a0
	if (cr6.eq) goto loc_824CE5A0;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
loc_824CE584:
	// lfs f13,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// beq cr6,0x824ce5a0
	if (cr6.eq) goto loc_824CE5A0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// blt cr6,0x824ce584
	if (cr6.lt) goto loc_824CE584;
loc_824CE5A0:
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bne cr6,0x824ce5b4
	if (!cr6.eq) goto loc_824CE5B4;
	// stfs f0,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
loc_824CE5B4:
	// slw r9,r15,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (r15.u32 << (ctx.r10.u8 & 0x3F));
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// andc r9,r29,r9
	ctx.r9.u64 = r29.u64 & ~ctx.r9.u64;
	// or r29,r9,r11
	r29.u64 = ctx.r9.u64 | r11.u64;
loc_824CE5C4:
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// blt cr6,0x824ce568
	if (cr6.lt) goto loc_824CE568;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,18,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x824ce5f0
	if (!cr6.eq) goto loc_824CE5F0;
	// srw. r11,r5,r31
	r11.u64 = r31.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (r31.u8 & 0x3F));
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824ce6c8
	if (cr0.eq) goto loc_824CE6C8;
loc_824CE5F0:
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lis r10,0
	ctx.r10.s64 = 0;
	// lis r11,-28311
	r11.s64 = -1855389696;
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// ori r10,r10,36262
	ctx.r10.u64 = ctx.r10.u64 | 36262;
	// ori r11,r11,5192
	r11.u64 = r11.u64 | 5192;
	// rldimi r11,r10,32,0
	r11.u64 = (__builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000) | (r11.u64 & 0xFFFFFFFF);
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwimi r11,r31,14,15,17
	r11.u64 = (__builtin_rotateleft32(r31.u32, 14) & 0x1C000) | (r11.u64 & 0xFFFFFFFFFFFE3FFF);
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// slw r10,r16,r31
	ctx.r10.u64 = r31.u8 & 0x20 ? 0 : (r16.u32 << (r31.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// clrldi r11,r10,32
	r11.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r8,r8,r11
	ctx.r8.u64 = r11.u8 & 0x40 ? 0 : (ctx.r8.u64 >> (r11.u8 & 0x7F));
	// srd r8,r8,r11
	ctx.r8.u64 = r11.u8 & 0x40 ? 0 : (ctx.r8.u64 >> (r11.u8 & 0x7F));
	// srd r11,r8,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r8.u64 >> (r11.u8 & 0x7F));
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// rlwimi r10,r11,13,16,18
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 13) & 0xE000) | (ctx.r10.u64 & 0xFFFFFFFFFFFF1FFF);
	// rlwimi r7,r10,1,27,30
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r10.u32, 1) & 0x1E) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFE1);
	// rlwimi r7,r10,1,15,17
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r10.u32, 1) & 0x1C000) | (ctx.r7.u64 & 0xFFFFFFFFFFFE3FFF);
	// stw r7,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r7.u32);
loc_824CE654:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824ce6c8
	if (cr6.eq) goto loc_824CE6C8;
	// lwz r11,16(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// subf r11,r22,r11
	r11.s64 = r11.s64 - r22.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824ce6c0
	if (cr0.eq) goto loc_824CE6C0;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r11,r10,27,24,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// rlwinm r10,r10,0,27,18
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFE01F;
	// rlwinm r8,r11,27,29,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x6;
	// rlwinm r7,r11,29,29,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x6;
	// rlwinm r6,r11,31,29,30
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x6;
	// rlwinm r11,r11,1,29,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x6;
	// srw r8,r29,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r29.u32 >> (ctx.r8.u8 & 0x3F));
	// srw r7,r29,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r29.u32 >> (ctx.r7.u8 & 0x3F));
	// srw r6,r29,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (r29.u32 >> (ctx.r6.u8 & 0x3F));
	// rlwimi r7,r8,2,28,29
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 2) & 0xC) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFF3);
	// srw r11,r29,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// clrlwi r8,r7,28
	ctx.r8.u64 = ctx.r7.u32 & 0xF;
	// rlwimi r6,r8,2,0,29
	ctx.r6.u64 = (__builtin_rotateleft32(ctx.r8.u32, 2) & 0xFFFFFFFC) | (ctx.r6.u64 & 0xFFFFFFFF00000003);
	// rlwimi r11,r6,2,0,29
	r11.u64 = (__builtin_rotateleft32(ctx.r6.u32, 2) & 0xFFFFFFFC) | (r11.u64 & 0xFFFFFFFF00000003);
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
loc_824CE6C0:
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// b 0x824ce654
	goto loc_824CE654;
loc_824CE6C8:
	// stw r30,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r30.u32);
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// b 0x824ce6ec
	goto loc_824CE6EC;
loc_824CE6D8:
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
loc_824CE6EC:
	// lwz r30,28(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// bne 0x824ce4b8
	if (!cr0.eq) goto loc_824CE4B8;
loc_824CE6F8:
	// addic. r26,r26,-1
	xer.ca = r26.u32 > 0;
	r26.s64 = r26.s64 + -1;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// bne 0x824ce4a4
	if (!cr0.eq) goto loc_824CE4A4;
	// lis r11,-32179
	r11.s64 = -2108882944;
	// li r5,4
	ctx.r5.s64 = 4;
	// addi r6,r11,-11112
	ctx.r6.s64 = r11.s64 + -11112;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x823a0760
	sub_823A0760(ctx, base);
	// mr r21,r22
	r21.u64 = r22.u64;
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// beq cr6,0x824cec30
	if (cr6.eq) goto loc_824CEC30;
	// mr r20,r17
	r20.u64 = r17.u64;
loc_824CE72C:
	// lwz r24,0(r20)
	r24.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// cmplwi r24,0
	cr0.compare<uint32_t>(r24.u32, 0, xer);
	// beq 0x824cec20
	if (cr0.eq) goto loc_824CEC20;
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// mr r30,r22
	r30.u64 = r22.u64;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// rlwinm r4,r11,18,29,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// beq cr6,0x824ce8d4
	if (cr6.eq) goto loc_824CE8D4;
	// mr r31,r17
	r31.u64 = r17.u64;
loc_824CE750:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824ce7e4
	if (cr0.eq) goto loc_824CE7E4;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x824ce7dc
	if (cr6.eq) goto loc_824CE7DC;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// addi r8,r24,40
	ctx.r8.s64 = r24.s64 + 40;
loc_824CE77C:
	// lfs f0,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	f0.f64 = double(temp.f32);
	// mr r11,r22
	r11.u64 = r22.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x824ce7c0
	if (cr6.eq) goto loc_824CE7C0;
	// addi r10,r3,40
	ctx.r10.s64 = ctx.r3.s64 + 40;
loc_824CE790:
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// beq cr6,0x824ce7b0
	if (cr6.eq) goto loc_824CE7B0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// blt cr6,0x824ce790
	if (cr6.lt) goto loc_824CE790;
	// b 0x824ce7c0
	goto loc_824CE7C0;
loc_824CE7B0:
	// slw r29,r15,r9
	r29.u64 = ctx.r9.u8 & 0x20 ? 0 : (r15.u32 << (ctx.r9.u8 & 0x3F));
	// slw r10,r11,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r9.u8 & 0x3F));
	// andc r5,r5,r29
	ctx.r5.u64 = ctx.r5.u64 & ~r29.u64;
	// or r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 | ctx.r10.u64;
loc_824CE7C0:
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// beq cr6,0x824ce7dc
	if (cr6.eq) goto loc_824CE7DC;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// cmplw cr6,r7,r4
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r4.u32, xer);
	// blt cr6,0x824ce77c
	if (cr6.lt) goto loc_824CE77C;
loc_824CE7DC:
	// cmplw cr6,r7,r4
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r4.u32, xer);
	// beq cr6,0x824ce7f8
	if (cr6.eq) goto loc_824CE7F8;
loc_824CE7E4:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplw cr6,r30,r21
	cr6.compare<uint32_t>(r30.u32, r21.u32, xer);
	// blt cr6,0x824ce750
	if (cr6.lt) goto loc_824CE750;
	// b 0x824ce8cc
	goto loc_824CE8CC;
loc_824CE7F8:
	// addi r8,r24,4
	ctx.r8.s64 = r24.s64 + 4;
loc_824CE7FC:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824ce8b4
	if (cr0.eq) goto loc_824CE8B4;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r22,r10
	ctx.r10.s64 = ctx.r10.s64 - r22.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824ce8a0
	if (cr0.eq) goto loc_824CE8A0;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824ce834
	goto loc_824CE834;
loc_824CE830:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824CE834:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824ce830
	if (!cr6.eq) goto loc_824CE830;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r3,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r3.u32);
	// rlwinm r10,r9,27,24,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0xFF;
	// rlwinm r9,r9,0,27,18
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFE01F;
	// rlwinm r7,r10,27,29,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x6;
	// rlwinm r6,r10,29,29,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x6;
	// rlwinm r4,r10,31,29,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x6;
	// rlwinm r10,r10,1,29,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x6;
	// srw r7,r5,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r7.u8 & 0x3F));
	// srw r6,r5,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r6.u8 & 0x3F));
	// srw r4,r5,r4
	ctx.r4.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r4.u8 & 0x3F));
	// rlwimi r6,r7,2,28,29
	ctx.r6.u64 = (__builtin_rotateleft32(ctx.r7.u32, 2) & 0xC) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFFF3);
	// srw r10,r5,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r10.u8 & 0x3F));
	// clrlwi r7,r6,28
	ctx.r7.u64 = ctx.r6.u32 & 0xF;
	// rlwimi r4,r7,2,0,29
	ctx.r4.u64 = (__builtin_rotateleft32(ctx.r7.u32, 2) & 0xFFFFFFFC) | (ctx.r4.u64 & 0xFFFFFFFF00000003);
	// rlwimi r10,r4,2,0,29
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r4.u32, 2) & 0xFFFFFFFC) | (ctx.r10.u64 & 0xFFFFFFFF00000003);
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_824CE8A0:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824ce7fc
	if (!cr6.eq) goto loc_824CE7FC;
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// b 0x824ce7fc
	goto loc_824CE7FC;
loc_824CE8B4:
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// stw r22,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r22.u32);
loc_824CE8CC:
	// cmplw cr6,r30,r21
	cr6.compare<uint32_t>(r30.u32, r21.u32, xer);
	// blt cr6,0x824cec20
	if (cr6.lt) goto loc_824CEC20;
loc_824CE8D4:
	// addi r23,r21,1
	r23.s64 = r21.s64 + 1;
loc_824CE8D8:
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// mr r31,r23
	r31.u64 = r23.u64;
	// mr r27,r22
	r27.u64 = r22.u64;
	// mr r30,r22
	r30.u64 = r22.u64;
	// li r28,-1
	r28.s64 = -1;
	// mr r25,r22
	r25.u64 = r22.u64;
	// rlwinm r29,r11,18,29,31
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// cmplw cr6,r31,r18
	cr6.compare<uint32_t>(r31.u32, r18.u32, xer);
	// bge cr6,0x824cec20
	if (!cr6.lt) goto loc_824CEC20;
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r26,r11,r17
	r26.u64 = r11.u64 + r17.u64;
loc_824CE904:
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824ce9dc
	if (cr0.eq) goto loc_824CE9DC;
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// rlwinm. r3,r11,18,29,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x824ce9b4
	if (cr0.eq) goto loc_824CE9B4;
	// mr r11,r22
	r11.u64 = r22.u64;
	// addi r5,r10,40
	ctx.r5.s64 = ctx.r10.s64 + 40;
loc_824CE934:
	// lfs f0,0(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	f0.f64 = double(temp.f32);
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824ce97c
	if (cr6.eq) goto loc_824CE97C;
	// addi r9,r24,40
	ctx.r9.s64 = r24.s64 + 40;
loc_824CE948:
	// lfs f13,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// beq cr6,0x824ce968
	if (cr6.eq) goto loc_824CE968;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplw cr6,r10,r29
	cr6.compare<uint32_t>(ctx.r10.u32, r29.u32, xer);
	// blt cr6,0x824ce948
	if (cr6.lt) goto loc_824CE948;
	// b 0x824ce97c
	goto loc_824CE97C;
loc_824CE968:
	// slw r14,r15,r11
	r14.u64 = r11.u8 & 0x20 ? 0 : (r15.u32 << (r11.u8 & 0x3F));
	// slw r9,r10,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// andc r8,r8,r14
	ctx.r8.u64 = ctx.r8.u64 & ~r14.u64;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// or r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 | ctx.r9.u64;
loc_824CE97C:
	// cmplw cr6,r10,r29
	cr6.compare<uint32_t>(ctx.r10.u32, r29.u32, xer);
	// bne cr6,0x824ce9a0
	if (!cr6.eq) goto loc_824CE9A0;
	// cmplwi cr6,r7,4
	cr6.compare<uint32_t>(ctx.r7.u32, 4, xer);
	// beq cr6,0x824ce9b4
	if (cr6.eq) goto loc_824CE9B4;
	// slw r9,r15,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (r15.u32 << (r11.u8 & 0x3F));
	// slw r10,r7,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (r11.u8 & 0x3F));
	// andc r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 & ~ctx.r9.u64;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// or r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 | ctx.r10.u64;
loc_824CE9A0:
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplw cr6,r4,r3
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r3.u32, xer);
	// blt cr6,0x824ce934
	if (cr6.lt) goto loc_824CE934;
loc_824CE9B4:
	// cmplw cr6,r4,r3
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r3.u32, xer);
	// bne cr6,0x824ce9dc
	if (!cr6.eq) goto loc_824CE9DC;
	// cmplw cr6,r7,r30
	cr6.compare<uint32_t>(ctx.r7.u32, r30.u32, xer);
	// blt cr6,0x824ce9dc
	if (cr6.lt) goto loc_824CE9DC;
	// cmplw cr6,r6,r27
	cr6.compare<uint32_t>(ctx.r6.u32, r27.u32, xer);
	// blt cr6,0x824ce9dc
	if (cr6.lt) goto loc_824CE9DC;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r28,r31
	r28.u64 = r31.u64;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
loc_824CE9DC:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// cmplw cr6,r31,r18
	cr6.compare<uint32_t>(r31.u32, r18.u32, xer);
	// blt cr6,0x824ce904
	if (cr6.lt) goto loc_824CE904;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824cec20
	if (cr6.eq) goto loc_824CEC20;
	// rlwinm r27,r28,2,0,29
	r27.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r29,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r4,r24,40
	ctx.r4.s64 = r24.s64 + 40;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwzx r28,r27,r17
	r28.u64 = PPC_LOAD_U32(r27.u32 + r17.u32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824cea50
	if (cr6.eq) goto loc_824CEA50;
	// mr r11,r22
	r11.u64 = r22.u64;
	// addi r10,r28,40
	ctx.r10.s64 = r28.s64 + 40;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
loc_824CEA20:
	// srw r9,r25,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (r25.u32 >> (r11.u8 & 0x3F));
	// clrlwi r9,r9,30
	ctx.r9.u64 = ctx.r9.u32 & 0x3;
	// cmplw cr6,r9,r29
	cr6.compare<uint32_t>(ctx.r9.u32, r29.u32, xer);
	// blt cr6,0x824cea40
	if (cr6.lt) goto loc_824CEA40;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// stfsx f0,r9,r7
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, temp.u32);
loc_824CEA40:
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne 0x824cea20
	if (!cr0.eq) goto loc_824CEA20;
loc_824CEA50:
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// stw r22,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r22.u32);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x82554df0
	sub_82554DF0(ctx, base);
	// lis r11,-28311
	r11.s64 = -1855389696;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r11,r11,5192
	r11.u64 = r11.u64 | 5192;
	// ori r10,r10,36262
	ctx.r10.u64 = ctx.r10.u64 | 36262;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// rldimi r11,r10,32,0
	r11.u64 = (__builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000) | (r11.u64 & 0xFFFFFFFF);
	// addi r8,r24,4
	ctx.r8.s64 = r24.s64 + 4;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lwz r7,8(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// slw r11,r16,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r16.u32 << (r30.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// clrldi r10,r11,32
	ctx.r10.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r9,r9,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r10.u8 & 0x7F));
	// srd r9,r9,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r10.u8 & 0x7F));
	// srd r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r10.u8 & 0x7F));
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// rlwimi r11,r10,13,16,18
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 13) & 0xE000) | (r11.u64 & 0xFFFFFFFFFFFF1FFF);
	// rlwimi r7,r11,1,27,30
	ctx.r7.u64 = (__builtin_rotateleft32(r11.u32, 1) & 0x1E) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFE1);
	// rlwimi r7,r11,1,15,17
	ctx.r7.u64 = (__builtin_rotateleft32(r11.u32, 1) & 0x1C000) | (ctx.r7.u64 & 0xFFFFFFFFFFFE3FFF);
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
loc_824CEABC:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824ceb2c
	if (cr0.eq) goto loc_824CEB2C;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r22,r10
	ctx.r10.s64 = ctx.r10.s64 - r22.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824ceb18
	if (cr0.eq) goto loc_824CEB18;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824ceaf4
	goto loc_824CEAF4;
loc_824CEAF0:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824CEAF4:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824ceaf0
	if (!cr6.eq) goto loc_824CEAF0;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r31,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r31.u32);
loc_824CEB18:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824ceabc
	if (!cr6.eq) goto loc_824CEABC;
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// b 0x824ceabc
	goto loc_824CEABC;
loc_824CEB2C:
	// addi r8,r28,4
	ctx.r8.s64 = r28.s64 + 4;
loc_824CEB30:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824cebe8
	if (cr0.eq) goto loc_824CEBE8;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r22,r10
	ctx.r10.s64 = ctx.r10.s64 - r22.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824cebd4
	if (cr0.eq) goto loc_824CEBD4;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824ceb68
	goto loc_824CEB68;
loc_824CEB64:
	// addi r10,r9,8
	ctx.r10.s64 = ctx.r9.s64 + 8;
loc_824CEB68:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x824ceb64
	if (!cr6.eq) goto loc_824CEB64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r31,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r31.u32);
	// rlwinm r10,r9,27,24,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0xFF;
	// rlwinm r9,r9,0,27,18
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFE01F;
	// rlwinm r7,r10,27,29,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x6;
	// rlwinm r6,r10,29,29,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x6;
	// rlwinm r5,r10,31,29,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x6;
	// rlwinm r10,r10,1,29,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x6;
	// srw r7,r25,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r25.u32 >> (ctx.r7.u8 & 0x3F));
	// srw r6,r25,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (r25.u32 >> (ctx.r6.u8 & 0x3F));
	// srw r5,r25,r5
	ctx.r5.u64 = ctx.r5.u8 & 0x20 ? 0 : (r25.u32 >> (ctx.r5.u8 & 0x3F));
	// rlwimi r6,r7,2,28,29
	ctx.r6.u64 = (__builtin_rotateleft32(ctx.r7.u32, 2) & 0xC) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFFF3);
	// srw r10,r25,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r25.u32 >> (ctx.r10.u8 & 0x3F));
	// clrlwi r7,r6,28
	ctx.r7.u64 = ctx.r6.u32 & 0xF;
	// rlwimi r5,r7,2,0,29
	ctx.r5.u64 = (__builtin_rotateleft32(ctx.r7.u32, 2) & 0xFFFFFFFC) | (ctx.r5.u64 & 0xFFFFFFFF00000003);
	// rlwimi r10,r5,2,0,29
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r5.u32, 2) & 0xFFFFFFFC) | (ctx.r10.u64 & 0xFFFFFFFF00000003);
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_824CEBD4:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824ceb30
	if (!cr6.eq) goto loc_824CEB30;
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// b 0x824ceb30
	goto loc_824CEB30;
loc_824CEBE8:
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r31,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r31.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
	// mr r24,r31
	r24.u64 = r31.u64;
	// stwx r22,r27,r17
	PPC_STORE_U32(r27.u32 + r17.u32, r22.u32);
	// b 0x824ce8d8
	goto loc_824CE8D8;
loc_824CEC20:
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// addi r20,r20,4
	r20.s64 = r20.s64 + 4;
	// cmplw cr6,r21,r18
	cr6.compare<uint32_t>(r21.u32, r18.u32, xer);
	// blt cr6,0x824ce72c
	if (cr6.lt) goto loc_824CE72C;
loc_824CEC30:
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_824CEC38"))) PPC_WEAK_FUNC(sub_824CEC38);
PPC_FUNC_IMPL(__imp__sub_824CEC38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r21,-1
	r21.s64 = -1;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r22,r4
	r22.u64 = ctx.r4.u64;
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r23,r21
	r23.u64 = r21.u64;
	// mr r20,r21
	r20.u64 = r21.u64;
	// li r25,0
	r25.s64 = 0;
	// subfic r24,r27,256
	xer.ca = r27.u32 <= 256;
	r24.s64 = 256 - r27.s64;
loc_824CEC68:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x824cdf98
	sub_824CDF98(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// beq cr6,0x824cee00
	if (cr6.eq) goto loc_824CEE00;
	// rlwinm r4,r31,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r3,r22,8
	ctx.r3.s64 = r22.s64 + 8;
	// bl 0x824cd4b8
	sub_824CD4B8(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824ceca0
	if (cr6.eq) goto loc_824CECA0;
	// rlwinm r28,r3,30,2,31
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// cmpwi cr6,r28,-1
	cr6.compare<int32_t>(r28.s32, -1, xer);
	// bne cr6,0x824ceca4
	if (!cr6.eq) goto loc_824CECA4;
loc_824CECA0:
	// add r28,r24,r27
	r28.u64 = r24.u64 + r27.u64;
loc_824CECA4:
	// add r11,r31,r27
	r11.u64 = r31.u64 + r27.u64;
	// rlwinm r26,r28,0,0,29
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFC;
	// rlwinm r10,r11,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// cmplw cr6,r10,r26
	cr6.compare<uint32_t>(ctx.r10.u32, r26.u32, xer);
	// bne cr6,0x824cecc0
	if (!cr6.eq) goto loc_824CECC0;
	// subf r29,r11,r28
	r29.s64 = r28.s64 - r11.s64;
	// b 0x824cecd8
	goto loc_824CECD8;
loc_824CECC0:
	// subf r29,r26,r28
	r29.s64 = r28.s64 - r26.s64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x824cecd8
	if (cr6.eq) goto loc_824CECD8;
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r29,r11,4
	r29.s64 = r11.s64 + 4;
loc_824CECD8:
	// cmplw cr6,r28,r24
	cr6.compare<uint32_t>(r28.u32, r24.u32, xer);
	// blt cr6,0x824cece8
	if (cr6.lt) goto loc_824CECE8;
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// b 0x824ced1c
	goto loc_824CED1C;
loc_824CECE8:
	// subf r10,r27,r28
	ctx.r10.s64 = r28.s64 - r27.s64;
	// rlwinm r11,r31,0,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFC;
	// rlwinm r9,r10,0,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x824ced04
	if (!cr6.eq) goto loc_824CED04;
	// subf r7,r31,r10
	ctx.r7.s64 = ctx.r10.s64 - r31.s64;
	// b 0x824ced1c
	goto loc_824CED1C;
loc_824CED04:
	// subf r7,r9,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x824ced1c
	if (cr6.eq) goto loc_824CED1C;
	// subf r11,r31,r11
	r11.s64 = r11.s64 - r31.s64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r7,r11,4
	ctx.r7.s64 = r11.s64 + 4;
loc_824CED1C:
	// addi r11,r31,3
	r11.s64 = r31.s64 + 3;
	// rlwinm r9,r31,0,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFC;
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// rlwinm r8,r11,0,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// beq cr6,0x824ced44
	if (cr6.eq) goto loc_824CED44;
	// subf r10,r31,r9
	ctx.r10.s64 = ctx.r9.s64 - r31.s64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_824CED44:
	// cmplw cr6,r8,r26
	cr6.compare<uint32_t>(ctx.r8.u32, r26.u32, xer);
	// bne cr6,0x824ced58
	if (!cr6.eq) goto loc_824CED58;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// add r10,r11,r28
	ctx.r10.u64 = r11.u64 + r28.u64;
	// b 0x824ced74
	goto loc_824CED74;
loc_824CED58:
	// subf r10,r26,r10
	ctx.r10.s64 = ctx.r10.s64 - r26.s64;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// beq cr6,0x824ced74
	if (cr6.eq) goto loc_824CED74;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
loc_824CED74:
	// cmplw cr6,r7,r29
	cr6.compare<uint32_t>(ctx.r7.u32, r29.u32, xer);
	// bge cr6,0x824ced84
	if (!cr6.lt) goto loc_824CED84;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// subf r31,r27,r28
	r31.s64 = r28.s64 - r27.s64;
loc_824CED84:
	// cmplw cr6,r10,r29
	cr6.compare<uint32_t>(ctx.r10.u32, r29.u32, xer);
	// bge cr6,0x824ced98
	if (!cr6.lt) goto loc_824CED98;
	// addi r11,r31,3
	r11.s64 = r31.s64 + 3;
	// mr r29,r10
	r29.u64 = ctx.r10.u64;
	// rlwinm r31,r11,0,0,29
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
loc_824CED98:
	// li r30,0
	r30.s64 = 0;
	// cmplw cr6,r28,r24
	cr6.compare<uint32_t>(r28.u32, r24.u32, xer);
	// bge cr6,0x824cedd0
	if (!cr6.lt) goto loc_824CEDD0;
	// li r6,4
	ctx.r6.s64 = 4;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x824cd878
	sub_824CD878(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824cedd0
	if (cr6.eq) goto loc_824CEDD0;
	// rlwinm r11,r3,28,4,31
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 28) & 0xFFFFFFF;
	// addi r10,r11,3
	ctx.r10.s64 = r11.s64 + 3;
	// rlwinm r10,r10,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// subf r30,r11,r10
	r30.s64 = ctx.r10.s64 - r11.s64;
loc_824CEDD0:
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// cmpwi cr6,r23,-1
	cr6.compare<int32_t>(r23.s32, -1, xer);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// beq cr6,0x824cede8
	if (cr6.eq) goto loc_824CEDE8;
	// cmplw cr6,r11,r20
	cr6.compare<uint32_t>(r11.u32, r20.u32, xer);
	// bge cr6,0x824cedf0
	if (!cr6.lt) goto loc_824CEDF0;
loc_824CEDE8:
	// mr r23,r31
	r23.u64 = r31.u64;
	// mr r20,r11
	r20.u64 = r11.u64;
loc_824CEDF0:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// subf r25,r26,r28
	r25.s64 = r28.s64 - r26.s64;
	// cmplw cr6,r28,r24
	cr6.compare<uint32_t>(r28.u32, r24.u32, xer);
	// ble cr6,0x824cec68
	if (!cr6.gt) goto loc_824CEC68;
loc_824CEE00:
	// cmpwi cr6,r23,-1
	cr6.compare<int32_t>(r23.s32, -1, xer);
	// bne cr6,0x824cee14
	if (!cr6.eq) goto loc_824CEE14;
	// li r4,3566
	ctx.r4.s64 = 3566;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CEE14:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x824cdeb0
	sub_824CDEB0(ctx, base);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_824CEE30"))) PPC_WEAK_FUNC(sub_824CEE30);
PPC_FUNC_IMPL(__imp__sub_824CEE30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r7,0
	ctx.r7.s64 = 0;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cee94
	if (!cr0.eq) goto loc_824CEE94;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824cee94
	if (cr0.eq) goto loc_824CEE94;
loc_824CEE48:
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824cee78
	if (cr0.eq) goto loc_824CEE78;
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
loc_824CEE5C:
	// lwz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmplw cr6,r6,r4
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r4.u32, xer);
	// beq cr6,0x824cee9c
	if (cr6.eq) goto loc_824CEE9C;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x824cee5c
	if (cr6.lt) goto loc_824CEE5C;
loc_824CEE78:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cee94
	if (!cr0.eq) goto loc_824CEE94;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824cee48
	if (!cr6.eq) goto loc_824CEE48;
loc_824CEE94:
	// li r3,-1
	ctx.r3.s64 = -1;
	// blr 
	return;
loc_824CEE9C:
	// add r3,r10,r7
	ctx.r3.u64 = ctx.r10.u64 + ctx.r7.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CEEA4"))) PPC_WEAK_FUNC(sub_824CEEA4);
PPC_FUNC_IMPL(__imp__sub_824CEEA4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CEEA8"))) PPC_WEAK_FUNC(sub_824CEEA8);
PPC_FUNC_IMPL(__imp__sub_824CEEA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,10
	cr6.compare<uint32_t>(ctx.r4.u32, 10, xer);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bgt cr6,0x824ceed0
	if (cr6.gt) goto loc_824CEED0;
	// li r30,10
	r30.s64 = 10;
loc_824CEED0:
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x824a9e88
	sub_824A9E88(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r30,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r30.u32);
	// rlwinm r11,r3,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFFFFFE;
	// ori r9,r31,1
	ctx.r9.u64 = r31.u64 | 1;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r8,r8,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CEF2C"))) PPC_WEAK_FUNC(sub_824CEF2C);
PPC_FUNC_IMPL(__imp__sub_824CEF2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CEF30"))) PPC_WEAK_FUNC(sub_824CEF30);
PPC_FUNC_IMPL(__imp__sub_824CEF30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x824ce288
	sub_824CE288(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x824cef64
	if (!cr6.eq) goto loc_824CEF64;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x824cd100
	sub_824CD100(ctx, base);
loc_824CEF64:
	// add r11,r31,r29
	r11.u64 = r31.u64 + r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r11,-1
	ctx.r5.s64 = r11.s64 + -1;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x824cd9e0
	sub_824CD9E0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CEF84"))) PPC_WEAK_FUNC(sub_824CEF84);
PPC_FUNC_IMPL(__imp__sub_824CEF84) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CEF88"))) PPC_WEAK_FUNC(sub_824CEF88);
PPC_FUNC_IMPL(__imp__sub_824CEF88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x824ce2f0
	sub_824CE2F0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x824cefc0
	if (!cr6.eq) goto loc_824CEFC0;
	// li r4,3569
	ctx.r4.s64 = 3569;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CEFC0:
	// add r11,r31,r29
	r11.u64 = r31.u64 + r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r11,-1
	ctx.r5.s64 = r11.s64 + -1;
	// addi r3,r30,8
	ctx.r3.s64 = r30.s64 + 8;
	// bl 0x824cdb50
	sub_824CDB50(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CEFE0"))) PPC_WEAK_FUNC(sub_824CEFE0);
PPC_FUNC_IMPL(__imp__sub_824CEFE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x824ce358
	sub_824CE358(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x824cf018
	if (!cr6.eq) goto loc_824CF018;
	// li r4,3570
	ctx.r4.s64 = 3570;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CF018:
	// add r11,r31,r29
	r11.u64 = r31.u64 + r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r11,-1
	ctx.r5.s64 = r11.s64 + -1;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x824cd9e0
	sub_824CD9E0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CF038"))) PPC_WEAK_FUNC(sub_824CF038);
PPC_FUNC_IMPL(__imp__sub_824CF038) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,1412(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1412);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824cf0ac
	if (cr0.eq) goto loc_824CF0AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824cee30
	sub_824CEE30(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824cf0ac
	if (cr6.eq) goto loc_824CF0AC;
	// clrlwi. r11,r3,31
	r11.u64 = ctx.r3.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cf0ac
	if (!cr0.eq) goto loc_824CF0AC;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r9,r3,1
	ctx.r9.s64 = ctx.r3.s64 + 1;
	// b 0x824cf084
	goto loc_824CF084;
loc_824CF078:
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
loc_824CF084:
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824cf090
	if (cr0.eq) goto loc_824CF090;
	// li r11,0
	r11.s64 = 0;
loc_824CF090:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x824cf078
	if (!cr6.lt) goto loc_824CF078;
	// addi r10,r9,4
	ctx.r10.s64 = ctx.r9.s64 + 4;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// b 0x824cf0b0
	goto loc_824CF0B0;
loc_824CF0AC:
	// li r3,0
	ctx.r3.s64 = 0;
loc_824CF0B0:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CF0C4"))) PPC_WEAK_FUNC(sub_824CF0C4);
PPC_FUNC_IMPL(__imp__sub_824CF0C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CF0C8"))) PPC_WEAK_FUNC(sub_824CF0C8);
PPC_FUNC_IMPL(__imp__sub_824CF0C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mulli r11,r4,40
	r11.s64 = ctx.r4.s64 * 40;
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// rlwinm r11,r9,28,29,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 28) & 0x7;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824cf22c
	if (cr6.lt) goto loc_824CF22C;
	// beq cr6,0x824cf200
	if (cr6.eq) goto loc_824CF200;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// blt cr6,0x824cf1f4
	if (cr6.lt) goto loc_824CF1F4;
	// beq cr6,0x824cf164
	if (cr6.eq) goto loc_824CF164;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lis r10,4080
	ctx.r10.s64 = 267386880;
	// rlwinm r11,r11,0,3,11
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1FF00000;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x824cf124
	if (cr6.gt) goto loc_824CF124;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CF124:
	// rlwinm. r11,r9,0,16,24
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFF80;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cf158
	if (cr0.eq) goto loc_824CF158;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824cf038
	sub_824CF038(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x824cf148
	if (!cr0.eq) goto loc_824CF148;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CF148:
	// lwz r5,24(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// li r4,4509
	ctx.r4.s64 = 4509;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CF158:
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// b 0x824cf1ec
	goto loc_824CF1EC;
loc_824CF164:
	// lwz r9,756(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 756);
	// lis r11,-1
	r11.s64 = -65536;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824cf17c
	if (cr0.eq) goto loc_824CF17C;
	// lwz r10,752(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 752);
	// b 0x824cf1a4
	goto loc_824CF1A4;
loc_824CF17C:
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// rlwinm r10,r10,0,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFF0000;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,31,27,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x10;
loc_824CF1A4:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824cf1b4
	if (cr6.eq) goto loc_824CF1B4;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// b 0x824cf1e4
	goto loc_824CF1E4;
loc_824CF1B4:
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// rlwinm r9,r9,0,0,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFF0000;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,28,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xC;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_824CF1E4:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
loc_824CF1EC:
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r11.u32);
	// b 0x824cf25c
	goto loc_824CF25C;
loc_824CF1F4:
	// li r11,0
	r11.s64 = 0;
	// li r10,256
	ctx.r10.s64 = 256;
	// b 0x824cf254
	goto loc_824CF254;
loc_824CF200:
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lis r10,-1
	ctx.r10.s64 = -65536;
	// rlwinm r11,r11,0,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF0000;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,31,27,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x10;
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// b 0x824cf254
	goto loc_824CF254;
loc_824CF22C:
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lis r10,-1
	ctx.r10.s64 = -65536;
	// rlwinm r11,r11,0,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF0000;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,2,24,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0x80;
	// addi r10,r11,128
	ctx.r10.s64 = r11.s64 + 128;
loc_824CF254:
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// stw r10,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
loc_824CF25C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CF270"))) PPC_WEAK_FUNC(sub_824CF270);
PPC_FUNC_IMPL(__imp__sub_824CF270) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x824cf0c8
	sub_824CF0C8(ctx, base);
	// lwz r10,12(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// mulli r11,r31,40
	r11.s64 = r31.s64 * 40;
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
	// li r29,63
	r29.s64 = 63;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// rlwinm r11,r11,28,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x7;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824cf2d8
	if (cr6.lt) goto loc_824CF2D8;
	// beq cr6,0x824cf2d0
	if (cr6.eq) goto loc_824CF2D0;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// blt cr6,0x824cf2c8
	if (cr6.lt) goto loc_824CF2C8;
	// bne cr6,0x824cf2dc
	if (!cr6.eq) goto loc_824CF2DC;
	// li r29,115
	r29.s64 = 115;
	// b 0x824cf2dc
	goto loc_824CF2DC;
loc_824CF2C8:
	// li r29,99
	r29.s64 = 99;
	// b 0x824cf2dc
	goto loc_824CF2DC;
loc_824CF2D0:
	// li r29,105
	r29.s64 = 105;
	// b 0x824cf2dc
	goto loc_824CF2DC;
loc_824CF2D8:
	// li r29,98
	r29.s64 = 98;
loc_824CF2DC:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824cf038
	sub_824CF038(ctx, base);
	// lwz r31,84(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824cf2fc
	if (cr0.eq) goto loc_824CF2FC;
	// lwz r5,24(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// b 0x824cf324
	goto loc_824CF324;
loc_824CF2FC:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// extsb r6,r29
	ctx.r6.s64 = r29.s8;
	// addi r5,r11,9768
	ctx.r5.s64 = r11.s64 + 9768;
	// rlwinm r11,r10,12,23,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x1FF;
	// li r4,256
	ctx.r4.s64 = 256;
	// subf r7,r31,r11
	ctx.r7.s64 = r11.s64 - r31.s64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x823a1348
	sub_823A1348(ctx, base);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
loc_824CF324:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bge cr6,0x824cf350
	if (!cr6.lt) goto loc_824CF350;
	// extsb r8,r29
	ctx.r8.s64 = r29.s8;
	// subf r11,r31,r11
	r11.s64 = r11.s64 - r31.s64;
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// li r4,3563
	ctx.r4.s64 = 3563;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CF350:
	// li r4,4509
	ctx.r4.s64 = 4509;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
}

__attribute__((alias("__imp__sub_824CF358"))) PPC_WEAK_FUNC(sub_824CF358);
PPC_FUNC_IMPL(__imp__sub_824CF358) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// add r28,r4,r5
	r28.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// cmplw cr6,r4,r28
	cr6.compare<uint32_t>(ctx.r4.u32, r28.u32, xer);
	// bge cr6,0x824cf3e8
	if (!cr6.lt) goto loc_824CF3E8;
	// rlwinm r31,r4,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
loc_824CF37C:
	// addi r10,r31,3
	ctx.r10.s64 = r31.s64 + 3;
	// li r9,2
	ctx.r9.s64 = 2;
	// clrldi r8,r10,58
	ctx.r8.u64 = ctx.r10.u64 & 0x3F;
	// rlwinm r10,r31,26,6,31
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 26) & 0x3FFFFFF;
	// clrldi r11,r31,58
	r11.u64 = r31.u64 & 0x3F;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// li r7,-1
	ctx.r7.s64 = -1;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// ldx r6,r10,r29
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r10.u32 + r29.u32);
	// sld r10,r9,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// sld r9,r7,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r7.u64 << (r11.u8 & 0x7F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 & ctx.r6.u64;
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// subfic r11,r11,15
	xer.ca = r11.u32 <= 15;
	r11.s64 = 15 - r11.s64;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824cf3d8
	if (cr0.eq) goto loc_824CF3D8;
	// rlwimi r11,r30,4,0,27
	r11.u64 = (__builtin_rotateleft32(r30.u32, 4) & 0xFFFFFFF0) | (r11.u64 & 0xFFFFFFFF0000000F);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// bl 0x824cde38
	sub_824CDE38(ctx, base);
loc_824CF3D8:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// blt cr6,0x824cf37c
	if (cr6.lt) goto loc_824CF37C;
loc_824CF3E8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824CF3F0"))) PPC_WEAK_FUNC(sub_824CF3F0);
PPC_FUNC_IMPL(__imp__sub_824CF3F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// add r7,r4,r5
	ctx.r7.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// cmplw cr6,r4,r7
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r7.u32, xer);
	// bge cr6,0x824cf488
	if (!cr6.lt) goto loc_824CF488;
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
loc_824CF408:
	// addi r10,r9,3
	ctx.r10.s64 = ctx.r9.s64 + 3;
	// li r6,2
	ctx.r6.s64 = 2;
	// clrldi r5,r10,58
	ctx.r5.u64 = ctx.r10.u64 & 0x3F;
	// rlwinm r10,r9,26,6,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 26) & 0x3FFFFFF;
	// clrldi r11,r9,58
	r11.u64 = ctx.r9.u64 & 0x3F;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// li r4,-1
	ctx.r4.s64 = -1;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// ldx r31,r10,r3
	r31.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r3.u32);
	// sld r10,r6,r5
	ctx.r10.u64 = ctx.r5.u8 & 0x40 ? 0 : (ctx.r6.u64 << (ctx.r5.u8 & 0x7F));
	// sld r6,r4,r11
	ctx.r6.u64 = r11.u8 & 0x40 ? 0 : (ctx.r4.u64 << (r11.u8 & 0x7F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 & r31.u64;
	// and r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 & ctx.r6.u64;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rotlwi. r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cf478
	if (cr0.eq) goto loc_824CF478;
	// rlwimi r11,r8,4,0,27
	r11.u64 = (__builtin_rotateleft32(ctx.r8.u32, 4) & 0xFFFFFFF0) | (r11.u64 & 0xFFFFFFFF0000000F);
	// rlwinm r10,r11,30,2,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFC;
	// clrldi r6,r11,60
	ctx.r6.u64 = r11.u64 & 0xF;
	// rlwinm r11,r10,26,6,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FFFFFF;
	// clrldi r10,r10,58
	ctx.r10.u64 = ctx.r10.u64 & 0x3F;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// sld r10,r6,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r6.u64 << (ctx.r10.u8 & 0x7F));
	// ldx r6,r11,r3
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + ctx.r3.u32);
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stdx r10,r11,r3
	PPC_STORE_U64(r11.u32 + ctx.r3.u32, ctx.r10.u64);
loc_824CF478:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// blt cr6,0x824cf408
	if (cr6.lt) goto loc_824CF408;
loc_824CF488:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824CF490"))) PPC_WEAK_FUNC(sub_824CF490);
PPC_FUNC_IMPL(__imp__sub_824CF490) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// add r29,r4,r5
	r29.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// cmplw cr6,r4,r29
	cr6.compare<uint32_t>(ctx.r4.u32, r29.u32, xer);
	// bge cr6,0x824cf50c
	if (!cr6.lt) goto loc_824CF50C;
loc_824CF4B0:
	// rlwinm r10,r31,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r11,r31,27
	r11.u64 = r31.u32 & 0x1F;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// li r9,2
	ctx.r9.s64 = 2;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r8,-1
	ctx.r8.s64 = -1;
	// lwzx r7,r10,r30
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// slw r10,r9,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// slw r9,r8,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// and r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 & ctx.r10.u64;
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// srw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824cf500
	if (cr0.eq) goto loc_824CF500;
	// rlwimi r11,r31,4,0,27
	r11.u64 = (__builtin_rotateleft32(r31.u32, 4) & 0xFFFFFFF0) | (r11.u64 & 0xFFFFFFFF0000000F);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// bl 0x824ce020
	sub_824CE020(ctx, base);
loc_824CF500:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// blt cr6,0x824cf4b0
	if (cr6.lt) goto loc_824CF4B0;
loc_824CF50C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CF514"))) PPC_WEAK_FUNC(sub_824CF514);
PPC_FUNC_IMPL(__imp__sub_824CF514) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CF518"))) PPC_WEAK_FUNC(sub_824CF518);
PPC_FUNC_IMPL(__imp__sub_824CF518) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// add r28,r4,r5
	r28.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// cmplw cr6,r4,r28
	cr6.compare<uint32_t>(ctx.r4.u32, r28.u32, xer);
	// bge cr6,0x824cf5a0
	if (!cr6.lt) goto loc_824CF5A0;
	// clrldi r30,r4,32
	r30.u64 = ctx.r4.u64 & 0xFFFFFFFF;
loc_824CF53C:
	// rlwinm r10,r31,26,6,31
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 26) & 0x3FFFFFF;
	// clrldi r11,r30,58
	r11.u64 = r30.u64 & 0x3F;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// li r9,2
	ctx.r9.s64 = 2;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// li r8,-1
	ctx.r8.s64 = -1;
	// ldx r7,r10,r29
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r10.u32 + r29.u32);
	// sld r10,r9,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// sld r9,r8,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r8.u64 << (r11.u8 & 0x7F));
	// and r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 & ctx.r10.u64;
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824cf590
	if (cr0.eq) goto loc_824CF590;
	// rlwimi r11,r31,4,0,27
	r11.u64 = (__builtin_rotateleft32(r31.u32, 4) & 0xFFFFFFF0) | (r11.u64 & 0xFFFFFFFF0000000F);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// bl 0x824ce0e8
	sub_824CE0E8(ctx, base);
loc_824CF590:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmplw cr6,r31,r28
	cr6.compare<uint32_t>(r31.u32, r28.u32, xer);
	// blt cr6,0x824cf53c
	if (cr6.lt) goto loc_824CF53C;
loc_824CF5A0:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824CF5A8"))) PPC_WEAK_FUNC(sub_824CF5A8);
PPC_FUNC_IMPL(__imp__sub_824CF5A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// add r29,r4,r5
	r29.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// cmplw cr6,r4,r29
	cr6.compare<uint32_t>(ctx.r4.u32, r29.u32, xer);
	// bge cr6,0x824cf624
	if (!cr6.lt) goto loc_824CF624;
loc_824CF5C8:
	// rlwinm r10,r31,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 27) & 0x7FFFFFF;
	// clrlwi r11,r31,27
	r11.u64 = r31.u32 & 0x1F;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// li r9,2
	ctx.r9.s64 = 2;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r8,-1
	ctx.r8.s64 = -1;
	// lwzx r7,r10,r30
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// slw r10,r9,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// slw r9,r8,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// and r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 & ctx.r10.u64;
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// srw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824cf618
	if (cr0.eq) goto loc_824CF618;
	// rlwimi r11,r31,4,0,27
	r11.u64 = (__builtin_rotateleft32(r31.u32, 4) & 0xFFFFFFF0) | (r11.u64 & 0xFFFFFFFF0000000F);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// bl 0x824ce020
	sub_824CE020(ctx, base);
loc_824CF618:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// blt cr6,0x824cf5c8
	if (cr6.lt) goto loc_824CF5C8;
loc_824CF624:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CF62C"))) PPC_WEAK_FUNC(sub_824CF62C);
PPC_FUNC_IMPL(__imp__sub_824CF62C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824CF630"))) PPC_WEAK_FUNC(sub_824CF630);
PPC_FUNC_IMPL(__imp__sub_824CF630) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// li r5,21
	ctx.r5.s64 = 21;
	// li r4,36
	ctx.r4.s64 = 36;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// bl 0x824a9858
	sub_824A9858(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r31,24
	r11.s64 = r31.s64 + 24;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// ori r8,r11,1
	ctx.r8.u64 = r11.u64 | 1;
	// ori r7,r9,1
	ctx.r7.u64 = ctx.r9.u64 | 1;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// stw r28,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r28.u32);
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
	// stw r10,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r10.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// stb r10,32(r31)
	PPC_STORE_U8(r31.u32 + 32, ctx.r10.u8);
	// bl 0x82527c60
	sub_82527C60(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824CF6A0"))) PPC_WEAK_FUNC(sub_824CF6A0);
PPC_FUNC_IMPL(__imp__sub_824CF6A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r11,0
	r11.s64 = 0;
	// addi r28,r29,8
	r28.s64 = r29.s64 + 8;
loc_824CF6B8:
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824cd4b8
	sub_824CD4B8(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824cf720
	if (cr6.eq) goto loc_824CF720;
	// rlwinm r30,r3,30,2,31
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// cmpwi cr6,r30,-1
	cr6.compare<int32_t>(r30.s32, -1, xer);
	// beq cr6,0x824cf720
	if (cr6.eq) goto loc_824CF720;
	// li r6,4
	ctx.r6.s64 = 4;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824cd878
	sub_824CD878(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// li r31,256
	r31.s64 = 256;
	// beq cr6,0x824cf6fc
	if (cr6.eq) goto loc_824CF6FC;
	// rlwinm r31,r3,28,4,31
	r31.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 28) & 0xFFFFFFF;
loc_824CF6FC:
	// addi r11,r31,3
	r11.s64 = r31.s64 + 3;
	// rlwinm r4,r30,0,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFC;
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// subf r5,r4,r11
	ctx.r5.s64 = r11.s64 - ctx.r4.s64;
	// bl 0x824cf358
	sub_824CF358(ctx, base);
	// mr r11,r31
	r11.u64 = r31.u64;
	// cmplwi cr6,r31,256
	cr6.compare<uint32_t>(r31.u32, 256, xer);
	// blt cr6,0x824cf6b8
	if (cr6.lt) goto loc_824CF6B8;
loc_824CF720:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824CF728"))) PPC_WEAK_FUNC(sub_824CF728);
PPC_FUNC_IMPL(__imp__sub_824CF728) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lwz r11,1412(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1412);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824cf770
	if (!cr6.eq) goto loc_824CF770;
	// li r5,44
	ctx.r5.s64 = 44;
	// li r4,8
	ctx.r4.s64 = 8;
	// bl 0x824b0748
	sub_824B0748(ctx, base);
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
	// ori r10,r3,1
	ctx.r10.u64 = ctx.r3.u64 | 1;
	// stw r3,1412(r31)
	PPC_STORE_U32(r31.u32 + 1412, ctx.r3.u32);
	// ori r9,r11,1
	ctx.r9.u64 = r11.u64 | 1;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
loc_824CF770:
	// lwz r3,1412(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1412);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cf7a4
	if (!cr0.eq) goto loc_824CF7A4;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-4
	xer.ca = r11.u32 > 3;
	r11.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cf7a4
	if (cr0.eq) goto loc_824CF7A4;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x824cf7b0
	if (!cr6.gt) goto loc_824CF7B0;
loc_824CF7A4:
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x824ceea8
	sub_824CEEA8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_824CF7B0:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// stwx r30,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r30.u32);
	// lwz r3,1412(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1412);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cf7fc
	if (!cr0.eq) goto loc_824CF7FC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r11,r11,-4
	xer.ca = r11.u32 > 3;
	r11.s64 = r11.s64 + -4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cf7fc
	if (cr0.eq) goto loc_824CF7FC;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x824cf808
	if (!cr6.gt) goto loc_824CF808;
loc_824CF7FC:
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x824ceea8
	sub_824CEEA8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_824CF808:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// stwx r29,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r29.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_824CF828"))) PPC_WEAK_FUNC(sub_824CF828);
PPC_FUNC_IMPL(__imp__sub_824CF828) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r17,r3
	r17.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mulli r11,r27,40
	r11.s64 = r27.s64 * 40;
	// lwz r10,12(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + 12);
	// stw r27,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, r27.u32);
	// add r21,r11,r10
	r21.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r4,r11,0,15,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1FFF8;
	// bl 0x824ce3c0
	sub_824CE3C0(ctx, base);
	// mr r18,r3
	r18.u64 = ctx.r3.u64;
	// lwz r11,16(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 16);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// stw r18,28(r21)
	PPC_STORE_U32(r21.u32 + 28, r18.u32);
	// beq 0x824cf878
	if (cr0.eq) goto loc_824CF878;
	// lwz r6,64(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// b 0x824cf87c
	goto loc_824CF87C;
loc_824CF878:
	// li r6,0
	ctx.r6.s64 = 0;
loc_824CF87C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824cf88c
	if (cr6.eq) goto loc_824CF88C;
	// lwz r5,60(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// b 0x824cf8ac
	goto loc_824CF8AC;
loc_824CF88C:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bne cr6,0x824cf8a8
	if (!cr6.eq) goto loc_824CF8A8;
	// lwz r11,24(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 24);
	// lwz r5,56(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// b 0x824cf8ac
	goto loc_824CF8AC;
loc_824CF8A8:
	// li r5,0
	ctx.r5.s64 = 0;
loc_824CF8AC:
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lwz r4,20(r21)
	ctx.r4.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// bl 0x824cf630
	sub_824CF630(ctx, base);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// li r23,0
	r23.s64 = 0;
	// li r16,0
	r16.s64 = 0;
	// li r15,0
	r15.s64 = 0;
	// li r14,0
	r14.s64 = 0;
loc_824CF8CC:
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824cf8ec
	if (!cr6.eq) goto loc_824CF8EC;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82527f38
	sub_82527F38(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// beq 0x824cf8f0
	if (cr0.eq) goto loc_824CF8F0;
loc_824CF8EC:
	// li r11,0
	r11.s64 = 0;
loc_824CF8F0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cfd14
	if (!cr0.eq) goto loc_824CFD14;
	// rlwinm r11,r23,3,0,28
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r10,32(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 32);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// add r11,r11,r18
	r11.u64 = r11.u64 + r18.u64;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwimi r10,r9,0,0,30
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0xFFFFFFFE) | (ctx.r10.u64 & 0xFFFFFFFF00000001);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// bl 0x82527f98
	sub_82527F98(ctx, base);
	// li r19,0
	r19.s64 = 0;
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,30
	cr6.compare<int32_t>(r20.s32, 30, xer);
	// stb r19,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r19.u8);
	// bgt cr6,0x824cf964
	if (cr6.gt) goto loc_824CF964;
	// cmpwi cr6,r20,29
	cr6.compare<int32_t>(r20.s32, 29, xer);
	// bge cr6,0x824cf94c
	if (!cr6.lt) goto loc_824CF94C;
	// cmpwi cr6,r20,24
	cr6.compare<int32_t>(r20.s32, 24, xer);
	// blt cr6,0x824cf978
	if (cr6.lt) goto loc_824CF978;
	// cmpwi cr6,r20,25
	cr6.compare<int32_t>(r20.s32, 25, xer);
	// ble cr6,0x824cf94c
	if (!cr6.gt) goto loc_824CF94C;
	// cmpwi cr6,r20,27
	cr6.compare<int32_t>(r20.s32, 27, xer);
	// bne cr6,0x824cf978
	if (!cr6.eq) goto loc_824CF978;
loc_824CF94C:
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// rlwinm r11,r11,0,25,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70;
	// cmplwi cr6,r11,112
	cr6.compare<uint32_t>(r11.u32, 112, xer);
	// bne cr6,0x824cfcd0
	if (!cr6.eq) goto loc_824CFCD0;
	// li r16,256
	r16.s64 = 256;
	// b 0x824cf9a4
	goto loc_824CF9A4;
loc_824CF964:
	// cmpwi cr6,r20,33
	cr6.compare<int32_t>(r20.s32, 33, xer);
	// beq cr6,0x824cf994
	if (cr6.eq) goto loc_824CF994;
	// ble cr6,0x824cf978
	if (!cr6.gt) goto loc_824CF978;
	// cmpwi cr6,r20,37
	cr6.compare<int32_t>(r20.s32, 37, xer);
	// ble cr6,0x824cf994
	if (!cr6.gt) goto loc_824CF994;
loc_824CF978:
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// rlwinm r11,r11,0,25,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70;
	// cmplwi cr6,r11,48
	cr6.compare<uint32_t>(r11.u32, 48, xer);
	// bne cr6,0x824cf9a4
	if (!cr6.eq) goto loc_824CF9A4;
	// li r4,3533
	ctx.r4.s64 = 3533;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CF994:
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// rlwinm r11,r11,0,25,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70;
	// cmplwi cr6,r11,48
	cr6.compare<uint32_t>(r11.u32, 48, xer);
	// bne cr6,0x824cfcdc
	if (!cr6.eq) goto loc_824CFCDC;
loc_824CF9A4:
	// lwz r4,12(r24)
	ctx.r4.u64 = PPC_LOAD_U32(r24.u32 + 12);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x824cfa80
	if (cr0.eq) goto loc_824CFA80;
	// clrlwi. r11,r15,24
	r11.u64 = r15.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cfa84
	if (!cr0.eq) goto loc_824CFA84;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x8251d278
	sub_8251D278(ctx, base);
	// b 0x824cf9e0
	goto loc_824CF9E0;
loc_824CF9C4:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmpwi cr6,r10,18
	cr6.compare<int32_t>(ctx.r10.s32, 18, xer);
	// beq cr6,0x824cf9ec
	if (cr6.eq) goto loc_824CF9EC;
	// lwz r4,12(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// bl 0x8251d2a0
	sub_8251D2A0(ctx, base);
loc_824CF9E0:
	// mr. r11,r3
	r11.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cf9c4
	if (!cr0.eq) goto loc_824CF9C4;
	// b 0x824cfa84
	goto loc_824CFA84;
loc_824CF9EC:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r31,20(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x824cfce8
	if (!cr6.eq) goto loc_824CFCE8;
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// rlwinm r5,r11,28,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x7;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x824cd248
	sub_824CD248(ctx, base);
	// lwz r9,4(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwimi r9,r8,4,25,27
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r8.u32, 4) & 0x70) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFF8F);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rlwimi r11,r10,20,3,11
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 20) & 0x1FF00000) | (r11.u64 & 0xFFFFFFFFE00FFFFF);
	// lis r10,4080
	ctx.r10.s64 = 267386880;
	// stw r9,4(r21)
	PPC_STORE_U32(r21.u32 + 4, ctx.r9.u32);
	// rlwinm r9,r11,0,3,11
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1FF00000;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// ble cr6,0x824cfa5c
	if (!cr6.gt) goto loc_824CFA5C;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x824cf728
	sub_824CF728(ctx, base);
loc_824CFA5C:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// clrlwi r10,r11,29
	ctx.r10.u64 = r11.u32 & 0x7;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bne cr6,0x824cfcf0
	if (!cr6.eq) goto loc_824CFCF0;
	// oris r11,r11,8
	r11.u64 = r11.u64 | 524288;
	// lbz r19,80(r1)
	r19.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// li r15,1
	r15.s64 = 1;
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// b 0x824cfa84
	goto loc_824CFA84;
loc_824CFA80:
	// li r14,1
	r14.s64 = 1;
loc_824CFA84:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r10,4(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// addi r31,r10,1
	r31.s64 = ctx.r10.s64 + 1;
	// lwz r25,28(r11)
	r25.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// lwz r28,32(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmplwi cr6,r25,1
	cr6.compare<uint32_t>(r25.u32, 1, xer);
	// ble cr6,0x824cfaa8
	if (!cr6.gt) goto loc_824CFAA8;
	// cmplwi cr6,r28,4
	cr6.compare<uint32_t>(r28.u32, 4, xer);
	// bgt cr6,0x824cfcfc
	if (cr6.gt) goto loc_824CFCFC;
loc_824CFAA8:
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// clrlwi r9,r9,29
	ctx.r9.u64 = ctx.r9.u32 & 0x7;
	// cmplwi cr6,r9,2
	cr6.compare<uint32_t>(ctx.r9.u32, 2, xer);
	// beq cr6,0x824cfae0
	if (cr6.eq) goto loc_824CFAE0;
	// cmplwi cr6,r25,1
	cr6.compare<uint32_t>(r25.u32, 1, xer);
	// ble cr6,0x824cfae0
	if (!cr6.gt) goto loc_824CFAE0;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm. r10,r11,22,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 22) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824cfae0
	if (cr0.eq) goto loc_824CFAE0;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r25,r28
	r25.u64 = r28.u64;
	// mr r28,r11
	r28.u64 = r11.u64;
loc_824CFAE0:
	// cmplwi cr6,r28,4
	cr6.compare<uint32_t>(r28.u32, 4, xer);
	// ble cr6,0x824cfaf4
	if (!cr6.gt) goto loc_824CFAF4;
	// addi r11,r28,3
	r11.s64 = r28.s64 + 3;
	// li r28,4
	r28.s64 = 4;
	// rlwinm r25,r11,30,2,31
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
loc_824CFAF4:
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// rlwinm r11,r11,28,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x7;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x824cfb1c
	if (!cr6.eq) goto loc_824CFB1C;
	// clrlwi. r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824cfd08
	if (!cr0.eq) goto loc_824CFD08;
	// cmplwi cr6,r25,1
	cr6.compare<uint32_t>(r25.u32, 1, xer);
	// bgt cr6,0x824cfd08
	if (cr6.gt) goto loc_824CFD08;
	// cmplwi cr6,r28,1
	cr6.compare<uint32_t>(r28.u32, 1, xer);
	// bgt cr6,0x824cfd08
	if (cr6.gt) goto loc_824CFD08;
loc_824CFB1C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824cfb2c
	if (!cr6.eq) goto loc_824CFB2C;
	// mr r25,r31
	r25.u64 = r31.u64;
	// li r28,1
	r28.s64 = 1;
loc_824CFB2C:
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// lwz r26,8(r24)
	r26.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// clrlwi. r11,r10,24
	r11.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// add r16,r25,r16
	r16.u64 = r25.u64 + r16.u64;
	// mr r22,r23
	r22.u64 = r23.u64;
	// bne 0x824cfbcc
	if (!cr0.eq) goto loc_824CFBCC;
loc_824CFB44:
	// cmplw cr6,r31,r28
	cr6.compare<uint32_t>(r31.u32, r28.u32, xer);
	// mr r30,r31
	r30.u64 = r31.u64;
	// blt cr6,0x824cfb54
	if (cr6.lt) goto loc_824CFB54;
	// mr r30,r28
	r30.u64 = r28.u64;
loc_824CFB54:
	// rlwinm r11,r23,3,0,28
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r31,r30,r31
	r31.s64 = r31.s64 - r30.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// add r29,r11,r18
	r29.u64 = r11.u64 + r18.u64;
loc_824CFB64:
	// mr r8,r19
	ctx.r8.u64 = r19.u64;
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// bl 0x824cd1a8
	sub_824CD1A8(ctx, base);
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824cfb9c
	if (cr6.eq) goto loc_824CFB9C;
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x824cfb9c
	if (!cr6.lt) goto loc_824CFB9C;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwimi r10,r11,27,1,4
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 27) & 0x78000000) | (ctx.r10.u64 & 0xFFFFFFFF87FFFFFF);
	// oris r11,r10,1024
	r11.u64 = ctx.r10.u64 | 67108864;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
loc_824CFB9C:
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// bne 0x824cfb64
	if (!cr0.eq) goto loc_824CFB64;
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r24)
	PPC_STORE_U32(r24.u32 + 20, r11.u32);
	// bne cr6,0x824cfb44
	if (!cr6.eq) goto loc_824CFB44;
	// b 0x824cfc7c
	goto loc_824CFC7C;
loc_824CFBCC:
	// lwz r27,20(r24)
	r27.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
loc_824CFBD8:
	// cmplw cr6,r31,r25
	cr6.compare<uint32_t>(r31.u32, r25.u32, xer);
	// mr r30,r31
	r30.u64 = r31.u64;
	// blt cr6,0x824cfbe8
	if (cr6.lt) goto loc_824CFBE8;
	// mr r30,r25
	r30.u64 = r25.u64;
loc_824CFBE8:
	// rlwinm r11,r23,3,0,28
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r31,r30,r31
	r31.s64 = r31.s64 - r30.s64;
	// add r29,r11,r18
	r29.u64 = r11.u64 + r18.u64;
loc_824CFBF4:
	// mr r8,r19
	ctx.r8.u64 = r19.u64;
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// bl 0x824cd1a8
	sub_824CD1A8(ctx, base);
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x824cfc38
	if (cr6.eq) goto loc_824CFC38;
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x824cfc2c
	if (!cr6.lt) goto loc_824CFC2C;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwimi r10,r11,27,1,4
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 27) & 0x78000000) | (ctx.r10.u64 & 0xFFFFFFFF87FFFFFF);
	// oris r11,r10,1024
	r11.u64 = ctx.r10.u64 | 67108864;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
loc_824CFC2C:
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r24)
	PPC_STORE_U32(r24.u32 + 20, r11.u32);
loc_824CFC38:
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// bne 0x824cfbf4
	if (!cr0.eq) goto loc_824CFBF4;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r27,20(r24)
	PPC_STORE_U32(r24.u32 + 20, r27.u32);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824cfbd8
	if (!cr6.eq) goto loc_824CFBD8;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// lwz r27,284(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// oris r11,r11,16384
	r11.u64 = r11.u64 | 1073741824;
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// add r11,r25,r11
	r11.u64 = r25.u64 + r11.u64;
	// stw r11,20(r24)
	PPC_STORE_U32(r24.u32 + 20, r11.u32);
loc_824CFC7C:
	// cmplwi cr6,r25,1
	cr6.compare<uint32_t>(r25.u32, 1, xer);
	// ble cr6,0x824cfcc4
	if (!cr6.gt) goto loc_824CFCC4;
	// cmplw cr6,r22,r23
	cr6.compare<uint32_t>(r22.u32, r23.u32, xer);
	// bge cr6,0x824cfcc4
	if (!cr6.lt) goto loc_824CFCC4;
	// rlwinm r11,r22,3,0,28
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r10,r22,r23
	ctx.r10.s64 = r23.s64 - r22.s64;
	// add r11,r11,r18
	r11.u64 = r11.u64 + r18.u64;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_824CFC9C:
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// clrlwi r9,r9,28
	ctx.r9.u64 = ctx.r9.u32 & 0xF;
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x824cfcb8
	if (!cr6.eq) goto loc_824CFCB8;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// ori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 | 1;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_824CFCB8:
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x824cfc9c
	if (!cr0.eq) goto loc_824CFC9C;
loc_824CFCC4:
	// li r11,0
	r11.s64 = 0;
	// stw r11,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r11.u32);
	// b 0x824cf8cc
	goto loc_824CF8CC;
loc_824CFCD0:
	// li r4,4801
	ctx.r4.s64 = 4801;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CFCDC:
	// li r4,3533
	ctx.r4.s64 = 3533;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CFCE8:
	// li r4,4801
	ctx.r4.s64 = 4801;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CFCF0:
	// li r4,4801
	ctx.r4.s64 = 4801;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CFCFC:
	// li r4,4801
	ctx.r4.s64 = 4801;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CFD08:
	// li r4,4801
	ctx.r4.s64 = 4801;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824CFD14:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,29,18,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x3FFF;
	// cmplw cr6,r23,r11
	cr6.compare<uint32_t>(r23.u32, r11.u32, xer);
	// bne cr6,0x824cfdc4
	if (!cr6.eq) goto loc_824CFDC4;
	// clrlwi. r11,r14,24
	r11.u64 = r14.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cfd34
	if (cr0.eq) goto loc_824CFD34;
	// clrlwi. r11,r15,24
	r11.u64 = r15.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cfdc4
	if (!cr0.eq) goto loc_824CFDC4;
loc_824CFD34:
	// addi r11,r17,924
	r11.s64 = r17.s64 + 924;
	// cmplwi cr6,r16,256
	cr6.compare<uint32_t>(r16.u32, 256, xer);
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
	// stw r24,32(r11)
	PPC_STORE_U32(r11.u32 + 32, r24.u32);
	// blt cr6,0x824cfd54
	if (cr6.lt) goto loc_824CFD54;
	// li r10,256
	ctx.r10.s64 = 256;
loc_824CFD54:
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// rlwimi r11,r10,16,7,15
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0x1FF0000) | (r11.u64 & 0xFFFFFFFFFE00FFFF);
	// stw r11,4(r21)
	PPC_STORE_U32(r21.u32 + 4, r11.u32);
	// rlwinm. r10,r11,0,25,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824cfdbc
	if (!cr0.eq) goto loc_824CFDBC;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm. r11,r11,0,15,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1FFF8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cfdbc
	if (cr0.eq) goto loc_824CFDBC;
	// li r11,0
	r11.s64 = 0;
loc_824CFD7C:
	// lwz r9,28(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 28);
	// li r7,1
	ctx.r7.s64 = 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// rlwimi r8,r7,18,12,13
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r7.u32, 18) & 0xC0000) | (ctx.r8.u64 & 0xFFFFFFFFFFF3FFFF);
	// stwx r8,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r8.u32);
	// lwz r9,28(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 28);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r8,r8,0,12,5
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFC0FFFFF;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r9,r9,29,18,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x3FFF;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x824cfd7c
	if (cr6.lt) goto loc_824CFD7C;
loc_824CFDBC:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_824CFDC4:
	// li r4,4801
	ctx.r4.s64 = 4801;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
}

__attribute__((alias("__imp__sub_824CFDD0"))) PPC_WEAK_FUNC(sub_824CFDD0);
PPC_FUNC_IMPL(__imp__sub_824CFDD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-608(r1)
	ea = -608 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r15,r3
	r15.u64 = ctx.r3.u64;
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824cfdf0
	if (cr6.eq) goto loc_824CFDF0;
	// bl 0x824ce468
	sub_824CE468(ctx, base);
loc_824CFDF0:
	// li r5,128
	ctx.r5.s64 = 128;
	// stw r15,320(r1)
	PPC_STORE_U32(ctx.r1.u32 + 320, r15.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,328
	ctx.r3.s64 = ctx.r1.s64 + 328;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r10,4(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// addi r11,r1,136
	r11.s64 = ctx.r1.s64 + 136;
	// stw r15,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r15.u32);
	// li r16,0
	r16.s64 = 0;
	// stw r15,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r15.u32);
	// addi r9,r1,92
	ctx.r9.s64 = ctx.r1.s64 + 92;
	// stw r15,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r15.u32);
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// mr r30,r16
	r30.u64 = r16.u64;
	// std r16,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r16.u64);
	// std r16,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r16.u64);
	// std r16,16(r11)
	PPC_STORE_U64(r11.u32 + 16, r16.u64);
	// std r16,24(r11)
	PPC_STORE_U64(r11.u32 + 24, r16.u64);
	// stw r16,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r16.u32);
	// stw r16,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r16.u32);
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cfe4c
	if (cr0.eq) goto loc_824CFE4C;
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
loc_824CFE4C:
	// lwz r31,20(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// li r14,1
	r14.s64 = 1;
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cfed8
	if (!cr0.eq) goto loc_824CFED8;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824cfed8
	if (cr0.eq) goto loc_824CFED8;
loc_824CFE64:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,15744
	cr6.compare<uint32_t>(r11.u32, 15744, xer);
	// bne cr6,0x824cfec0
	if (!cr6.eq) goto loc_824CFEC0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r31,-8
	r11.s64 = r31.s64 + -8;
	// lwz r10,12(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// lwzx r11,r3,r11
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// clrlwi r11,r11,17
	r11.u64 = r11.u32 & 0x7FFF;
	// mulli r11,r11,40
	r11.s64 = r11.s64 * 40;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,36(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// oris r10,r10,8192
	ctx.r10.u64 = ctx.r10.u64 | 536870912;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// beq cr6,0x824cfec0
	if (cr6.eq) goto loc_824CFEC0;
	// mr r30,r14
	r30.u64 = r14.u64;
loc_824CFEC0:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824cfed8
	if (!cr0.eq) goto loc_824CFED8;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824cfe64
	if (!cr6.eq) goto loc_824CFE64;
loc_824CFED8:
	// clrlwi. r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d0178
	if (cr0.eq) goto loc_824D0178;
	// lwz r7,16(r15)
	ctx.r7.u64 = PPC_LOAD_U32(r15.u32 + 16);
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// cmplwi r7,0
	cr0.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq 0x824cffac
	if (cr0.eq) goto loc_824CFFAC;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
loc_824CFEF4:
	// lwz r10,12(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r5,r4,r10
	ctx.r5.u64 = ctx.r4.u64 + ctx.r10.u64;
	// lwz r9,36(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 36);
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824cff98
	if (cr0.eq) goto loc_824CFF98;
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwinm. r11,r11,0,2,2
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824cff98
	if (cr0.eq) goto loc_824CFF98;
	// mr r11,r16
	r11.u64 = r16.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x824cff48
	if (cr6.eq) goto loc_824CFF48;
	// addi r10,r10,36
	ctx.r10.s64 = ctx.r10.s64 + 36;
loc_824CFF24:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// beq cr6,0x824cff40
	if (cr6.eq) goto loc_824CFF40;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,40
	ctx.r10.s64 = ctx.r10.s64 + 40;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// blt cr6,0x824cff24
	if (cr6.lt) goto loc_824CFF24;
loc_824CFF40:
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// blt cr6,0x824cff98
	if (cr6.lt) goto loc_824CFF98;
loc_824CFF48:
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x824cff98
	if (cr6.eq) goto loc_824CFF98;
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
loc_824CFF58:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// lwz r9,36(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 36);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r7,36(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// bne cr6,0x824cff84
	if (!cr6.eq) goto loc_824CFF84;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r7,r9,0,2,2
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000000;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x824cff84
	if (!cr0.eq) goto loc_824CFF84;
	// oris r9,r9,8192
	ctx.r9.u64 = ctx.r9.u64 | 536870912;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_824CFF84:
	// lwz r11,16(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 16);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r10,r10,40
	ctx.r10.s64 = ctx.r10.s64 + 40;
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// blt cr6,0x824cff58
	if (cr6.lt) goto loc_824CFF58;
loc_824CFF98:
	// lwz r7,16(r15)
	ctx.r7.u64 = PPC_LOAD_U32(r15.u32 + 16);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r4,r4,40
	ctx.r4.s64 = ctx.r4.s64 + 40;
	// cmplw cr6,r6,r7
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r7.u32, xer);
	// blt cr6,0x824cfef4
	if (cr6.lt) goto loc_824CFEF4;
loc_824CFFAC:
	// lwz r11,16(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 16);
	// mr r28,r16
	r28.u64 = r16.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x824d0178
	if (!cr6.gt) goto loc_824D0178;
	// mr r26,r16
	r26.u64 = r16.u64;
loc_824CFFC0:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r27,r26,r11
	r27.u64 = r26.u64 + r11.u64;
	// lwz r11,36(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824d0164
	if (cr6.eq) goto loc_824D0164;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm. r10,r11,0,2,2
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d0164
	if (cr0.eq) goto loc_824D0164;
	// rlwinm. r11,r11,0,15,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1FFF8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r31,r16
	r31.u64 = r16.u64;
	// mr r29,r16
	r29.u64 = r16.u64;
	// beq 0x824d0164
	if (cr0.eq) goto loc_824D0164;
loc_824CFFF0:
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82555010
	sub_82555010(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
loc_824D000C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824d0054
	if (cr6.eq) goto loc_824D0054;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// subf r10,r16,r10
	ctx.r10.s64 = ctx.r10.s64 - r16.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824d004c
	if (cr0.eq) goto loc_824D004C;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x824d0150
	if (!cr0.eq) goto loc_824D0150;
loc_824D004C:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824d000c
	goto loc_824D000C;
loc_824D0054:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824d00dc
	if (!cr6.eq) goto loc_824D00DC;
	// lwz r4,4(r15)
	ctx.r4.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// clrlwi. r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d006c
	if (cr0.eq) goto loc_824D006C;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
loc_824D006C:
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,116
	ctx.r6.s64 = 116;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82553560
	sub_82553560(ctx, base);
	// lwz r11,4(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d0098
	if (cr0.eq) goto loc_824D0098;
	// mr r11,r16
	r11.u64 = r16.u64;
loc_824D0098:
	// addi r11,r11,24
	r11.s64 = r11.s64 + 24;
	// rlwinm r10,r31,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// ori r6,r11,1
	ctx.r6.u64 = r11.u64 | 1;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r7,r7,0,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,32(r7)
	PPC_STORE_U32(ctx.r7.u32 + 32, ctx.r9.u32);
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// oris r11,r11,768
	r11.u64 = r11.u64 | 50331648;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_824D00DC:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824ba568
	sub_824BA568(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824ba7b8
	sub_824BA7B8(ctx, base);
	// lis r11,-28311
	r11.s64 = -1855389696;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r11,r11,5192
	r11.u64 = r11.u64 | 5192;
	// ori r10,r10,36262
	ctx.r10.u64 = ctx.r10.u64 | 36262;
	// lis r8,128
	ctx.r8.s64 = 8388608;
	// rldimi r11,r10,32,0
	r11.u64 = (__builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000) | (r11.u64 & 0xFFFFFFFF);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r10,r11,18,29,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r7,r11,0,31,26
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFE1;
	// rlwinm r7,r7,0,18,14
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFFFFE3FFF;
	// slw r11,r14,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r14.u32 << (ctx.r10.u8 & 0x3F));
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// clrldi r11,r10,32
	r11.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// clrlwi r10,r10,28
	ctx.r10.u64 = ctx.r10.u32 & 0xF;
	// srd r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// srd r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// srd r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (r11.u8 & 0x7F));
	// rlwimi r8,r11,13,16,18
	ctx.r8.u64 = (__builtin_rotateleft32(r11.u32, 13) & 0xE000) | (ctx.r8.u64 & 0xFFFFFFFFFFFF1FFF);
	// or r11,r8,r10
	r11.u64 = ctx.r8.u64 | ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
loc_824D0150:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// rlwinm r11,r11,29,18,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x3FFF;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// blt cr6,0x824cfff0
	if (cr6.lt) goto loc_824CFFF0;
loc_824D0164:
	// lwz r11,16(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 16);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r26,r26,40
	r26.s64 = r26.s64 + 40;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// blt cr6,0x824cffc0
	if (cr6.lt) goto loc_824CFFC0;
loc_824D0178:
	// lwz r11,4(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// mr r17,r16
	r17.u64 = r16.u64;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d018c
	if (cr0.eq) goto loc_824D018C;
	// mr r11,r16
	r11.u64 = r16.u64;
loc_824D018C:
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d07a0
	if (!cr0.eq) goto loc_824D07A0;
	// mr. r20,r11
	r20.u64 = r11.u64;
	cr0.compare<int32_t>(r20.s32, 0, xer);
	// beq 0x824d07a0
	if (cr0.eq) goto loc_824D07A0;
loc_824D01A0:
	// lwz r11,8(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,15744
	cr6.compare<uint32_t>(r11.u32, 15744, xer);
	// bne cr6,0x824d0708
	if (!cr6.eq) goto loc_824D0708;
	// lwz r11,8(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 8);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// addi r22,r20,-8
	r22.s64 = r20.s64 + -8;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r11,8(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 8);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// lwzx r10,r10,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r22.u32);
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// clrlwi r30,r10,17
	r30.u64 = ctx.r10.u32 & 0x7FFF;
	// lwz r10,12(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// mulli r21,r30,40
	r21.s64 = r30.s64 * 40;
	// add r25,r21,r10
	r25.u64 = r21.u64 + ctx.r10.u64;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// lwzx r11,r3,r22
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + r22.u32);
	// addi r19,r20,4
	r19.s64 = r20.s64 + 4;
	// rlwinm r11,r11,17,18,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 17) & 0x3FFF;
	// mr r28,r16
	r28.u64 = r16.u64;
	// addi r18,r11,1
	r18.s64 = r11.s64 + 1;
	// lwz r31,0(r19)
	r31.u64 = PPC_LOAD_U32(r19.u32 + 0);
loc_824D0214:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824d02bc
	if (cr6.eq) goto loc_824D02BC;
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r11,r16,r3
	r11.s64 = ctx.r3.s64 - r16.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d02b4
	if (cr0.eq) goto loc_824D02B4;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r10,r11,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824d02b4
	if (cr0.eq) goto loc_824D02B4;
	// rlwinm. r11,r11,0,27,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x18;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d0280
	if (cr0.eq) goto loc_824D0280;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r10,28(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// rlwinm r11,r11,0,15,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1FFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,-8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// rlwinm r11,r11,28,18,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x3FFF;
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// bgt cr6,0x824d07b8
	if (cr6.gt) goto loc_824D07B8;
	// addi r18,r11,1
	r18.s64 = r11.s64 + 1;
loc_824D0280:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824d029c
	if (cr6.lt) goto loc_824D029C;
	// cmplwi cr6,r11,101
	cr6.compare<uint32_t>(r11.u32, 101, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// ble cr6,0x824d02a0
	if (!cr6.gt) goto loc_824D02A0;
loc_824D029C:
	// mr r11,r16
	r11.u64 = r16.u64;
loc_824D02A0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d02b4
	if (cr0.eq) goto loc_824D02B4;
	// bl 0x824cd058
	sub_824CD058(ctx, base);
	// slw r11,r14,r3
	r11.u64 = ctx.r3.u8 & 0x20 ? 0 : (r14.u32 << (ctx.r3.u8 & 0x3F));
	// or r28,r11,r28
	r28.u64 = r11.u64 | r28.u64;
loc_824D02B4:
	// lwz r31,8(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// b 0x824d0214
	goto loc_824D0214;
loc_824D02BC:
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// rlwinm r10,r11,28,29,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x7;
	// cmplwi cr6,r10,7
	cr6.compare<uint32_t>(ctx.r10.u32, 7, xer);
	// beq cr6,0x824d02f0
	if (cr6.eq) goto loc_824D02F0;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// bne cr6,0x824d02e8
	if (!cr6.eq) goto loc_824D02E8;
	// cmplwi cr6,r28,64
	cr6.compare<uint32_t>(r28.u32, 64, xer);
	// bne cr6,0x824d02e8
	if (!cr6.eq) goto loc_824D02E8;
	// rlwimi r11,r10,5,25,27
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 5) & 0x70) | (r11.u64 & 0xFFFFFFFFFFFFFF8F);
	// stw r11,4(r25)
	PPC_STORE_U32(r25.u32 + 4, r11.u32);
	// b 0x824d02f0
	goto loc_824D02F0;
loc_824D02E8:
	// slw r11,r14,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r14.u32 << (ctx.r10.u8 & 0x3F));
	// or r28,r11,r28
	r28.u64 = r11.u64 | r28.u64;
loc_824D02F0:
	// rlwinm. r11,r28,0,24,24
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x80;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d0304
	if (cr0.eq) goto loc_824D0304;
	// rlwinm. r28,r28,0,25,23
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFFFFFFFF7F;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// bne 0x824d030c
	if (!cr0.eq) goto loc_824D030C;
	// ori r28,r28,4
	r28.u64 = r28.u64 | 4;
loc_824D0304:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824d053c
	if (cr6.eq) goto loc_824D053C;
loc_824D030C:
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// andc r11,r28,r11
	r11.u64 = r28.u64 & ~r11.u64;
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d0508
	if (!cr0.eq) goto loc_824D0508;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// rlwinm. r10,r11,0,12,12
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d0820
	if (!cr0.eq) goto loc_824D0820;
	// lwz r10,36(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 36);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824d0820
	if (!cr6.eq) goto loc_824D0820;
	// rlwinm r11,r11,0,15,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1FFF8;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bne cr6,0x824d07c4
	if (!cr6.eq) goto loc_824D07C4;
loc_824D0348:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824d0364
	if (cr6.eq) goto loc_824D0364;
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// andc r11,r28,r11
	r11.u64 = r28.u64 & ~r11.u64;
	// subf. r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// beq 0x824d0368
	if (cr0.eq) goto loc_824D0368;
loc_824D0364:
	// mr r11,r16
	r11.u64 = r16.u64;
loc_824D0368:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d0500
	if (!cr0.eq) goto loc_824D0500;
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// andc r11,r28,r11
	r11.u64 = r28.u64 & ~r11.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// subf r28,r11,r28
	r28.s64 = r28.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r24,r11,31
	xer.ca = r11.u32 <= 31;
	r24.s64 = 31 - r11.s64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// bl 0x82553d48
	sub_82553D48(ctx, base);
	// lwz r11,8(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 8);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// mr r23,r19
	r23.u64 = r19.u64;
	// add r26,r3,r22
	r26.u64 = ctx.r3.u64 + r22.u64;
loc_824D03B8:
	// lwz r31,0(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824d0348
	if (cr0.eq) goto loc_824D0348;
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r11,r16,r3
	r11.s64 = ctx.r3.s64 - r16.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d04ec
	if (cr0.eq) goto loc_824D04EC;
	// lwz r30,0(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r29,r30,7,29,31
	r29.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 7) & 0x7;
	// cntlzw r11,r29
	r11.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d04ec
	if (cr0.eq) goto loc_824D04EC;
	// bl 0x824cd058
	sub_824CD058(ctx, base);
	// cmpw cr6,r3,r24
	cr6.compare<int32_t>(ctx.r3.s32, r24.s32, xer);
	// bne cr6,0x824d04ec
	if (!cr6.eq) goto loc_824D04EC;
	// mr r9,r16
	ctx.r9.u64 = r16.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824d045c
	if (cr6.eq) goto loc_824D045C;
	// rotlwi r11,r30,0
	r11.u64 = __builtin_rotateleft32(r30.u32, 0);
	// rlwinm r8,r30,27,24,31
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 27) & 0xFF;
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
	// rlwinm r11,r11,7,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x7;
loc_824D0424:
	// srw r7,r8,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// clrlwi r7,r7,30
	ctx.r7.u64 = ctx.r7.u32 & 0x3;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// slw r7,r14,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r14.u32 << (ctx.r7.u8 & 0x3F));
	// or r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 | ctx.r9.u64;
	// bne 0x824d0424
	if (!cr0.eq) goto loc_824D0424;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824d045c
	if (cr6.eq) goto loc_824D045C;
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// andc r11,r9,r11
	r11.u64 = ctx.r9.u64 & ~r11.u64;
	// subf. r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r14
	r11.u64 = r14.u64;
	// beq 0x824d0460
	if (cr0.eq) goto loc_824D0460;
loc_824D045C:
	// mr r11,r16
	r11.u64 = r16.u64;
loc_824D0460:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// beq 0x824d0818
	if (cr0.eq) goto loc_824D0818;
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// rlwinm r10,r30,27,30,31
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 27) & 0x3;
	// lwz r9,12(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// li r8,40
	ctx.r8.s64 = 40;
	// rlwinm r11,r11,26,18,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFF;
	// subf r9,r9,r27
	ctx.r9.s64 = r27.s64 - ctx.r9.s64;
	// add r5,r11,r10
	ctx.r5.u64 = r11.u64 + ctx.r10.u64;
	// divw r4,r9,r8
	ctx.r4.s32 = ctx.r9.s32 / ctx.r8.s32;
	// li r6,1
	ctx.r6.s64 = 1;
	// bl 0x82555010
	sub_82555010(ctx, base);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r11,r11,0,30,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFE3;
	// rlwinm r11,r11,0,17,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFE7FFF;
	// oris r11,r11,256
	r11.u64 = r11.u64 | 16777216;
	// ori r11,r11,16386
	r11.u64 = r11.u64 | 16386;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// b 0x824d04bc
	goto loc_824D04BC;
loc_824D04B8:
	// addi r11,r10,8
	r11.s64 = ctx.r10.s64 + 8;
loc_824D04BC:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// bne cr6,0x824d04b8
	if (!cr6.eq) goto loc_824D04B8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r31,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r31.u32);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r3,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r3.u32);
	// rlwinm r11,r11,0,27,18
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFE01F;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_824D04EC:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bne cr6,0x824d03b8
	if (!cr6.eq) goto loc_824D03B8;
	// addi r23,r31,8
	r23.s64 = r31.s64 + 8;
	// b 0x824d03b8
	goto loc_824D03B8;
loc_824D0500:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r25,r21,r11
	r25.u64 = r21.u64 + r11.u64;
loc_824D0508:
	// cmplwi cr6,r28,64
	cr6.compare<uint32_t>(r28.u32, 64, xer);
	// bne cr6,0x824d051c
	if (!cr6.eq) goto loc_824D051C;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// rlwinm. r11,r11,0,12,12
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d0884
	if (cr0.eq) goto loc_824D0884;
loc_824D051C:
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// rlwinm r10,r11,0,25,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70;
	// cmplwi cr6,r10,112
	cr6.compare<uint32_t>(ctx.r10.u32, 112, xer);
	// bne cr6,0x824d053c
	if (!cr6.eq) goto loc_824D053C;
	// cntlzw r10,r28
	ctx.r10.u64 = r28.u32 == 0 ? 32 : __builtin_clz(r28.u32);
	// subfic r10,r10,-1
	xer.ca = ctx.r10.u32 <= 4294967295;
	ctx.r10.s64 = -1 - ctx.r10.s64;
	// rlwimi r11,r10,4,25,27
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 4) & 0x70) | (r11.u64 & 0xFFFFFFFFFFFFFF8F);
	// stw r11,4(r25)
	PPC_STORE_U32(r25.u32 + 4, r11.u32);
loc_824D053C:
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// rlwinm r10,r11,25,23,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1FF;
	// cmplw cr6,r18,r10
	cr6.compare<uint32_t>(r18.u32, ctx.r10.u32, xer);
	// ble cr6,0x824d0554
	if (!cr6.gt) goto loc_824D0554;
	// rlwimi r11,r18,7,16,24
	r11.u64 = (__builtin_rotateleft32(r18.u32, 7) & 0xFF80) | (r11.u64 & 0xFFFFFFFFFFFF007F);
	// stw r11,4(r25)
	PPC_STORE_U32(r25.u32 + 4, r11.u32);
loc_824D0554:
	// lwz r31,4(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// rlwinm r11,r31,28,29,31
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 28) & 0x7;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x824d06b0
	if (!cr6.eq) goto loc_824D06B0;
	// lwz r11,8(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 8);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// add r30,r3,r22
	r30.u64 = ctx.r3.u64 + r22.u64;
	// rlwinm. r11,r31,0,28,28
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d05b4
	if (!cr0.eq) goto loc_824D05B4;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// rlwinm r9,r11,20,15,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 20) & 0x1FFF8;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lwz r11,28(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// lwzx r11,r9,r11
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// rlwinm r11,r11,12,20,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// rlwimi r10,r11,4,26,27
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 4) & 0x30) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFCF);
	// stw r10,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r10.u32);
loc_824D05B4:
	// lwz r29,0(r19)
	r29.u64 = PPC_LOAD_U32(r19.u32 + 0);
loc_824D05B8:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x824d0708
	if (cr6.eq) goto loc_824D0708;
	// lwz r31,16(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// subf r11,r16,r31
	r11.s64 = r31.s64 - r16.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d06a8
	if (cr0.eq) goto loc_824D06A8;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,96
	cr6.compare<uint32_t>(r11.u32, 96, xer);
	// beq cr6,0x824d0600
	if (cr6.eq) goto loc_824D0600;
	// cmplwi cr6,r11,98
	cr6.compare<uint32_t>(r11.u32, 98, xer);
	// beq cr6,0x824d0600
	if (cr6.eq) goto loc_824D0600;
	// cmplwi cr6,r11,99
	cr6.compare<uint32_t>(r11.u32, 99, xer);
	// mr r11,r16
	r11.u64 = r16.u64;
	// bne cr6,0x824d0604
	if (!cr6.eq) goto loc_824D0604;
loc_824D0600:
	// mr r11,r14
	r11.u64 = r14.u64;
loc_824D0604:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// beq 0x824d0890
	if (cr0.eq) goto loc_824D0890;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r31,-24
	r11.s64 = r31.s64 + -24;
	// add r10,r3,r11
	ctx.r10.u64 = ctx.r3.u64 + r11.u64;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// clrlwi r11,r11,27
	r11.u64 = r11.u32 & 0x1F;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x824d0654
	if (cr6.eq) goto loc_824D0654;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// ble cr6,0x824d06a8
	if (!cr6.gt) goto loc_824D06A8;
	// cmplwi cr6,r11,17
	cr6.compare<uint32_t>(r11.u32, 17, xer);
	// ble cr6,0x824d0654
	if (!cr6.gt) goto loc_824D0654;
	// cmplwi cr6,r11,19
	cr6.compare<uint32_t>(r11.u32, 19, xer);
	// bne cr6,0x824d06a8
	if (!cr6.eq) goto loc_824D06A8;
loc_824D0654:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm. r9,r11,0,0,0
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824d067c
	if (!cr0.eq) goto loc_824D067C;
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// rlwimi r10,r11,22,26,27
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 22) & 0x30) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFCF);
	// stw r10,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r10.u32);
	// b 0x824d06a8
	goto loc_824D06A8;
loc_824D067C:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// rlwinm. r11,r11,28,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d0698
	if (!cr0.eq) goto loc_824D0698;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r9,r9,0,16,17
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xC000;
	// cmplwi cr6,r9,16384
	cr6.compare<uint32_t>(ctx.r9.u32, 16384, xer);
	// beq cr6,0x824d06a8
	if (cr6.eq) goto loc_824D06A8;
loc_824D0698:
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r10,r10,18,30,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x824d0898
	if (!cr6.eq) goto loc_824D0898;
loc_824D06A8:
	// lwz r29,8(r29)
	r29.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// b 0x824d05b8
	goto loc_824D05B8;
loc_824D06B0:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x824d0708
	if (!cr6.eq) goto loc_824D0708;
	// lwz r30,0(r19)
	r30.u64 = PPC_LOAD_U32(r19.u32 + 0);
loc_824D06BC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824d0708
	if (cr6.eq) goto loc_824D0708;
	// lwz r31,16(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// subf r11,r16,r31
	r11.s64 = r31.s64 - r16.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d0700
	if (cr0.eq) goto loc_824D0700;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b0570
	sub_824B0570(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d0700
	if (cr0.eq) goto loc_824D0700;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10624
	cr6.compare<uint32_t>(r11.u32, 10624, xer);
	// bne cr6,0x824d08a4
	if (!cr6.eq) goto loc_824D08A4;
loc_824D0700:
	// lwz r30,8(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// b 0x824d06bc
	goto loc_824D06BC;
loc_824D0708:
	// cmplwi cr6,r17,0
	cr6.compare<uint32_t>(r17.u32, 0, xer);
	// beq cr6,0x824d0750
	if (cr6.eq) goto loc_824D0750;
	// rlwinm r11,r17,0,0,30
	r11.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r16
	r11.u64 = r16.u64;
	// bne 0x824d0728
	if (!cr0.eq) goto loc_824D0728;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_824D0728:
	// cmplw cr6,r20,r11
	cr6.compare<uint32_t>(r20.u32, r11.u32, xer);
	// bne cr6,0x824d0740
	if (!cr6.eq) goto loc_824D0740;
loc_824D0730:
	// rlwinm r11,r20,0,0,30
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r17,r20
	r17.u64 = r20.u64;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// b 0x824d078c
	goto loc_824D078C;
loc_824D0740:
	// clrlwi. r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d07a0
	if (!cr0.eq) goto loc_824D07A0;
	// mr r20,r10
	r20.u64 = ctx.r10.u64;
	// b 0x824d0798
	goto loc_824D0798;
loc_824D0750:
	// lwz r11,4(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
	// clrlwi. r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824d0764
	if (!cr0.eq) goto loc_824D0764;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_824D0764:
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// clrlwi. r8,r10,31
	ctx.r8.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824d0774
	if (cr0.eq) goto loc_824D0774;
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
loc_824D0774:
	// cmplw cr6,r20,r10
	cr6.compare<uint32_t>(r20.u32, ctx.r10.u32, xer);
	// beq cr6,0x824d0730
	if (cr6.eq) goto loc_824D0730;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824d0788
	if (cr6.eq) goto loc_824D0788;
	// mr r11,r16
	r11.u64 = r16.u64;
loc_824D0788:
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
loc_824D078C:
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d07a0
	if (!cr0.eq) goto loc_824D07A0;
	// mr r20,r11
	r20.u64 = r11.u64;
loc_824D0798:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x824d01a0
	if (!cr6.eq) goto loc_824D01A0;
loc_824D07A0:
	// lwz r10,756(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 756);
	// lis r11,-1
	r11.s64 = -65536;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824d08b0
	if (cr0.eq) goto loc_824D08B0;
	// lwz r30,752(r15)
	r30.u64 = PPC_LOAD_U32(r15.u32 + 752);
	// b 0x824d08d8
	goto loc_824D08D8;
loc_824D07B8:
	// li r4,3566
	ctx.r4.s64 = 3566;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D07C4:
	// rlwinm. r11,r28,0,30,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// beq 0x824d07d8
	if (cr0.eq) goto loc_824D07D8;
	// li r4,3548
	ctx.r4.s64 = 3548;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D07D8:
	// rlwinm. r11,r28,0,25,25
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d07e8
	if (cr0.eq) goto loc_824D07E8;
	// li r4,3598
	ctx.r4.s64 = 3598;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D07E8:
	// rlwinm. r11,r28,0,28,28
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d07f8
	if (cr0.eq) goto loc_824D07F8;
	// li r4,3533
	ctx.r4.s64 = 3533;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D07F8:
	// clrlwi. r11,r28,31
	r11.u64 = r28.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d0810
	if (cr0.eq) goto loc_824D0810;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r4,3500
	ctx.r4.s64 = 3500;
	// addi r5,r11,28196
	ctx.r5.s64 = r11.s64 + 28196;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D0810:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D0818:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D0820:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824cf038
	sub_824CF038(ctx, base);
	// mr. r11,r3
	r11.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d0844
	if (cr0.eq) goto loc_824D0844;
	// li r4,4556
	ctx.r4.s64 = 4556;
	// lwz r5,24(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D0844:
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lwz r9,4(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// lis r11,-32139
	r11.s64 = -2106261504;
	// lwz r8,0(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// addi r5,r10,28188
	ctx.r5.s64 = ctx.r10.s64 + 28188;
	// addi r11,r11,-68
	r11.s64 = r11.s64 + -68;
	// rlwinm r10,r9,30,27,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x1C;
	// rlwinm r7,r8,12,23,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0x1FF;
	// li r4,10
	ctx.r4.s64 = 10;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lwzx r6,r10,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// bl 0x823a1348
	sub_823A1348(ctx, base);
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// li r4,4556
	ctx.r4.s64 = 4556;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D0884:
	// li r4,3599
	ctx.r4.s64 = 3599;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D0890:
	// li r4,3512
	ctx.r4.s64 = 3512;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D0898:
	// li r4,4539
	ctx.r4.s64 = 4539;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D08A4:
	// li r4,3548
	ctx.r4.s64 = 3548;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D08B0:
	// lwz r9,44(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 44);
	// rlwinm r9,r9,0,0,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFF0000;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 ^ 1;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r30,r9,31,27,27
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x10;
loc_824D08D8:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824d0910
	if (!cr6.eq) goto loc_824D0910;
	// lwz r10,44(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 44);
	// rlwinm r10,r10,0,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFF0000;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// subfic r10,r10,0
	xer.ca = ctx.r10.u32 <= 0;
	ctx.r10.s64 = 0 - ctx.r10.s64;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r10,r10,0,28,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xC;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_824D0910:
	// lwz r9,44(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 44);
	// add r31,r10,r30
	r31.u64 = ctx.r10.u64 + r30.u64;
	// li r5,128
	ctx.r5.s64 = 128;
	// rlwinm r10,r9,0,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFF0000;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d0950
	if (cr0.eq) goto loc_824D0950;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x824ce150
	sub_824CE150(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// b 0x824d095c
	goto loc_824D095C;
loc_824D0950:
	// li r4,128
	ctx.r4.s64 = 128;
	// bl 0x824ce150
	sub_824CE150(ctx, base);
	// li r4,16
	ctx.r4.s64 = 16;
loc_824D095C:
	// li r5,16
	ctx.r5.s64 = 16;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x824ce1b0
	sub_824CE1B0(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824d0980
	if (cr6.eq) goto loc_824D0980;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x824ce088
	sub_824CE088(ctx, base);
loc_824D0980:
	// cmplwi cr6,r31,32
	cr6.compare<uint32_t>(r31.u32, 32, xer);
	// bge cr6,0x824d0998
	if (!cr6.lt) goto loc_824D0998;
	// subfic r5,r31,32
	xer.ca = r31.u32 <= 32;
	ctx.r5.s64 = 32 - r31.s64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x824ce088
	sub_824CE088(ctx, base);
loc_824D0998:
	// lwz r21,16(r15)
	r21.u64 = PPC_LOAD_U32(r15.u32 + 16);
	// mr r23,r16
	r23.u64 = r16.u64;
	// mr r30,r16
	r30.u64 = r16.u64;
	// cmplwi r21,0
	cr0.compare<uint32_t>(r21.u32, 0, xer);
	// stw r21,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r21.u32);
	// beq 0x824d0ad0
	if (cr0.eq) goto loc_824D0AD0;
	// mr r29,r16
	r29.u64 = r16.u64;
loc_824D09B4:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r31,r29,r11
	r31.u64 = r29.u64 + r11.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// or r23,r10,r23
	r23.u64 = ctx.r10.u64 | r23.u64;
	// rlwinm. r11,r11,0,12,12
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d0ac0
	if (cr0.eq) goto loc_824D0AC0;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r11,r11,0,14,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3FFF0;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// beq 0x824d09f4
	if (cr0.eq) goto loc_824D09F4;
	// rlwinm. r10,r11,0,16,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFF80;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d0ea4
	if (!cr0.eq) goto loc_824D0EA4;
	// rlwinm r11,r11,0,16,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFE00FFFF;
	// b 0x824d0abc
	goto loc_824D0ABC;
loc_824D09F4:
	// rlwinm r10,r11,0,25,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70;
	// cmplwi cr6,r10,96
	cr6.compare<uint32_t>(ctx.r10.u32, 96, xer);
	// beq cr6,0x824d0ab8
	if (cr6.eq) goto loc_824D0AB8;
	// addi r6,r1,108
	ctx.r6.s64 = ctx.r1.s64 + 108;
	// addi r5,r1,104
	ctx.r5.s64 = ctx.r1.s64 + 104;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824cf0c8
	sub_824CF0C8(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r8,108(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// rlwinm. r10,r11,25,23,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1FF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d0a50
	if (cr0.eq) goto loc_824D0A50;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r9,r9,12,23,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0x1FF;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// bgt cr6,0x824d0a44
	if (cr6.gt) goto loc_824D0A44;
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x824d0a50
	if (!cr6.lt) goto loc_824D0A50;
loc_824D0A44:
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x824d0eb0
	if (!cr6.eq) goto loc_824D0EB0;
loc_824D0A50:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r4,r10,12,23,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x1FF;
	// cmplwi cr6,r4,255
	cr6.compare<uint32_t>(ctx.r4.u32, 255, xer);
	// bgt cr6,0x824d0ac0
	if (cr6.gt) goto loc_824D0AC0;
	// subf r10,r4,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r4.s64;
	// rlwinm r5,r11,16,23,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0x1FF;
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// bge cr6,0x824d0a74
	if (!cr6.lt) goto loc_824D0A74;
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
loc_824D0A74:
	// rlwimi r11,r5,16,7,15
	r11.u64 = (__builtin_rotateleft32(ctx.r5.u32, 16) & 0x1FF0000) | (r11.u64 & 0xFFFFFFFFFE00FFFF);
	// rlwinm r10,r11,28,29,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x7;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// blt cr6,0x824d0aac
	if (cr6.lt) goto loc_824D0AAC;
	// beq cr6,0x824d0aa0
	if (cr6.eq) goto loc_824D0AA0;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// bne cr6,0x824d0ac0
	if (!cr6.eq) goto loc_824D0AC0;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x824cf490
	sub_824CF490(ctx, base);
	// b 0x824d0ab4
	goto loc_824D0AB4;
loc_824D0AA0:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x824cf5a8
	sub_824CF5A8(ctx, base);
	// b 0x824d0ab4
	goto loc_824D0AB4;
loc_824D0AAC:
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x824cf518
	sub_824CF518(ctx, base);
loc_824D0AB4:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
loc_824D0AB8:
	// oris r11,r11,512
	r11.u64 = r11.u64 | 33554432;
loc_824D0ABC:
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
loc_824D0AC0:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r29,r29,40
	r29.s64 = r29.s64 + 40;
	// cmplw cr6,r30,r21
	cr6.compare<uint32_t>(r30.u32, r21.u32, xer);
	// blt cr6,0x824d09b4
	if (cr6.lt) goto loc_824D09B4;
loc_824D0AD0:
	// li r5,128
	ctx.r5.s64 = 128;
	// stw r15,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r15.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,184
	ctx.r3.s64 = ctx.r1.s64 + 184;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r24,r16
	r24.u64 = r16.u64;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x824d0be4
	if (cr6.eq) goto loc_824D0BE4;
	// mr r25,r16
	r25.u64 = r16.u64;
loc_824D0AF4:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r26,r25,r11
	r26.u64 = r25.u64 + r11.u64;
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// rlwinm. r11,r11,0,6,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d0bd4
	if (!cr0.eq) goto loc_824D0BD4;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm. r10,r11,0,12,12
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d0bd4
	if (cr0.eq) goto loc_824D0BD4;
	// rlwinm r11,r11,0,3,11
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1FF00000;
	// lis r10,4080
	ctx.r10.s64 = 267386880;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x824d0bd4
	if (cr6.gt) goto loc_824D0BD4;
	// addi r3,r1,184
	ctx.r3.s64 = ctx.r1.s64 + 184;
	// addi r4,r1,328
	ctx.r4.s64 = ctx.r1.s64 + 328;
	// li r5,128
	ctx.r5.s64 = 128;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824cf6a0
	sub_824CF6A0(ctx, base);
	// mr r27,r16
	r27.u64 = r16.u64;
	// mr r28,r16
	r28.u64 = r16.u64;
loc_824D0B48:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r30,r28,r11
	r30.u64 = r28.u64 + r11.u64;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm. r10,r11,0,12,12
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d0bc4
	if (cr0.eq) goto loc_824D0BC4;
	// lwz r29,4(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// rlwinm. r10,r29,0,6,6
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x2000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d0bc4
	if (!cr0.eq) goto loc_824D0BC4;
	// lwz r10,36(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// lwz r9,36(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 36);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x824d0bc4
	if (!cr6.eq) goto loc_824D0BC4;
	// rlwinm r31,r11,12,23,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x1FF;
	// cmplwi cr6,r31,255
	cr6.compare<uint32_t>(r31.u32, 255, xer);
	// bgt cr6,0x824d0bc4
	if (cr6.gt) goto loc_824D0BC4;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,184
	ctx.r3.s64 = ctx.r1.s64 + 184;
	// bl 0x824cd4b8
	sub_824CD4B8(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824d0ba8
	if (cr6.eq) goto loc_824D0BA8;
	// rlwinm r11,r29,16,23,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0x1FF;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// blt cr6,0x824d0ebc
	if (cr6.lt) goto loc_824D0EBC;
loc_824D0BA8:
	// rlwinm r5,r29,16,23,31
	ctx.r5.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0x1FF;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,320
	ctx.r3.s64 = ctx.r1.s64 + 320;
	// bl 0x824cf358
	sub_824CF358(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// oris r11,r11,512
	r11.u64 = r11.u64 | 33554432;
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
loc_824D0BC4:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r28,r28,40
	r28.s64 = r28.s64 + 40;
	// cmplw cr6,r27,r21
	cr6.compare<uint32_t>(r27.u32, r21.u32, xer);
	// blt cr6,0x824d0b48
	if (cr6.lt) goto loc_824D0B48;
loc_824D0BD4:
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r25,r25,40
	r25.s64 = r25.s64 + 40;
	// cmplw cr6,r24,r21
	cr6.compare<uint32_t>(r24.u32, r21.u32, xer);
	// blt cr6,0x824d0af4
	if (cr6.lt) goto loc_824D0AF4;
loc_824D0BE4:
	// li r14,-1
	r14.s64 = -1;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x824d0e00
	if (cr6.eq) goto loc_824D0E00;
	// mr r22,r16
	r22.u64 = r16.u64;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x824d0df4
	if (cr6.eq) goto loc_824D0DF4;
	// mr r23,r16
	r23.u64 = r16.u64;
loc_824D0C00:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r24,r23,r11
	r24.u64 = r23.u64 + r11.u64;
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// rlwinm. r10,r11,0,6,6
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d0de4
	if (!cr0.eq) goto loc_824D0DE4;
	// lwz r10,36(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 36);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824d0de4
	if (cr6.eq) goto loc_824D0DE4;
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d0de4
	if (!cr0.eq) goto loc_824D0DE4;
	// addi r3,r1,184
	ctx.r3.s64 = ctx.r1.s64 + 184;
	// addi r4,r1,328
	ctx.r4.s64 = ctx.r1.s64 + 328;
	// li r5,128
	ctx.r5.s64 = 128;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r26,r14
	r26.u64 = r14.u64;
	// mr r25,r16
	r25.u64 = r16.u64;
	// mr r31,r16
	r31.u64 = r16.u64;
	// mr r30,r21
	r30.u64 = r21.u64;
loc_824D0C48:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm. r9,r10,0,6,6
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2000000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824d0c80
	if (cr0.eq) goto loc_824D0C80;
	// lwz r9,36(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// lwz r8,36(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 36);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bne cr6,0x824d0c80
	if (!cr6.eq) goto loc_824D0C80;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r5,r10,16,23,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0x1FF;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// rlwinm r4,r11,12,23,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x1FF;
	// bl 0x824cf3f0
	sub_824CF3F0(ctx, base);
loc_824D0C80:
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// addi r31,r31,40
	r31.s64 = r31.s64 + 40;
	// bne 0x824d0c48
	if (!cr0.eq) goto loc_824D0C48;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824cf6a0
	sub_824CF6A0(ctx, base);
	// mr r30,r16
	r30.u64 = r16.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
loc_824D0CA0:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r31,r30,r11
	r31.u64 = r30.u64 + r11.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm. r10,r11,0,6,6
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d0d04
	if (cr0.eq) goto loc_824D0D04;
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r9,36(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 36);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x824d0d04
	if (!cr6.eq) goto loc_824D0D04;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r5,r11,16,23,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0x1FF;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// rlwinm r4,r10,12,23,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x1FF;
	// bl 0x824cf358
	sub_824CF358(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r11,r11,12,23,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x1FF;
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bge cr6,0x824d0cec
	if (!cr6.lt) goto loc_824D0CEC;
	// mr r26,r11
	r26.u64 = r11.u64;
loc_824D0CEC:
	// lhz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// clrlwi r10,r10,23
	ctx.r10.u64 = ctx.r10.u32 & 0x1FF;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r25
	cr6.compare<uint32_t>(r11.u32, r25.u32, xer);
	// ble cr6,0x824d0d04
	if (!cr6.gt) goto loc_824D0D04;
	// mr r25,r11
	r25.u64 = r11.u64;
loc_824D0D04:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r30,r30,40
	r30.s64 = r30.s64 + 40;
	// bne 0x824d0ca0
	if (!cr0.eq) goto loc_824D0CA0;
	// mr r27,r16
	r27.u64 = r16.u64;
	// mr r28,r16
	r28.u64 = r16.u64;
loc_824D0D18:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r31,r28,r11
	r31.u64 = r28.u64 + r11.u64;
	// lwz r29,4(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm. r11,r29,0,6,6
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x2000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d0dd4
	if (!cr0.eq) goto loc_824D0DD4;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r10,36(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 36);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x824d0dd4
	if (!cr6.eq) goto loc_824D0DD4;
	// mr r3,r14
	ctx.r3.u64 = r14.u64;
	// cmpwi cr6,r26,-1
	cr6.compare<int32_t>(r26.s32, -1, xer);
	// beq cr6,0x824d0d58
	if (cr6.eq) goto loc_824D0D58;
	// rlwinm r5,r29,16,23,31
	ctx.r5.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0x1FF;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x824cdf98
	sub_824CDF98(ctx, base);
loc_824D0D58:
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824d0d74
	if (cr6.eq) goto loc_824D0D74;
	// cmpwi cr6,r26,-1
	cr6.compare<int32_t>(r26.s32, -1, xer);
	// beq cr6,0x824d0d90
	if (cr6.eq) goto loc_824D0D90;
	// cmplw cr6,r3,r25
	cr6.compare<uint32_t>(ctx.r3.u32, r25.u32, xer);
	// ble cr6,0x824d0d90
	if (!cr6.gt) goto loc_824D0D90;
loc_824D0D74:
	// rlwinm r5,r29,16,23,31
	ctx.r5.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0x1FF;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x824cdf98
	sub_824CDF98(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x824d0d90
	if (cr6.eq) goto loc_824D0D90;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_824D0D90:
	// cmpwi cr6,r30,-1
	cr6.compare<int32_t>(r30.s32, -1, xer);
	// beq cr6,0x824d0ec8
	if (cr6.eq) goto loc_824D0EC8;
	// rlwinm r5,r29,16,23,31
	ctx.r5.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0x1FF;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,320
	ctx.r3.s64 = ctx.r1.s64 + 320;
	// bl 0x824cdeb0
	sub_824CDEB0(ctx, base);
	// lhz r11,4(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// clrlwi r5,r11,23
	ctx.r5.u64 = r11.u32 & 0x1FF;
	// bl 0x824cdeb0
	sub_824CDEB0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwimi r11,r30,20,3,11
	r11.u64 = (__builtin_rotateleft32(r30.u32, 20) & 0x1FF00000) | (r11.u64 & 0xFFFFFFFFE00FFFFF);
	// oris r10,r10,512
	ctx.r10.u64 = ctx.r10.u64 | 33554432;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
loc_824D0DD4:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r28,r28,40
	r28.s64 = r28.s64 + 40;
	// cmplw cr6,r27,r21
	cr6.compare<uint32_t>(r27.u32, r21.u32, xer);
	// blt cr6,0x824d0d18
	if (cr6.lt) goto loc_824D0D18;
loc_824D0DE4:
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// addi r23,r23,40
	r23.s64 = r23.s64 + 40;
	// cmplw cr6,r22,r21
	cr6.compare<uint32_t>(r22.u32, r21.u32, xer);
	// blt cr6,0x824d0c00
	if (cr6.lt) goto loc_824D0C00;
loc_824D0DF4:
	// addi r4,r1,320
	ctx.r4.s64 = ctx.r1.s64 + 320;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824cf6a0
	sub_824CF6A0(ctx, base);
loc_824D0E00:
	// lwz r11,40(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 40);
	// stb r16,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r16.u8);
	// rlwinm. r11,r11,0,28,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1294
	if (cr0.eq) goto loc_824D1294;
	// lwz r11,4(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// mr r24,r16
	r24.u64 = r16.u64;
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d1294
	if (!cr0.eq) goto loc_824D1294;
	// mr. r16,r11
	r16.u64 = r11.u64;
	cr0.compare<int32_t>(r16.s32, 0, xer);
	// beq 0x824d1294
	if (cr0.eq) goto loc_824D1294;
loc_824D0E28:
	// lwz r11,28(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 28);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d11b8
	if (!cr0.eq) goto loc_824D11B8;
	// mr. r17,r11
	r17.u64 = r11.u64;
	cr0.compare<int32_t>(r17.s32, 0, xer);
	// beq 0x824d11b8
	if (cr0.eq) goto loc_824D11B8;
	// lwz r31,84(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_824D0E40:
	// lwz r11,8(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,12672
	cr6.compare<uint32_t>(r11.u32, 12672, xer);
	// bne cr6,0x824d119c
	if (!cr6.eq) goto loc_824D119C;
	// lwz r19,40(r17)
	r19.u64 = PPC_LOAD_U32(r17.u32 + 40);
	// lwz r18,12(r19)
	r18.u64 = PPC_LOAD_U32(r19.u32 + 12);
	// lwz r11,8(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// addi r10,r11,-123
	ctx.r10.s64 = r11.s64 + -123;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d119c
	if (!cr0.eq) goto loc_824D119C;
	// cmplwi cr6,r11,119
	cr6.compare<uint32_t>(r11.u32, 119, xer);
	// bne cr6,0x824d11d8
	if (!cr6.eq) goto loc_824D11D8;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// bne cr6,0x824d1134
	if (!cr6.eq) goto loc_824D1134;
	// lwz r11,4(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d0e90
	if (cr0.eq) goto loc_824D0E90;
	// li r11,0
	r11.s64 = 0;
loc_824D0E90:
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d0ed4
	if (cr0.eq) goto loc_824D0ED4;
loc_824D0E9C:
	// li r24,0
	r24.s64 = 0;
	// b 0x824d0f58
	goto loc_824D0F58;
loc_824D0EA4:
	// li r4,4801
	ctx.r4.s64 = 4801;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D0EB0:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824cf270
	sub_824CF270(ctx, base);
loc_824D0EBC:
	// li r4,3613
	ctx.r4.s64 = 3613;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D0EC8:
	// li r4,3566
	ctx.r4.s64 = 3566;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D0ED4:
	// mr. r24,r11
	r24.u64 = r11.u64;
	cr0.compare<int32_t>(r24.s32, 0, xer);
	// beq 0x824d0f58
	if (cr0.eq) goto loc_824D0F58;
loc_824D0EDC:
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-15744
	r11.s64 = r11.s64 + -15744;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d0f30
	if (cr0.eq) goto loc_824D0F30;
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r24,-8
	r11.s64 = r24.s64 + -8;
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r10,r10,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d0f30
	if (cr0.eq) goto loc_824D0F30;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r11,r11,0,26,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x30;
	// cmplwi cr6,r11,48
	cr6.compare<uint32_t>(r11.u32, 48, xer);
	// bne cr6,0x824d0f50
	if (!cr6.eq) goto loc_824D0F50;
loc_824D0F30:
	// rlwinm r11,r24,0,0,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d0e9c
	if (!cr0.eq) goto loc_824D0E9C;
	// mr r24,r11
	r24.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824d0edc
	if (!cr6.eq) goto loc_824D0EDC;
	// b 0x824d0f58
	goto loc_824D0F58;
loc_824D0F50:
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// bne cr6,0x824d1134
	if (!cr6.eq) goto loc_824D1134;
loc_824D0F58:
	// mr r25,r14
	r25.u64 = r14.u64;
	// mr r5,r14
	ctx.r5.u64 = r14.u64;
	// li r21,0
	r21.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824d112c
	if (cr6.eq) goto loc_824D112C;
	// lwz r20,12(r15)
	r20.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// mr r23,r20
	r23.u64 = r20.u64;
loc_824D0F74:
	// lwz r11,4(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 4);
	// rlwinm r10,r11,0,25,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70;
	// cmplwi cr6,r10,48
	cr6.compare<uint32_t>(ctx.r10.u32, 48, xer);
	// bne cr6,0x824d10b8
	if (!cr6.eq) goto loc_824D10B8;
	// lwz r28,0(r23)
	r28.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm. r10,r28,0,2,2
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x20000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d10b8
	if (!cr0.eq) goto loc_824D10B8;
	// rlwinm. r22,r11,16,23,31
	r22.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0x1FF;
	cr0.compare<int32_t>(r22.s32, 0, xer);
	// beq 0x824d10b8
	if (cr0.eq) goto loc_824D10B8;
	// rlwinm. r26,r28,29,18,31
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 29) & 0x3FFF;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// li r29,0
	r29.s64 = 0;
	// beq 0x824d10a0
	if (cr0.eq) goto loc_824D10A0;
	// lwz r27,28(r23)
	r27.u64 = PPC_LOAD_U32(r23.u32 + 28);
loc_824D0FA8:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r10,r11,12,26,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3F;
	// cmpwi cr6,r10,33
	cr6.compare<int32_t>(ctx.r10.s32, 33, xer);
	// beq cr6,0x824d1090
	if (cr6.eq) goto loc_824D1090;
	// cmpwi cr6,r10,37
	cr6.compare<int32_t>(ctx.r10.s32, 37, xer);
	// beq cr6,0x824d1090
	if (cr6.eq) goto loc_824D1090;
	// rlwinm. r10,r28,0,12,12
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d10a0
	if (cr0.eq) goto loc_824D10A0;
	// rlwinm r10,r11,28,18,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x3FFF;
	// cmplw cr6,r10,r22
	cr6.compare<uint32_t>(ctx.r10.u32, r22.u32, xer);
	// bge cr6,0x824d1090
	if (!cr6.lt) goto loc_824D1090;
	// rlwinm r11,r28,12,23,31
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 12) & 0x1FF;
	// li r3,0
	ctx.r3.s64 = 0;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// mr r8,r20
	ctx.r8.u64 = r20.u64;
loc_824D0FE4:
	// lwz r31,4(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// rlwinm r11,r31,0,25,27
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x70;
	// cmplwi cr6,r11,48
	cr6.compare<uint32_t>(r11.u32, 48, xer);
	// bne cr6,0x824d1070
	if (!cr6.eq) goto loc_824D1070;
	// lwz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// rlwinm. r11,r4,0,12,12
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1070
	if (cr0.eq) goto loc_824D1070;
	// rlwinm. r11,r4,0,2,2
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x20000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1070
	if (cr0.eq) goto loc_824D1070;
	// cmplw cr6,r8,r23
	cr6.compare<uint32_t>(ctx.r8.u32, r23.u32, xer);
	// beq cr6,0x824d1070
	if (cr6.eq) goto loc_824D1070;
	// rlwinm. r6,r4,29,18,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 29) & 0x3FFF;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// li r9,0
	ctx.r9.s64 = 0;
	// beq 0x824d1068
	if (cr0.eq) goto loc_824D1068;
	// lwz r7,28(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
loc_824D1020:
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lis r10,592
	ctx.r10.s64 = 38797312;
	// rlwinm r14,r11,0,6,11
	r14.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F00000;
	// cmplw cr6,r14,r10
	cr6.compare<uint32_t>(r14.u32, ctx.r10.u32, xer);
	// bne cr6,0x824d1054
	if (!cr6.eq) goto loc_824D1054;
	// rlwinm r11,r11,28,18,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x3FFF;
	// rlwinm r10,r31,16,23,31
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 16) & 0x1FF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x824d1054
	if (!cr6.lt) goto loc_824D1054;
	// rlwinm r10,r4,12,23,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 12) & 0x1FF;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x824d1064
	if (cr6.eq) goto loc_824D1064;
loc_824D1054:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// cmplw cr6,r9,r6
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, xer);
	// blt cr6,0x824d1020
	if (cr6.lt) goto loc_824D1020;
loc_824D1064:
	// li r14,-1
	r14.s64 = -1;
loc_824D1068:
	// cmplw cr6,r9,r6
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, xer);
	// blt cr6,0x824d1084
	if (cr6.lt) goto loc_824D1084;
loc_824D1070:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r8,r8,40
	ctx.r8.s64 = ctx.r8.s64 + 40;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// blt cr6,0x824d0fe4
	if (cr6.lt) goto loc_824D0FE4;
loc_824D1084:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bge cr6,0x824d10a0
	if (!cr6.lt) goto loc_824D10A0;
loc_824D1090:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r27,r27,8
	r27.s64 = r27.s64 + 8;
	// cmplw cr6,r29,r26
	cr6.compare<uint32_t>(r29.u32, r26.u32, xer);
	// blt cr6,0x824d0fa8
	if (cr6.lt) goto loc_824D0FA8;
loc_824D10A0:
	// cmplw cr6,r29,r26
	cr6.compare<uint32_t>(r29.u32, r26.u32, xer);
	// bge cr6,0x824d10b8
	if (!cr6.lt) goto loc_824D10B8;
	// cmplw cr6,r29,r5
	cr6.compare<uint32_t>(r29.u32, ctx.r5.u32, xer);
	// bge cr6,0x824d10b8
	if (!cr6.lt) goto loc_824D10B8;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r25,r21
	r25.u64 = r21.u64;
loc_824D10B8:
	// lwz r31,84(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// addi r23,r23,40
	r23.s64 = r23.s64 + 40;
	// cmplw cr6,r21,r31
	cr6.compare<uint32_t>(r21.u32, r31.u32, xer);
	// blt cr6,0x824d0f74
	if (cr6.lt) goto loc_824D0F74;
	// cmpwi cr6,r25,-1
	cr6.compare<int32_t>(r25.s32, -1, xer);
	// beq cr6,0x824d112c
	if (cr6.eq) goto loc_824D112C;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82555010
	sub_82555010(ctx, base);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mulli r11,r25,40
	r11.s64 = r25.s64 * 40;
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rlwinm r10,r10,0,30,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFFE3;
	// rlwinm r10,r10,0,17,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFE7FFF;
	// oris r10,r10,256
	ctx.r10.u64 = ctx.r10.u64 | 16777216;
	// ori r10,r10,16386
	ctx.r10.u64 = ctx.r10.u64 | 16386;
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// lwz r10,12(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// oris r9,r9,8192
	ctx.r9.u64 = ctx.r9.u64 | 536870912;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,12(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwimi r10,r9,7,16,24
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 7) & 0xFF80) | (ctx.r10.u64 & 0xFFFFFFFFFFFF007F);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
loc_824D112C:
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x824d11e4
	if (cr6.eq) goto loc_824D11E4;
loc_824D1134:
	// lwz r11,12(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 12);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// b 0x824d1144
	goto loc_824D1144;
loc_824D1140:
	// addi r11,r10,8
	r11.s64 = ctx.r10.s64 + 8;
loc_824D1144:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r10,r19
	cr6.compare<uint32_t>(ctx.r10.u32, r19.u32, xer);
	// bne cr6,0x824d1140
	if (!cr6.eq) goto loc_824D1140;
	// lwz r10,8(r19)
	ctx.r10.u64 = PPC_LOAD_U32(r19.u32 + 8);
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// stw r11,8(r19)
	PPC_STORE_U32(r19.u32 + 8, r11.u32);
	// stw r19,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r19.u32);
	// lwz r11,0(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 0);
	// stw r24,12(r19)
	PPC_STORE_U32(r19.u32 + 12, r24.u32);
	// rlwinm r11,r11,0,19,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFE001FFF;
	// rlwinm r11,r11,0,2,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r11,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r11.u32);
	// bl 0x824a98e8
	sub_824A98E8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d119c
	if (cr0.eq) goto loc_824D119C;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x8255a198
	sub_8255A198(ctx, base);
loc_824D119C:
	// rlwinm r11,r17,0,0,30
	r11.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d11b8
	if (!cr0.eq) goto loc_824D11B8;
	// mr r17,r11
	r17.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824d0e40
	if (!cr6.eq) goto loc_824D0E40;
loc_824D11B8:
	// rlwinm r11,r16,0,0,30
	r11.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d1294
	if (!cr0.eq) goto loc_824D1294;
	// mr r16,r11
	r16.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824d0e28
	if (!cr6.eq) goto loc_824D0E28;
	// b 0x824d1294
	goto loc_824D1294;
loc_824D11D8:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D11E4:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1280
	if (cr0.eq) goto loc_824D1280;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824d1288
	if (cr6.eq) goto loc_824D1288;
	// lwz r10,12(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 12);
loc_824D1200:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm. r9,r11,0,12,12
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824d1254
	if (cr0.eq) goto loc_824D1254;
	// lwz r8,28(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// rlwinm. r9,r9,0,14,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x3FFF0;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824d1254
	if (!cr0.eq) goto loc_824D1254;
	// rlwinm. r11,r11,12,23,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x1FF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1254
	if (!cr0.eq) goto loc_824D1254;
	// lhz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// clrlwi r9,r9,23
	ctx.r9.u64 = ctx.r9.u32 & 0x1FF;
	// add. r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x824d1254
	if (cr0.eq) goto loc_824D1254;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,12,26,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3F;
	// cmpwi cr6,r11,37
	cr6.compare<int32_t>(r11.s32, 37, xer);
	// beq cr6,0x824d1268
	if (cr6.eq) goto loc_824D1268;
	// cmpwi cr6,r11,33
	cr6.compare<int32_t>(r11.s32, 33, xer);
	// beq cr6,0x824d1274
	if (cr6.eq) goto loc_824D1274;
loc_824D1254:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r10,r10,40
	ctx.r10.s64 = ctx.r10.s64 + 40;
	// cmplw cr6,r7,r31
	cr6.compare<uint32_t>(ctx.r7.u32, r31.u32, xer);
	// blt cr6,0x824d1200
	if (cr6.lt) goto loc_824D1200;
	// b 0x824d1288
	goto loc_824D1288;
loc_824D1268:
	// li r4,3601
	ctx.r4.s64 = 3601;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D1274:
	// li r4,3602
	ctx.r4.s64 = 3602;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D1280:
	// li r11,1
	r11.s64 = 1;
	// stb r11,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r11.u8);
loc_824D1288:
	// li r4,3600
	ctx.r4.s64 = 3600;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496da0
	sub_82496DA0(ctx, base);
loc_824D1294:
	// lwz r25,84(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x824d1448
	if (cr6.eq) goto loc_824D1448;
	// li r26,0
	r26.s64 = 0;
loc_824D12A8:
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// add r31,r26,r11
	r31.u64 = r26.u64 + r11.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm. r10,r11,0,6,6
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d1438
	if (!cr0.eq) goto loc_824D1438;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm. r10,r10,0,2,2
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d1438
	if (cr0.eq) goto loc_824D1438;
	// rlwinm r10,r11,28,29,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x7;
	// rlwinm r29,r11,25,23,31
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1FF;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// blt cr6,0x824d1414
	if (cr6.lt) goto loc_824D1414;
	// beq cr6,0x824d1404
	if (cr6.eq) goto loc_824D1404;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// blt cr6,0x824d13f0
	if (cr6.lt) goto loc_824D13F0;
	// bne cr6,0x824d14d4
	if (!cr6.eq) goto loc_824D14D4;
	// lbz r11,80(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// li r28,0
	r28.s64 = 0;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d13b4
	if (cr0.eq) goto loc_824D13B4;
	// lwz r11,4(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d1308
	if (cr0.eq) goto loc_824D1308;
	// li r11,0
	r11.s64 = 0;
loc_824D1308:
	// lwz r30,20(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d13b4
	if (!cr0.eq) goto loc_824D13B4;
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x824d13b4
	if (cr0.eq) goto loc_824D13B4;
loc_824D131C:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-15744
	r11.s64 = r11.s64 + -15744;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d137c
	if (cr0.eq) goto loc_824D137C;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r30,-8
	r11.s64 = r30.s64 + -8;
	// add r10,r3,r11
	ctx.r10.u64 = ctx.r3.u64 + r11.u64;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// clrlwi r9,r11,17
	ctx.r9.u64 = r11.u32 & 0x7FFF;
	// cmplw cr6,r9,r27
	cr6.compare<uint32_t>(ctx.r9.u32, r27.u32, xer);
	// bne cr6,0x824d137c
	if (!cr6.eq) goto loc_824D137C;
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d137c
	if (cr0.eq) goto loc_824D137C;
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// rlwinm r11,r11,0,26,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x30;
	// cmplwi cr6,r11,48
	cr6.compare<uint32_t>(r11.u32, 48, xer);
	// beq cr6,0x824d1398
	if (cr6.eq) goto loc_824D1398;
loc_824D137C:
	// rlwinm r11,r30,0,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r30,36(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d13b4
	if (!cr0.eq) goto loc_824D13B4;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824d131c
	if (!cr6.eq) goto loc_824D131C;
	// b 0x824d13b4
	goto loc_824D13B4;
loc_824D1398:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d13b4
	if (!cr0.eq) goto loc_824D13B4;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x824ce020
	sub_824CE020(ctx, base);
	// li r28,1
	r28.s64 = 1;
loc_824D13B4:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x824cef30
	sub_824CEF30(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwimi r11,r3,20,3,11
	r11.u64 = (__builtin_rotateleft32(ctx.r3.u32, 20) & 0x1FF00000) | (r11.u64 & 0xFFFFFFFFE00FFFFF);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// clrlwi. r10,r28,24
	ctx.r10.u64 = r28.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// oris r10,r9,512
	ctx.r10.u64 = ctx.r9.u64 | 33554432;
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// beq 0x824d1438
	if (cr0.eq) goto loc_824D1438;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// b 0x824d1438
	goto loc_824D1438;
loc_824D13F0:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// addi r4,r1,320
	ctx.r4.s64 = ctx.r1.s64 + 320;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824cec38
	sub_824CEC38(ctx, base);
	// b 0x824d1420
	goto loc_824D1420;
loc_824D1404:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x824cefe0
	sub_824CEFE0(ctx, base);
	// b 0x824d1420
	goto loc_824D1420;
loc_824D1414:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x824cef88
	sub_824CEF88(ctx, base);
loc_824D1420:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// oris r10,r10,512
	ctx.r10.u64 = ctx.r10.u64 | 33554432;
	// rlwimi r11,r3,20,3,11
	r11.u64 = (__builtin_rotateleft32(ctx.r3.u32, 20) & 0x1FF00000) | (r11.u64 & 0xFFFFFFFFE00FFFFF);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_824D1438:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r26,r26,40
	r26.s64 = r26.s64 + 40;
	// cmplw cr6,r27,r25
	cr6.compare<uint32_t>(r27.u32, r25.u32, xer);
	// blt cr6,0x824d12a8
	if (cr6.lt) goto loc_824D12A8;
loc_824D1448:
	// addi r4,r1,320
	ctx.r4.s64 = ctx.r1.s64 + 320;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824cf6a0
	sub_824CF6A0(ctx, base);
	// lwz r11,4(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d1464
	if (cr0.eq) goto loc_824D1464;
	// li r11,0
	r11.s64 = 0;
loc_824D1464:
	// lwz r31,20(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1590
	if (!cr0.eq) goto loc_824D1590;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x824d1590
	if (cr0.eq) goto loc_824D1590;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lwz r27,104(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r28,r11,27924
	r28.s64 = r11.s64 + 27924;
loc_824D1484:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// addi r10,r11,-124
	ctx.r10.s64 = r11.s64 + -124;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d14e0
	if (cr0.eq) goto loc_824D14E0;
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r3,r1,320
	ctx.r3.s64 = ctx.r1.s64 + 320;
	// bl 0x824cdf18
	sub_824CDF18(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r9,1
	ctx.r9.s64 = 1;
	// rlwinm r10,r10,18,29,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x7;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// rlwimi r11,r27,0,28,31
	r11.u64 = (__builtin_rotateleft32(r27.u32, 0) & 0xF) | (r11.u64 & 0xFFFFFFFFFFFFFFF0);
	// slw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// addi r27,r10,-1
	r27.s64 = ctx.r10.s64 + -1;
	// rlwimi r27,r11,0,0,27
	r27.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFF0) | (r27.u64 & 0xFFFFFFFF0000000F);
	// clrlwi r11,r27,28
	r11.u64 = r27.u32 & 0xF;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// b 0x824d1568
	goto loc_824D1568;
loc_824D14D4:
	// li r4,4800
	ctx.r4.s64 = 4800;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D14E0:
	// cmplwi cr6,r11,123
	cr6.compare<uint32_t>(r11.u32, 123, xer);
	// bne cr6,0x824d1578
	if (!cr6.eq) goto loc_824D1578;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824d8728
	sub_824D8728(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// addi r30,r31,-8
	r30.s64 = r31.s64 + -8;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// rlwimi r10,r9,21,14,27
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 21) & 0x3FFF0) | (ctx.r10.u64 & 0xFFFFFFFFFFFC000F);
	// clrlwi r29,r10,14
	r29.u64 = ctx.r10.u32 & 0x3FFFF;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,12(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// lwzx r11,r11,r30
	r11.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// clrlwi r11,r11,17
	r11.u64 = r11.u32 & 0x7FFF;
	// mulli r11,r11,40
	r11.s64 = r11.s64 * 40;
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// rlwinm r11,r11,0,19,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1FF0;
	// add r4,r11,r29
	ctx.r4.u64 = r11.u64 + r29.u64;
	// rlwimi r4,r29,0,28,31
	ctx.r4.u64 = (__builtin_rotateleft32(r29.u32, 0) & 0xF) | (ctx.r4.u64 & 0xFFFFFFFFFFFFFFF0);
	// clrlwi r11,r4,28
	r11.u64 = ctx.r4.u32 & 0xF;
loc_824D1568:
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lbz r5,-1(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// bl 0x825518d8
	sub_825518D8(ctx, base);
loc_824D1578:
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1590
	if (!cr0.eq) goto loc_824D1590;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824d1484
	if (!cr6.eq) goto loc_824D1484;
loc_824D1590:
	// lwz r11,36(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 36);
	// rlwinm. r11,r11,0,22,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d16b0
	if (cr0.eq) goto loc_824D16B0;
	// lwz r11,4(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d16b0
	if (!cr0.eq) goto loc_824D16B0;
	// mr. r28,r11
	r28.u64 = r11.u64;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824d16b0
	if (cr0.eq) goto loc_824D16B0;
loc_824D15B0:
	// lwz r11,28(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1694
	if (!cr0.eq) goto loc_824D1694;
	// lwz r11,24(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 24);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r31,r11,-36
	xer.ca = r11.u32 > 35;
	r31.s64 = r11.s64 + -36;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824d1694
	if (cr0.eq) goto loc_824D1694;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// cmplwi cr6,r11,10624
	cr6.compare<uint32_t>(r11.u32, 10624, xer);
	// bne cr6,0x824d1694
	if (!cr6.eq) goto loc_824D1694;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,83
	ctx.r4.s64 = 83;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r31,-28
	r11.s64 = r31.s64 + -28;
	// lwz r30,0(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r29,r3,r11
	r29.u64 = ctx.r3.u64 + r11.u64;
loc_824D15FC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x824d167c
	if (cr6.eq) goto loc_824D167C;
	// lwz r31,12(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-15744
	r11.s64 = r11.s64 + -15744;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1660
	if (cr0.eq) goto loc_824D1660;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// rlwinm r6,r11,18,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x7;
	// rlwinm r5,r11,13,29,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	// rlwinm r4,r11,25,25,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// bl 0x824a9340
	sub_824A9340(ctx, base);
	// addi r11,r31,-8
	r11.s64 = r31.s64 + -8;
	// lwz r10,12(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// lwzx r11,r3,r11
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// clrlwi r11,r11,17
	r11.u64 = r11.u32 & 0x7FFF;
	// mulli r11,r11,40
	r11.s64 = r11.s64 * 40;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r11,r11,0,25,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x70;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// beq cr6,0x824d1668
	if (cr6.eq) goto loc_824D1668;
loc_824D1660:
	// lwz r30,4(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// b 0x824d15fc
	goto loc_824D15FC;
loc_824D1668:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r10,12(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// rlwimi r10,r11,15,27,31
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 15) & 0x1F) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE0);
	// stw r10,12(r29)
	PPC_STORE_U32(r29.u32 + 12, ctx.r10.u32);
	// b 0x824d1694
	goto loc_824D1694;
loc_824D167C:
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x824ce210
	sub_824CE210(ctx, base);
	// lwz r11,12(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// rlwimi r11,r3,28,27,31
	r11.u64 = (__builtin_rotateleft32(ctx.r3.u32, 28) & 0x1F) | (r11.u64 & 0xFFFFFFFFFFFFFFE0);
	// stw r11,12(r29)
	PPC_STORE_U32(r29.u32 + 12, r11.u32);
loc_824D1694:
	// rlwinm r11,r28,0,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d16b0
	if (!cr0.eq) goto loc_824D16B0;
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824d15b0
	if (!cr6.eq) goto loc_824D15B0;
loc_824D16B0:
	// addi r1,r1,608
	ctx.r1.s64 = ctx.r1.s64 + 608;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_824D16B8"))) PPC_WEAK_FUNC(sub_824D16B8);
PPC_FUNC_IMPL(__imp__sub_824D16B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824d16d4
	if (cr6.lt) goto loc_824D16D4;
	// cmplwi cr6,r11,81
	cr6.compare<uint32_t>(r11.u32, 81, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824d16d8
	if (!cr6.gt) goto loc_824D16D8;
loc_824D16D4:
	// li r11,0
	r11.s64 = 0;
loc_824D16D8:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r3,r11,1
	ctx.r3.u64 = r11.u64 ^ 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824D16EC"))) PPC_WEAK_FUNC(sub_824D16EC);
PPC_FUNC_IMPL(__imp__sub_824D16EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824D16F0"))) PPC_WEAK_FUNC(sub_824D16F0);
PPC_FUNC_IMPL(__imp__sub_824D16F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcf8
	// lwz r11,40(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// lwz r5,44(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,12(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r6,12(r5)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// rlwinm. r11,r10,2,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d17ac
	if (cr0.eq) goto loc_824D17AC;
	// rlwinm r11,r3,0,0,19
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFFF000;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,148(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 148);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// rlwinm. r11,r11,25,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d17ac
	if (cr0.eq) goto loc_824D17AC;
	// li r30,0
	r30.s64 = 0;
	// rlwinm. r11,r10,7,29,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// li r29,1
	r29.s64 = 1;
	// beq 0x824d1764
	if (cr0.eq) goto loc_824D1764;
	// rlwinm r9,r10,27,24,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_824D1748:
	// srw r8,r9,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r10.u8 & 0x3F));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// clrlwi r8,r8,30
	ctx.r8.u64 = ctx.r8.u32 & 0x3;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// slw r8,r29,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r8.u8 & 0x3F));
	// or r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 | ctx.r7.u64;
	// bne 0x824d1748
	if (!cr0.eq) goto loc_824D1748;
loc_824D1764:
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// rlwinm. r11,r10,7,29,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1798
	if (cr0.eq) goto loc_824D1798;
	// rlwinm r8,r10,27,24,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_824D177C:
	// srw r6,r8,r10
	ctx.r6.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// clrlwi r6,r6,30
	ctx.r6.u64 = ctx.r6.u32 & 0x3;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// slw r6,r29,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r6.u8 & 0x3F));
	// or r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 | ctx.r9.u64;
	// bne 0x824d177c
	if (!cr0.eq) goto loc_824D177C;
loc_824D1798:
	// and. r11,r9,r7
	r11.u64 = ctx.r9.u64 & ctx.r7.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d19d0
	if (!cr0.eq) goto loc_824D19D0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r30,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r30.u8);
	// b 0x824d19d4
	goto loc_824D19D4;
loc_824D17AC:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,0
	r30.s64 = 0;
	// li r29,1
	r29.s64 = 1;
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// li r3,15744
	ctx.r3.s64 = 15744;
	// cmpwi cr6,r11,19
	cr6.compare<int32_t>(r11.s32, 19, xer);
	// beq cr6,0x824d17d8
	if (cr6.eq) goto loc_824D17D8;
	// cmpwi cr6,r11,29
	cr6.compare<int32_t>(r11.s32, 29, xer);
	// beq cr6,0x824d17d8
	if (cr6.eq) goto loc_824D17D8;
	// cmpwi cr6,r11,112
	cr6.compare<int32_t>(r11.s32, 112, xer);
	// bne cr6,0x824d1870
	if (!cr6.eq) goto loc_824D1870;
loc_824D17D8:
	// rlwinm. r7,r10,7,29,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x7;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// beq 0x824d180c
	if (cr0.eq) goto loc_824D180C;
	// rlwinm r8,r10,27,24,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// mr r11,r30
	r11.u64 = r30.u64;
loc_824D17EC:
	// srw r28,r8,r11
	r28.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (r11.u8 & 0x3F));
	// clrlwi r28,r28,30
	r28.u64 = r28.u32 & 0x3;
	// cmpw cr6,r28,r9
	cr6.compare<int32_t>(r28.s32, ctx.r9.s32, xer);
	// bne cr6,0x824d18c4
	if (!cr6.eq) goto loc_824D18C4;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// blt cr6,0x824d17ec
	if (cr6.lt) goto loc_824D17EC;
loc_824D180C:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_824D1810:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d19d0
	if (!cr0.eq) goto loc_824D19D0;
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r3,r11
	r11.s64 = r11.s64 - ctx.r3.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d19d0
	if (cr0.eq) goto loc_824D19D0;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x824d186c
	if (cr6.eq) goto loc_824D186C;
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwinm r10,r10,27,24,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// rlwinm r9,r11,27,24,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0xFF;
	// mr r11,r30
	r11.u64 = r30.u64;
loc_824D184C:
	// xor r28,r9,r10
	r28.u64 = ctx.r9.u64 ^ ctx.r10.u64;
	// srw r28,r28,r11
	r28.u64 = r11.u8 & 0x20 ? 0 : (r28.u32 >> (r11.u8 & 0x3F));
	// clrlwi. r28,r28,30
	r28.u64 = r28.u32 & 0x3;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x824d19d0
	if (cr0.eq) goto loc_824D19D0;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// blt cr6,0x824d184c
	if (cr6.lt) goto loc_824D184C;
loc_824D186C:
	// stb r30,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r30.u8);
loc_824D1870:
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// subf r11,r3,r11
	r11.s64 = r11.s64 - ctx.r3.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d18cc
	if (cr0.eq) goto loc_824D18CC;
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwinm. r11,r10,7,29,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d18d4
	if (cr0.eq) goto loc_824D18D4;
	// rlwinm r8,r10,27,24,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xFF;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_824D18A4:
	// srw r7,r8,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// clrlwi r7,r7,30
	ctx.r7.u64 = ctx.r7.u32 & 0x3;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// slw r7,r29,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r7.u8 & 0x3F));
	// or r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 | ctx.r9.u64;
	// bne 0x824d18a4
	if (!cr0.eq) goto loc_824D18A4;
	// b 0x824d18d4
	goto loc_824D18D4;
loc_824D18C4:
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x824d1810
	goto loc_824D1810;
loc_824D18CC:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwinm r6,r11,7,29,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x7;
loc_824D18D4:
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 4);
loc_824D18D8:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x824d1990
	if (cr6.eq) goto loc_824D1990;
	// lwz r10,16(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// subf r11,r30,r10
	r11.s64 = ctx.r10.s64 - r30.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d1988
	if (cr0.eq) goto loc_824D1988;
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// rlwinm r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d1988
	if (cr0.eq) goto loc_824D1988;
	// lwz r8,8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r11,r8,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,31
	cr6.compare<uint32_t>(r11.u32, 31, xer);
	// blt cr6,0x824d1934
	if (cr6.lt) goto loc_824D1934;
	// cmplwi cr6,r11,81
	cr6.compare<uint32_t>(r11.u32, 81, xer);
	// mr r11,r29
	r11.u64 = r29.u64;
	// ble cr6,0x824d1938
	if (!cr6.gt) goto loc_824D1938;
loc_824D1934:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_824D1938:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1988
	if (cr0.eq) goto loc_824D1988;
	// rlwinm r11,r8,0,10,12
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x380000;
	// lis r8,16
	ctx.r8.s64 = 1048576;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x824d1988
	if (!cr6.eq) goto loc_824D1988;
	// lwz r11,40(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r10,r10,0,18,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F80;
	// subf r10,r3,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r3.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm. r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d1984
	if (cr0.eq) goto loc_824D1984;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,27,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x3;
	// slw r11,r29,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 << (r11.u8 & 0x3F));
	// or r9,r11,r9
	ctx.r9.u64 = r11.u64 | ctx.r9.u64;
	// b 0x824d1988
	goto loc_824D1988;
loc_824D1984:
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
loc_824D1988:
	// lwz r7,8(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// b 0x824d18d8
	goto loc_824D18D8;
loc_824D1990:
	// clrldi r11,r9,32
	r11.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lis r10,-28311
	ctx.r10.s64 = -1855389696;
	// lis r9,0
	ctx.r9.s64 = 0;
	// ori r10,r10,5192
	ctx.r10.u64 = ctx.r10.u64 | 5192;
	// ori r9,r9,36262
	ctx.r9.u64 = ctx.r9.u64 | 36262;
	// rldimi r10,r9,32,0
	ctx.r10.u64 = (__builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r10.u64 & 0xFFFFFFFF);
	// li r9,4
	ctx.r9.s64 = 4;
	// srd r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// srd r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// subfc r11,r9,r11
	xer.ca = r11.u32 >= ctx.r9.u32;
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// clrlwi r3,r11,31
	ctx.r3.u64 = r11.u32 & 0x1;
	// b 0x824d19d4
	goto loc_824D19D4;
loc_824D19D0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_824D19D4:
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824D19D8"))) PPC_WEAK_FUNC(sub_824D19D8);
PPC_FUNC_IMPL(__imp__sub_824D19D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r8,0
	ctx.r8.s64 = 0;
	// rlwinm. r7,r11,13,29,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0x7;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x824d1a38
	if (cr0.eq) goto loc_824D1A38;
	// addi r10,r3,40
	ctx.r10.s64 = ctx.r3.s64 + 40;
loc_824D19EC:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r11,12(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,124
	cr6.compare<uint32_t>(r11.u32, 124, xer);
	// beq cr6,0x824d1a10
	if (cr6.eq) goto loc_824D1A10;
	// cmplwi cr6,r11,123
	cr6.compare<uint32_t>(r11.u32, 123, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824d1a14
	if (!cr6.eq) goto loc_824D1A14;
loc_824D1A10:
	// li r11,1
	r11.s64 = 1;
loc_824D1A14:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1a28
	if (cr0.eq) goto loc_824D1A28;
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm. r11,r11,0,28,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1a40
	if (!cr0.eq) goto loc_824D1A40;
loc_824D1A28:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// blt cr6,0x824d19ec
	if (cr6.lt) goto loc_824D19EC;
loc_824D1A38:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_824D1A40:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824D1A48"))) PPC_WEAK_FUNC(sub_824D1A48);
PPC_FUNC_IMPL(__imp__sub_824D1A48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x824b37b8
	sub_824B37B8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1a9c
	if (!cr0.eq) goto loc_824D1A9C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r10,r11,25,25,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r10,82
	cr6.compare<uint32_t>(ctx.r10.u32, 82, xer);
	// blt cr6,0x824d1a84
	if (cr6.lt) goto loc_824D1A84;
	// cmplwi cr6,r10,94
	cr6.compare<uint32_t>(ctx.r10.u32, 94, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824d1a88
	if (!cr6.gt) goto loc_824D1A88;
loc_824D1A84:
	// li r11,0
	r11.s64 = 0;
loc_824D1A88:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1a9c
	if (!cr0.eq) goto loc_824D1A9C;
	// cmplwi cr6,r10,116
	cr6.compare<uint32_t>(ctx.r10.u32, 116, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824d1aa0
	if (!cr6.eq) goto loc_824D1AA0;
loc_824D1A9C:
	// li r11,1
	r11.s64 = 1;
loc_824D1AA0:
	// clrlwi r3,r11,24
	ctx.r3.u64 = r11.u32 & 0xFF;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824D1AB8"))) PPC_WEAK_FUNC(sub_824D1AB8);
PPC_FUNC_IMPL(__imp__sub_824D1AB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r8,0
	ctx.r8.s64 = 0;
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,95
	cr6.compare<uint32_t>(r11.u32, 95, xer);
	// blt cr6,0x824d1ad8
	if (cr6.lt) goto loc_824D1AD8;
	// cmplwi cr6,r11,101
	cr6.compare<uint32_t>(r11.u32, 101, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824d1adc
	if (!cr6.gt) goto loc_824D1ADC;
loc_824D1AD8:
	// li r11,0
	r11.s64 = 0;
loc_824D1ADC:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1af8
	if (cr0.eq) goto loc_824D1AF8;
	// li r8,1
	ctx.r8.s64 = 1;
loc_824D1AE8:
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// rlwimi r11,r8,13,6,18
	r11.u64 = (__builtin_rotateleft32(ctx.r8.u32, 13) & 0x3FFE000) | (r11.u64 & 0xFFFFFFFFFC001FFF);
	// stw r11,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r11.u32);
	// blr 
	return;
loc_824D1AF8:
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
loc_824D1AFC:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824d1ae8
	if (cr6.eq) goto loc_824D1AE8;
	// lwz r10,12(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,95
	cr6.compare<uint32_t>(r11.u32, 95, xer);
	// blt cr6,0x824d1b24
	if (cr6.lt) goto loc_824D1B24;
	// cmplwi cr6,r11,101
	cr6.compare<uint32_t>(r11.u32, 101, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824d1b28
	if (!cr6.gt) goto loc_824D1B28;
loc_824D1B24:
	// li r11,0
	r11.s64 = 0;
loc_824D1B28:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1b38
	if (cr0.eq) goto loc_824D1B38;
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x824d1b60
	goto loc_824D1B60;
loc_824D1B38:
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r10,r10,19,19,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 19) & 0x1FFF;
	// rlwinm r11,r11,4,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0x3;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// beq cr6,0x824d1b5c
	if (cr6.eq) goto loc_824D1B5C;
	// cmplwi cr6,r10,8191
	cr6.compare<uint32_t>(ctx.r10.u32, 8191, xer);
	// ble cr6,0x824d1b60
	if (!cr6.gt) goto loc_824D1B60;
loc_824D1B5C:
	// li r10,8191
	ctx.r10.s64 = 8191;
loc_824D1B60:
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// ble cr6,0x824d1b6c
	if (!cr6.gt) goto loc_824D1B6C;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_824D1B6C:
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x824d1afc
	goto loc_824D1AFC;
}

__attribute__((alias("__imp__sub_824D1B74"))) PPC_WEAK_FUNC(sub_824D1B74);
PPC_FUNC_IMPL(__imp__sub_824D1B74) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824D1B78"))) PPC_WEAK_FUNC(sub_824D1B78);
PPC_FUNC_IMPL(__imp__sub_824D1B78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
loc_824D1B9C:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r30,r9
	cr6.compare<uint32_t>(r30.u32, ctx.r9.u32, xer);
	// bne cr6,0x824d1bb0
	if (!cr6.eq) goto loc_824D1BB0;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_824D1BB0:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// blt cr6,0x824d1b9c
	if (cr6.lt) goto loc_824D1B9C;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b1b78
	sub_824B1B78(ctx, base);
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824b3a78
	sub_824B3A78(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824D1BF4"))) PPC_WEAK_FUNC(sub_824D1BF4);
PPC_FUNC_IMPL(__imp__sub_824D1BF4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824D1BF8"))) PPC_WEAK_FUNC(sub_824D1BF8);
PPC_FUNC_IMPL(__imp__sub_824D1BF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r28,r30
	r28.u64 = r30.u64;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// b 0x824d1c2c
	goto loc_824D1C2C;
loc_824D1C14:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824a90a0
	sub_824A90A0(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1c38
	if (!cr0.eq) goto loc_824D1C38;
	// addi r28,r31,28
	r28.s64 = r31.s64 + 28;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
loc_824D1C2C:
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x824d1c14
	if (!cr0.eq) goto loc_824D1C14;
	// b 0x824d1c70
	goto loc_824D1C70;
loc_824D1C38:
	// addi r29,r30,4
	r29.s64 = r30.s64 + 4;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b1d38
	sub_824B1D38(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1c5c
	if (!cr0.eq) goto loc_824D1C5C;
	// li r4,4800
	ctx.r4.s64 = 4800;
	// lwz r3,52(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 52);
	// bl 0x82496e98
	sub_82496E98(ctx, base);
loc_824D1C5C:
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// bl 0x824d1b78
	sub_824D1B78(ctx, base);
loc_824D1C70:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824D1C7C"))) PPC_WEAK_FUNC(sub_824D1C7C);
PPC_FUNC_IMPL(__imp__sub_824D1C7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824D1C80"))) PPC_WEAK_FUNC(sub_824D1C80);
PPC_FUNC_IMPL(__imp__sub_824D1C80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// addi r11,r4,4
	r11.s64 = ctx.r4.s64 + 4;
	// li r8,0
	ctx.r8.s64 = 0;
loc_824D1C88:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x824d1d08
	if (cr0.eq) goto loc_824D1D08;
	// lwz r11,16(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// subf r10,r8,r11
	ctx.r10.s64 = r11.s64 - ctx.r8.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824d1d00
	if (cr0.eq) goto loc_824D1D00;
	// lbz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 12);
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d1d00
	if (!cr0.eq) goto loc_824D1D00;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x824d1cf4
	if (cr0.eq) goto loc_824D1CF4;
	// lwz r7,24(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// bne cr6,0x824d1cec
	if (!cr6.eq) goto loc_824D1CEC;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,0,18,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F80;
	// addi r11,r11,-14208
	r11.s64 = r11.s64 + -14208;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1cf4
	if (cr0.eq) goto loc_824D1CF4;
loc_824D1CEC:
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x824d1cf8
	goto loc_824D1CF8;
loc_824D1CF4:
	// li r11,1
	r11.s64 = 1;
loc_824D1CF8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1d08
	if (!cr0.eq) goto loc_824D1D08;
loc_824D1D00:
	// addi r11,r9,8
	r11.s64 = ctx.r9.s64 + 8;
	// b 0x824d1c88
	goto loc_824D1C88;
loc_824D1D08:
	// subf r11,r8,r9
	r11.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// clrlwi r3,r11,24
	ctx.r3.u64 = r11.u32 & 0xFF;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824D1D1C"))) PPC_WEAK_FUNC(sub_824D1D1C);
PPC_FUNC_IMPL(__imp__sub_824D1D1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824D1D20"))) PPC_WEAK_FUNC(sub_824D1D20);
PPC_FUNC_IMPL(__imp__sub_824D1D20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r9,8(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r10,r11,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// rlwinm r11,r9,27,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x824d1d54
	if (cr6.eq) goto loc_824D1D54;
loc_824D1D4C:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x824d2208
	goto loc_824D2208;
loc_824D1D54:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824d1dd8
	if (cr6.eq) goto loc_824D1DD8;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
loc_824D1D60:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824d1d8c
	if (cr6.eq) goto loc_824D1D8C;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,4,6
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x824d1d8c
	if (!cr0.eq) goto loc_824D1D8C;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824d1d60
	goto loc_824D1D60;
loc_824D1D8C:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// rlwinm r10,r10,19,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 19) & 0xFFF;
loc_824D1D98:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824d1dc4
	if (cr6.eq) goto loc_824D1DC4;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r8,0,4,6
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xE000000;
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 ^ 1;
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne 0x824d1dc4
	if (!cr0.eq) goto loc_824D1DC4;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x824d1d98
	goto loc_824D1D98;
loc_824D1DC4:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// rlwinm r11,r11,19,20,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 19) & 0xFF0;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x824d1d4c
	if (!cr6.eq) goto loc_824D1D4C;
loc_824D1DD8:
	// rlwinm. r11,r9,15,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r30,0
	r30.s64 = 0;
	// beq 0x824d1df0
	if (cr0.eq) goto loc_824D1DF0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// lwz r30,12(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
loc_824D1DF0:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// li r31,0
	r31.s64 = 0;
	// rlwinm. r11,r11,15,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1e0c
	if (cr0.eq) goto loc_824D1E0C;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824b07d8
	sub_824B07D8(ctx, base);
	// lwz r31,12(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
loc_824D1E0C:
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// bne cr6,0x824d1d4c
	if (!cr6.eq) goto loc_824D1D4C;
	// lwz r11,24(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// lwz r11,76(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// rlwinm. r11,r11,10,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1e78
	if (cr0.eq) goto loc_824D1E78;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x824d1e40
	if (!cr6.eq) goto loc_824D1E40;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne 0x824d1e44
	if (!cr0.eq) goto loc_824D1E44;
loc_824D1E40:
	// li r11,0
	r11.s64 = 0;
loc_824D1E44:
	// clrlwi r30,r11,24
	r30.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x824d1e64
	if (!cr6.eq) goto loc_824D1E64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824a9130
	sub_824A9130(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne 0x824d1e68
	if (!cr0.eq) goto loc_824D1E68;
loc_824D1E64:
	// li r11,0
	r11.s64 = 0;
loc_824D1E68:
	// clrlwi. r10,r30,24
	ctx.r10.u64 = r30.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d1d4c
	if (!cr0.eq) goto loc_824D1D4C;
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1d4c
	if (!cr0.eq) goto loc_824D1D4C;
loc_824D1E78:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824a90a0
	sub_824A90A0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824a90a0
	sub_824A90A0(ctx, base);
	// clrlwi. r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1e9c
	if (!cr0.eq) goto loc_824D1E9C;
	// clrlwi. r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d1f24
	if (cr0.eq) goto loc_824D1F24;
loc_824D1E9C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824d1eac
	if (cr6.eq) goto loc_824D1EAC;
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1d4c
	if (!cr0.eq) goto loc_824D1D4C;
loc_824D1EAC:
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,25
	cr6.compare<uint32_t>(r11.u32, 25, xer);
	// blt cr6,0x824d1ec4
	if (cr6.lt) goto loc_824D1EC4;
	// cmplwi cr6,r11,28
	cr6.compare<uint32_t>(r11.u32, 28, xer);
	// ble cr6,0x824d1ed4
	if (!cr6.gt) goto loc_824D1ED4;
loc_824D1EC4:
	// cmplwi cr6,r11,66
	cr6.compare<uint32_t>(r11.u32, 66, xer);
	// blt cr6,0x824d1edc
	if (cr6.lt) goto loc_824D1EDC;
	// cmplwi cr6,r11,70
	cr6.compare<uint32_t>(r11.u32, 70, xer);
	// bgt cr6,0x824d1edc
	if (cr6.gt) goto loc_824D1EDC;
loc_824D1ED4:
	// li r11,1
	r11.s64 = 1;
	// b 0x824d1ee0
	goto loc_824D1EE0;
loc_824D1EDC:
	// li r11,0
	r11.s64 = 0;
loc_824D1EE0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1d4c
	if (!cr0.eq) goto loc_824D1D4C;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,25
	cr6.compare<uint32_t>(r11.u32, 25, xer);
	// blt cr6,0x824d1f00
	if (cr6.lt) goto loc_824D1F00;
	// cmplwi cr6,r11,28
	cr6.compare<uint32_t>(r11.u32, 28, xer);
	// ble cr6,0x824d1f10
	if (!cr6.gt) goto loc_824D1F10;
loc_824D1F00:
	// cmplwi cr6,r11,66
	cr6.compare<uint32_t>(r11.u32, 66, xer);
	// blt cr6,0x824d1f18
	if (cr6.lt) goto loc_824D1F18;
	// cmplwi cr6,r11,70
	cr6.compare<uint32_t>(r11.u32, 70, xer);
	// bgt cr6,0x824d1f18
	if (cr6.gt) goto loc_824D1F18;
loc_824D1F10:
	// li r11,1
	r11.s64 = 1;
	// b 0x824d1f1c
	goto loc_824D1F1C;
loc_824D1F18:
	// li r11,0
	r11.s64 = 0;
loc_824D1F1C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1d4c
	if (!cr0.eq) goto loc_824D1D4C;
loc_824D1F24:
	// lwz r4,8(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm. r11,r4,0,25,25
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1f38
	if (!cr0.eq) goto loc_824D1F38;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x824d1f60
	goto loc_824D1F60;
loc_824D1F38:
	// rlwinm r11,r4,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// beq cr6,0x824d1f58
	if (cr6.eq) goto loc_824D1F58;
	// cmplwi cr6,r11,54
	cr6.compare<uint32_t>(r11.u32, 54, xer);
	// beq cr6,0x824d1f58
	if (cr6.eq) goto loc_824D1F58;
	// cmplwi cr6,r11,55
	cr6.compare<uint32_t>(r11.u32, 55, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824d1f5c
	if (!cr6.eq) goto loc_824D1F5C;
loc_824D1F58:
	// li r11,1
	r11.s64 = 1;
loc_824D1F5C:
	// clrlwi r10,r11,24
	ctx.r10.u64 = r11.u32 & 0xFF;
loc_824D1F60:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm. r9,r11,0,25,25
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824d1f74
	if (!cr0.eq) goto loc_824D1F74;
	// li r11,0
	r11.s64 = 0;
	// b 0x824d1f9c
	goto loc_824D1F9C;
loc_824D1F74:
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// beq cr6,0x824d1f94
	if (cr6.eq) goto loc_824D1F94;
	// cmplwi cr6,r11,54
	cr6.compare<uint32_t>(r11.u32, 54, xer);
	// beq cr6,0x824d1f94
	if (cr6.eq) goto loc_824D1F94;
	// cmplwi cr6,r11,55
	cr6.compare<uint32_t>(r11.u32, 55, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824d1f98
	if (!cr6.eq) goto loc_824D1F98;
loc_824D1F94:
	// li r11,1
	r11.s64 = 1;
loc_824D1F98:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
loc_824D1F9C:
	// clrlwi. r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d1fac
	if (cr0.eq) goto loc_824D1FAC;
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1d4c
	if (!cr0.eq) goto loc_824D1D4C;
loc_824D1FAC:
	// rlwinm r11,r4,25,25,31
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824d1fc4
	if (cr6.lt) goto loc_824D1FC4;
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824d1fc8
	if (!cr6.gt) goto loc_824D1FC8;
loc_824D1FC4:
	// li r11,0
	r11.s64 = 0;
loc_824D1FC8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d1fdc
	if (cr0.eq) goto loc_824D1FDC;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mr r11,r28
	r11.u64 = r28.u64;
	// b 0x824d1fe4
	goto loc_824D1FE4;
loc_824D1FDC:
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_824D1FE4:
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r9,r9,13,29,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 13) & 0x7;
	// cmplwi cr6,r9,3
	cr6.compare<uint32_t>(ctx.r9.u32, 3, xer);
	// bne cr6,0x824d2008
	if (!cr6.eq) goto loc_824D2008;
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lis r7,8
	ctx.r7.s64 = 524288;
	// rlwinm r8,r8,0,10,12
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x380000;
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bgt cr6,0x824d1d4c
	if (cr6.gt) goto loc_824D1D4C;
loc_824D2008:
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm. r8,r8,0,10,12
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x380000;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x824d2174
	if (cr0.eq) goto loc_824D2174;
	// lwz r5,40(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmplwi cr6,r9,3
	cr6.compare<uint32_t>(ctx.r9.u32, 3, xer);
	// lwz r11,12(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// bne cr6,0x824d203c
	if (!cr6.eq) goto loc_824D203C;
	// lwz r11,48(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// xor r11,r10,r11
	r11.u64 = ctx.r10.u64 ^ r11.u64;
	// rlwinm. r11,r11,0,27,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1E;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// b 0x824d1d4c
	goto loc_824D1D4C;
loc_824D203C:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,124
	cr6.compare<uint32_t>(r11.u32, 124, xer);
	// beq cr6,0x824d2058
	if (cr6.eq) goto loc_824D2058;
	// cmplwi cr6,r11,123
	cr6.compare<uint32_t>(r11.u32, 123, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824d205c
	if (!cr6.eq) goto loc_824D205C;
loc_824D2058:
	// li r11,1
	r11.s64 = 1;
loc_824D205C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d2174
	if (cr0.eq) goto loc_824D2174;
	// lwz r6,40(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// ble cr6,0x824d2078
	if (!cr6.gt) goto loc_824D2078;
	// lwz r7,44(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// b 0x824d207c
	goto loc_824D207C;
loc_824D2078:
	// li r7,0
	ctx.r7.s64 = 0;
loc_824D207C:
	// lwz r11,12(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	// cmplwi r7,0
	cr0.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq 0x824d2090
	if (cr0.eq) goto loc_824D2090;
	// lwz r10,12(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// b 0x824d2094
	goto loc_824D2094;
loc_824D2090:
	// li r10,0
	ctx.r10.s64 = 0;
loc_824D2094:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,124
	cr6.compare<uint32_t>(r11.u32, 124, xer);
	// beq cr6,0x824d20b0
	if (cr6.eq) goto loc_824D20B0;
	// cmplwi cr6,r11,123
	cr6.compare<uint32_t>(r11.u32, 123, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824d20b4
	if (!cr6.eq) goto loc_824D20B4;
loc_824D20B0:
	// li r11,1
	r11.s64 = 1;
loc_824D20B4:
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x824d20e8
	if (cr6.eq) goto loc_824D20E8;
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,124
	cr6.compare<uint32_t>(r11.u32, 124, xer);
	// beq cr6,0x824d20dc
	if (cr6.eq) goto loc_824D20DC;
	// cmplwi cr6,r11,123
	cr6.compare<uint32_t>(r11.u32, 123, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x824d20e0
	if (!cr6.eq) goto loc_824D20E0;
loc_824D20DC:
	// li r11,1
	r11.s64 = 1;
loc_824D20E0:
	// clrlwi r8,r11,24
	ctx.r8.u64 = r11.u32 & 0xFF;
	// b 0x824d20ec
	goto loc_824D20EC;
loc_824D20E8:
	// li r8,0
	ctx.r8.s64 = 0;
loc_824D20EC:
	// clrlwi. r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824d20fc
	if (!cr0.eq) goto loc_824D20FC;
	// clrlwi. r11,r8,24
	r11.u64 = ctx.r8.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d2174
	if (cr0.eq) goto loc_824D2174;
loc_824D20FC:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824d2110
	if (cr6.eq) goto loc_824D2110;
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm. r11,r11,31,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d2124
	if (!cr0.eq) goto loc_824D2124;
loc_824D2110:
	// clrlwi. r11,r8,24
	r11.u64 = ctx.r8.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d212c
	if (cr0.eq) goto loc_824D212C;
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// rlwinm. r11,r11,31,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d212c
	if (cr0.eq) goto loc_824D212C;
loc_824D2124:
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x824d2130
	goto loc_824D2130;
loc_824D212C:
	// li r10,0
	ctx.r10.s64 = 0;
loc_824D2130:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// rlwinm r5,r11,31,31,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x1;
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// bne cr6,0x824d1d4c
	if (!cr6.eq) goto loc_824D1D4C;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824d215c
	if (cr6.eq) goto loc_824D215C;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// xor r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 ^ r11.u64;
	// rlwinm. r10,r10,0,27,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x18;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d1d4c
	if (!cr0.eq) goto loc_824D1D4C;
loc_824D215C:
	// clrlwi. r10,r8,24
	ctx.r10.u64 = ctx.r8.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d2174
	if (cr0.eq) goto loc_824D2174;
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// xor r11,r10,r11
	r11.u64 = ctx.r10.u64 ^ r11.u64;
	// rlwinm. r11,r11,0,27,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x18;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d1d4c
	if (!cr0.eq) goto loc_824D1D4C;
loc_824D2174:
	// li r11,0
	r11.s64 = 0;
	// rlwinm r8,r4,13,29,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 13) & 0x7;
	// addi r10,r29,40
	ctx.r10.s64 = r29.s64 + 40;
loc_824D2180:
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x824d2204
	if (!cr6.lt) goto loc_824D2204;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm. r9,r9,0,28,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824d21a4
	if (!cr0.eq) goto loc_824D21A4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824d2180
	goto loc_824D2180;
loc_824D21A4:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824ba1b0
	sub_824BA1B0(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x824d2204
	if (cr0.eq) goto loc_824D2204;
	// lwz r10,8(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// li r11,0
	r11.s64 = 0;
	// rlwinm r8,r10,13,29,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 13) & 0x7;
	// addi r10,r28,40
	ctx.r10.s64 = r28.s64 + 40;
loc_824D21C4:
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x824d2204
	if (!cr6.lt) goto loc_824D2204;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm. r9,r9,0,28,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824d21e8
	if (!cr0.eq) goto loc_824D21E8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x824d21c4
	goto loc_824D21C4;
loc_824D21E8:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824ba1b0
	sub_824BA1B0(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x824d2204
	if (cr0.eq) goto loc_824D2204;
	// cmplw cr6,r3,r31
	cr6.compare<uint32_t>(ctx.r3.u32, r31.u32, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// bne cr6,0x824d2208
	if (!cr6.eq) goto loc_824D2208;
loc_824D2204:
	// li r3,1
	ctx.r3.s64 = 1;
loc_824D2208:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824D2210"))) PPC_WEAK_FUNC(sub_824D2210);
PPC_FUNC_IMPL(__imp__sub_824D2210) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// li r6,1
	ctx.r6.s64 = 1;
	// bl 0x824d19d8
	sub_824D19D8(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d22f4
	if (cr0.eq) goto loc_824D22F4;
	// lwz r9,4(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
loc_824D223C:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824d22a0
	if (cr6.eq) goto loc_824D22A0;
	// lwz r11,16(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// addi r11,r11,0
	r11.s64 = r11.s64 + 0;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d2294
	if (cr0.eq) goto loc_824D2294;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm. r11,r10,0,1,1
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d2278
	if (cr0.eq) goto loc_824D2278;
	// rlwinm. r11,r10,0,4,6
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// beq 0x824d227c
	if (cr0.eq) goto loc_824D227C;
loc_824D2278:
	// li r11,0
	r11.s64 = 0;
loc_824D227C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d2294
	if (cr0.eq) goto loc_824D2294;
	// rlwinm. r11,r10,19,20,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 19) & 0xFFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x824d2294
	if (cr0.lt) goto loc_824D2294;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// ble cr6,0x824d229c
	if (!cr6.gt) goto loc_824D229C;
loc_824D2294:
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// b 0x824d223c
	goto loc_824D223C;
loc_824D229C:
	// li r6,0
	ctx.r6.s64 = 0;
loc_824D22A0:
	// clrlwi. r11,r6,24
	r11.u64 = ctx.r6.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d22f4
	if (cr0.eq) goto loc_824D22F4;
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
loc_824D22AC:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x824d22f4
	if (cr6.eq) goto loc_824D22F4;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm. r11,r10,0,1,1
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d22cc
	if (cr0.eq) goto loc_824D22CC;
	// rlwinm. r11,r10,0,4,6
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// beq 0x824d22d0
	if (cr0.eq) goto loc_824D22D0;
loc_824D22CC:
	// li r11,0
	r11.s64 = 0;
loc_824D22D0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d22e8
	if (cr0.eq) goto loc_824D22E8;
	// rlwinm. r11,r10,19,20,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 19) & 0xFFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x824d22e8
	if (cr0.lt) goto loc_824D22E8;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// ble cr6,0x824d22f0
	if (!cr6.gt) goto loc_824D22F0;
loc_824D22E8:
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x824d22ac
	goto loc_824D22AC;
loc_824D22F0:
	// li r6,0
	ctx.r6.s64 = 0;
loc_824D22F4:
	// clrlwi. r11,r6,24
	r11.u64 = ctx.r6.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d2308
	if (cr0.eq) goto loc_824D2308;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x824d1d20
	sub_824D1D20(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
loc_824D2308:
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824D2320"))) PPC_WEAK_FUNC(sub_824D2320);
PPC_FUNC_IMPL(__imp__sub_824D2320) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// lwz r28,0(r29)
	r28.u64 = PPC_LOAD_U32(r29.u32 + 0);
loc_824D233C:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x824d2414
	if (cr6.eq) goto loc_824D2414;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// rlwinm. r10,r11,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d235c
	if (cr0.eq) goto loc_824D235C;
	// rlwinm. r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// beq 0x824d2360
	if (cr0.eq) goto loc_824D2360;
loc_824D235C:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_824D2360:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d240c
	if (cr0.eq) goto loc_824D240C;
	// lwz r30,12(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
loc_824D2370:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824d23e4
	if (cr6.eq) goto loc_824D23E4;
	// lwz r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r11,r26,r4
	r11.s64 = ctx.r4.s64 - r26.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d23dc
	if (cr0.eq) goto loc_824D23DC;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm. r10,r11,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d23ac
	if (cr0.eq) goto loc_824D23AC;
	// rlwinm. r11,r11,0,4,6
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// beq 0x824d23b0
	if (cr0.eq) goto loc_824D23B0;
loc_824D23AC:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_824D23B0:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d23dc
	if (cr0.eq) goto loc_824D23DC;
	// cmplw cr6,r4,r29
	cr6.compare<uint32_t>(ctx.r4.u32, r29.u32, xer);
	// beq cr6,0x824d23dc
	if (cr6.eq) goto loc_824D23DC;
	// lbz r11,12(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 12);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d23dc
	if (cr0.eq) goto loc_824D23DC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x824b4400
	sub_824B4400(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d240c
	if (!cr0.eq) goto loc_824D240C;
loc_824D23DC:
	// lwz r31,8(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// b 0x824d2370
	goto loc_824D2370;
loc_824D23E4:
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// addi r11,r27,4
	r11.s64 = r27.s64 + 4;
loc_824D23EC:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r30,r9
	cr6.compare<uint32_t>(r30.u32, ctx.r9.u32, xer);
	// bne cr6,0x824d23fc
	if (!cr6.eq) goto loc_824D23FC;
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
loc_824D23FC:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// blt cr6,0x824d23ec
	if (cr6.lt) goto loc_824D23EC;
loc_824D240C:
	// lwz r28,4(r28)
	r28.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// b 0x824d233c
	goto loc_824D233C;
loc_824D2414:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x824b1b78
	sub_824B1B78(ctx, base);
	// lwz r31,4(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 4);
loc_824D2424:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x824d2460
	if (cr6.eq) goto loc_824D2460;
	// lwz r5,16(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r11,r26,r5
	r11.s64 = ctx.r5.s64 - r26.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x824d2458
	if (cr0.eq) goto loc_824D2458;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x824b1fd0
	sub_824B1FD0(ctx, base);
loc_824D2458:
	// lwz r31,8(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// b 0x824d2424
	goto loc_824D2424;
loc_824D2460:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x824b3a78
	sub_824B3A78(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_824D2478"))) PPC_WEAK_FUNC(sub_824D2478);
PPC_FUNC_IMPL(__imp__sub_824D2478) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r9,20(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// clrlwi. r11,r9,31
	r11.u64 = ctx.r9.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bnelr 
	if (!cr0.eq) return;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beqlr 
	if (cr0.eq) return;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r8,r11,-25468
	ctx.r8.s64 = r11.s64 + -25468;
loc_824D2494:
	// addi r10,r9,4
	ctx.r10.s64 = ctx.r9.s64 + 4;
	// b 0x824d24a8
	goto loc_824D24A8;
loc_824D249C:
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// beq cr6,0x824d24b4
	if (cr6.eq) goto loc_824D24B4;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
loc_824D24A8:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x824d249c
	if (!cr0.eq) goto loc_824D249C;
loc_824D24B4:
	// li r11,0
	r11.s64 = 0;
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,36(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bnelr 
	if (!cr0.eq) return;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x824d2494
	if (!cr6.eq) goto loc_824D2494;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824D24DC"))) PPC_WEAK_FUNC(sub_824D24DC);
PPC_FUNC_IMPL(__imp__sub_824D24DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824D24E0"))) PPC_WEAK_FUNC(sub_824D24E0);
PPC_FUNC_IMPL(__imp__sub_824D24E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// addi r28,r29,4
	r28.s64 = r29.s64 + 4;
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// li r5,48
	ctx.r5.s64 = 48;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r11,r31
	r11.u64 = r31.u64;
loc_824D2510:
	// rlwinm r10,r11,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// clrlwi. r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824d2548
	if (!cr0.eq) goto loc_824D2548;
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// addic. r10,r10,-36
	xer.ca = ctx.r10.u32 > 35;
	ctx.r10.s64 = ctx.r10.s64 + -36;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d2548
	if (cr0.eq) goto loc_824D2548;
	// lwz r9,16(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// clrlwi. r9,r9,19
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFF;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x824d2548
	if (!cr0.eq) goto loc_824D2548;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// b 0x824d2510
	goto loc_824D2510;
loc_824D2548:
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// clrlwi. r10,r31,31
	ctx.r10.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d2584
	if (cr0.eq) goto loc_824D2584;
	// li r31,0
	r31.s64 = 0;
	// b 0x824d2584
	goto loc_824D2584;
loc_824D2560:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x824d2320
	sub_824D2320(ctx, base);
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d2580
	if (cr0.eq) goto loc_824D2580;
	// li r31,0
	r31.s64 = 0;
loc_824D2580:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
loc_824D2584:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x824d2560
	if (!cr6.eq) goto loc_824D2560;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_824D2594"))) PPC_WEAK_FUNC(sub_824D2594);
PPC_FUNC_IMPL(__imp__sub_824D2594) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_824D2598"))) PPC_WEAK_FUNC(sub_824D2598);
PPC_FUNC_IMPL(__imp__sub_824D2598) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824d25b8
	if (cr6.lt) goto loc_824D25B8;
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// ble cr6,0x824d25bc
	if (!cr6.gt) goto loc_824D25BC;
loc_824D25B8:
	// li r10,0
	ctx.r10.s64 = 0;
loc_824D25BC:
	// clrlwi. r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d2658
	if (cr0.eq) goto loc_824D2658;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824d25d8
	if (cr6.lt) goto loc_824D25D8;
	// cmplwi cr6,r11,81
	cr6.compare<uint32_t>(r11.u32, 81, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824d25dc
	if (!cr6.gt) goto loc_824D25DC;
loc_824D25D8:
	// li r11,0
	r11.s64 = 0;
loc_824D25DC:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d2658
	if (cr0.eq) goto loc_824D2658;
	// rlwinm r11,r9,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d264c
	if (!cr0.eq) goto loc_824D264C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824d264c
	if (cr6.eq) goto loc_824D264C;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r9,16(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// clrlwi. r10,r10,19
	ctx.r10.u64 = ctx.r10.u32 & 0x1FFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d264c
	if (!cr0.eq) goto loc_824D264C;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,31
	cr6.compare<uint32_t>(r11.u32, 31, xer);
	// blt cr6,0x824d263c
	if (cr6.lt) goto loc_824D263C;
	// cmplwi cr6,r11,81
	cr6.compare<uint32_t>(r11.u32, 81, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824d2640
	if (!cr6.gt) goto loc_824D2640;
loc_824D263C:
	// li r11,0
	r11.s64 = 0;
loc_824D2640:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// bne 0x824d2650
	if (!cr0.eq) goto loc_824D2650;
loc_824D264C:
	// li r11,1
	r11.s64 = 1;
loc_824D2650:
	// clrlwi r3,r11,24
	ctx.r3.u64 = r11.u32 & 0xFF;
	// blr 
	return;
loc_824D2658:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_824D2660"))) PPC_WEAK_FUNC(sub_824D2660);
PPC_FUNC_IMPL(__imp__sub_824D2660) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824d2694
	if (cr6.lt) goto loc_824D2694;
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// ble cr6,0x824d2698
	if (!cr6.gt) goto loc_824D2698;
loc_824D2694:
	// li r10,0
	ctx.r10.s64 = 0;
loc_824D2698:
	// clrlwi. r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x824d2788
	if (cr0.eq) goto loc_824D2788;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x824d26b4
	if (cr6.lt) goto loc_824D26B4;
	// cmplwi cr6,r11,81
	cr6.compare<uint32_t>(r11.u32, 81, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824d26b8
	if (!cr6.gt) goto loc_824D26B8;
loc_824D26B4:
	// li r11,0
	r11.s64 = 0;
loc_824D26B8:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d2788
	if (cr0.eq) goto loc_824D2788;
	// rlwinm r11,r31,0,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d2724
	if (!cr0.eq) goto loc_824D2724;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x824d2724
	if (cr6.eq) goto loc_824D2724;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// clrlwi. r10,r10,19
	ctx.r10.u64 = ctx.r10.u32 & 0x1FFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x824d2724
	if (!cr0.eq) goto loc_824D2724;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r11,r11,25,25,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x7F;
	// cmplwi cr6,r11,31
	cr6.compare<uint32_t>(r11.u32, 31, xer);
	// blt cr6,0x824d2718
	if (cr6.lt) goto loc_824D2718;
	// cmplwi cr6,r11,81
	cr6.compare<uint32_t>(r11.u32, 81, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x824d271c
	if (!cr6.gt) goto loc_824D271C;
loc_824D2718:
	// li r11,0
	r11.s64 = 0;
loc_824D271C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x824d2788
	if (!cr0.eq) goto loc_824D2788;
loc_824D2724:
	// addi r4,r30,4
	ctx.r4.s64 = r30.s64 + 4;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// li r5,48
	ctx.r5.s64 = 48;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824d2320
	sub_824D2320(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824b1d38
	sub_824B1D38(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d2788
	if (cr0.eq) goto loc_824D2788;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824b2028
	sub_824B2028(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x824b1d38
	sub_824B1D38(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x824d2788
	if (cr0.eq) goto loc_824D2788;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r3,44(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 44);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x824d1d20
	sub_824D1D20(ctx, base);
	// b 0x824d278c
	goto loc_824D278C;
loc_824D2788:
	// li r3,0
	ctx.r3.s64 = 0;
loc_824D278C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd4c
	return;
}

