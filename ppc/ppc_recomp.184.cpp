#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_826E44F8"))) PPC_WEAK_FUNC(sub_826E44F8);
PPC_FUNC_IMPL(__imp__sub_826E44F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-26044(r10)
	PPC_STORE_U32(ctx.r10.u32 + -26044, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E450C"))) PPC_WEAK_FUNC(sub_826E450C);
PPC_FUNC_IMPL(__imp__sub_826E450C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4510"))) PPC_WEAK_FUNC(sub_826E4510);
PPC_FUNC_IMPL(__imp__sub_826E4510) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-26040(r10)
	PPC_STORE_U32(ctx.r10.u32 + -26040, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4524"))) PPC_WEAK_FUNC(sub_826E4524);
PPC_FUNC_IMPL(__imp__sub_826E4524) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4528"))) PPC_WEAK_FUNC(sub_826E4528);
PPC_FUNC_IMPL(__imp__sub_826E4528) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-26032(r10)
	PPC_STORE_U32(ctx.r10.u32 + -26032, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E453C"))) PPC_WEAK_FUNC(sub_826E453C);
PPC_FUNC_IMPL(__imp__sub_826E453C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4540"))) PPC_WEAK_FUNC(sub_826E4540);
PPC_FUNC_IMPL(__imp__sub_826E4540) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-26028(r10)
	PPC_STORE_U32(ctx.r10.u32 + -26028, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4554"))) PPC_WEAK_FUNC(sub_826E4554);
PPC_FUNC_IMPL(__imp__sub_826E4554) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4558"))) PPC_WEAK_FUNC(sub_826E4558);
PPC_FUNC_IMPL(__imp__sub_826E4558) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-26020(r10)
	PPC_STORE_U32(ctx.r10.u32 + -26020, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E456C"))) PPC_WEAK_FUNC(sub_826E456C);
PPC_FUNC_IMPL(__imp__sub_826E456C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4570"))) PPC_WEAK_FUNC(sub_826E4570);
PPC_FUNC_IMPL(__imp__sub_826E4570) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-26016(r10)
	PPC_STORE_U32(ctx.r10.u32 + -26016, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4584"))) PPC_WEAK_FUNC(sub_826E4584);
PPC_FUNC_IMPL(__imp__sub_826E4584) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4588"))) PPC_WEAK_FUNC(sub_826E4588);
PPC_FUNC_IMPL(__imp__sub_826E4588) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-26008(r10)
	PPC_STORE_U32(ctx.r10.u32 + -26008, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E459C"))) PPC_WEAK_FUNC(sub_826E459C);
PPC_FUNC_IMPL(__imp__sub_826E459C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E45A0"))) PPC_WEAK_FUNC(sub_826E45A0);
PPC_FUNC_IMPL(__imp__sub_826E45A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-26004(r10)
	PPC_STORE_U32(ctx.r10.u32 + -26004, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E45B4"))) PPC_WEAK_FUNC(sub_826E45B4);
PPC_FUNC_IMPL(__imp__sub_826E45B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E45B8"))) PPC_WEAK_FUNC(sub_826E45B8);
PPC_FUNC_IMPL(__imp__sub_826E45B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25952(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25952, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E45CC"))) PPC_WEAK_FUNC(sub_826E45CC);
PPC_FUNC_IMPL(__imp__sub_826E45CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E45D0"))) PPC_WEAK_FUNC(sub_826E45D0);
PPC_FUNC_IMPL(__imp__sub_826E45D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25948(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25948, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E45E4"))) PPC_WEAK_FUNC(sub_826E45E4);
PPC_FUNC_IMPL(__imp__sub_826E45E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E45E8"))) PPC_WEAK_FUNC(sub_826E45E8);
PPC_FUNC_IMPL(__imp__sub_826E45E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25940(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25940, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E45FC"))) PPC_WEAK_FUNC(sub_826E45FC);
PPC_FUNC_IMPL(__imp__sub_826E45FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4600"))) PPC_WEAK_FUNC(sub_826E4600);
PPC_FUNC_IMPL(__imp__sub_826E4600) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25936(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25936, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4614"))) PPC_WEAK_FUNC(sub_826E4614);
PPC_FUNC_IMPL(__imp__sub_826E4614) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4618"))) PPC_WEAK_FUNC(sub_826E4618);
PPC_FUNC_IMPL(__imp__sub_826E4618) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25888(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25888, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E462C"))) PPC_WEAK_FUNC(sub_826E462C);
PPC_FUNC_IMPL(__imp__sub_826E462C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4630"))) PPC_WEAK_FUNC(sub_826E4630);
PPC_FUNC_IMPL(__imp__sub_826E4630) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25884(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25884, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4644"))) PPC_WEAK_FUNC(sub_826E4644);
PPC_FUNC_IMPL(__imp__sub_826E4644) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4648"))) PPC_WEAK_FUNC(sub_826E4648);
PPC_FUNC_IMPL(__imp__sub_826E4648) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25828(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25828, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E465C"))) PPC_WEAK_FUNC(sub_826E465C);
PPC_FUNC_IMPL(__imp__sub_826E465C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4660"))) PPC_WEAK_FUNC(sub_826E4660);
PPC_FUNC_IMPL(__imp__sub_826E4660) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25824(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25824, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4674"))) PPC_WEAK_FUNC(sub_826E4674);
PPC_FUNC_IMPL(__imp__sub_826E4674) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4678"))) PPC_WEAK_FUNC(sub_826E4678);
PPC_FUNC_IMPL(__imp__sub_826E4678) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25760(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25760, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E468C"))) PPC_WEAK_FUNC(sub_826E468C);
PPC_FUNC_IMPL(__imp__sub_826E468C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4690"))) PPC_WEAK_FUNC(sub_826E4690);
PPC_FUNC_IMPL(__imp__sub_826E4690) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25756(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25756, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E46A4"))) PPC_WEAK_FUNC(sub_826E46A4);
PPC_FUNC_IMPL(__imp__sub_826E46A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E46A8"))) PPC_WEAK_FUNC(sub_826E46A8);
PPC_FUNC_IMPL(__imp__sub_826E46A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25700(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25700, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E46BC"))) PPC_WEAK_FUNC(sub_826E46BC);
PPC_FUNC_IMPL(__imp__sub_826E46BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E46C0"))) PPC_WEAK_FUNC(sub_826E46C0);
PPC_FUNC_IMPL(__imp__sub_826E46C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25696(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25696, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E46D4"))) PPC_WEAK_FUNC(sub_826E46D4);
PPC_FUNC_IMPL(__imp__sub_826E46D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E46D8"))) PPC_WEAK_FUNC(sub_826E46D8);
PPC_FUNC_IMPL(__imp__sub_826E46D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25648(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25648, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E46EC"))) PPC_WEAK_FUNC(sub_826E46EC);
PPC_FUNC_IMPL(__imp__sub_826E46EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E46F0"))) PPC_WEAK_FUNC(sub_826E46F0);
PPC_FUNC_IMPL(__imp__sub_826E46F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25644(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25644, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4704"))) PPC_WEAK_FUNC(sub_826E4704);
PPC_FUNC_IMPL(__imp__sub_826E4704) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4708"))) PPC_WEAK_FUNC(sub_826E4708);
PPC_FUNC_IMPL(__imp__sub_826E4708) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25600(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25600, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E471C"))) PPC_WEAK_FUNC(sub_826E471C);
PPC_FUNC_IMPL(__imp__sub_826E471C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4720"))) PPC_WEAK_FUNC(sub_826E4720);
PPC_FUNC_IMPL(__imp__sub_826E4720) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25596(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25596, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4734"))) PPC_WEAK_FUNC(sub_826E4734);
PPC_FUNC_IMPL(__imp__sub_826E4734) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4738"))) PPC_WEAK_FUNC(sub_826E4738);
PPC_FUNC_IMPL(__imp__sub_826E4738) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25548(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25548, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E474C"))) PPC_WEAK_FUNC(sub_826E474C);
PPC_FUNC_IMPL(__imp__sub_826E474C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4750"))) PPC_WEAK_FUNC(sub_826E4750);
PPC_FUNC_IMPL(__imp__sub_826E4750) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25544(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25544, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4764"))) PPC_WEAK_FUNC(sub_826E4764);
PPC_FUNC_IMPL(__imp__sub_826E4764) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4768"))) PPC_WEAK_FUNC(sub_826E4768);
PPC_FUNC_IMPL(__imp__sub_826E4768) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25496(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25496, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E477C"))) PPC_WEAK_FUNC(sub_826E477C);
PPC_FUNC_IMPL(__imp__sub_826E477C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4780"))) PPC_WEAK_FUNC(sub_826E4780);
PPC_FUNC_IMPL(__imp__sub_826E4780) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25492(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25492, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4794"))) PPC_WEAK_FUNC(sub_826E4794);
PPC_FUNC_IMPL(__imp__sub_826E4794) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4798"))) PPC_WEAK_FUNC(sub_826E4798);
PPC_FUNC_IMPL(__imp__sub_826E4798) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25444(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25444, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E47AC"))) PPC_WEAK_FUNC(sub_826E47AC);
PPC_FUNC_IMPL(__imp__sub_826E47AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E47B0"))) PPC_WEAK_FUNC(sub_826E47B0);
PPC_FUNC_IMPL(__imp__sub_826E47B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25440(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25440, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E47C4"))) PPC_WEAK_FUNC(sub_826E47C4);
PPC_FUNC_IMPL(__imp__sub_826E47C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E47C8"))) PPC_WEAK_FUNC(sub_826E47C8);
PPC_FUNC_IMPL(__imp__sub_826E47C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25388(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25388, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E47DC"))) PPC_WEAK_FUNC(sub_826E47DC);
PPC_FUNC_IMPL(__imp__sub_826E47DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E47E0"))) PPC_WEAK_FUNC(sub_826E47E0);
PPC_FUNC_IMPL(__imp__sub_826E47E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25384(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25384, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E47F4"))) PPC_WEAK_FUNC(sub_826E47F4);
PPC_FUNC_IMPL(__imp__sub_826E47F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E47F8"))) PPC_WEAK_FUNC(sub_826E47F8);
PPC_FUNC_IMPL(__imp__sub_826E47F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25332(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25332, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E480C"))) PPC_WEAK_FUNC(sub_826E480C);
PPC_FUNC_IMPL(__imp__sub_826E480C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4810"))) PPC_WEAK_FUNC(sub_826E4810);
PPC_FUNC_IMPL(__imp__sub_826E4810) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-25328(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25328, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4824"))) PPC_WEAK_FUNC(sub_826E4824);
PPC_FUNC_IMPL(__imp__sub_826E4824) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4828"))) PPC_WEAK_FUNC(sub_826E4828);
PPC_FUNC_IMPL(__imp__sub_826E4828) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-24760(r10)
	PPC_STORE_U32(ctx.r10.u32 + -24760, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E483C"))) PPC_WEAK_FUNC(sub_826E483C);
PPC_FUNC_IMPL(__imp__sub_826E483C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4840"))) PPC_WEAK_FUNC(sub_826E4840);
PPC_FUNC_IMPL(__imp__sub_826E4840) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-24756(r10)
	PPC_STORE_U32(ctx.r10.u32 + -24756, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4854"))) PPC_WEAK_FUNC(sub_826E4854);
PPC_FUNC_IMPL(__imp__sub_826E4854) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4858"))) PPC_WEAK_FUNC(sub_826E4858);
PPC_FUNC_IMPL(__imp__sub_826E4858) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-24748(r10)
	PPC_STORE_U32(ctx.r10.u32 + -24748, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E486C"))) PPC_WEAK_FUNC(sub_826E486C);
PPC_FUNC_IMPL(__imp__sub_826E486C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4870"))) PPC_WEAK_FUNC(sub_826E4870);
PPC_FUNC_IMPL(__imp__sub_826E4870) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-23188
	r11.s64 = r11.s64 + -23188;
	// stw r11,-24744(r10)
	PPC_STORE_U32(ctx.r10.u32 + -24744, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4884"))) PPC_WEAK_FUNC(sub_826E4884);
PPC_FUNC_IMPL(__imp__sub_826E4884) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4888"))) PPC_WEAK_FUNC(sub_826E4888);
PPC_FUNC_IMPL(__imp__sub_826E4888) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32243
	r11.s64 = -2113077248;
	// lis r10,-32137
	ctx.r10.s64 = -2106130432;
	// addi r11,r11,4400
	r11.s64 = r11.s64 + 4400;
	// stw r11,5868(r10)
	PPC_STORE_U32(ctx.r10.u32 + 5868, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E489C"))) PPC_WEAK_FUNC(sub_826E489C);
PPC_FUNC_IMPL(__imp__sub_826E489C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E48A0"))) PPC_WEAK_FUNC(sub_826E48A0);
PPC_FUNC_IMPL(__imp__sub_826E48A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r10,r11,18572
	ctx.r10.s64 = r11.s64 + 18572;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r11,r11,-13676
	r11.s64 = r11.s64 + -13676;
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// b 0x82120818
	sub_82120818(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_826E48BC"))) PPC_WEAK_FUNC(sub_826E48BC);
PPC_FUNC_IMPL(__imp__sub_826E48BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E48C0"))) PPC_WEAK_FUNC(sub_826E48C0);
PPC_FUNC_IMPL(__imp__sub_826E48C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r31,r11,-13620
	r31.s64 = r11.s64 + -13620;
	// addi r3,r31,28
	ctx.r3.s64 = r31.s64 + 28;
	// bl 0x823260f8
	sub_823260F8(ctx, base);
	// addi r3,r31,28
	ctx.r3.s64 = r31.s64 + 28;
	// bl 0x82317760
	sub_82317760(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E48FC"))) PPC_WEAK_FUNC(sub_826E48FC);
PPC_FUNC_IMPL(__imp__sub_826E48FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4900"))) PPC_WEAK_FUNC(sub_826E4900);
PPC_FUNC_IMPL(__imp__sub_826E4900) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r3,r11,-13364
	ctx.r3.s64 = r11.s64 + -13364;
	// b 0x822f5468
	sub_822F5468(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_826E490C"))) PPC_WEAK_FUNC(sub_826E490C);
PPC_FUNC_IMPL(__imp__sub_826E490C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4910"))) PPC_WEAK_FUNC(sub_826E4910);
PPC_FUNC_IMPL(__imp__sub_826E4910) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r3,r11,-13336
	ctx.r3.s64 = r11.s64 + -13336;
	// b 0x826a46e8
	sub_826A46E8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_826E491C"))) PPC_WEAK_FUNC(sub_826E491C);
PPC_FUNC_IMPL(__imp__sub_826E491C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4920"))) PPC_WEAK_FUNC(sub_826E4920);
PPC_FUNC_IMPL(__imp__sub_826E4920) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r3,r11,-13328
	ctx.r3.s64 = r11.s64 + -13328;
	// b 0x826a46e8
	sub_826A46E8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_826E492C"))) PPC_WEAK_FUNC(sub_826E492C);
PPC_FUNC_IMPL(__imp__sub_826E492C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4930"))) PPC_WEAK_FUNC(sub_826E4930);
PPC_FUNC_IMPL(__imp__sub_826E4930) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32242
	r11.s64 = -2113011712;
	// lis r10,-32137
	ctx.r10.s64 = -2106130432;
	// addi r11,r11,-25964
	r11.s64 = r11.s64 + -25964;
	// stw r11,10592(r10)
	PPC_STORE_U32(ctx.r10.u32 + 10592, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E4944"))) PPC_WEAK_FUNC(sub_826E4944);
PPC_FUNC_IMPL(__imp__sub_826E4944) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E4948"))) PPC_WEAK_FUNC(sub_826E4948);
PPC_FUNC_IMPL(__imp__sub_826E4948) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32242
	r11.s64 = -2113011712;
	// lis r10,-32137
	ctx.r10.s64 = -2106130432;
	// addi r11,r11,-25964
	r11.s64 = r11.s64 + -25964;
	// stw r11,10624(r10)
	PPC_STORE_U32(ctx.r10.u32 + 10624, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E5600"))) PPC_WEAK_FUNC(sub_826E5600);
PPC_FUNC_IMPL(__imp__sub_826E5600) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E5604"))) PPC_WEAK_FUNC(sub_826E5604);
PPC_FUNC_IMPL(__imp__sub_826E5604) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E5608"))) PPC_WEAK_FUNC(sub_826E5608);
PPC_FUNC_IMPL(__imp__sub_826E5608) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r22,0
	r22.s64 = 0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// lwz r30,0(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r25,4(r26)
	r25.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// addi r24,r11,1
	r24.s64 = r11.s64 + 1;
	// lwz r23,36(r26)
	r23.u64 = PPC_LOAD_U32(r26.u32 + 36);
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// beq cr6,0x826e57f8
	if (cr6.eq) goto loc_826E57F8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r28,0(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rldicl r11,r9,10,54
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 10) & 0x3FF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e5720
	if (cr6.lt) goto loc_826E5720;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826e5718
	if (!cr6.lt) goto loc_826E5718;
loc_826E5680:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826e56ac
	if (cr6.lt) goto loc_826E56AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826e5680
	if (cr6.eq) goto loc_826E5680;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e5768
	goto loc_826E5768;
loc_826E56AC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826E5718:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e5768
	goto loc_826E5768;
loc_826E5720:
	// li r4,10
	ctx.r4.s64 = 10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r30,r11,32768
	r30.u64 = r11.u64 | 32768;
loc_826E5734:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r30
	r11.u64 = r29.u64 + r30.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e5734
	if (cr6.lt) goto loc_826E5734;
loc_826E5768:
	// mr r21,r29
	r21.u64 = r29.u64;
	// cmplw cr6,r29,r25
	cr6.compare<uint32_t>(r29.u32, r25.u32, xer);
	// bne cr6,0x826e5780
	if (!cr6.eq) goto loc_826E5780;
loc_826E5774:
	// li r3,-1
	ctx.r3.s64 = -1;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_826E5780:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge 0x826e57ac
	if (!cr0.lt) goto loc_826E57AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E57AC:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// cmplw cr6,r29,r24
	cr6.compare<uint32_t>(r29.u32, r24.u32, xer);
	// lhzx r11,r11,r23
	r11.u64 = PPC_LOAD_U16(r11.u32 + r23.u32);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// clrlwi r25,r9,24
	r25.u64 = ctx.r9.u32 & 0xFF;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// blt cr6,0x826e57e4
	if (cr6.lt) goto loc_826E57E4;
	// lwz r9,16(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// lbzx r9,r9,r25
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r25.u32);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// add r30,r9,r11
	r30.u64 = ctx.r9.u64 + r11.u64;
	// b 0x826e5ca4
	goto loc_826E5CA4;
loc_826E57E4:
	// lwz r9,12(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 12);
	// lbzx r9,r9,r25
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r25.u32);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// add r30,r9,r11
	r30.u64 = ctx.r9.u64 + r11.u64;
	// b 0x826e5ca4
	goto loc_826E5CA4;
loc_826E57F8:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge 0x826e5824
	if (!cr0.lt) goto loc_826E5824;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E5824:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826e59c8
	if (cr6.eq) goto loc_826E59C8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r28,0(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rldicl r11,r9,10,54
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 10) & 0x3FF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e590c
	if (cr6.lt) goto loc_826E590C;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r7,r11,r10
	ctx.r7.s64 = ctx.r10.s64 - r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// bge cr6,0x826e5904
	if (!cr6.lt) goto loc_826E5904;
loc_826E586C:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826e5898
	if (cr6.lt) goto loc_826E5898;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826e586c
	if (cr6.eq) goto loc_826E586C;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e5954
	goto loc_826E5954;
loc_826E5898:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826E5904:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e5954
	goto loc_826E5954;
loc_826E590C:
	// li r4,10
	ctx.r4.s64 = 10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r30,r11,32768
	r30.u64 = r11.u64 | 32768;
loc_826E5920:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r30
	r11.u64 = r29.u64 + r30.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e5920
	if (cr6.lt) goto loc_826E5920;
loc_826E5954:
	// mr r21,r29
	r21.u64 = r29.u64;
	// cmplw cr6,r29,r25
	cr6.compare<uint32_t>(r29.u32, r25.u32, xer);
	// beq cr6,0x826e5774
	if (cr6.eq) goto loc_826E5774;
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r29,r24
	cr6.compare<uint32_t>(r29.u32, r24.u32, xer);
	// lhzx r11,r11,r23
	r11.u64 = PPC_LOAD_U16(r11.u32 + r23.u32);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// srawi r30,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	r30.s64 = ctx.r9.s32 >> 8;
	// blt cr6,0x826e5984
	if (cr6.lt) goto loc_826E5984;
	// lwz r10,24(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 24);
	// b 0x826e5988
	goto loc_826E5988;
loc_826E5984:
	// lwz r10,20(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 20);
loc_826E5988:
	// lbzx r10,r10,r30
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r30.u32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r25,r11,1
	r25.s64 = r11.s64 + 1;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// bge 0x826e59c0
	if (!cr0.lt) goto loc_826E59C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E59C0:
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// b 0x826e5ca4
	goto loc_826E5CA4;
loc_826E59C8:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x826e59f4
	if (!cr0.lt) goto loc_826E59F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E59F4:
	// add r11,r30,r24
	r11.u64 = r30.u64 + r24.u64;
	// addi r21,r11,-1
	r21.s64 = r11.s64 + -1;
	// lbz r11,1187(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 1187);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e5a14
	if (cr6.eq) goto loc_826E5A14;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82611c10
	sub_82611C10(ctx, base);
	// stb r22,1187(r27)
	PPC_STORE_U8(r27.u32 + 1187, r22.u8);
loc_826E5A14:
	// lwz r30,0(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r28,r22
	r28.u64 = r22.u64;
	// lbz r29,1184(r27)
	r29.u64 = PPC_LOAD_U8(r27.u32 + 1184);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x826e5a38
	if (!cr6.eq) goto loc_826E5A38;
	// mr r25,r22
	r25.u64 = r22.u64;
	// b 0x826e5ad8
	goto loc_826E5AD8;
loc_826E5A38:
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x826e5a98
	if (!cr6.gt) goto loc_826E5A98;
loc_826E5A40:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e5a98
	if (cr6.eq) goto loc_826E5A98;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e5a88
	if (!cr0.lt) goto loc_826E5A88;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E5A88:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e5a40
	if (cr6.gt) goto loc_826E5A40;
loc_826E5A98:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x826e5ad4
	if (!cr0.lt) goto loc_826E5AD4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E5AD4:
	// mr r25,r29
	r25.u64 = r29.u64;
loc_826E5AD8:
	// lwz r3,0(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e5b04
	if (!cr0.lt) goto loc_826E5B04;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E5B04:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r30,0(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lbz r29,1183(r27)
	r29.u64 = PPC_LOAD_U8(r27.u32 + 1183);
	// mr r28,r22
	r28.u64 = r22.u64;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826e5bd8
	if (cr6.eq) goto loc_826E5BD8;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x826e5b34
	if (!cr6.eq) goto loc_826E5B34;
	// mr r29,r22
	r29.u64 = r22.u64;
	// neg r11,r29
	r11.s64 = -r29.s64;
	// b 0x826e5c88
	goto loc_826E5C88;
loc_826E5B34:
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x826e5b94
	if (!cr6.gt) goto loc_826E5B94;
loc_826E5B3C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e5b94
	if (cr6.eq) goto loc_826E5B94;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e5b84
	if (!cr0.lt) goto loc_826E5B84;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E5B84:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e5b3c
	if (cr6.gt) goto loc_826E5B3C;
loc_826E5B94:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x826e5bd0
	if (!cr0.lt) goto loc_826E5BD0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E5BD0:
	// neg r11,r29
	r11.s64 = -r29.s64;
	// b 0x826e5c88
	goto loc_826E5C88;
loc_826E5BD8:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x826e5be8
	if (!cr6.eq) goto loc_826E5BE8;
	// mr r11,r22
	r11.u64 = r22.u64;
	// b 0x826e5c88
	goto loc_826E5C88;
loc_826E5BE8:
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x826e5c48
	if (!cr6.gt) goto loc_826E5C48;
loc_826E5BF0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e5c48
	if (cr6.eq) goto loc_826E5C48;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e5c38
	if (!cr0.lt) goto loc_826E5C38;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E5C38:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e5bf0
	if (cr6.gt) goto loc_826E5BF0;
loc_826E5C48:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x826e5c84
	if (!cr0.lt) goto loc_826E5C84;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E5C84:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_826E5C88:
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r9,r9,1
	xer.ca = ctx.r9.u32 <= 1;
	ctx.r9.s64 = 1 - ctx.r9.s64;
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// clrlwi r30,r11,24
	r30.u64 = r11.u32 & 0xFF;
	// srawi r22,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r22.s64 = r11.s32 >> 8;
loc_826E5CA4:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826e5774
	if (!cr6.eq) goto loc_826E5774;
	// rlwinm r11,r22,12,0,19
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 12) & 0xFFFFF000;
	// or r11,r11,r21
	r11.u64 = r11.u64 | r21.u64;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// or r11,r11,r30
	r11.u64 = r11.u64 | r30.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// rlwinm r11,r11,7,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// or r3,r11,r25
	ctx.r3.u64 = r11.u64 | r25.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_826E5CD8"))) PPC_WEAK_FUNC(sub_826E5CD8);
PPC_FUNC_IMPL(__imp__sub_826E5CD8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,2928(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2928);
	// lwz r10,2088(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2088);
	// addi r9,r11,726
	ctx.r9.s64 = r11.s64 + 726;
	// lwz r8,3960(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r10,263
	ctx.r10.s64 = ctx.r10.s64 + 263;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r8,3
	cr6.compare<int32_t>(ctx.r8.s32, 3, xer);
	// stw r9,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, ctx.r9.u32);
	// lwzx r9,r7,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + r31.u32);
	// stw r9,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, ctx.r9.u32);
	// lwz r11,2100(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 2100);
	// stw r11,2092(r31)
	PPC_STORE_U32(r31.u32 + 2092, r11.u32);
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(r31.u32 + 2096, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// beq cr6,0x826e5d48
	if (cr6.eq) goto loc_826E5D48;
	// li r11,1
	r11.s64 = 1;
loc_826E5D48:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
	// bl 0x825ebc08
	sub_825EBC08(ctx, base);
	// lwz r11,3960(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826e5d70
	if (cr6.eq) goto loc_826E5D70;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x826e5d74
	if (!cr6.eq) goto loc_826E5D74;
loc_826E5D70:
	// li r11,1
	r11.s64 = 1;
loc_826E5D74:
	// lwz r10,1972(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r11,76(r10)
	PPC_STORE_U32(ctx.r10.u32 + 76, r11.u32);
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// lwz r3,1972(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// bl 0x8265b9d0
	sub_8265B9D0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// bl 0x82628f88
	sub_82628F88(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82612140
	sub_82612140(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826E5DB4"))) PPC_WEAK_FUNC(sub_826E5DB4);
PPC_FUNC_IMPL(__imp__sub_826E5DB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826E5DB8"))) PPC_WEAK_FUNC(sub_826E5DB8);
PPC_FUNC_IMPL(__imp__sub_826E5DB8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-416(r1)
	ea = -416 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r14,r5
	r14.u64 = ctx.r5.u64;
	// li r16,0
	r16.s64 = 0;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r16
	r31.u64 = r16.u64;
	// lwz r11,268(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 268);
	// stw r25,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, r25.u32);
	// stw r14,452(r1)
	PPC_STORE_U32(ctx.r1.u32 + 452, r14.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x82615470
	sub_82615470(ctx, base);
	// lwz r11,21556(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21556);
	// mr r28,r16
	r28.u64 = r16.u64;
	// mr r27,r16
	r27.u64 = r16.u64;
	// mr r15,r16
	r15.u64 = r16.u64;
	// stw r11,20(r14)
	PPC_STORE_U32(r14.u32 + 20, r11.u32);
	// lwz r11,21568(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21568);
	// stw r28,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r28.u32);
	// stw r27,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r27.u32);
	// stw r15,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r15.u32);
	// stw r11,24(r14)
	PPC_STORE_U32(r14.u32 + 24, r11.u32);
	// lwz r11,21560(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21560);
	// stw r11,28(r14)
	PPC_STORE_U32(r14.u32 + 28, r11.u32);
	// lwz r11,21572(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21572);
	// stw r16,0(r14)
	PPC_STORE_U32(r14.u32 + 0, r16.u32);
	// stw r16,4(r14)
	PPC_STORE_U32(r14.u32 + 4, r16.u32);
	// sth r16,16(r14)
	PPC_STORE_U16(r14.u32 + 16, r16.u16);
	// sth r16,18(r14)
	PPC_STORE_U16(r14.u32 + 18, r16.u16);
	// stw r11,32(r14)
	PPC_STORE_U32(r14.u32 + 32, r11.u32);
	// lhz r11,52(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 52);
	// lhz r10,50(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r11,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r11.u32);
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// beq cr6,0x826ea3e0
	if (cr6.eq) goto loc_826EA3E0;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r11,r11,29840
	r11.s64 = r11.s64 + 29840;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r11,r11,11448
	r11.s64 = r11.s64 + 11448;
	// stw r11,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r11.u32);
loc_826E5E6C:
	// li r11,-1
	r11.s64 = -1;
	// stw r28,8(r14)
	PPC_STORE_U32(r14.u32 + 8, r28.u32);
	// stw r27,12(r14)
	PPC_STORE_U32(r14.u32 + 12, r27.u32);
	// sth r16,18(r14)
	PPC_STORE_U16(r14.u32 + 18, r16.u16);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// addi r11,r30,1376
	r11.s64 = r30.s64 + 1376;
	// stw r11,1416(r30)
	PPC_STORE_U32(r30.u32 + 1416, r11.u32);
	// lwz r11,21236(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826e6094
	if (cr6.eq) goto loc_826E6094;
	// lwz r11,1240(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1240);
	// rlwinm r26,r15,2,0,29
	r26.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r26,r11
	r11.u64 = PPC_LOAD_U32(r26.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826e6084
	if (cr6.eq) goto loc_826E6084;
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// ld r10,104(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 104);
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,112(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,116(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,120(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,124(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,128(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,132(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,136(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,144(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,148(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,152(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826e5fc4
	if (cr6.eq) goto loc_826E5FC4;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r29,1
	r29.s64 = 1;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826e5f9c
	if (!cr6.lt) goto loc_826E5F9C;
loc_826E5F5C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e5f9c
	if (cr6.eq) goto loc_826E5F9C;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x826e5f8c
	if (!cr0.lt) goto loc_826E5F8C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E5F8C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e5f5c
	if (cr6.gt) goto loc_826E5F5C;
loc_826E5F9C:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r29,32
	ctx.r10.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x826e5fc4
	if (!cr0.lt) goto loc_826E5FC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E5FC4:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82639b10
	sub_82639B10(ctx, base);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,104(r30)
	PPC_STORE_U64(r30.u32 + 104, r11.u64);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,112(r30)
	PPC_STORE_U32(r30.u32 + 112, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,116(r30)
	PPC_STORE_U32(r30.u32 + 116, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,120(r30)
	PPC_STORE_U32(r30.u32 + 120, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,124(r30)
	PPC_STORE_U32(r30.u32 + 124, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,128(r30)
	PPC_STORE_U32(r30.u32 + 128, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,132(r30)
	PPC_STORE_U32(r30.u32 + 132, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,136(r30)
	PPC_STORE_U32(r30.u32 + 136, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,140(r30)
	PPC_STORE_U32(r30.u32 + 140, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,144(r30)
	PPC_STORE_U32(r30.u32 + 144, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,148(r30)
	PPC_STORE_U32(r30.u32 + 148, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stb r10,1187(r30)
	PPC_STORE_U8(r30.u32 + 1187, ctx.r10.u8);
	// stw r11,152(r30)
	PPC_STORE_U32(r30.u32 + 152, r11.u32);
	// bne cr6,0x826ea3d4
	if (!cr6.eq) goto loc_826EA3D4;
loc_826E6084:
	// lwz r11,1240(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1240);
	// lwzx r11,r26,r11
	r11.u64 = PPC_LOAD_U32(r26.u32 + r11.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
loc_826E6094:
	// lwz r11,3932(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 3932);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826e61d8
	if (cr6.eq) goto loc_826E61D8;
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// ld r10,104(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 104);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,112(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,116(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,120(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,124(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,128(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,132(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,136(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,144(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,148(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r10,152(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// bl 0x82639ef0
	sub_82639EF0(ctx, base);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,104(r30)
	PPC_STORE_U64(r30.u32 + 104, r11.u64);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,112(r30)
	PPC_STORE_U32(r30.u32 + 112, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,116(r30)
	PPC_STORE_U32(r30.u32 + 116, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,120(r30)
	PPC_STORE_U32(r30.u32 + 120, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,124(r30)
	PPC_STORE_U32(r30.u32 + 124, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,128(r30)
	PPC_STORE_U32(r30.u32 + 128, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,132(r30)
	PPC_STORE_U32(r30.u32 + 132, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,136(r30)
	PPC_STORE_U32(r30.u32 + 136, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,140(r30)
	PPC_STORE_U32(r30.u32 + 140, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,144(r30)
	PPC_STORE_U32(r30.u32 + 144, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,148(r30)
	PPC_STORE_U32(r30.u32 + 148, r11.u32);
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stw r11,152(r30)
	PPC_STORE_U32(r30.u32 + 152, r11.u32);
	// bne cr6,0x826ea3d4
	if (!cr6.eq) goto loc_826EA3D4;
loc_826E61D8:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r16,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r16.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ea36c
	if (cr6.eq) goto loc_826EA36C;
loc_826E61E8:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r10,128
	ctx.r10.s64 = 128;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// dcbt r10,r11
	// lwz r23,80(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lbz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 24);
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// stb r10,4(r23)
	PPC_STORE_U8(r23.u32 + 4, ctx.r10.u8);
	// rlwinm r11,r11,0,10,7
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFF3FFFFF;
	// rlwinm r11,r11,0,4,2
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFEFFFFFFF;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// lbz r11,25(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 25);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826e625c
	if (!cr6.eq) goto loc_826E625C;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e624c
	if (!cr0.lt) goto loc_826E624C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E624C:
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// extsb r11,r31
	r11.s64 = r31.s8;
	// rlwimi r10,r11,8,21,23
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 8) & 0x700) | (ctx.r10.u64 & 0xFFFFFFFFFFFFF8FF);
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
loc_826E625C:
	// lbz r11,26(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 26);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826e62a0
	if (!cr6.eq) goto loc_826E62A0;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e6294
	if (!cr0.lt) goto loc_826E6294;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E6294:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwimi r11,r31,31,0,0
	r11.u64 = (__builtin_rotateleft32(r31.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
loc_826E62A0:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// lwz r20,4(r30)
	r20.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// rlwinm r10,r11,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// rlwinm r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r19,r10,27,31,31
	r19.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// stw r19,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r19.u32);
	// beq cr6,0x826e7a54
	if (cr6.eq) goto loc_826E7A54;
	// li r17,0
	r17.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r21,r17
	r21.u64 = r17.u64;
	// beq cr6,0x826e674c
	if (cr6.eq) goto loc_826E674C;
	// stb r17,5(r23)
	PPC_STORE_U8(r23.u32 + 5, r17.u8);
	// lhz r11,36(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 36);
	// lwz r10,0(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// lhz r27,16(r14)
	r27.u64 = PPC_LOAD_U16(r14.u32 + 16);
	// add r24,r11,r10
	r24.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lhz r26,18(r14)
	r26.u64 = PPC_LOAD_U16(r14.u32 + 18);
	// and r10,r27,r11
	ctx.r10.u64 = r27.u64 & r11.u64;
	// lhz r28,50(r30)
	r28.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// lwz r25,188(r30)
	r25.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// addi r11,r24,-1
	r11.s64 = r24.s64 + -1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826e63f4
	if (!cr6.eq) goto loc_826E63F4;
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826e63e8
	if (cr6.eq) goto loc_826E63E8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r25
	r11.u64 = PPC_LOAD_U32(r11.u32 + r25.u32);
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// beq cr6,0x826e63e8
	if (cr6.eq) goto loc_826E63E8;
	// lwz r9,16(r14)
	ctx.r9.u64 = PPC_LOAD_U32(r14.u32 + 16);
	// rlwinm r7,r11,1,15,15
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x10000;
	// lwz r8,1400(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 1400);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r9,r9,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r6,1404(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 1404);
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// subf r9,r9,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r9.s64;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r9,r9,0,0,16
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r9,r9,0,16,0
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826e63e8
	if (cr6.eq) goto loc_826E63E8;
	// srawi r9,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 16;
	// lhz r8,52(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 52);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// rlwinm r11,r28,5,0,26
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r6,r26,5,0,26
	ctx.r6.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 5) & 0xFFFFFFE0;
	// rotlwi r8,r8,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 5);
	// addi r5,r11,-4
	ctx.r5.s64 = r11.s64 + -4;
	// rlwinm r7,r27,5,0,26
	ctx.r7.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r6,r10
	r11.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r4,r8,-4
	ctx.r4.s64 = ctx.r8.s64 + -4;
	// add r8,r7,r9
	ctx.r8.u64 = ctx.r7.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,-60
	cr6.compare<int32_t>(r11.s32, -60, xer);
	// bge cr6,0x826e63a4
	if (!cr6.lt) goto loc_826E63A4;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r10,r11,-60
	ctx.r10.s64 = r11.s64 + -60;
	// b 0x826e63b4
	goto loc_826E63B4;
loc_826E63A4:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// ble cr6,0x826e63b4
	if (!cr6.gt) goto loc_826E63B4;
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_826E63B4:
	// cmpwi cr6,r8,-60
	cr6.compare<int32_t>(ctx.r8.s32, -60, xer);
	// bge cr6,0x826e63d4
	if (!cr6.lt) goto loc_826E63D4;
	// subf r11,r8,r9
	r11.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r9,r11,-60
	ctx.r9.s64 = r11.s64 + -60;
	// rlwimi r10,r9,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// srawi r8,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 16;
	// b 0x826e66e8
	goto loc_826E66E8;
loc_826E63D4:
	// cmpw cr6,r8,r4
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r4.s32, xer);
	// ble cr6,0x826e63e4
	if (!cr6.gt) goto loc_826E63E4;
	// subf r11,r8,r4
	r11.s64 = ctx.r4.s64 - ctx.r8.s64;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
loc_826E63E4:
	// rlwimi r10,r9,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
loc_826E63E8:
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// srawi r8,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 16;
	// b 0x826e66e8
	goto loc_826E66E8;
loc_826E63F4:
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826e6408
	if (cr6.eq) goto loc_826E6408;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r11,r25
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + r25.u32);
loc_826E6408:
	// lwz r10,1416(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 1416);
	// subf r11,r28,r24
	r11.s64 = r24.s64 - r28.s64;
	// rlwinm r9,r6,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 31) & 0x7FFFFFFF;
	// xor r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r6.u64;
	// lbz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// rlwinm r9,r9,0,17,17
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x4000;
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r11,r25
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + r25.u32);
	// lwzx r11,r10,r25
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r25.u32);
	// rlwinm r10,r7,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r8,r11,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// xor r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r7.u64;
	// xor r5,r8,r11
	ctx.r5.u64 = ctx.r8.u64 ^ r11.u64;
	// rlwinm r8,r10,0,17,17
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4000;
	// rlwinm r10,r5,0,17,17
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x4000;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826e6474
	if (!cr6.gt) goto loc_826E6474;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// bne cr6,0x826e659c
	if (!cr6.eq) goto loc_826E659C;
	// cmplwi cr6,r7,16384
	cr6.compare<uint32_t>(ctx.r7.u32, 16384, xer);
	// bne cr6,0x826e657c
	if (!cr6.eq) goto loc_826E657C;
	// mr r7,r17
	ctx.r7.u64 = r17.u64;
loc_826E6474:
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// lwz r4,16(r14)
	ctx.r4.u64 = PPC_LOAD_U32(r14.u32 + 16);
	// subf r31,r6,r11
	r31.s64 = r11.s64 - ctx.r6.s64;
	// lwz r29,1404(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 1404);
	// subf r21,r7,r6
	r21.s64 = ctx.r6.s64 - ctx.r7.s64;
	// lwz r22,1400(r30)
	r22.u64 = PPC_LOAD_U32(r30.u32 + 1400);
	// rlwinm r9,r7,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r8,r6,16,0,15
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 16) & 0xFFFF0000;
	// xor r21,r21,r5
	r21.u64 = r21.u64 ^ ctx.r5.u64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// subf r5,r9,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r9.s64;
	// rlwinm r10,r11,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r4,r4,5,0,26
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r3,r9,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r18,r8,r10
	r18.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// srawi r5,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r31.s32 >> 31;
	// srawi r31,r21,31
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r21.s32 >> 31;
	// subf r21,r4,r29
	r21.s64 = r29.s64 - ctx.r4.s64;
	// xor r29,r18,r3
	r29.u64 = r18.u64 ^ ctx.r3.u64;
	// lwz r18,84(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// xor r18,r18,r3
	r18.u64 = r18.u64 ^ ctx.r3.u64;
	// srawi r3,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r29.s32 >> 31;
	// srawi r29,r18,31
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r18.s32 >> 31;
	// or r18,r5,r31
	r18.u64 = ctx.r5.u64 | r31.u64;
	// and r10,r3,r10
	ctx.r10.u64 = ctx.r3.u64 & ctx.r10.u64;
	// andc r5,r6,r18
	ctx.r5.u64 = ctx.r6.u64 & ~r18.u64;
	// and r9,r29,r9
	ctx.r9.u64 = r29.u64 & ctx.r9.u64;
	// or r11,r5,r11
	r11.u64 = ctx.r5.u64 | r11.u64;
	// and r5,r31,r7
	ctx.r5.u64 = r31.u64 & ctx.r7.u64;
	// or r11,r11,r5
	r11.u64 = r11.u64 | ctx.r5.u64;
	// or r5,r3,r29
	ctx.r5.u64 = ctx.r3.u64 | r29.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// andc r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 & ~ctx.r5.u64;
	// rlwinm r11,r11,16,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// rlwinm r10,r10,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// rlwinm r10,r11,1,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x10000;
	// subf r9,r11,r21
	ctx.r9.s64 = r21.s64 - r11.s64;
	// subf r10,r10,r22
	ctx.r10.s64 = r22.s64 - ctx.r10.s64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// rlwinm r10,r10,0,0,16
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r10,r10,0,16,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826e65f8
	if (cr6.eq) goto loc_826E65F8;
	// srawi r9,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 16;
	// lhz r8,52(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 52);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// rlwinm r11,r28,5,0,26
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r4,r26,5,0,26
	ctx.r4.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 5) & 0xFFFFFFE0;
	// rotlwi r8,r8,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 5);
	// addi r3,r11,-4
	ctx.r3.s64 = r11.s64 + -4;
	// rlwinm r5,r27,5,0,26
	ctx.r5.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r4,r10
	r11.u64 = ctx.r4.u64 + ctx.r10.u64;
	// addi r31,r8,-4
	r31.s64 = ctx.r8.s64 + -4;
	// add r8,r5,r9
	ctx.r8.u64 = ctx.r5.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,-60
	cr6.compare<int32_t>(r11.s32, -60, xer);
	// bge cr6,0x826e65bc
	if (!cr6.lt) goto loc_826E65BC;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r10,r11,-60
	ctx.r10.s64 = r11.s64 + -60;
	// b 0x826e65cc
	goto loc_826E65CC;
loc_826E657C:
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x826e658c
	if (!cr6.eq) goto loc_826E658C;
	// mr r11,r17
	r11.u64 = r17.u64;
	// b 0x826e6474
	goto loc_826E6474;
loc_826E658C:
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// bne cr6,0x826e6474
	if (!cr6.eq) goto loc_826E6474;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// b 0x826e6474
	goto loc_826E6474;
loc_826E659C:
	// cmplwi cr6,r7,16384
	cr6.compare<uint32_t>(ctx.r7.u32, 16384, xer);
	// bne cr6,0x826e65a8
	if (!cr6.eq) goto loc_826E65A8;
	// mr r7,r17
	ctx.r7.u64 = r17.u64;
loc_826E65A8:
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// bne cr6,0x826e65b4
	if (!cr6.eq) goto loc_826E65B4;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
loc_826E65B4:
	// mr r11,r17
	r11.u64 = r17.u64;
	// b 0x826e65f8
	goto loc_826E65F8;
loc_826E65BC:
	// cmpw cr6,r11,r3
	cr6.compare<int32_t>(r11.s32, ctx.r3.s32, xer);
	// ble cr6,0x826e65cc
	if (!cr6.gt) goto loc_826E65CC;
	// subf r11,r11,r3
	r11.s64 = ctx.r3.s64 - r11.s64;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_826E65CC:
	// cmpwi cr6,r8,-60
	cr6.compare<int32_t>(ctx.r8.s32, -60, xer);
	// bge cr6,0x826e65e0
	if (!cr6.lt) goto loc_826E65E0;
	// subf r11,r8,r9
	r11.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r9,r11,-60
	ctx.r9.s64 = r11.s64 + -60;
	// b 0x826e65f0
	goto loc_826E65F0;
loc_826E65E0:
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// ble cr6,0x826e65f0
	if (!cr6.gt) goto loc_826E65F0;
	// subf r11,r8,r31
	r11.s64 = r31.s64 - ctx.r8.s64;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
loc_826E65F0:
	// rlwimi r10,r9,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_826E65F8:
	// rlwinm r10,r11,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// srawi r8,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 16;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826e66e4
	if (cr6.eq) goto loc_826E66E4;
	// rlwinm r11,r6,16,0,15
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 16) & 0xFFFF0000;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r6,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 16;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// subf r6,r6,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r6.s64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// srawi r9,r6,31
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r6.s32 >> 31;
	// xor r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r9.u64;
	// subf r9,r9,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r9.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// bgt cr6,0x826e6678
	if (cr6.gt) goto loc_826E6678;
	// rlwinm r11,r7,16,0,15
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 16) & 0xFFFF0000;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r7,16
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 16;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// srawi r9,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 31;
	// xor r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826e66e4
	if (!cr6.gt) goto loc_826E66E4;
loc_826E6678:
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e66a4
	if (!cr0.lt) goto loc_826E66A4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E66A4:
	// addi r11,r31,19
	r11.s64 = r31.s64 + 19;
	// lwz r25,188(r30)
	r25.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// subf r11,r11,r24
	r11.s64 = r24.s64 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// cmpwi cr6,r9,16384
	cr6.compare<int32_t>(ctx.r9.s32, 16384, xer);
	// bne cr6,0x826e66e8
	if (!cr6.eq) goto loc_826E66E8;
	// mr r8,r17
	ctx.r8.u64 = r17.u64;
	// mr r9,r17
	ctx.r9.u64 = r17.u64;
	// b 0x826e66e8
	goto loc_826E66E8;
loc_826E66E4:
	// srawi r9,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 16;
loc_826E66E8:
	// lbz r10,30(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 30);
	// rlwinm r7,r24,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r11,62(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 62);
	// mr r24,r17
	r24.u64 = r17.u64;
	// lhz r6,66(r30)
	ctx.r6.u64 = PPC_LOAD_U16(r30.u32 + 66);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lhz r5,68(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 68);
	// rotlwi r5,r5,16
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 16);
	// slw r10,r17,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r17.u32 << (ctx.r10.u8 & 0x3F));
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// and r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 & ctx.r6.u64;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// lhz r11,64(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 64);
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// rotlwi r11,r11,16
	r11.u64 = __builtin_rotateleft32(r11.u32, 16);
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// rlwinm r10,r10,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// and r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 & ctx.r5.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwx r11,r7,r25
	PPC_STORE_U32(ctx.r7.u32 + r25.u32, r11.u32);
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// stb r17,5(r20)
	PPC_STORE_U8(r20.u32 + 5, r17.u8);
	// b 0x826e79f8
	goto loc_826E79F8;
loc_826E674C:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,176(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 176);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,10,54
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 10) & 0x3FF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e6844
	if (cr6.lt) goto loc_826E6844;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826e6834
	if (!cr6.lt) goto loc_826E6834;
loc_826E6794:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826e67c8
	if (cr6.lt) goto loc_826E67C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826e6794
	if (cr6.eq) goto loc_826E6794;
	// lis r11,0
	r11.s64 = 0;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// ori r18,r11,32768
	r18.u64 = r11.u64 | 32768;
	// b 0x826e688c
	goto loc_826E688C;
loc_826E67C8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826E6834:
	// lis r11,0
	r11.s64 = 0;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// ori r18,r11,32768
	r18.u64 = r11.u64 | 32768;
	// b 0x826e688c
	goto loc_826E688C;
loc_826E6844:
	// li r4,10
	ctx.r4.s64 = 10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r18,r11,32768
	r18.u64 = r11.u64 | 32768;
loc_826E6858:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r18
	r11.u64 = r29.u64 + r18.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e6858
	if (cr6.lt) goto loc_826E6858;
loc_826E688C:
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpwi cr6,r11,37
	cr6.compare<int32_t>(r11.s32, 37, xer);
	// bge cr6,0x826e68a0
	if (!cr6.lt) goto loc_826E68A0;
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
loc_826E68A0:
	// extsw r22,r10
	r22.s64 = ctx.r10.s32;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826e68b0
	if (cr6.eq) goto loc_826E68B0;
	// addi r11,r11,-37
	r11.s64 = r11.s64 + -37;
loc_826E68B0:
	// mr r23,r17
	r23.u64 = r17.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826e68c4
	if (!cr6.eq) goto loc_826E68C4;
	// mr r11,r17
	r11.u64 = r17.u64;
	// b 0x826e6b2c
	goto loc_826E6B2C;
loc_826E68C4:
	// cmpwi cr6,r11,35
	cr6.compare<int32_t>(r11.s32, 35, xer);
	// bge cr6,0x826e6a14
	if (!cr6.lt) goto loc_826E6A14;
	// lwz r10,12(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// srawi r9,r11,24
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 24;
	// srawi r8,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r8.s64 = r11.s32 >> 4;
	// clrlwi r10,r11,28
	ctx.r10.u64 = r11.u32 & 0xF;
	// clrlwi r27,r8,28
	r27.u64 = ctx.r8.u32 & 0xF;
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// add r29,r10,r27
	r29.u64 = ctx.r10.u64 + r27.u64;
	// clrlwi r26,r9,24
	r26.u64 = ctx.r9.u32 & 0xFF;
	// clrlwi r25,r8,24
	r25.u64 = ctx.r8.u32 & 0xFF;
	// clrlwi r24,r11,24
	r24.u64 = r11.u32 & 0xFF;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x826e69c0
	if (!cr6.gt) goto loc_826E69C0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r28,r17
	r28.u64 = r17.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826e69c0
	if (cr6.eq) goto loc_826E69C0;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x826e697c
	if (!cr6.gt) goto loc_826E697C;
loc_826E6924:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e697c
	if (cr6.eq) goto loc_826E697C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e696c
	if (!cr0.lt) goto loc_826E696C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E696C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e6924
	if (cr6.gt) goto loc_826E6924;
loc_826E697C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e69b8
	if (!cr0.lt) goto loc_826E69B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E69B8:
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// b 0x826e69c4
	goto loc_826E69C4;
loc_826E69C0:
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
loc_826E69C4:
	// and r11,r10,r26
	r11.u64 = ctx.r10.u64 & r26.u64;
	// rlwinm r8,r24,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r11,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// sraw r10,r10,r27
	temp.u32 = r27.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// clrlwi r7,r11,31
	ctx.r7.u64 = r11.u32 & 0x1;
	// rlwinm r8,r9,15,0,16
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 15) & 0xFFFF8000;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// neg r11,r7
	r11.s64 = -ctx.r7.s64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// rlwinm r7,r11,16,0,15
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// rlwinm r11,r11,16,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// b 0x826e6b28
	goto loc_826E6B28;
loc_826E6A14:
	// cmpwi cr6,r11,36
	cr6.compare<int32_t>(r11.s32, 36, xer);
	// bne cr6,0x826e6a28
	if (!cr6.eq) goto loc_826E6A28;
	// li r23,1
	r23.s64 = 1;
	// mr r11,r17
	r11.u64 = r17.u64;
	// b 0x826e6b2c
	goto loc_826E6B2C;
loc_826E6A28:
	// lbz r11,30(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 30);
	// mr r28,r17
	r28.u64 = r17.u64;
	// lhz r10,70(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 70);
	// lhz r9,72(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 72);
	// subf r26,r11,r10
	r26.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r27,r11,r9
	r27.s64 = ctx.r9.s64 - r11.s64;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// add r29,r27,r26
	r29.u64 = r27.u64 + r26.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x826e6a5c
	if (!cr6.eq) goto loc_826E6A5C;
	// mr r11,r17
	r11.u64 = r17.u64;
	// b 0x826e6afc
	goto loc_826E6AFC;
loc_826E6A5C:
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x826e6abc
	if (!cr6.gt) goto loc_826E6ABC;
loc_826E6A64:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e6abc
	if (cr6.eq) goto loc_826E6ABC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e6aac
	if (!cr0.lt) goto loc_826E6AAC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E6AAC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e6a64
	if (cr6.gt) goto loc_826E6A64;
loc_826E6ABC:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e6af8
	if (!cr0.lt) goto loc_826E6AF8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E6AF8:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_826E6AFC:
	// li r9,1
	ctx.r9.s64 = 1;
	// rlwinm r8,r11,16,0,15
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// sraw r11,r11,r27
	temp.u32 = r27.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// slw r10,r9,r26
	ctx.r10.u64 = r26.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r26.u8 & 0x3F));
	// slw r9,r9,r27
	ctx.r9.u64 = r27.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r27.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// rlwinm r9,r9,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 16) & 0xFFFF0000;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// addis r10,r9,-1
	ctx.r10.s64 = ctx.r9.s64 + -65536;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
loc_826E6B28:
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
loc_826E6B2C:
	// rldicr r8,r22,8,55
	ctx.r8.u64 = __builtin_rotateleft64(r22.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lhz r10,36(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 36);
	// lwz r9,0(r14)
	ctx.r9.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// or r8,r8,r23
	ctx.r8.u64 = ctx.r8.u64 | r23.u64;
	// lhz r27,16(r14)
	r27.u64 = PPC_LOAD_U16(r14.u32 + 16);
	// add r24,r10,r9
	r24.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lhz r26,18(r14)
	r26.u64 = PPC_LOAD_U16(r14.u32 + 18);
	// rldicr r10,r8,32,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000;
	// lhz r28,50(r30)
	r28.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// lwz r25,188(r30)
	r25.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// or r23,r10,r11
	r23.u64 = ctx.r10.u64 | r11.u64;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// and r10,r27,r11
	ctx.r10.u64 = r27.u64 & r11.u64;
	// addi r11,r24,-1
	r11.s64 = r24.s64 + -1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826e6c64
	if (!cr6.eq) goto loc_826E6C64;
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826e6c44
	if (cr6.eq) goto loc_826E6C44;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r25
	r11.u64 = PPC_LOAD_U32(r11.u32 + r25.u32);
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// beq cr6,0x826e6c44
	if (cr6.eq) goto loc_826E6C44;
	// lwz r9,16(r14)
	ctx.r9.u64 = PPC_LOAD_U32(r14.u32 + 16);
	// rlwinm r7,r11,1,15,15
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x10000;
	// lwz r8,1400(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 1400);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r9,r9,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r6,1404(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 1404);
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// subf r9,r9,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r9.s64;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r9,r9,0,0,16
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r9,r9,0,16,0
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826e6c44
	if (cr6.eq) goto loc_826E6C44;
	// srawi r9,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 16;
	// lhz r8,52(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 52);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// rlwinm r11,r28,5,0,26
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r6,r26,5,0,26
	ctx.r6.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 5) & 0xFFFFFFE0;
	// rotlwi r8,r8,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 5);
	// addi r5,r11,-4
	ctx.r5.s64 = r11.s64 + -4;
	// rlwinm r7,r27,5,0,26
	ctx.r7.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r6,r10
	r11.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r4,r8,-4
	ctx.r4.s64 = ctx.r8.s64 + -4;
	// add r8,r7,r9
	ctx.r8.u64 = ctx.r7.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,-60
	cr6.compare<int32_t>(r11.s32, -60, xer);
	// bge cr6,0x826e6c0c
	if (!cr6.lt) goto loc_826E6C0C;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r10,r11,-60
	ctx.r10.s64 = r11.s64 + -60;
	// b 0x826e6c1c
	goto loc_826E6C1C;
loc_826E6C0C:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// ble cr6,0x826e6c1c
	if (!cr6.gt) goto loc_826E6C1C;
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_826E6C1C:
	// cmpwi cr6,r8,-60
	cr6.compare<int32_t>(ctx.r8.s32, -60, xer);
	// bge cr6,0x826e6c30
	if (!cr6.lt) goto loc_826E6C30;
	// subf r11,r8,r9
	r11.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r9,r11,-60
	ctx.r9.s64 = r11.s64 + -60;
	// b 0x826e6c40
	goto loc_826E6C40;
loc_826E6C30:
	// cmpw cr6,r8,r4
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r4.s32, xer);
	// ble cr6,0x826e6c40
	if (!cr6.gt) goto loc_826E6C40;
	// subf r11,r8,r4
	r11.s64 = ctx.r4.s64 - ctx.r8.s64;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
loc_826E6C40:
	// rlwimi r10,r9,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
loc_826E6C44:
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,32,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r23,r12
	r11.u64 = r23.u64 & r12.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// bne cr6,0x826e6f88
	if (!cr6.eq) goto loc_826E6F88;
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// srawi r8,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 16;
	// b 0x826e6f9c
	goto loc_826E6F9C;
loc_826E6C64:
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826e6c78
	if (cr6.eq) goto loc_826E6C78;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r11,r25
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + r25.u32);
loc_826E6C78:
	// lwz r10,1416(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 1416);
	// subf r11,r28,r24
	r11.s64 = r24.s64 - r28.s64;
	// rlwinm r9,r6,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 31) & 0x7FFFFFFF;
	// xor r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r6.u64;
	// lbz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// rlwinm r9,r9,0,17,17
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x4000;
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r11,r25
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + r25.u32);
	// lwzx r11,r10,r25
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r25.u32);
	// rlwinm r10,r7,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r8,r11,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// xor r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r7.u64;
	// xor r5,r8,r11
	ctx.r5.u64 = ctx.r8.u64 ^ r11.u64;
	// rlwinm r8,r10,0,17,17
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4000;
	// rlwinm r10,r5,0,17,17
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x4000;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826e6ce4
	if (!cr6.gt) goto loc_826E6CE4;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// bne cr6,0x826e6e0c
	if (!cr6.eq) goto loc_826E6E0C;
	// cmplwi cr6,r7,16384
	cr6.compare<uint32_t>(ctx.r7.u32, 16384, xer);
	// bne cr6,0x826e6dec
	if (!cr6.eq) goto loc_826E6DEC;
	// mr r7,r17
	ctx.r7.u64 = r17.u64;
loc_826E6CE4:
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// lwz r4,16(r14)
	ctx.r4.u64 = PPC_LOAD_U32(r14.u32 + 16);
	// subf r31,r6,r11
	r31.s64 = r11.s64 - ctx.r6.s64;
	// lwz r29,1404(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 1404);
	// subf r18,r7,r6
	r18.s64 = ctx.r6.s64 - ctx.r7.s64;
	// lwz r22,1400(r30)
	r22.u64 = PPC_LOAD_U32(r30.u32 + 1400);
	// rlwinm r9,r7,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r8,r6,16,0,15
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 16) & 0xFFFF0000;
	// xor r18,r18,r5
	r18.u64 = r18.u64 ^ ctx.r5.u64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// subf r5,r9,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r9.s64;
	// rlwinm r10,r11,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r4,r4,5,0,26
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r3,r9,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r17,r8,r10
	r17.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// srawi r5,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r31.s32 >> 31;
	// srawi r31,r18,31
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r18.s32 >> 31;
	// subf r18,r4,r29
	r18.s64 = r29.s64 - ctx.r4.s64;
	// xor r29,r17,r3
	r29.u64 = r17.u64 ^ ctx.r3.u64;
	// lwz r17,84(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// xor r17,r17,r3
	r17.u64 = r17.u64 ^ ctx.r3.u64;
	// srawi r3,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r29.s32 >> 31;
	// srawi r29,r17,31
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r17.s32 >> 31;
	// or r17,r5,r31
	r17.u64 = ctx.r5.u64 | r31.u64;
	// and r10,r3,r10
	ctx.r10.u64 = ctx.r3.u64 & ctx.r10.u64;
	// andc r5,r6,r17
	ctx.r5.u64 = ctx.r6.u64 & ~r17.u64;
	// and r9,r29,r9
	ctx.r9.u64 = r29.u64 & ctx.r9.u64;
	// or r11,r5,r11
	r11.u64 = ctx.r5.u64 | r11.u64;
	// and r5,r31,r7
	ctx.r5.u64 = r31.u64 & ctx.r7.u64;
	// or r11,r11,r5
	r11.u64 = r11.u64 | ctx.r5.u64;
	// or r5,r3,r29
	ctx.r5.u64 = ctx.r3.u64 | r29.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// andc r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 & ~ctx.r5.u64;
	// rlwinm r11,r11,16,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// rlwinm r10,r10,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// rlwinm r10,r11,1,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x10000;
	// subf r9,r11,r18
	ctx.r9.s64 = r18.s64 - r11.s64;
	// subf r10,r10,r22
	ctx.r10.s64 = r22.s64 - ctx.r10.s64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// rlwinm r10,r10,0,0,16
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r10,r10,0,16,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826e6e68
	if (cr6.eq) goto loc_826E6E68;
	// srawi r9,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 16;
	// lhz r8,52(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 52);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// rlwinm r11,r28,5,0,26
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r4,r26,5,0,26
	ctx.r4.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 5) & 0xFFFFFFE0;
	// rotlwi r8,r8,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 5);
	// addi r3,r11,-4
	ctx.r3.s64 = r11.s64 + -4;
	// rlwinm r5,r27,5,0,26
	ctx.r5.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r4,r10
	r11.u64 = ctx.r4.u64 + ctx.r10.u64;
	// addi r31,r8,-4
	r31.s64 = ctx.r8.s64 + -4;
	// add r8,r5,r9
	ctx.r8.u64 = ctx.r5.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,-60
	cr6.compare<int32_t>(r11.s32, -60, xer);
	// bge cr6,0x826e6e2c
	if (!cr6.lt) goto loc_826E6E2C;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r10,r11,-60
	ctx.r10.s64 = r11.s64 + -60;
	// b 0x826e6e3c
	goto loc_826E6E3C;
loc_826E6DEC:
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x826e6dfc
	if (!cr6.eq) goto loc_826E6DFC;
	// mr r11,r17
	r11.u64 = r17.u64;
	// b 0x826e6ce4
	goto loc_826E6CE4;
loc_826E6DFC:
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// bne cr6,0x826e6ce4
	if (!cr6.eq) goto loc_826E6CE4;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// b 0x826e6ce4
	goto loc_826E6CE4;
loc_826E6E0C:
	// cmplwi cr6,r7,16384
	cr6.compare<uint32_t>(ctx.r7.u32, 16384, xer);
	// bne cr6,0x826e6e18
	if (!cr6.eq) goto loc_826E6E18;
	// mr r7,r17
	ctx.r7.u64 = r17.u64;
loc_826E6E18:
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// bne cr6,0x826e6e24
	if (!cr6.eq) goto loc_826E6E24;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
loc_826E6E24:
	// mr r11,r17
	r11.u64 = r17.u64;
	// b 0x826e6e74
	goto loc_826E6E74;
loc_826E6E2C:
	// cmpw cr6,r11,r3
	cr6.compare<int32_t>(r11.s32, ctx.r3.s32, xer);
	// ble cr6,0x826e6e3c
	if (!cr6.gt) goto loc_826E6E3C;
	// subf r11,r11,r3
	r11.s64 = ctx.r3.s64 - r11.s64;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_826E6E3C:
	// cmpwi cr6,r8,-60
	cr6.compare<int32_t>(ctx.r8.s32, -60, xer);
	// bge cr6,0x826e6e50
	if (!cr6.lt) goto loc_826E6E50;
	// subf r11,r8,r9
	r11.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r9,r11,-60
	ctx.r9.s64 = r11.s64 + -60;
	// b 0x826e6e60
	goto loc_826E6E60;
loc_826E6E50:
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// ble cr6,0x826e6e60
	if (!cr6.gt) goto loc_826E6E60;
	// subf r11,r8,r31
	r11.s64 = r31.s64 - ctx.r8.s64;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
loc_826E6E60:
	// rlwimi r10,r9,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_826E6E68:
	// lis r10,0
	ctx.r10.s64 = 0;
	// li r17,0
	r17.s64 = 0;
	// ori r18,r10,32768
	r18.u64 = ctx.r10.u64 | 32768;
loc_826E6E74:
	// rlwinm r10,r11,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// srawi r8,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 16;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826e6f74
	if (cr6.eq) goto loc_826E6F74;
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,32,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r23,r12
	r11.u64 = r23.u64 & r12.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// bne cr6,0x826e6f74
	if (!cr6.eq) goto loc_826E6F74;
	// rlwinm r11,r6,16,0,15
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 16) & 0xFFFF0000;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r6,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 16;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// subf r6,r6,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r6.s64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// srawi r9,r6,31
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r6.s32 >> 31;
	// xor r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r9.u64;
	// subf r9,r9,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r9.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// bgt cr6,0x826e6f08
	if (cr6.gt) goto loc_826E6F08;
	// rlwinm r11,r7,16,0,15
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 16) & 0xFFFF0000;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r7,16
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 16;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// srawi r9,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 31;
	// xor r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826e6f98
	if (!cr6.gt) goto loc_826E6F98;
loc_826E6F08:
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e6f34
	if (!cr0.lt) goto loc_826E6F34;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E6F34:
	// addi r11,r31,19
	r11.s64 = r31.s64 + 19;
	// lwz r25,188(r30)
	r25.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// subf r11,r11,r24
	r11.s64 = r24.s64 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// cmpwi cr6,r9,16384
	cr6.compare<int32_t>(ctx.r9.s32, 16384, xer);
	// bne cr6,0x826e6f9c
	if (!cr6.eq) goto loc_826E6F9C;
	// mr r8,r17
	ctx.r8.u64 = r17.u64;
	// mr r9,r17
	ctx.r9.u64 = r17.u64;
	// b 0x826e6f9c
	goto loc_826E6F9C;
loc_826E6F74:
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,32,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r23,r12
	r11.u64 = r23.u64 & r12.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x826e6f98
	if (cr6.eq) goto loc_826E6F98;
loc_826E6F88:
	// rlwinm r10,r24,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,16384
	r11.s64 = 16384;
	// stwx r11,r10,r25
	PPC_STORE_U32(ctx.r10.u32 + r25.u32, r11.u32);
	// b 0x826e7000
	goto loc_826E7000;
loc_826E6F98:
	// srawi r9,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 16;
loc_826E6F9C:
	// rldicl r6,r23,48,16
	ctx.r6.u64 = __builtin_rotateleft64(r23.u64, 48) & 0xFFFFFFFFFFFF;
	// lbz r11,30(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 30);
	// clrlwi r7,r23,16
	ctx.r7.u64 = r23.u32 & 0xFFFF;
	// lhz r10,62(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 62);
	// clrlwi r3,r6,16
	ctx.r3.u64 = ctx.r6.u32 & 0xFFFF;
	// lhz r6,68(r30)
	ctx.r6.u64 = PPC_LOAD_U16(r30.u32 + 68);
	// lhz r5,66(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 66);
	// rlwinm r4,r24,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r31,r6,16
	r31.u64 = __builtin_rotateleft32(ctx.r6.u32, 16);
	// slw r6,r7,r11
	ctx.r6.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (r11.u8 & 0x3F));
	// slw r7,r3,r11
	ctx.r7.u64 = r11.u8 & 0x20 ? 0 : (ctx.r3.u32 << (r11.u8 & 0x3F));
	// add r11,r6,r10
	r11.u64 = ctx.r6.u64 + ctx.r10.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// and r11,r11,r5
	r11.u64 = r11.u64 & ctx.r5.u64;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// lhz r11,64(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 64);
	// add r9,r7,r11
	ctx.r9.u64 = ctx.r7.u64 + r11.u64;
	// rotlwi r11,r11,16
	r11.u64 = __builtin_rotateleft32(r11.u32, 16);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// rlwinm r9,r9,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 16) & 0xFFFF0000;
	// and r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 & r31.u64;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwx r11,r4,r25
	PPC_STORE_U32(ctx.r4.u32 + r25.u32, r11.u32);
loc_826E7000:
	// rldicl r10,r23,32,32
	ctx.r10.u64 = __builtin_rotateleft64(r23.u64, 32) & 0xFFFFFFFF;
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// clrlwi r24,r10,31
	r24.u64 = ctx.r10.u32 & 0x1;
	// rldicl r10,r23,24,40
	ctx.r10.u64 = __builtin_rotateleft64(r23.u64, 24) & 0xFFFFFF;
	// mr r11,r24
	r11.u64 = r24.u64;
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stb r11,5(r20)
	PPC_STORE_U8(r20.u32 + 5, r11.u8);
	// bne cr6,0x826e7354
	if (!cr6.eq) goto loc_826E7354;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// oris r10,r10,16384
	ctx.r10.u64 = ctx.r10.u64 | 1073741824;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lbz r11,27(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 27);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e7308
	if (cr6.eq) goto loc_826E7308;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x826e79ec
	if (cr6.eq) goto loc_826E79EC;
	// lbz r11,1181(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1181);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e7088
	if (cr6.eq) goto loc_826E7088;
	// rlwinm r10,r10,20,12,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFFFFF;
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e7074
	if (cr6.eq) goto loc_826E7074;
	// lbz r11,1182(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// b 0x826e72e8
	goto loc_826E72E8;
loc_826E7074:
	// lbz r11,1180(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1180);
	// lbz r10,1185(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826e72e8
	goto loc_826E72E8;
loc_826E7088:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r28,r17
	r28.u64 = r17.u64;
	// lbz r11,1186(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826e717c
	if (cr6.eq) goto loc_826E717C;
	// li r29,1
	r29.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826e7108
	if (!cr6.lt) goto loc_826E7108;
loc_826E70B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e7108
	if (cr6.eq) goto loc_826E7108;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e70f8
	if (!cr0.lt) goto loc_826E70F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E70F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e70b0
	if (cr6.gt) goto loc_826E70B0;
loc_826E7108:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e7144
	if (!cr0.lt) goto loc_826E7144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E7144:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826e7160
	if (cr6.eq) goto loc_826E7160;
	// lbz r11,1182(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// b 0x826e72ec
	goto loc_826E72EC;
loc_826E7160:
	// lbz r11,1180(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1180);
	// lbz r10,1185(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// b 0x826e72ec
	goto loc_826E72EC;
loc_826E717C:
	// li r29,3
	r29.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826e71e0
	if (!cr6.lt) goto loc_826E71E0;
loc_826E7188:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e71e0
	if (cr6.eq) goto loc_826E71E0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e71d0
	if (!cr0.lt) goto loc_826E71D0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E71D0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e7188
	if (cr6.gt) goto loc_826E7188;
loc_826E71E0:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e721c
	if (!cr0.lt) goto loc_826E721C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E721C:
	// cmpwi cr6,r29,7
	cr6.compare<int32_t>(r29.s32, 7, xer);
	// bne cr6,0x826e72dc
	if (!cr6.eq) goto loc_826E72DC;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r29,5
	r29.s64 = 5;
	// mr r28,r17
	r28.u64 = r17.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826e7298
	if (!cr6.lt) goto loc_826E7298;
loc_826E7240:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e7298
	if (cr6.eq) goto loc_826E7298;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e7288
	if (!cr0.lt) goto loc_826E7288;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E7288:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e7240
	if (cr6.gt) goto loc_826E7240;
loc_826E7298:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e72d4
	if (!cr0.lt) goto loc_826E72D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E72D4:
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x826e72e4
	goto loc_826E72E4;
loc_826E72DC:
	// lbz r11,1180(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1180);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
loc_826E72E4:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826E72E8:
	// addi r10,r11,255
	ctx.r10.s64 = r11.s64 + 255;
loc_826E72EC:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stb r10,4(r11)
	PPC_STORE_U8(r11.u32 + 4, ctx.r10.u8);
	// clrlwi r11,r10,24
	r11.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x826e77cc
	if (cr6.lt) goto loc_826E77CC;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// bgt cr6,0x826e77cc
	if (cr6.gt) goto loc_826E77CC;
loc_826E7308:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x826e79ec
	if (cr6.eq) goto loc_826E79EC;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e733c
	if (!cr0.lt) goto loc_826E733C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E733C:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r10,r31,24
	ctx.r10.u64 = r31.u32 & 0xFF;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwimi r9,r10,3,27,28
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 3) & 0x18) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// b 0x826e79ec
	goto loc_826E79EC;
loc_826E7354:
	// lbz r10,29(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 29);
	// lbz r27,28(r30)
	r27.u64 = PPC_LOAD_U8(r30.u32 + 28);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826e7374
	if (cr6.eq) goto loc_826E7374;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// li r26,1
	r26.s64 = 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e7378
	if (cr6.eq) goto loc_826E7378;
loc_826E7374:
	// mr r26,r17
	r26.u64 = r17.u64;
loc_826E7378:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x826e73c0
	if (cr6.eq) goto loc_826E73C0;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e73ac
	if (!cr0.lt) goto loc_826E73AC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E73AC:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r10,r31,24
	ctx.r10.u64 = r31.u32 & 0xFF;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwimi r9,r10,3,27,28
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 3) & 0x18) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_826E73C0:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,196(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 196);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,8,56
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e74a8
	if (cr6.lt) goto loc_826E74A8;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826e74a0
	if (!cr6.lt) goto loc_826E74A0;
loc_826E7408:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826e7434
	if (cr6.lt) goto loc_826E7434;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826e7408
	if (cr6.eq) goto loc_826E7408;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e74e8
	goto loc_826E74E8;
loc_826E7434:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826E74A0:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e74e8
	goto loc_826E74E8;
loc_826E74A8:
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826E74B4:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r18
	r11.u64 = r29.u64 + r18.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e74b4
	if (cr6.lt) goto loc_826E74B4;
loc_826E74E8:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r21,r29
	r21.u64 = r29.u64;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826e77cc
	if (!cr6.eq) goto loc_826E77CC;
	// lbz r11,27(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 27);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e77d8
	if (cr6.eq) goto loc_826E77D8;
	// lbz r11,1181(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1181);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e7550
	if (cr6.eq) goto loc_826E7550;
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// rlwinm r10,r10,20,12,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFFFFF;
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e753c
	if (cr6.eq) goto loc_826E753C;
	// lbz r11,1182(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// b 0x826e77b0
	goto loc_826E77B0;
loc_826E753C:
	// lbz r11,1180(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1180);
	// lbz r10,1185(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826e77b0
	goto loc_826E77B0;
loc_826E7550:
	// lbz r11,1186(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1186);
	// mr r28,r17
	r28.u64 = r17.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826e7640
	if (cr6.eq) goto loc_826E7640;
	// li r29,1
	r29.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826e75cc
	if (!cr6.lt) goto loc_826E75CC;
loc_826E7574:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e75cc
	if (cr6.eq) goto loc_826E75CC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e75bc
	if (!cr0.lt) goto loc_826E75BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E75BC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e7574
	if (cr6.gt) goto loc_826E7574;
loc_826E75CC:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e7608
	if (!cr0.lt) goto loc_826E7608;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E7608:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826e7624
	if (cr6.eq) goto loc_826E7624;
	// lbz r11,1182(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1182);
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826e77b4
	goto loc_826E77B4;
loc_826E7624:
	// lbz r11,1180(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1180);
	// lbz r10,1185(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826e77b4
	goto loc_826E77B4;
loc_826E7640:
	// li r29,3
	r29.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826e76a4
	if (!cr6.lt) goto loc_826E76A4;
loc_826E764C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e76a4
	if (cr6.eq) goto loc_826E76A4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e7694
	if (!cr0.lt) goto loc_826E7694;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E7694:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e764c
	if (cr6.gt) goto loc_826E764C;
loc_826E76A4:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e76e0
	if (!cr0.lt) goto loc_826E76E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E76E0:
	// cmpwi cr6,r29,7
	cr6.compare<int32_t>(r29.s32, 7, xer);
	// bne cr6,0x826e77a0
	if (!cr6.eq) goto loc_826E77A0;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r29,5
	r29.s64 = 5;
	// mr r28,r17
	r28.u64 = r17.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826e775c
	if (!cr6.lt) goto loc_826E775C;
loc_826E7704:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e775c
	if (cr6.eq) goto loc_826E775C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e774c
	if (!cr0.lt) goto loc_826E774C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E774C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e7704
	if (cr6.gt) goto loc_826E7704;
loc_826E775C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e7798
	if (!cr0.lt) goto loc_826E7798;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E7798:
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x826e77a8
	goto loc_826E77A8;
loc_826E77A0:
	// lbz r11,1180(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1180);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
loc_826E77A8:
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826E77B0:
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
loc_826E77B4:
	// stb r11,4(r25)
	PPC_STORE_U8(r25.u32 + 4, r11.u8);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x826e77cc
	if (cr6.lt) goto loc_826E77CC;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// ble cr6,0x826e77dc
	if (!cr6.gt) goto loc_826E77DC;
loc_826E77CC:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// b 0x8239bd10
	return;
loc_826E77D8:
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_826E77DC:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// rlwinm r11,r11,0,2,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// beq cr6,0x826e7864
	if (cr6.eq) goto loc_826E7864;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e781c
	if (!cr0.lt) goto loc_826E781C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E781C:
	// mr r11,r29
	r11.u64 = r29.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x826e7858
	if (cr6.eq) goto loc_826E7858;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e7854
	if (!cr0.lt) goto loc_826E7854;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E7854:
	// add r11,r29,r31
	r11.u64 = r29.u64 + r31.u64;
loc_826E7858:
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// rlwimi r10,r11,22,8,9
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 22) & 0xC00000) | (ctx.r10.u64 & 0xFFFFFFFFFF3FFFFF);
	// stw r10,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r10.u32);
loc_826E7864:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826e79ec
	if (cr6.eq) goto loc_826E79EC;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,200(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 200);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,8,56
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e7954
	if (cr6.lt) goto loc_826E7954;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826e794c
	if (!cr6.lt) goto loc_826E794C;
loc_826E78B4:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826e78e0
	if (cr6.lt) goto loc_826E78E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826e78b4
	if (cr6.eq) goto loc_826E78B4;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e7994
	goto loc_826E7994;
loc_826E78E0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826E794C:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e7994
	goto loc_826E7994;
loc_826E7954:
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826E7960:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r18
	r11.u64 = r29.u64 + r18.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e7960
	if (cr6.lt) goto loc_826E7960;
loc_826E7994:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826e77cc
	if (!cr6.eq) goto loc_826E77CC;
	// cmpwi cr6,r29,8
	cr6.compare<int32_t>(r29.s32, 8, xer);
	// li r9,1
	ctx.r9.s64 = 1;
	// blt cr6,0x826e79b4
	if (cr6.lt) goto loc_826E79B4;
	// mr r9,r17
	ctx.r9.u64 = r17.u64;
loc_826E79B4:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// clrlwi r10,r29,29
	ctx.r10.u64 = r29.u32 & 0x7;
	// rlwimi r11,r9,28,3,3
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 28) & 0x10000000) | (r11.u64 & 0xFFFFFFFFEFFFFFFF);
	// add r9,r10,r30
	ctx.r9.u64 = ctx.r10.u64 + r30.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// lbz r9,516(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 516);
	// rlwimi r11,r9,24,5,7
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 24) & 0x7000000) | (r11.u64 & 0xFFFFFFFFF8FFFFFF);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// lbz r11,524(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 524);
	// rlwimi r9,r11,20,10,11
	ctx.r9.u64 = (__builtin_rotateleft32(r11.u32, 20) & 0x300000) | (ctx.r9.u64 & 0xFFFFFFFFFFCFFFFF);
	// rlwinm r11,r9,0,5,3
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFF7FFFFFF;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
loc_826E79EC:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// stb r21,5(r10)
	PPC_STORE_U8(ctx.r10.u32 + 5, r21.u8);
loc_826E79F8:
	// lwz r10,0(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// clrlwi r11,r24,24
	r11.u64 = r24.u32 & 0xFF;
	// lwz r9,188(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// mr r25,r24
	r25.u64 = r24.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// lhz r8,50(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// rotlwi r8,r8,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 2);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// stw r10,-4(r9)
	PPC_STORE_U32(ctx.r9.u32 + -4, ctx.r10.u32);
	// stb r11,13(r20)
	PPC_STORE_U8(r20.u32 + 13, r11.u8);
	// stb r11,21(r20)
	PPC_STORE_U8(r20.u32 + 21, r11.u8);
	// stb r11,29(r20)
	PPC_STORE_U8(r20.u32 + 29, r11.u8);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826e77cc
	if (!cr6.eq) goto loc_826E77CC;
	// mr r31,r17
	r31.u64 = r17.u64;
	// b 0x826e902c
	goto loc_826E902C;
loc_826E7A54:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// beq cr6,0x826e7f44
	if (cr6.eq) goto loc_826E7F44;
	// li r29,0
	r29.s64 = 0;
	// addi r21,r30,36
	r21.s64 = r30.s64 + 36;
	// mr r25,r29
	r25.u64 = r29.u64;
	// stb r29,5(r23)
	PPC_STORE_U8(r23.u32 + 5, r29.u8);
loc_826E7A70:
	// lhz r11,18(r14)
	r11.u64 = PPC_LOAD_U16(r14.u32 + 18);
	// clrlwi r31,r25,31
	r31.u64 = r25.u32 & 0x1;
	// lhz r27,16(r14)
	r27.u64 = PPC_LOAD_U16(r14.u32 + 16);
	// srawi r26,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	r26.s64 = r25.s32 >> 1;
	// add r24,r11,r31
	r24.u64 = r11.u64 + r31.u64;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lhz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U16(r21.u32 + 0);
	// lwz r9,0(r14)
	ctx.r9.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// and r11,r27,r11
	r11.u64 = r27.u64 & r11.u64;
	// lhz r28,50(r30)
	r28.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// add r22,r10,r9
	r22.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r23,188(r30)
	r23.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// add. r10,r11,r26
	ctx.r10.u64 = r11.u64 + r26.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r22,-1
	r11.s64 = r22.s64 + -1;
	// bne 0x826e7bac
	if (!cr0.eq) goto loc_826E7BAC;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// beq cr6,0x826e7ba0
	if (cr6.eq) goto loc_826E7BA0;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r23
	r11.u64 = PPC_LOAD_U32(r11.u32 + r23.u32);
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// beq cr6,0x826e7ba0
	if (cr6.eq) goto loc_826E7BA0;
	// lwz r9,16(r14)
	ctx.r9.u64 = PPC_LOAD_U32(r14.u32 + 16);
	// rlwinm r10,r25,15,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 15) & 0x10000;
	// lwz r8,1392(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 1392);
	// rlwinm r7,r11,1,15,15
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x10000;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r6,1396(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 1396);
	// subf r10,r7,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r7.s64;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r9,r9,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r7,r11,r6
	ctx.r7.s64 = ctx.r6.s64 - r11.s64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r9,r9,0,0,16
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r9,r9,0,16,0
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826e7ba0
	if (cr6.eq) goto loc_826E7BA0;
	// clrlwi r8,r26,31
	ctx.r8.u64 = r26.u32 & 0x1;
	// lhz r7,52(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 52);
	// srawi r9,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 16;
	// add r5,r8,r27
	ctx.r5.u64 = ctx.r8.u64 + r27.u64;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// rlwinm r11,r28,5,0,26
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 5) & 0xFFFFFFE0;
	// rotlwi r8,r7,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r7.u32, 5);
	// rlwinm r6,r24,5,0,26
	ctx.r6.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r7,r5,5,0,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r5,r11,-4
	ctx.r5.s64 = r11.s64 + -4;
	// add r11,r6,r10
	r11.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r4,r8,-4
	ctx.r4.s64 = ctx.r8.s64 + -4;
	// add r8,r7,r9
	ctx.r8.u64 = ctx.r7.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,-28
	cr6.compare<int32_t>(r11.s32, -28, xer);
	// bge cr6,0x826e7b5c
	if (!cr6.lt) goto loc_826E7B5C;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r10,r11,-28
	ctx.r10.s64 = r11.s64 + -28;
	// b 0x826e7b6c
	goto loc_826E7B6C;
loc_826E7B5C:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// ble cr6,0x826e7b6c
	if (!cr6.gt) goto loc_826E7B6C;
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_826E7B6C:
	// cmpwi cr6,r8,-28
	cr6.compare<int32_t>(ctx.r8.s32, -28, xer);
	// bge cr6,0x826e7b8c
	if (!cr6.lt) goto loc_826E7B8C;
	// subf r11,r8,r9
	r11.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r9,r11,-28
	ctx.r9.s64 = r11.s64 + -28;
	// rlwimi r10,r9,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// srawi r8,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 16;
	// b 0x826e7eb8
	goto loc_826E7EB8;
loc_826E7B8C:
	// cmpw cr6,r8,r4
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r4.s32, xer);
	// ble cr6,0x826e7b9c
	if (!cr6.gt) goto loc_826E7B9C;
	// subf r11,r8,r4
	r11.s64 = ctx.r4.s64 - ctx.r8.s64;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
loc_826E7B9C:
	// rlwimi r10,r9,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
loc_826E7BA0:
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// srawi r8,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 16;
	// b 0x826e7eb8
	goto loc_826E7EB8;
loc_826E7BAC:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// beq cr6,0x826e7bbc
	if (cr6.eq) goto loc_826E7BBC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r11,r23
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + r23.u32);
loc_826E7BBC:
	// lwz r10,1416(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 1416);
	// subf r11,r28,r22
	r11.s64 = r22.s64 - r28.s64;
	// rlwinm r9,r6,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 31) & 0x7FFFFFFF;
	// xor r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r6.u64;
	// lbzx r10,r10,r25
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r25.u32);
	// rlwinm r9,r9,0,17,17
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x4000;
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r11,r23
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + r23.u32);
	// lwzx r11,r10,r23
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r23.u32);
	// rlwinm r10,r7,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r8,r11,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// xor r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r7.u64;
	// xor r5,r8,r11
	ctx.r5.u64 = ctx.r8.u64 ^ r11.u64;
	// rlwinm r8,r10,0,17,17
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4000;
	// rlwinm r10,r5,0,17,17
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x4000;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826e7c28
	if (!cr6.gt) goto loc_826E7C28;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// bne cr6,0x826e7d64
	if (!cr6.eq) goto loc_826E7D64;
	// cmplwi cr6,r7,16384
	cr6.compare<uint32_t>(ctx.r7.u32, 16384, xer);
	// bne cr6,0x826e7d44
	if (!cr6.eq) goto loc_826E7D44;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
loc_826E7C28:
	// lwz r3,16(r14)
	ctx.r3.u64 = PPC_LOAD_U32(r14.u32 + 16);
	// rlwinm r4,r25,15,15,15
	ctx.r4.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 15) & 0x10000;
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// lwz r18,1392(r30)
	r18.u64 = PPC_LOAD_U32(r30.u32 + 1392);
	// subf r29,r6,r11
	r29.s64 = r11.s64 - ctx.r6.s64;
	// lwz r17,1396(r30)
	r17.u64 = PPC_LOAD_U32(r30.u32 + 1396);
	// subf r14,r7,r6
	r14.s64 = ctx.r6.s64 - ctx.r7.s64;
	// rlwinm r9,r7,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r8,r6,16,0,15
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 16) & 0xFFFF0000;
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// xor r3,r29,r5
	ctx.r3.u64 = r29.u64 ^ ctx.r5.u64;
	// xor r14,r14,r5
	r14.u64 = r14.u64 ^ ctx.r5.u64;
	// subf r5,r9,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r9.s64;
	// rlwinm r10,r11,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// add r31,r4,r31
	r31.u64 = ctx.r4.u64 + r31.u64;
	// subf r4,r9,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r29,r8,r10
	r29.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// srawi r5,r3,31
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r3.s32 >> 31;
	// srawi r3,r14,31
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r14.s32 >> 31;
	// lwz r14,84(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// xor r29,r29,r4
	r29.u64 = r29.u64 ^ ctx.r4.u64;
	// xor r14,r14,r4
	r14.u64 = r14.u64 ^ ctx.r4.u64;
	// srawi r4,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r29.s32 >> 31;
	// srawi r29,r14,31
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r14.s32 >> 31;
	// or r14,r5,r3
	r14.u64 = ctx.r5.u64 | ctx.r3.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// andc r5,r6,r14
	ctx.r5.u64 = ctx.r6.u64 & ~r14.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// or r11,r5,r11
	r11.u64 = ctx.r5.u64 | r11.u64;
	// and r5,r3,r7
	ctx.r5.u64 = ctx.r3.u64 & ctx.r7.u64;
	// and r9,r29,r9
	ctx.r9.u64 = r29.u64 & ctx.r9.u64;
	// or r11,r11,r5
	r11.u64 = r11.u64 | ctx.r5.u64;
	// or r5,r4,r29
	ctx.r5.u64 = ctx.r4.u64 | r29.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// andc r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 & ~ctx.r5.u64;
	// rlwinm r11,r11,16,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// rlwinm r31,r31,5,0,26
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 5) & 0xFFFFFFE0;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// rlwinm r10,r10,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// rlwinm r10,r11,1,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x10000;
	// subf r9,r11,r17
	ctx.r9.s64 = r17.s64 - r11.s64;
	// subf r10,r10,r18
	ctx.r10.s64 = r18.s64 - ctx.r10.s64;
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - r31.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// rlwinm r10,r10,0,0,16
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r10,r10,0,16,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826e7dc0
	if (cr6.eq) goto loc_826E7DC0;
	// clrlwi r8,r26,31
	ctx.r8.u64 = r26.u32 & 0x1;
	// lhz r5,52(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 52);
	// srawi r9,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 16;
	// add r3,r8,r27
	ctx.r3.u64 = ctx.r8.u64 + r27.u64;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// rlwinm r11,r28,5,0,26
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 5) & 0xFFFFFFE0;
	// rotlwi r8,r5,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r5.u32, 5);
	// rlwinm r4,r24,5,0,26
	ctx.r4.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r5,r3,5,0,26
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r3,r11,-4
	ctx.r3.s64 = r11.s64 + -4;
	// add r11,r4,r10
	r11.u64 = ctx.r4.u64 + ctx.r10.u64;
	// addi r31,r8,-4
	r31.s64 = ctx.r8.s64 + -4;
	// add r8,r5,r9
	ctx.r8.u64 = ctx.r5.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,-28
	cr6.compare<int32_t>(r11.s32, -28, xer);
	// bge cr6,0x826e7d84
	if (!cr6.lt) goto loc_826E7D84;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r10,r11,-28
	ctx.r10.s64 = r11.s64 + -28;
	// b 0x826e7d94
	goto loc_826E7D94;
loc_826E7D44:
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x826e7d54
	if (!cr6.eq) goto loc_826E7D54;
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x826e7c28
	goto loc_826E7C28;
loc_826E7D54:
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// bne cr6,0x826e7c28
	if (!cr6.eq) goto loc_826E7C28;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// b 0x826e7c28
	goto loc_826E7C28;
loc_826E7D64:
	// cmplwi cr6,r7,16384
	cr6.compare<uint32_t>(ctx.r7.u32, 16384, xer);
	// bne cr6,0x826e7d70
	if (!cr6.eq) goto loc_826E7D70;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
loc_826E7D70:
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// bne cr6,0x826e7d7c
	if (!cr6.eq) goto loc_826E7D7C;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
loc_826E7D7C:
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x826e7dc8
	goto loc_826E7DC8;
loc_826E7D84:
	// cmpw cr6,r11,r3
	cr6.compare<int32_t>(r11.s32, ctx.r3.s32, xer);
	// ble cr6,0x826e7d94
	if (!cr6.gt) goto loc_826E7D94;
	// subf r11,r11,r3
	r11.s64 = ctx.r3.s64 - r11.s64;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_826E7D94:
	// cmpwi cr6,r8,-28
	cr6.compare<int32_t>(ctx.r8.s32, -28, xer);
	// bge cr6,0x826e7da8
	if (!cr6.lt) goto loc_826E7DA8;
	// subf r11,r8,r9
	r11.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r9,r11,-28
	ctx.r9.s64 = r11.s64 + -28;
	// b 0x826e7db8
	goto loc_826E7DB8;
loc_826E7DA8:
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// ble cr6,0x826e7db8
	if (!cr6.gt) goto loc_826E7DB8;
	// subf r11,r8,r31
	r11.s64 = r31.s64 - ctx.r8.s64;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
loc_826E7DB8:
	// rlwimi r10,r9,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_826E7DC0:
	// lwz r14,452(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// li r29,0
	r29.s64 = 0;
loc_826E7DC8:
	// rlwinm r10,r11,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// srawi r8,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 16;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x826e7eb4
	if (cr6.eq) goto loc_826E7EB4;
	// rlwinm r11,r6,16,0,15
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 16) & 0xFFFF0000;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r6,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 16;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// subf r6,r6,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r6.s64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// srawi r9,r6,31
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r6.s32 >> 31;
	// xor r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r9.u64;
	// subf r9,r9,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r9.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// bgt cr6,0x826e7e48
	if (cr6.gt) goto loc_826E7E48;
	// rlwinm r11,r7,16,0,15
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 16) & 0xFFFF0000;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r7,16
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 16;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// srawi r9,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 31;
	// xor r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826e7eb4
	if (!cr6.gt) goto loc_826E7EB4;
loc_826E7E48:
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e7e74
	if (!cr0.lt) goto loc_826E7E74;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E7E74:
	// addi r11,r31,19
	r11.s64 = r31.s64 + 19;
	// lwz r23,188(r30)
	r23.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// subf r11,r11,r22
	r11.s64 = r22.s64 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// cmpwi cr6,r9,16384
	cr6.compare<int32_t>(ctx.r9.s32, 16384, xer);
	// bne cr6,0x826e7eb8
	if (!cr6.eq) goto loc_826E7EB8;
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// b 0x826e7eb8
	goto loc_826E7EB8;
loc_826E7EB4:
	// srawi r9,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 16;
loc_826E7EB8:
	// lbz r10,30(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 30);
	// rlwinm r5,r22,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r7,68(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 68);
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// lhz r11,62(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 62);
	// addi r21,r21,2
	r21.s64 = r21.s64 + 2;
	// rotlwi r4,r7,16
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r7.u32, 16);
	// lhz r6,66(r30)
	ctx.r6.u64 = PPC_LOAD_U16(r30.u32 + 66);
	// cmpwi cr6,r25,4
	cr6.compare<int32_t>(r25.s32, 4, xer);
	// slw r10,r29,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r10.u8 & 0x3F));
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + r11.u64;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// and r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 & ctx.r6.u64;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// lhz r11,64(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 64);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// rotlwi r11,r11,16
	r11.u64 = __builtin_rotateleft32(r11.u32, 16);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// rlwinm r10,r10,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// and r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 & ctx.r4.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwx r11,r5,r23
	PPC_STORE_U32(ctx.r5.u32 + r23.u32, r11.u32);
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// stb r29,5(r20)
	PPC_STORE_U8(r20.u32 + 5, r29.u8);
	// addi r20,r20,8
	r20.s64 = r20.s64 + 8;
	// blt cr6,0x826e7a70
	if (cr6.lt) goto loc_826E7A70;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r25,r29
	r25.u64 = r29.u64;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826e77cc
	if (!cr6.eq) goto loc_826E77CC;
	// mr r31,r29
	r31.u64 = r29.u64;
	// b 0x826e9028
	goto loc_826E9028;
loc_826E7F44:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,196(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 196);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,8,56
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r15,r11
	r15.s64 = r11.s16;
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// blt cr6,0x826e802c
	if (cr6.lt) goto loc_826E802C;
	// clrlwi r11,r15,28
	r11.u64 = r15.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826e8024
	if (!cr6.lt) goto loc_826E8024;
loc_826E7F8C:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826e7fb8
	if (cr6.lt) goto loc_826E7FB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826e7f8c
	if (cr6.eq) goto loc_826E7F8C;
	// srawi r15,r15,4
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0xF) != 0);
	r15.s64 = r15.s32 >> 4;
	// b 0x826e8074
	goto loc_826E8074;
loc_826E7FB8:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826E8024:
	// srawi r15,r15,4
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0xF) != 0);
	r15.s64 = r15.s32 >> 4;
	// b 0x826e8074
	goto loc_826E8074;
loc_826E802C:
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r27,r11,32768
	r27.u64 = r11.u64 | 32768;
loc_826E8040:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r15
	r29.u64 = r11.u64 + r15.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r27
	r11.u64 = r29.u64 + r27.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r15,r11
	r15.s64 = r11.s16;
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// blt cr6,0x826e8040
	if (cr6.lt) goto loc_826E8040;
loc_826E8074:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// stw r15,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r15.u32);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826e77cc
	if (!cr6.eq) goto loc_826E77CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r20,0
	r20.s64 = 0;
	// mr r18,r15
	r18.u64 = r15.u64;
	// mr r16,r20
	r16.u64 = r20.u64;
	// mr r14,r20
	r14.u64 = r20.u64;
	// mr r17,r20
	r17.u64 = r20.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r19,r30,36
	r19.s64 = r30.s64 + 36;
	// stw r20,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r20.u32);
	// rlwinm r10,r10,0,2,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r16,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r16.u32);
	// stw r14,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r14.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_826E80BC:
	// clrlwi r11,r18,31
	r11.u64 = r18.u32 & 0x1;
	// li r25,0
	r25.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826e84b0
	if (cr6.eq) goto loc_826E84B0;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,176(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 176);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,10,54
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 10) & 0x3FF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e81b4
	if (cr6.lt) goto loc_826E81B4;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826e81ac
	if (!cr6.lt) goto loc_826E81AC;
loc_826E8114:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826e8140
	if (cr6.lt) goto loc_826E8140;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826e8114
	if (cr6.eq) goto loc_826E8114;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e81fc
	goto loc_826E81FC;
loc_826E8140:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826E81AC:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e81fc
	goto loc_826E81FC;
loc_826E81B4:
	// li r4,10
	ctx.r4.s64 = 10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r27,r11,32768
	r27.u64 = r11.u64 | 32768;
loc_826E81C8:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r27
	r11.u64 = r29.u64 + r27.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e81c8
	if (cr6.lt) goto loc_826E81C8;
loc_826E81FC:
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpwi cr6,r11,37
	cr6.compare<int32_t>(r11.s32, 37, xer);
	// bge cr6,0x826e8210
	if (!cr6.lt) goto loc_826E8210;
	// li r10,0
	ctx.r10.s64 = 0;
loc_826E8210:
	// extsw r22,r10
	r22.s64 = ctx.r10.s32;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826e8220
	if (cr6.eq) goto loc_826E8220;
	// addi r11,r11,-37
	r11.s64 = r11.s64 + -37;
loc_826E8220:
	// li r23,0
	r23.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826e849c
	if (cr6.eq) goto loc_826E849C;
	// cmpwi cr6,r11,35
	cr6.compare<int32_t>(r11.s32, 35, xer);
	// bge cr6,0x826e8380
	if (!cr6.lt) goto loc_826E8380;
	// lwz r10,12(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// srawi r9,r11,24
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 24;
	// srawi r8,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r8.s64 = r11.s32 >> 4;
	// clrlwi r10,r11,28
	ctx.r10.u64 = r11.u32 & 0xF;
	// clrlwi r27,r8,28
	r27.u64 = ctx.r8.u32 & 0xF;
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// add r29,r10,r27
	r29.u64 = ctx.r10.u64 + r27.u64;
	// clrlwi r26,r9,24
	r26.u64 = ctx.r9.u32 & 0xFF;
	// clrlwi r25,r8,24
	r25.u64 = ctx.r8.u32 & 0xFF;
	// clrlwi r24,r11,24
	r24.u64 = r11.u32 & 0xFF;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x826e832c
	if (!cr6.gt) goto loc_826E832C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// beq cr6,0x826e832c
	if (cr6.eq) goto loc_826E832C;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x826e82e8
	if (!cr6.gt) goto loc_826E82E8;
loc_826E828C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e82e8
	if (cr6.eq) goto loc_826E82E8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e82d8
	if (!cr0.lt) goto loc_826E82D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E82D8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e828c
	if (cr6.gt) goto loc_826E828C;
loc_826E82E8:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e8324
	if (!cr0.lt) goto loc_826E8324;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8324:
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// b 0x826e8330
	goto loc_826E8330;
loc_826E832C:
	// li r10,0
	ctx.r10.s64 = 0;
loc_826E8330:
	// and r11,r10,r26
	r11.u64 = ctx.r10.u64 & r26.u64;
	// rlwinm r8,r24,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r11,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// sraw r10,r10,r27
	temp.u32 = r27.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// clrlwi r7,r11,31
	ctx.r7.u64 = r11.u32 & 0x1;
	// rlwinm r8,r9,15,0,16
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 15) & 0xFFFF8000;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// neg r11,r7
	r11.s64 = -ctx.r7.s64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// rlwinm r7,r11,16,0,15
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// rlwinm r11,r11,16,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// b 0x826e8498
	goto loc_826E8498;
loc_826E8380:
	// cmpwi cr6,r11,36
	cr6.compare<int32_t>(r11.s32, 36, xer);
	// bne cr6,0x826e8394
	if (!cr6.eq) goto loc_826E8394;
	// li r23,1
	r23.s64 = 1;
	// li r11,0
	r11.s64 = 0;
	// b 0x826e849c
	goto loc_826E849C;
loc_826E8394:
	// lbz r11,30(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 30);
	// li r28,0
	r28.s64 = 0;
	// lhz r10,70(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 70);
	// lhz r9,72(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 72);
	// subf r26,r11,r10
	r26.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r27,r11,r9
	r27.s64 = ctx.r9.s64 - r11.s64;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// add r29,r27,r26
	r29.u64 = r27.u64 + r26.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x826e83c8
	if (!cr6.eq) goto loc_826E83C8;
	// li r11,0
	r11.s64 = 0;
	// b 0x826e846c
	goto loc_826E846C;
loc_826E83C8:
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x826e842c
	if (!cr6.gt) goto loc_826E842C;
loc_826E83D0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e842c
	if (cr6.eq) goto loc_826E842C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e841c
	if (!cr0.lt) goto loc_826E841C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E841C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e83d0
	if (cr6.gt) goto loc_826E83D0;
loc_826E842C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e8468
	if (!cr0.lt) goto loc_826E8468;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8468:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_826E846C:
	// li r9,1
	ctx.r9.s64 = 1;
	// rlwinm r8,r11,16,0,15
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// sraw r11,r11,r27
	temp.u32 = r27.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// slw r10,r9,r26
	ctx.r10.u64 = r26.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r26.u8 & 0x3F));
	// slw r9,r9,r27
	ctx.r9.u64 = r27.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r27.u8 & 0x3F));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// rlwinm r9,r9,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 16) & 0xFFFF0000;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// addis r10,r9,-1
	ctx.r10.s64 = ctx.r9.s64 + -65536;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
loc_826E8498:
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
loc_826E849C:
	// rldicr r10,r22,8,55
	ctx.r10.u64 = __builtin_rotateleft64(r22.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// or r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 | r23.u64;
	// rldicr r10,r10,32,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// or r25,r10,r11
	r25.u64 = ctx.r10.u64 | r11.u64;
loc_826E84B0:
	// rldicl r11,r25,32,32
	r11.u64 = __builtin_rotateleft64(r25.u64, 32) & 0xFFFFFFFF;
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r7,452(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// clrlwi r24,r20,31
	r24.u64 = r20.u32 & 0x1;
	// clrlwi r22,r11,31
	r22.u64 = r11.u32 & 0x1;
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// rldicl r11,r25,24,40
	r11.u64 = __builtin_rotateleft64(r25.u64, 24) & 0xFFFFFF;
	// srawi r18,r18,1
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x1) != 0);
	r18.s64 = r18.s32 >> 1;
	// clrlwi r21,r11,31
	r21.u64 = r11.u32 & 0x1;
	// srawi r23,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r23.s64 = r20.s32 >> 1;
	// stb r22,5(r10)
	PPC_STORE_U8(ctx.r10.u32 + 5, r22.u8);
	// lhz r11,0(r19)
	r11.u64 = PPC_LOAD_U16(r19.u32 + 0);
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r28,188(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// add r27,r11,r10
	r27.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,16(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 16);
	// lhz r10,18(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 18);
	// and r11,r11,r9
	r11.u64 = r11.u64 & ctx.r9.u64;
	// lhz r9,50(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// add r26,r10,r24
	r26.u64 = ctx.r10.u64 + r24.u64;
	// add. r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r11,r27,-1
	r11.s64 = r27.s64 + -1;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bne 0x826e85a4
	if (!cr0.eq) goto loc_826E85A4;
	// li r3,0
	ctx.r3.s64 = 0;
	// beq cr6,0x826e8584
	if (cr6.eq) goto loc_826E8584;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r11,r28
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + r28.u32);
	// cmpwi cr6,r6,16384
	cr6.compare<int32_t>(ctx.r6.s32, 16384, xer);
	// beq cr6,0x826e8584
	if (cr6.eq) goto loc_826E8584;
	// rlwinm r11,r20,15,15,15
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 15) & 0x10000;
	// lwz r10,16(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// lwz r8,1392(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 1392);
	// rlwinm r9,r6,1,15,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x10000;
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// lwz r5,1396(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 1396);
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r9.s64;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// subf r9,r6,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r6.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// rlwinm r11,r11,0,0,16
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r11,r11,0,16,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e8584
	if (cr6.eq) goto loc_826E8584;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82612030
	sub_82612030(ctx, base);
loc_826E8584:
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,32,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r25,r12
	r11.u64 = r25.u64 & r12.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// bne cr6,0x826e8870
	if (!cr6.eq) goto loc_826E8870;
	// extsh r9,r3
	ctx.r9.s64 = ctx.r3.s16;
	// srawi r8,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = ctx.r3.s32 >> 16;
	// b 0x826e8884
	goto loc_826E8884;
loc_826E85A4:
	// li r29,0
	r29.s64 = 0;
	// beq cr6,0x826e85b4
	if (cr6.eq) goto loc_826E85B4;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r29,r11,r28
	r29.u64 = PPC_LOAD_U32(r11.u32 + r28.u32);
loc_826E85B4:
	// lwz r10,1416(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 1416);
	// subf r11,r9,r27
	r11.s64 = r27.s64 - ctx.r9.s64;
	// rlwinm r9,r29,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// xor r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 ^ r29.u64;
	// lbzx r10,r10,r20
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r20.u32);
	// rlwinm r9,r9,0,17,17
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x4000;
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r31,r11,r28
	r31.u64 = PPC_LOAD_U32(r11.u32 + r28.u32);
	// lwzx r11,r10,r28
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r28.u32);
	// rlwinm r10,r31,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r8,r11,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// xor r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 ^ r31.u64;
	// xor r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 ^ r11.u64;
	// rlwinm r8,r10,0,17,17
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4000;
	// rlwinm r10,r7,0,17,17
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x4000;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826e8620
	if (!cr6.gt) goto loc_826E8620;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// bne cr6,0x826e883c
	if (!cr6.eq) goto loc_826E883C;
	// cmplwi cr6,r31,16384
	cr6.compare<uint32_t>(r31.u32, 16384, xer);
	// bne cr6,0x826e881c
	if (!cr6.eq) goto loc_826E881C;
	// li r31,0
	r31.s64 = 0;
loc_826E8620:
	// lwz r9,452(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// rlwinm r6,r20,15,15,15
	ctx.r6.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 15) & 0x10000;
	// subf r7,r31,r11
	ctx.r7.s64 = r11.s64 - r31.s64;
	// lwz r16,1392(r30)
	r16.u64 = PPC_LOAD_U32(r30.u32 + 1392);
	// add r6,r6,r24
	ctx.r6.u64 = ctx.r6.u64 + r24.u64;
	// lwz r15,1396(r30)
	r15.u64 = PPC_LOAD_U32(r30.u32 + 1396);
	// subf r3,r29,r11
	ctx.r3.s64 = r11.s64 - r29.s64;
	// subf r14,r31,r29
	r14.s64 = r29.s64 - r31.s64;
	// lwz r5,16(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// rlwinm r8,r29,16,0,15
	ctx.r8.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r9,r31,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 16) & 0xFFFF0000;
	// add r4,r6,r5
	ctx.r4.u64 = ctx.r6.u64 + ctx.r5.u64;
	// xor r5,r3,r7
	ctx.r5.u64 = ctx.r3.u64 ^ ctx.r7.u64;
	// xor r14,r14,r7
	r14.u64 = r14.u64 ^ ctx.r7.u64;
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// rlwinm r10,r11,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r4,r4,5,0,26
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r6,r9,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r3,r8,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// srawi r7,r5,31
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r5.s32 >> 31;
	// srawi r5,r14,31
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r14.s32 >> 31;
	// lwz r14,84(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// xor r3,r3,r6
	ctx.r3.u64 = ctx.r3.u64 ^ ctx.r6.u64;
	// xor r14,r14,r6
	r14.u64 = r14.u64 ^ ctx.r6.u64;
	// srawi r6,r3,31
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r3.s32 >> 31;
	// srawi r3,r14,31
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r14.s32 >> 31;
	// or r14,r7,r5
	r14.u64 = ctx.r7.u64 | ctx.r5.u64;
	// and r11,r7,r11
	r11.u64 = ctx.r7.u64 & r11.u64;
	// andc r7,r29,r14
	ctx.r7.u64 = r29.u64 & ~r14.u64;
	// and r9,r3,r9
	ctx.r9.u64 = ctx.r3.u64 & ctx.r9.u64;
	// or r11,r7,r11
	r11.u64 = ctx.r7.u64 | r11.u64;
	// and r7,r5,r31
	ctx.r7.u64 = ctx.r5.u64 & r31.u64;
	// and r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 & ctx.r10.u64;
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// or r7,r6,r3
	ctx.r7.u64 = ctx.r6.u64 | ctx.r3.u64;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// andc r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ~ctx.r7.u64;
	// rlwinm r11,r11,16,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// rlwinm r10,r10,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// or r6,r11,r10
	ctx.r6.u64 = r11.u64 | ctx.r10.u64;
	// rlwinm r11,r6,1,15,15
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x10000;
	// subf r10,r6,r15
	ctx.r10.s64 = r15.s64 - ctx.r6.s64;
	// subf r11,r11,r16
	r11.s64 = r16.s64 - r11.s64;
	// subf r10,r4,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r4.s64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// rlwinm r11,r11,0,0,16
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r11,r11,0,16,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e8710
	if (cr6.eq) goto loc_826E8710;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// lwz r7,452(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82612030
	sub_82612030(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
loc_826E8710:
	// lwz r15,104(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r14,108(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r16,112(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
loc_826E871C:
	// rlwinm r11,r6,16,0,15
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 16) & 0xFFFF0000;
	// srawi r8,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = ctx.r6.s32 >> 16;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826e885c
	if (cr6.eq) goto loc_826E885C;
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,32,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// and r10,r25,r12
	ctx.r10.u64 = r25.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// bne cr6,0x826e885c
	if (!cr6.eq) goto loc_826E885C;
	// rlwinm r10,r29,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0xFFFF0000;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// srawi r10,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 16;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r7,r29,16
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r29.s32 >> 16;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// srawi r9,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 31;
	// xor r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// bgt cr6,0x826e87b0
	if (cr6.gt) goto loc_826E87B0;
	// rlwinm r10,r31,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 16) & 0xFFFF0000;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// srawi r10,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 16;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r7,r31,16
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r31.s32 >> 16;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// srawi r9,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 31;
	// xor r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// ble cr6,0x826e8880
	if (!cr6.gt) goto loc_826E8880;
loc_826E87B0:
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e87dc
	if (!cr0.lt) goto loc_826E87DC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E87DC:
	// addi r11,r31,19
	r11.s64 = r31.s64 + 19;
	// lwz r28,188(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// subf r11,r11,r27
	r11.s64 = r27.s64 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// cmpwi cr6,r9,16384
	cr6.compare<int32_t>(ctx.r9.s32, 16384, xer);
	// bne cr6,0x826e8884
	if (!cr6.eq) goto loc_826E8884;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x826e8884
	goto loc_826E8884;
loc_826E881C:
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x826e882c
	if (!cr6.eq) goto loc_826E882C;
	// li r11,0
	r11.s64 = 0;
	// b 0x826e8620
	goto loc_826E8620;
loc_826E882C:
	// cmplwi cr6,r29,16384
	cr6.compare<uint32_t>(r29.u32, 16384, xer);
	// bne cr6,0x826e8620
	if (!cr6.eq) goto loc_826E8620;
	// li r29,0
	r29.s64 = 0;
	// b 0x826e8620
	goto loc_826E8620;
loc_826E883C:
	// cmplwi cr6,r31,16384
	cr6.compare<uint32_t>(r31.u32, 16384, xer);
	// bne cr6,0x826e8848
	if (!cr6.eq) goto loc_826E8848;
	// li r31,0
	r31.s64 = 0;
loc_826E8848:
	// cmplwi cr6,r29,16384
	cr6.compare<uint32_t>(r29.u32, 16384, xer);
	// bne cr6,0x826e8854
	if (!cr6.eq) goto loc_826E8854;
	// li r29,0
	r29.s64 = 0;
loc_826E8854:
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x826e871c
	goto loc_826E871C;
loc_826E885C:
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,32,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// and r10,r25,r12
	ctx.r10.u64 = r25.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x826e8880
	if (cr6.eq) goto loc_826E8880;
loc_826E8870:
	// rlwinm r10,r27,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,16384
	r11.s64 = 16384;
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + r28.u32, r11.u32);
	// b 0x826e88e8
	goto loc_826E88E8;
loc_826E8880:
	// srawi r9,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 16;
loc_826E8884:
	// rldicl r6,r25,48,16
	ctx.r6.u64 = __builtin_rotateleft64(r25.u64, 48) & 0xFFFFFFFFFFFF;
	// lbz r11,30(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 30);
	// clrlwi r7,r25,16
	ctx.r7.u64 = r25.u32 & 0xFFFF;
	// lhz r10,62(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 62);
	// clrlwi r3,r6,16
	ctx.r3.u64 = ctx.r6.u32 & 0xFFFF;
	// lhz r6,68(r30)
	ctx.r6.u64 = PPC_LOAD_U16(r30.u32 + 68);
	// lhz r5,66(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 66);
	// rlwinm r4,r27,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r31,r6,16
	r31.u64 = __builtin_rotateleft32(ctx.r6.u32, 16);
	// slw r6,r7,r11
	ctx.r6.u64 = r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (r11.u8 & 0x3F));
	// slw r7,r3,r11
	ctx.r7.u64 = r11.u8 & 0x20 ? 0 : (ctx.r3.u32 << (r11.u8 & 0x3F));
	// add r11,r6,r10
	r11.u64 = ctx.r6.u64 + ctx.r10.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// and r11,r11,r5
	r11.u64 = r11.u64 & ctx.r5.u64;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// lhz r11,64(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 64);
	// add r9,r7,r11
	ctx.r9.u64 = ctx.r7.u64 + r11.u64;
	// rotlwi r11,r11,16
	r11.u64 = __builtin_rotateleft32(r11.u32, 16);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// rlwinm r9,r9,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 16) & 0xFFFF0000;
	// and r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 & r31.u64;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwx r11,r4,r28
	PPC_STORE_U32(ctx.r4.u32 + r28.u32, r11.u32);
loc_826E88E8:
	// lwz r7,92(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// beq cr6,0x826e8978
	if (cr6.eq) goto loc_826E8978;
	// lwz r10,452(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lhz r11,0(r19)
	r11.u64 = PPC_LOAD_U16(r19.u32 + 0);
	// lhz r9,16(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 16);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// and r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 & ctx.r6.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add. r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x826e8944
	if (cr0.eq) goto loc_826E8944;
	// lhz r10,50(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// lwz r9,188(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lhz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// cmplwi cr6,r10,16384
	cr6.compare<uint32_t>(ctx.r10.u32, 16384, xer);
	// bne cr6,0x826e8944
	if (!cr6.eq) goto loc_826E8944;
	// li r8,1
	ctx.r8.s64 = 1;
loc_826E8944:
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// add. r10,r24,r10
	ctx.r10.u64 = r24.u64 + ctx.r10.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x826e896c
	if (cr0.eq) goto loc_826E896C;
	// lwz r10,188(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,-2(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x826e896c
	if (!cr6.eq) goto loc_826E896C;
	// li r8,1
	ctx.r8.s64 = 1;
loc_826E896C:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// or r11,r8,r11
	r11.u64 = ctx.r8.u64 | r11.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
loc_826E8978:
	// or r11,r22,r17
	r11.u64 = r22.u64 | r17.u64;
	// or r10,r21,r16
	ctx.r10.u64 = r21.u64 | r16.u64;
	// rlwinm r17,r11,1,0,30
	r17.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r14,r22,r14
	r14.u64 = r22.u64 + r14.u64;
	// rlwinm r16,r10,1,0,30
	r16.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r7,8
	r11.s64 = ctx.r7.s64 + 8;
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// addi r19,r19,2
	r19.s64 = r19.s64 + 2;
	// stw r14,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r14.u32);
	// cmpwi cr6,r20,4
	cr6.compare<int32_t>(r20.s32, 4, xer);
	// stw r16,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r16.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// blt cr6,0x826e80bc
	if (cr6.lt) goto loc_826E80BC;
	// srawi r10,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	ctx.r10.s64 = r17.s32 >> 1;
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// srawi r9,r16,1
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x1) != 0);
	ctx.r9.s64 = r16.s32 >> 1;
	// rlwinm r8,r15,0,26,27
	ctx.r8.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 0) & 0x30;
	// cmpwi cr6,r14,3
	cr6.compare<int32_t>(r14.s32, 3, xer);
	// lbzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// li r11,0
	r11.s64 = 0;
	// or r24,r9,r8
	r24.u64 = ctx.r9.u64 | ctx.r8.u64;
	// blt cr6,0x826e89d8
	if (cr6.lt) goto loc_826E89D8;
	// li r11,48
	r11.s64 = 48;
loc_826E89D8:
	// lbz r9,28(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 28);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826e89f4
	if (cr6.eq) goto loc_826E89F4;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// li r27,1
	r27.s64 = 1;
	// bne cr6,0x826e89f8
	if (!cr6.eq) goto loc_826E89F8;
loc_826E89F4:
	// li r27,0
	r27.s64 = 0;
loc_826E89F8:
	// lbz r10,29(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 29);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826e8a14
	if (cr6.eq) goto loc_826E8A14;
	// andc r11,r24,r11
	r11.u64 = r24.u64 & ~r11.u64;
	// li r26,1
	r26.s64 = 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826e8a18
	if (!cr6.eq) goto loc_826E8A18;
loc_826E8A14:
	// li r26,0
	r26.s64 = 0;
loc_826E8A18:
	// cmpwi cr6,r14,2
	cr6.compare<int32_t>(r14.s32, 2, xer);
	// ble cr6,0x826e8aa4
	if (!cr6.gt) goto loc_826E8AA4;
	// lwz r11,452(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// li r25,1
	r25.s64 = 1;
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826e8a6c
	if (cr6.eq) goto loc_826E8A6C;
	// lhz r10,50(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// lwz r9,192(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lhz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// cmplwi cr6,r10,16384
	cr6.compare<uint32_t>(ctx.r10.u32, 16384, xer);
	// bne cr6,0x826e8a6c
	if (!cr6.eq) goto loc_826E8A6C;
	// li r8,1
	ctx.r8.s64 = 1;
loc_826E8A6C:
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826e8a94
	if (cr6.eq) goto loc_826E8A94;
	// lwz r10,192(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,-2(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x826e8a94
	if (!cr6.eq) goto loc_826E8A94;
	// li r8,1
	ctx.r8.s64 = 1;
loc_826E8A94:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// or r11,r8,r11
	r11.u64 = ctx.r8.u64 | r11.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// b 0x826e8aa8
	goto loc_826E8AA8;
loc_826E8AA4:
	// li r25,0
	r25.s64 = 0;
loc_826E8AA8:
	// lbz r11,27(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 27);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e8d98
	if (cr6.eq) goto loc_826E8D98;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x826e8ac4
	if (!cr6.eq) goto loc_826E8AC4;
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// beq cr6,0x826e8d98
	if (cr6.eq) goto loc_826E8D98;
loc_826E8AC4:
	// lbz r11,1181(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1181);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e8b1c
	if (cr6.eq) goto loc_826E8B1C;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r10,r10,20,12,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFFFFF;
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e8b00
	if (cr6.eq) goto loc_826E8B00;
	// lbz r11,1182(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// stb r11,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, r11.u8);
	// b 0x826e8d80
	goto loc_826E8D80;
loc_826E8B00:
	// lbz r11,1180(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1180);
	// lbz r10,1185(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// stb r11,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, r11.u8);
	// b 0x826e8d80
	goto loc_826E8D80;
loc_826E8B1C:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r28,0
	r28.s64 = 0;
	// lbz r11,1186(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826e8c08
	if (cr6.eq) goto loc_826E8C08;
	// li r29,1
	r29.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826e8b9c
	if (!cr6.lt) goto loc_826E8B9C;
loc_826E8B44:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e8b9c
	if (cr6.eq) goto loc_826E8B9C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e8b8c
	if (!cr0.lt) goto loc_826E8B8C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8B8C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e8b44
	if (cr6.gt) goto loc_826E8B44;
loc_826E8B9C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e8bd8
	if (!cr0.lt) goto loc_826E8BD8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8BD8:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826e8bf0
	if (cr6.eq) goto loc_826E8BF0;
	// lbz r11,1182(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826e8d78
	goto loc_826E8D78;
loc_826E8BF0:
	// lbz r11,1180(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1180);
	// lbz r10,1185(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826e8d78
	goto loc_826E8D78;
loc_826E8C08:
	// li r29,3
	r29.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826e8c6c
	if (!cr6.lt) goto loc_826E8C6C;
loc_826E8C14:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e8c6c
	if (cr6.eq) goto loc_826E8C6C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e8c5c
	if (!cr0.lt) goto loc_826E8C5C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8C5C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e8c14
	if (cr6.gt) goto loc_826E8C14;
loc_826E8C6C:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e8ca8
	if (!cr0.lt) goto loc_826E8CA8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8CA8:
	// cmpwi cr6,r29,7
	cr6.compare<int32_t>(r29.s32, 7, xer);
	// bne cr6,0x826e8d68
	if (!cr6.eq) goto loc_826E8D68;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r29,5
	r29.s64 = 5;
	// li r28,0
	r28.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826e8d24
	if (!cr6.lt) goto loc_826E8D24;
loc_826E8CCC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e8d24
	if (cr6.eq) goto loc_826E8D24;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e8d14
	if (!cr0.lt) goto loc_826E8D14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8D14:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e8ccc
	if (cr6.gt) goto loc_826E8CCC;
loc_826E8D24:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e8d60
	if (!cr0.lt) goto loc_826E8D60;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8D60:
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x826e8d70
	goto loc_826E8D70;
loc_826E8D68:
	// lbz r11,1180(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1180);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
loc_826E8D70:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
loc_826E8D78:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stb r11,4(r10)
	PPC_STORE_U8(ctx.r10.u32 + 4, r11.u8);
loc_826E8D80:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lbz r11,4(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x826e77cc
	if (cr6.lt) goto loc_826E77CC;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// bgt cr6,0x826e77cc
	if (cr6.gt) goto loc_826E77CC;
loc_826E8D98:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826e8de4
	if (cr6.eq) goto loc_826E8DE4;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e8dd0
	if (!cr0.lt) goto loc_826E8DD0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8DD0:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r10,r31,24
	ctx.r10.u64 = r31.u32 & 0xFF;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwimi r9,r10,3,27,28
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 3) & 0x18) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_826E8DE4:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x826e8e64
	if (cr6.eq) goto loc_826E8E64;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e8e18
	if (!cr0.lt) goto loc_826E8E18;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8E18:
	// mr r11,r29
	r11.u64 = r29.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x826e8e54
	if (cr6.eq) goto loc_826E8E54;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826e8e50
	if (!cr0.lt) goto loc_826E8E50;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E8E50:
	// add r11,r31,r29
	r11.u64 = r31.u64 + r29.u64;
loc_826E8E54:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwimi r9,r11,22,8,9
	ctx.r9.u64 = (__builtin_rotateleft32(r11.u32, 22) & 0xC00000) | (ctx.r9.u64 & 0xFFFFFFFFFF3FFFFF);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
loc_826E8E64:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826e8ffc
	if (cr6.eq) goto loc_826E8FFC;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,200(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 200);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,8,56
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e8f54
	if (cr6.lt) goto loc_826E8F54;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826e8f4c
	if (!cr6.lt) goto loc_826E8F4C;
loc_826E8EB4:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826e8ee0
	if (cr6.lt) goto loc_826E8EE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826e8eb4
	if (cr6.eq) goto loc_826E8EB4;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e8f9c
	goto loc_826E8F9C;
loc_826E8EE0:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826E8F4C:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e8f9c
	goto loc_826E8F9C;
loc_826E8F54:
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r27,r11,32768
	r27.u64 = r11.u64 | 32768;
loc_826E8F68:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r27
	r11.u64 = r29.u64 + r27.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e8f68
	if (cr6.lt) goto loc_826E8F68;
loc_826E8F9C:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826e77cc
	if (!cr6.eq) goto loc_826E77CC;
	// cmpwi cr6,r29,8
	cr6.compare<int32_t>(r29.s32, 8, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// blt cr6,0x826e8fbc
	if (cr6.lt) goto loc_826E8FBC;
	// li r10,0
	ctx.r10.s64 = 0;
loc_826E8FBC:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r11,r29,29
	r11.u64 = r29.u32 & 0x7;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwimi r8,r10,28,3,3
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r10.u32, 28) & 0x10000000) | (ctx.r8.u64 & 0xFFFFFFFFEFFFFFFF);
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// add r8,r11,r30
	ctx.r8.u64 = r11.u64 + r30.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lbz r8,516(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 516);
	// rlwimi r10,r8,24,5,7
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r8.u32, 24) & 0x7000000) | (ctx.r10.u64 & 0xFFFFFFFFF8FFFFFF);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lbz r11,524(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 524);
	// rlwimi r10,r11,20,10,11
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 20) & 0x300000) | (ctx.r10.u64 & 0xFFFFFFFFFFCFFFFF);
	// rlwinm r11,r10,0,5,3
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFF7FFFFFF;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
loc_826E8FFC:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stb r24,5(r10)
	PPC_STORE_U8(ctx.r10.u32 + 5, r24.u8);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826e77cc
	if (!cr6.eq) goto loc_826E77CC;
	// lwz r14,452(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// li r31,0
	r31.s64 = 0;
	// lwz r15,120(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r16,116(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r19,144(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
loc_826E9028:
	// lwz r23,80(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_826E902C:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// rlwinm r9,r25,14,0,17
	ctx.r9.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 14) & 0xFFFFC000;
	// stb r25,45(r11)
	PPC_STORE_U8(r11.u32 + 45, r25.u8);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lbz r10,45(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// stb r10,37(r11)
	PPC_STORE_U8(r11.u32 + 37, ctx.r10.u8);
	// lwz r11,4(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 4);
	// lwz r10,192(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// lbz r10,5(r23)
	ctx.r10.u64 = PPC_LOAD_U8(r23.u32 + 5);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826e90b8
	if (!cr6.eq) goto loc_826E90B8;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// beq cr6,0x826e90b8
	if (cr6.eq) goto loc_826E90B8;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x826e90b8
	if (!cr6.eq) goto loc_826E90B8;
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// li r9,16256
	ctx.r9.s64 = 16256;
	// lbz r10,1260(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 1260);
	// rlwinm r11,r11,24,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0x7;
	// lbz r8,4(r23)
	ctx.r8.u64 = PPC_LOAD_U8(r23.u32 + 4);
	// lwz r7,4(r14)
	ctx.r7.u64 = PPC_LOAD_U32(r14.u32 + 4);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,1248(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 1248);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwimi r9,r11,1,25,25
	ctx.r9.u64 = (__builtin_rotateleft32(r11.u32, 1) & 0x40) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFFBF);
	// clrlwi r11,r9,24
	r11.u64 = ctx.r9.u32 & 0xFF;
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rldicr r11,r11,48,15
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFF000000000000;
	// stdx r11,r10,r7
	PPC_STORE_U64(ctx.r10.u32 + ctx.r7.u32, r11.u64);
	// b 0x826ea2e8
	goto loc_826EA2E8;
loc_826E90B8:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r18,r10
	r18.u64 = ctx.r10.u64;
	// lbz r10,28(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 28);
	// li r20,0
	r20.s64 = 0;
	// lbz r9,29(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 29);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lbz r23,34(r30)
	r23.u64 = PPC_LOAD_U8(r30.u32 + 34);
	// mr r24,r20
	r24.u64 = r20.u64;
	// lwz r17,4(r30)
	r17.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// stw r20,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r20.u32);
	// rlwinm r10,r11,12,30,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// rlwinm r21,r11,4,31,31
	r21.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0x1;
	// stw r23,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r23.u32);
	// std r24,208(r1)
	PPC_STORE_U64(ctx.r1.u32 + 208, r24.u64);
	// stw r27,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r27.u32);
	// stw r10,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r10.u32);
	// stw r21,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r21.u32);
	// beq cr6,0x826e9120
	if (cr6.eq) goto loc_826E9120;
	// lwz r7,228(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 228);
	// rlwinm r10,r11,12,28,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xC;
	// lwz r8,232(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 232);
	// add r22,r7,r10
	r22.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r19,r8,r10
	r19.u64 = ctx.r8.u64 + ctx.r10.u64;
	// b 0x826e9128
	goto loc_826E9128;
loc_826E9120:
	// addi r22,r30,236
	r22.s64 = r30.s64 + 236;
	// addi r19,r30,248
	r19.s64 = r30.s64 + 248;
loc_826E9128:
	// stw r22,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r22.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826e913c
	if (cr6.eq) goto loc_826E913C;
	// rlwinm r23,r11,8,29,31
	r23.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0x7;
	// stw r23,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r23.u32);
loc_826E913C:
	// stw r20,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, r20.u32);
loc_826E9140:
	// clrlwi r10,r18,31
	ctx.r10.u64 = r18.u32 & 0x1;
	// lbz r11,5(r17)
	r11.u64 = PPC_LOAD_U8(r17.u32 + 5);
	// addi r17,r17,8
	r17.s64 = r17.s64 + 8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r10.u32);
	// beq cr6,0x826e9e30
	if (cr6.eq) goto loc_826E9E30;
	// lwz r7,4(r14)
	ctx.r7.u64 = PPC_LOAD_U32(r14.u32 + 4);
	// rlwinm r5,r15,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,1248(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 1248);
	// clrlwi r10,r15,31
	ctx.r10.u64 = r15.u32 & 0x1;
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r6,1240(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 1240);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r4,r20,0,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0xFFFFFFFC;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lhz r9,50(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// stw r16,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, r16.u32);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lbz r11,4(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// stw r8,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r8.u32);
	// lwzx r8,r6,r5
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r5.u32);
	// neg r5,r10
	ctx.r5.s64 = -ctx.r10.s64;
	// lwz r6,220(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 220);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stw r11,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, r11.u32);
	// and r7,r8,r15
	ctx.r7.u64 = ctx.r8.u64 & r15.u64;
	// rotlwi r8,r11,2
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 2);
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r7,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r7.u32);
	// and r8,r5,r9
	ctx.r8.u64 = ctx.r5.u64 & ctx.r9.u64;
	// add r3,r11,r6
	ctx.r3.u64 = r11.u64 + ctx.r6.u64;
	// mullw r9,r9,r15
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r15.s32);
	// bne cr6,0x826e9258
	if (!cr6.eq) goto loc_826E9258;
	// addi r4,r20,18
	ctx.r4.s64 = r20.s64 + 18;
	// lwz r6,264(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 264);
	// rlwinm r31,r10,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r5,188(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 188);
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,1160(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 1160);
	// add r9,r9,r16
	ctx.r9.u64 = ctx.r9.u64 + r16.u64;
	// stw r19,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r19.u32);
	// srawi r11,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r11.s64 = r20.s32 >> 1;
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// add r4,r8,r16
	ctx.r4.u64 = ctx.r8.u64 + r16.u64;
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r30
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r30.u32);
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// clrlwi r4,r20,31
	ctx.r4.u64 = r20.u32 & 0x1;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r4,r4,r16
	ctx.r4.u64 = ctx.r4.u64 + r16.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r4,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r4.u32);
	// li r4,119
	ctx.r4.s64 = 119;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// stw r4,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r4.u32);
	// or r4,r31,r11
	ctx.r4.u64 = r31.u64 | r11.u64;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// stw r10,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r10.u32);
	// stw r11,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r11.u32);
	// addi r11,r4,104
	r11.s64 = ctx.r4.s64 + 104;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r7,r11,r30
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// rlwinm r11,r9,5,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r11.u32);
	// extsh r11,r7
	r11.s64 = ctx.r7.s16;
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// b 0x826e92b8
	goto loc_826E92B8;
loc_826E9258:
	// lwz r11,1164(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1164);
	// addi r5,r10,102
	ctx.r5.s64 = ctx.r10.s64 + 102;
	// addi r6,r20,63
	ctx.r6.s64 = r20.s64 + 63;
	// lwz r7,192(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// srawi r10,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// stw r22,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r22.u32);
	// add r10,r10,r16
	ctx.r10.u64 = ctx.r10.u64 + r16.u64;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// srawi r11,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r11.s64 = ctx.r8.s32 >> 1;
	// rlwinm r8,r6,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r6,r5,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lwzx r10,r8,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + r30.u32);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lhzx r8,r6,r30
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r6.u32 + r30.u32);
	// stw r9,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r9.u32);
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r11.u32);
	// extsh r11,r8
	r11.s64 = ctx.r8.s16;
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// li r11,119
	r11.s64 = 119;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
loc_826E92B8:
	// lwz r11,28(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 28);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// addi r11,r11,-128
	r11.s64 = r11.s64 + -128;
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// stw r11,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r11.u32);
	// stw r11,28(r14)
	PPC_STORE_U32(r14.u32 + 28, r11.u32);
	// dcbzl r0,r11
	memset(base + ((r11.u32) & ~127), 0, 128);
	// li r27,0
	r27.s64 = 0;
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lbz r4,8(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// lwz r28,0(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r28.u32);
	// extsh r29,r10
	r29.s64 = ctx.r10.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e93cc
	if (cr6.lt) goto loc_826E93CC;
	// clrlwi r10,r29,28
	ctx.r10.u64 = r29.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x826e93c4
	if (!cr6.lt) goto loc_826E93C4;
loc_826E932C:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826e9358
	if (cr6.lt) goto loc_826E9358;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826e932c
	if (cr6.eq) goto loc_826E932C;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e9410
	goto loc_826E9410;
loc_826E9358:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826E93C4:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826e9410
	goto loc_826E9410;
loc_826E93CC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
loc_826E93DC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r26
	r11.u64 = r29.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e93dc
	if (cr6.lt) goto loc_826E93DC;
loc_826E9410:
	// clrlwi r29,r29,16
	r29.u64 = r29.u32 & 0xFFFF;
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r26,r29
	r26.u64 = r29.u64;
	// cmpw cr6,r26,r11
	cr6.compare<int32_t>(r26.s32, r11.s32, xer);
	// beq cr6,0x826e9560
	if (cr6.eq) goto loc_826E9560;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x826e9680
	if (cr6.eq) goto loc_826E9680;
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x826e9478
	if (!cr6.eq) goto loc_826E9478;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x826e9464
	if (!cr0.lt) goto loc_826E9464;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E9464:
	// rlwinm r11,r26,1,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826e953c
	goto loc_826E953C;
loc_826E9478:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x826e9540
	if (!cr6.eq) goto loc_826E9540;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r29,2
	r29.s64 = 2;
	// li r28,0
	r28.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x826e94f0
	if (!cr6.lt) goto loc_826E94F0;
loc_826E9498:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e94f0
	if (cr6.eq) goto loc_826E94F0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e94e0
	if (!cr0.lt) goto loc_826E94E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E94E0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e9498
	if (cr6.gt) goto loc_826E9498;
loc_826E94F0:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e952c
	if (!cr0.lt) goto loc_826E952C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E952C:
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-3
	r11.s64 = r11.s64 + -3;
loc_826E953C:
	// clrlwi r29,r11,16
	r29.u64 = r11.u32 & 0xFFFF;
loc_826E9540:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r28,r8,0
	r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x826e965c
	goto loc_826E965C;
loc_826E9560:
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bgt cr6,0x826e9578
	if (cr6.gt) goto loc_826E9578;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// subfic r11,r11,3
	xer.ca = r11.u32 <= 3;
	r11.s64 = 3 - r11.s64;
	// b 0x826e957c
	goto loc_826E957C;
loc_826E9578:
	// li r11,0
	r11.s64 = 0;
loc_826E957C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r29,r11,8
	r29.s64 = r11.s64 + 8;
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x826e959c
	if (!cr6.eq) goto loc_826E959C;
	// li r11,0
	r11.s64 = 0;
	// b 0x826e963c
	goto loc_826E963C;
loc_826E959C:
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x826e95fc
	if (!cr6.gt) goto loc_826E95FC;
loc_826E95A4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e95fc
	if (cr6.eq) goto loc_826E95FC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826e95ec
	if (!cr0.lt) goto loc_826E95EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E95EC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826e95a4
	if (cr6.gt) goto loc_826E95A4;
loc_826E95FC:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826e9638
	if (!cr0.lt) goto loc_826E9638;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E9638:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_826E963C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrlwi r29,r11,16
	r29.u64 = r11.u32 & 0xFFFF;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r28,r11,0
	r28.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
loc_826E965C:
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// bge 0x826e966c
	if (!cr0.lt) goto loc_826E966C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826E966C:
	// rlwinm r11,r28,1,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r10,r29
	ctx.r10.s64 = r29.s16;
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// extsh r27,r11
	r27.s64 = r11.s16;
loc_826E9680:
	// lwz r5,196(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// sth r27,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, r27.u16);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826ea3d0
	if (!cr6.eq) goto loc_826EA3D0;
	// lwz r21,188(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x826e96c0
	if (cr6.eq) goto loc_826E96C0;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r6,276(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 276);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x826fcd00
	sub_826FCD00(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x826ea3d0
	if (cr6.lt) goto loc_826EA3D0;
loc_826E96C0:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r22,1
	r22.s64 = 1;
	// lwz r20,172(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// li r25,0
	r25.s64 = 0;
	// lhz r26,50(r30)
	r26.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// li r24,0
	r24.s64 = 0;
	// lwz r7,152(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// srawi r11,r20,2
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x3) != 0);
	r11.s64 = r20.s32 >> 2;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// rlwinm r10,r10,0,27,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x18;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r23,r10,27,31,31
	r23.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// srw r8,r26,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (r26.u32 >> (r11.u8 & 0x3F));
	// beq cr6,0x826e9734
	if (cr6.eq) goto loc_826E9734;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r7
	r11.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// bne cr6,0x826e9734
	if (!cr6.eq) goto loc_826E9734;
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// li r22,8
	r22.s64 = 8;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r25,r11,r10
	r25.s64 = ctx.r10.s64 - r11.s64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
loc_826E9734:
	// lwz r11,168(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// lwz r31,128(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// beq cr6,0x826e99b4
	if (cr6.eq) goto loc_826E99B4;
	// lwz r6,-4(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + -4);
	// cmpwi cr6,r6,16384
	cr6.compare<int32_t>(ctx.r6.s32, 16384, xer);
	// bne cr6,0x826e99b4
	if (!cr6.eq) goto loc_826E99B4;
	// addi r24,r10,-32
	r24.s64 = ctx.r10.s64 + -32;
	// li r22,1
	r22.s64 = 1;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x826e9bdc
	if (cr6.eq) goto loc_826E9BDC;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x826e99b4
	if (cr6.eq) goto loc_826E99B4;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r8,16384
	cr6.compare<int32_t>(ctx.r8.s32, 16384, xer);
	// bne cr6,0x826e9794
	if (!cr6.eq) goto loc_826E9794;
	// lhz r8,-16(r25)
	ctx.r8.u64 = PPC_LOAD_U16(r25.u32 + -16);
	// extsh r3,r8
	ctx.r3.s64 = ctx.r8.s16;
loc_826E9794:
	// lhz r8,16(r25)
	ctx.r8.u64 = PPC_LOAD_U16(r25.u32 + 16);
	// lhz r7,0(r24)
	ctx.r7.u64 = PPC_LOAD_U16(r24.u32 + 0);
	// lbz r6,27(r30)
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + 27);
	// extsh r29,r8
	r29.s64 = ctx.r8.s16;
	// extsh r28,r7
	r28.s64 = ctx.r7.s16;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x826e9978
	if (cr6.eq) goto loc_826E9978;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x826e98a8
	if (cr6.eq) goto loc_826E98A8;
	// cmpwi cr6,r20,4
	cr6.compare<int32_t>(r20.s32, 4, xer);
	// beq cr6,0x826e98a8
	if (cr6.eq) goto loc_826E98A8;
	// cmpwi cr6,r20,5
	cr6.compare<int32_t>(r20.s32, 5, xer);
	// beq cr6,0x826e98a8
	if (cr6.eq) goto loc_826E98A8;
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmpwi cr6,r20,1
	cr6.compare<int32_t>(r20.s32, 1, xer);
	// lis r16,2
	r16.s64 = 131072;
	// bne cr6,0x826e9840
	if (!cr6.eq) goto loc_826E9840;
	// rlwinm r7,r26,2,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFF8;
	// lwz r8,220(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 220);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r7,r31
	ctx.r7.s64 = r31.s64 - ctx.r7.s64;
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// clrlwi r7,r7,26
	ctx.r7.u64 = ctx.r7.u32 & 0x3F;
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,16(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// rlwinm r4,r6,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lwzx r7,r4,r27
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + r27.u32);
	// lwz r8,16(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// mullw r6,r8,r29
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(r29.s32);
	// mullw r8,r7,r8
	ctx.r8.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// mullw r8,r8,r3
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r3.s32);
	// mullw r7,r7,r6
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// add r8,r8,r16
	ctx.r8.u64 = ctx.r8.u64 + r16.u64;
	// add r7,r7,r16
	ctx.r7.u64 = ctx.r7.u64 + r16.u64;
	// srawi r3,r8,18
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 18;
	// srawi r29,r7,18
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFFF) != 0);
	r29.s64 = ctx.r7.s32 >> 18;
	// b 0x826e9980
	goto loc_826E9980;
loc_826E9840:
	// cmpwi cr6,r20,2
	cr6.compare<int32_t>(r20.s32, 2, xer);
	// bne cr6,0x826e9980
	if (!cr6.eq) goto loc_826E9980;
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r7,-8(r31)
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + -8);
	// lwz r8,220(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 220);
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// clrlwi r7,r7,26
	ctx.r7.u64 = ctx.r7.u32 & 0x3F;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r7,16(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r7,r27
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + r27.u32);
	// lwz r7,16(r6)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// mullw r6,r7,r28
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(r28.s32);
	// mullw r7,r8,r7
	ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// mullw r7,r7,r3
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r3.s32);
	// mullw r8,r8,r6
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// add r7,r7,r16
	ctx.r7.u64 = ctx.r7.u64 + r16.u64;
	// add r8,r8,r16
	ctx.r8.u64 = ctx.r8.u64 + r16.u64;
	// srawi r3,r7,18
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 18;
	// srawi r28,r8,18
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFFF) != 0);
	r28.s64 = ctx.r8.s32 >> 18;
	// b 0x826e9980
	goto loc_826E9980;
loc_826E98A8:
	// rlwinm r7,r26,2,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFF8;
	// lbz r6,-8(r31)
	ctx.r6.u64 = PPC_LOAD_U8(r31.u32 + -8);
	// lis r5,2
	ctx.r5.s64 = 131072;
	// lwz r8,220(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 220);
	// subf r7,r7,r31
	ctx.r7.s64 = r31.s64 - ctx.r7.s64;
	// mr r14,r5
	r14.u64 = ctx.r5.u64;
	// mr r16,r5
	r16.u64 = ctx.r5.u64;
	// mr r15,r5
	r15.u64 = ctx.r5.u64;
	// clrlwi r6,r6,26
	ctx.r6.u64 = ctx.r6.u32 & 0x3F;
	// lbz r5,-8(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + -8);
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// clrlwi r5,r5,26
	ctx.r5.u64 = ctx.r5.u32 & 0x3F;
	// rlwinm r27,r4,2,0,29
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r7,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r7.u32);
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,128(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// add r27,r27,r8
	r27.u64 = r27.u64 + ctx.r8.u64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// clrlwi r4,r4,26
	ctx.r4.u64 = ctx.r4.u32 & 0x3F;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lwz r7,16(r27)
	ctx.r7.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,16(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// mullw r6,r6,r28
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r28.s32);
	// lwzx r7,r7,r27
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + r27.u32);
	// rlwinm r28,r5,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r28
	ctx.r5.u64 = ctx.r5.u64 + r28.u64;
	// rlwinm r28,r4,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// add r28,r5,r8
	r28.u64 = ctx.r5.u64 + ctx.r8.u64;
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// lwz r8,16(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// mullw r4,r7,r8
	ctx.r4.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// lwz r8,16(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// mullw r8,r7,r8
	ctx.r8.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// mullw r5,r4,r3
	ctx.r5.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r3.s32);
	// mullw r8,r8,r29
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r29.s32);
	// mullw r7,r7,r6
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// add r6,r5,r16
	ctx.r6.u64 = ctx.r5.u64 + r16.u64;
	// add r8,r8,r15
	ctx.r8.u64 = ctx.r8.u64 + r15.u64;
	// add r7,r7,r14
	ctx.r7.u64 = ctx.r7.u64 + r14.u64;
	// srawi r3,r6,18
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = ctx.r6.s32 >> 18;
	// lwz r14,452(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// srawi r29,r8,18
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFFF) != 0);
	r29.s64 = ctx.r8.s32 >> 18;
	// srawi r28,r7,18
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFFF) != 0);
	r28.s64 = ctx.r7.s32 >> 18;
	// b 0x826e997c
	goto loc_826E997C;
loc_826E9978:
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
loc_826E997C:
	// lis r16,2
	r16.s64 = 131072;
loc_826E9980:
	// subf r7,r29,r3
	ctx.r7.s64 = ctx.r3.s64 - r29.s64;
	// subf r8,r28,r3
	ctx.r8.s64 = ctx.r3.s64 - r28.s64;
	// srawi r6,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// srawi r5,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 31;
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// xor r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r5.u64;
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826e99bc
	if (!cr6.lt) goto loc_826E99BC;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// li r22,8
	r22.s64 = 8;
	// b 0x826e99bc
	goto loc_826E99BC;
loc_826E99B4:
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lis r16,2
	r16.s64 = 131072;
loc_826E99BC:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826e9bdc
	if (cr6.eq) goto loc_826E9BDC;
	// lbz r8,27(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 27);
	// neg r7,r23
	ctx.r7.s64 = -r23.s64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// or r22,r7,r22
	r22.u64 = ctx.r7.u64 | r22.u64;
	// beq cr6,0x826e9bd0
	if (cr6.eq) goto loc_826E9BD0;
	// cmplw cr6,r9,r24
	cr6.compare<uint32_t>(ctx.r9.u32, r24.u32, xer);
	// bne cr6,0x826e9ad4
	if (!cr6.eq) goto loc_826E9AD4;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x826e9a28
	if (cr6.eq) goto loc_826E9A28;
	// cmpwi cr6,r20,2
	cr6.compare<int32_t>(r20.s32, 2, xer);
	// beq cr6,0x826e9a28
	if (cr6.eq) goto loc_826E9A28;
	// cmpwi cr6,r20,4
	cr6.compare<int32_t>(r20.s32, 4, xer);
	// beq cr6,0x826e9a28
	if (cr6.eq) goto loc_826E9A28;
	// cmpwi cr6,r20,5
	cr6.compare<int32_t>(r20.s32, 5, xer);
	// beq cr6,0x826e9a28
	if (cr6.eq) goto loc_826E9A28;
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// li r8,16
	ctx.r8.s64 = 16;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_826E9A0C:
	// lhz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x826e9a0c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826E9A0C;
	// addi r9,r1,224
	ctx.r9.s64 = ctx.r1.s64 + 224;
	// b 0x826e9bdc
	goto loc_826E9BDC;
loc_826E9A28:
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r8,-8(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + -8);
	// lwz r5,220(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 220);
	// addi r6,r1,226
	ctx.r6.s64 = ctx.r1.s64 + 226;
	// clrlwi r8,r8,26
	ctx.r8.u64 = ctx.r8.u32 & 0x3F;
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r31,r7
	r31.s64 = ctx.r7.s16;
	// lwzx r3,r4,r27
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + r27.u32);
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r9,2
	ctx.r7.s64 = ctx.r9.s64 + 2;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// li r9,15
	ctx.r9.s64 = 15;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r5
	ctx.r4.u64 = ctx.r4.u64 + ctx.r5.u64;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// lwz r4,16(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwzx r5,r5,r27
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + r27.u32);
	// mullw r11,r11,r31
	r11.s64 = int64_t(r11.s32) * int64_t(r31.s32);
	// mullw r11,r5,r11
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// sth r11,224(r1)
	PPC_STORE_U16(ctx.r1.u32 + 224, r11.u16);
loc_826E9A94:
	// lhz r11,0(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// mullw r11,r3,r11
	r11.s64 = int64_t(ctx.r3.s32) * int64_t(r11.s32);
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// sth r11,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, r11.u16);
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// bne cr6,0x826e9a94
	if (!cr6.eq) goto loc_826E9A94;
	// lhz r11,224(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 224);
	// addi r9,r1,224
	ctx.r9.s64 = ctx.r1.s64 + 224;
	// sth r11,240(r1)
	PPC_STORE_U16(ctx.r1.u32 + 240, r11.u16);
	// b 0x826e9bdc
	goto loc_826E9BDC;
loc_826E9AD4:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x826e9b1c
	if (cr6.eq) goto loc_826E9B1C;
	// cmpwi cr6,r20,1
	cr6.compare<int32_t>(r20.s32, 1, xer);
	// beq cr6,0x826e9b1c
	if (cr6.eq) goto loc_826E9B1C;
	// cmpwi cr6,r20,4
	cr6.compare<int32_t>(r20.s32, 4, xer);
	// beq cr6,0x826e9b1c
	if (cr6.eq) goto loc_826E9B1C;
	// cmpwi cr6,r20,5
	cr6.compare<int32_t>(r20.s32, 5, xer);
	// beq cr6,0x826e9b1c
	if (cr6.eq) goto loc_826E9B1C;
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// li r8,16
	ctx.r8.s64 = 16;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_826E9B00:
	// lhz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x826e9b00
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826E9B00;
	// addi r9,r1,240
	ctx.r9.s64 = ctx.r1.s64 + 240;
	// b 0x826e9bdc
	goto loc_826E9BDC;
loc_826E9B1C:
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,220(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 220);
	// rlwinm r8,r26,2,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFF8;
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r6,r1,226
	ctx.r6.s64 = ctx.r1.s64 + 226;
	// subf r8,r8,r31
	ctx.r8.s64 = r31.s64 - ctx.r8.s64;
	// extsh r31,r7
	r31.s64 = ctx.r7.s16;
	// lwzx r3,r4,r27
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + r27.u32);
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r9,2
	ctx.r7.s64 = ctx.r9.s64 + 2;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lbz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// li r9,15
	ctx.r9.s64 = 15;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r8,r8,26
	ctx.r8.u64 = ctx.r8.u32 & 0x3F;
	// add r4,r11,r5
	ctx.r4.u64 = r11.u64 + ctx.r5.u64;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,16(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwzx r5,r4,r27
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + r27.u32);
	// mullw r11,r11,r31
	r11.s64 = int64_t(r11.s32) * int64_t(r31.s32);
	// mullw r11,r5,r11
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// sth r11,224(r1)
	PPC_STORE_U16(ctx.r1.u32 + 224, r11.u16);
loc_826E9B90:
	// lhz r11,0(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// mullw r11,r3,r11
	r11.s64 = int64_t(ctx.r3.s32) * int64_t(r11.s32);
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// sth r11,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, r11.u16);
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// bne cr6,0x826e9b90
	if (!cr6.eq) goto loc_826E9B90;
	// lhz r11,224(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 224);
	// addi r9,r1,240
	ctx.r9.s64 = ctx.r1.s64 + 240;
	// sth r11,240(r1)
	PPC_STORE_U16(ctx.r1.u32 + 240, r11.u16);
	// b 0x826e9bdc
	goto loc_826E9BDC;
loc_826E9BD0:
	// cmplw cr6,r9,r25
	cr6.compare<uint32_t>(ctx.r9.u32, r25.u32, xer);
	// bne cr6,0x826e9bdc
	if (!cr6.eq) goto loc_826E9BDC;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
loc_826E9BDC:
	// lwz r11,28(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 28);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826e9d84
	if (cr6.eq) goto loc_826E9D84;
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmpwi cr6,r22,1
	cr6.compare<int32_t>(r22.s32, 1, xer);
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// sth r8,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r8.u16);
	// sth r8,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r8.u16);
	// bne cr6,0x826e9cb8
	if (!cr6.eq) goto loc_826E9CB8;
	// lhz r8,2(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lhz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r8.u16);
	// sth r8,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r8.u16);
	// lhz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lhz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,4(r11)
	PPC_STORE_U16(r11.u32 + 4, ctx.r8.u16);
	// sth r8,4(r10)
	PPC_STORE_U16(ctx.r10.u32 + 4, ctx.r8.u16);
	// lhz r8,6(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// lhz r7,6(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,6(r11)
	PPC_STORE_U16(r11.u32 + 6, ctx.r8.u16);
	// sth r8,6(r10)
	PPC_STORE_U16(ctx.r10.u32 + 6, ctx.r8.u16);
	// lhz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 8);
	// lhz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,8(r11)
	PPC_STORE_U16(r11.u32 + 8, ctx.r8.u16);
	// sth r8,8(r10)
	PPC_STORE_U16(ctx.r10.u32 + 8, ctx.r8.u16);
	// lhz r8,10(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 10);
	// lhz r7,10(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,10(r11)
	PPC_STORE_U16(r11.u32 + 10, ctx.r8.u16);
	// sth r8,10(r10)
	PPC_STORE_U16(ctx.r10.u32 + 10, ctx.r8.u16);
	// lhz r8,12(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 12);
	// lhz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 12);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,12(r11)
	PPC_STORE_U16(r11.u32 + 12, ctx.r8.u16);
	// sth r8,12(r10)
	PPC_STORE_U16(ctx.r10.u32 + 12, ctx.r8.u16);
	// lhz r9,14(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 14);
	// lhz r8,14(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// sth r9,14(r11)
	PPC_STORE_U16(r11.u32 + 14, ctx.r9.u16);
	// sth r9,14(r10)
	PPC_STORE_U16(ctx.r10.u32 + 14, ctx.r9.u16);
	// b 0x826e9da8
	goto loc_826E9DA8;
loc_826E9CB8:
	// cmpwi cr6,r22,8
	cr6.compare<int32_t>(r22.s32, 8, xer);
	// bne cr6,0x826e9d90
	if (!cr6.eq) goto loc_826E9D90;
	// lhz r8,2(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// sth r8,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r8.u16);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// ld r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// std r8,8(r10)
	PPC_STORE_U64(ctx.r10.u32 + 8, ctx.r8.u64);
	// lhz r8,2(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lhz r7,16(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 16);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,16(r11)
	PPC_STORE_U16(r11.u32 + 16, ctx.r8.u16);
	// sth r8,18(r10)
	PPC_STORE_U16(ctx.r10.u32 + 18, ctx.r8.u16);
	// lhz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lhz r7,32(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,32(r11)
	PPC_STORE_U16(r11.u32 + 32, ctx.r8.u16);
	// sth r8,20(r10)
	PPC_STORE_U16(ctx.r10.u32 + 20, ctx.r8.u16);
	// lhz r8,6(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// lhz r7,48(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 48);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,48(r11)
	PPC_STORE_U16(r11.u32 + 48, ctx.r8.u16);
	// sth r8,22(r10)
	PPC_STORE_U16(ctx.r10.u32 + 22, ctx.r8.u16);
	// lhz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 8);
	// lhz r7,64(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 64);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,64(r11)
	PPC_STORE_U16(r11.u32 + 64, ctx.r8.u16);
	// sth r8,24(r10)
	PPC_STORE_U16(ctx.r10.u32 + 24, ctx.r8.u16);
	// lhz r8,10(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 10);
	// lhz r7,80(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 80);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,80(r11)
	PPC_STORE_U16(r11.u32 + 80, ctx.r8.u16);
	// sth r8,26(r10)
	PPC_STORE_U16(ctx.r10.u32 + 26, ctx.r8.u16);
	// lhz r8,12(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 12);
	// lhz r7,96(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 96);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,96(r11)
	PPC_STORE_U16(r11.u32 + 96, ctx.r8.u16);
	// sth r8,28(r10)
	PPC_STORE_U16(ctx.r10.u32 + 28, ctx.r8.u16);
	// lhz r9,14(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 14);
	// lhz r8,112(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 112);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// sth r9,112(r11)
	PPC_STORE_U16(r11.u32 + 112, ctx.r9.u16);
	// sth r9,30(r10)
	PPC_STORE_U16(ctx.r10.u32 + 30, ctx.r9.u16);
	// b 0x826e9de0
	goto loc_826E9DE0;
loc_826E9D84:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// sth r9,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r9.u16);
loc_826E9D90:
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// sth r9,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r9.u16);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r9,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r9.u32);
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// std r9,8(r10)
	PPC_STORE_U64(ctx.r10.u32 + 8, ctx.r9.u64);
loc_826E9DA8:
	// lhz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 16);
	// sth r9,18(r10)
	PPC_STORE_U16(ctx.r10.u32 + 18, ctx.r9.u16);
	// lhz r9,32(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 32);
	// sth r9,20(r10)
	PPC_STORE_U16(ctx.r10.u32 + 20, ctx.r9.u16);
	// lhz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 48);
	// sth r9,22(r10)
	PPC_STORE_U16(ctx.r10.u32 + 22, ctx.r9.u16);
	// lhz r9,64(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 64);
	// sth r9,24(r10)
	PPC_STORE_U16(ctx.r10.u32 + 24, ctx.r9.u16);
	// lhz r9,80(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 80);
	// sth r9,26(r10)
	PPC_STORE_U16(ctx.r10.u32 + 26, ctx.r9.u16);
	// lhz r9,96(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 96);
	// sth r9,28(r10)
	PPC_STORE_U16(ctx.r10.u32 + 28, ctx.r9.u16);
	// lhz r11,112(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 112);
	// sth r11,30(r10)
	PPC_STORE_U16(ctx.r10.u32 + 30, r11.u16);
loc_826E9DE0:
	// lwz r15,120(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// rlwinm r11,r20,12,0,19
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 12) & 0xFFFFF000;
	// lwz r16,116(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// extsw r9,r21
	ctx.r9.s64 = r21.s32;
	// or r11,r11,r15
	r11.u64 = r11.u64 | r15.u64;
	// lwz r10,32(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 32);
	// ld r8,208(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 208);
	// ori r9,r9,128
	ctx.r9.u64 = ctx.r9.u64 | 128;
	// rlwinm r11,r11,16,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// lwz r27,156(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// lwz r23,92(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// or r24,r9,r8
	r24.u64 = ctx.r9.u64 | ctx.r8.u64;
	// or r11,r11,r16
	r11.u64 = r11.u64 | r16.u64;
	// lwz r21,184(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r22,132(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,32(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r11,32(r14)
	PPC_STORE_U32(r14.u32 + 32, r11.u32);
	// b 0x826ea258
	goto loc_826EA258;
loc_826E9E30:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826ea258
	if (cr6.eq) goto loc_826EA258;
	// addi r11,r27,-1
	r11.s64 = r27.s64 + -1;
	// and r11,r11,r21
	r11.u64 = r11.u64 & r21.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826e9fb4
	if (cr6.eq) goto loc_826E9FB4;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,440(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 440);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,6,58
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 6) & 0x3F;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e9f40
	if (cr6.lt) goto loc_826E9F40;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826e9f30
	if (!cr6.lt) goto loc_826E9F30;
loc_826E9E90:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826e9ec4
	if (cr6.lt) goto loc_826E9EC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826e9e90
	if (cr6.eq) goto loc_826E9E90;
	// lis r11,0
	r11.s64 = 0;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
	// b 0x826e9f88
	goto loc_826E9F88;
loc_826E9EC4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826E9F30:
	// lis r11,0
	r11.s64 = 0;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
	// b 0x826e9f88
	goto loc_826E9F88;
loc_826E9F40:
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
loc_826E9F54:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r26
	r11.u64 = r29.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826e9f54
	if (cr6.lt) goto loc_826E9F54;
loc_826E9F88:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826ea3d0
	if (!cr6.eq) goto loc_826EA3D0;
	// add r11,r29,r30
	r11.u64 = r29.u64 + r30.u64;
	// add r10,r29,r30
	ctx.r10.u64 = r29.u64 + r30.u64;
	// lbz r23,516(r11)
	r23.u64 = PPC_LOAD_U8(r11.u32 + 516);
	// lbz r11,524(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 524);
	// stw r23,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r23.u32);
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// b 0x826e9fbc
	goto loc_826E9FBC;
loc_826E9FB4:
	// lis r11,0
	r11.s64 = 0;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
loc_826E9FBC:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x826ea008
	if (!cr6.eq) goto loc_826EA008;
	// lwz r31,20(r14)
	r31.u64 = PPC_LOAD_U32(r14.u32 + 20);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r4,0(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// lbz r5,924(r30)
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + 924);
	// bl 0x826fc940
	sub_826FC940(ctx, base);
	// rlwinm r11,r3,1,24,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFE;
	// lwz r10,24(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 24);
	// li r8,0
	ctx.r8.s64 = 0;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// ori r24,r24,1
	r24.u64 = r24.u64 | 1;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// stw r11,20(r14)
	PPC_STORE_U32(r14.u32 + 20, r11.u32);
	// stb r3,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r3.u8);
	// lwz r11,24(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// b 0x826ea24c
	goto loc_826EA24C;
loc_826EA008:
	// cmpwi cr6,r23,2
	cr6.compare<int32_t>(r23.s32, 2, xer);
	// bgt cr6,0x826ea094
	if (cr6.gt) goto loc_826EA094;
	// or r11,r21,r27
	r11.u64 = r21.u64 | r27.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ea024
	if (cr6.eq) goto loc_826EA024;
	// lwz r11,164(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// b 0x826ea1d0
	goto loc_826EA1D0;
loc_826EA024:
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826ea050
	if (!cr0.lt) goto loc_826EA050;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EA050:
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// bne cr6,0x826ea08c
	if (!cr6.eq) goto loc_826EA08C;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826ea084
	if (!cr0.lt) goto loc_826EA084;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EA084:
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// b 0x826ea1d0
	goto loc_826EA1D0;
loc_826EA08C:
	// li r11,3
	r11.s64 = 3;
	// b 0x826ea1d0
	goto loc_826EA1D0;
loc_826EA094:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,444(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 444);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,6,58
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 6) & 0x3F;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826ea17c
	if (cr6.lt) goto loc_826EA17C;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826ea174
	if (!cr6.lt) goto loc_826EA174;
loc_826EA0DC:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826ea108
	if (cr6.lt) goto loc_826EA108;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826ea0dc
	if (cr6.eq) goto loc_826EA0DC;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826ea1bc
	goto loc_826EA1BC;
loc_826EA108:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EA174:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826ea1bc
	goto loc_826EA1BC;
loc_826EA17C:
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826EA188:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r26
	r11.u64 = r29.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826ea188
	if (cr6.lt) goto loc_826EA188;
loc_826EA1BC:
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826ea3d0
	if (!cr6.eq) goto loc_826EA3D0;
loc_826EA1D0:
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// lwz r27,24(r14)
	r27.u64 = PPC_LOAD_U32(r14.u32 + 24);
	// rlwinm r10,r23,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r29,20(r14)
	r29.u64 = PPC_LOAD_U32(r14.u32 + 20);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lwz r26,0(r22)
	r26.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// add r8,r23,r30
	ctx.r8.u64 = r23.u64 + r30.u64;
	// lbz r28,160(r9)
	r28.u64 = PPC_LOAD_U8(ctx.r9.u32 + 160);
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// li r31,0
	r31.s64 = 0;
	// or r24,r11,r24
	r24.u64 = r11.u64 | r24.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// lbz r25,924(r8)
	r25.u64 = PPC_LOAD_U8(ctx.r8.u32 + 924);
	// ble cr6,0x826ea238
	if (!cr6.gt) goto loc_826EA238;
loc_826EA20C:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x826fc940
	sub_826FC940(ctx, base);
	// stbx r3,r31,r27
	PPC_STORE_U8(r31.u32 + r27.u32, ctx.r3.u8);
	// rlwinm r11,r3,1,24,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFE;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
	// blt cr6,0x826ea20c
	if (cr6.lt) goto loc_826EA20C;
loc_826EA238:
	// cmpwi cr6,r29,-1
	cr6.compare<int32_t>(r29.s32, -1, xer);
	// beq cr6,0x826ea3d0
	if (cr6.eq) goto loc_826EA3D0;
	// lwz r11,24(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 24);
	// stw r29,20(r14)
	PPC_STORE_U32(r14.u32 + 20, r29.u32);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
loc_826EA24C:
	// li r27,0
	r27.s64 = 0;
	// stw r11,24(r14)
	PPC_STORE_U32(r14.u32 + 24, r11.u32);
	// stw r27,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r27.u32);
loc_826EA258:
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// rldicr r24,r24,8,55
	r24.u64 = __builtin_rotateleft64(r24.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// srawi r18,r18,1
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x1) != 0);
	r18.s64 = r18.s32 >> 1;
	// cmpwi cr6,r20,6
	cr6.compare<int32_t>(r20.s32, 6, xer);
	// stw r20,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, r20.u32);
	// std r24,208(r1)
	PPC_STORE_U64(ctx.r1.u32 + 208, r24.u64);
	// blt cr6,0x826e9140
	if (cr6.lt) goto loc_826E9140;
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rldicl r3,r24,56,8
	ctx.r3.u64 = __builtin_rotateleft64(r24.u64, 56) & 0xFFFFFFFFFFFFFF;
	// lwz r9,144(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lbz r10,1260(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 1260);
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r31,84(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// lbz r6,5(r8)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + 5);
	// rlwinm r9,r11,24,29,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0x7;
	// lbz r11,4(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 4);
	// rldicr r5,r11,8,63
	ctx.r5.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r10,4(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 4);
	// lwz r9,1248(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 1248);
	// neg r4,r11
	ctx.r4.s64 = -r11.s64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// xor r11,r4,r11
	r11.u64 = ctx.r4.u64 ^ r11.u64;
	// srawi r11,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 31;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// or r11,r7,r11
	r11.u64 = ctx.r7.u64 | r11.u64;
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// or r11,r11,r6
	r11.u64 = r11.u64 | ctx.r6.u64;
	// clrldi r11,r11,56
	r11.u64 = r11.u64 & 0xFF;
	// or r11,r5,r11
	r11.u64 = ctx.r5.u64 | r11.u64;
	// rldicr r11,r11,48,15
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFF000000000000;
	// or r11,r11,r3
	r11.u64 = r11.u64 | ctx.r3.u64;
	// stdx r11,r10,r9
	PPC_STORE_U64(ctx.r10.u32 + ctx.r9.u32, r11.u64);
	// bne cr6,0x826ea3d4
	if (!cr6.eq) goto loc_826EA3D4;
loc_826EA2E8:
	// lhz r10,18(r14)
	ctx.r10.u64 = PPC_LOAD_U16(r14.u32 + 18);
	// addi r7,r30,1381
	ctx.r7.s64 = r30.s64 + 1381;
	// lwz r11,0(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// addi r6,r10,2
	ctx.r6.s64 = ctx.r10.s64 + 2;
	// lwz r9,4(r14)
	ctx.r9.u64 = PPC_LOAD_U32(r14.u32 + 4);
	// addi r5,r11,2
	ctx.r5.s64 = r11.s64 + 2;
	// lwz r10,8(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 8);
	// lwz r11,12(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 12);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// sth r6,18(r14)
	PPC_STORE_U16(r14.u32 + 18, ctx.r6.u16);
	// stw r5,0(r14)
	PPC_STORE_U32(r14.u32 + 0, ctx.r5.u32);
	// stw r9,4(r14)
	PPC_STORE_U32(r14.u32 + 4, ctx.r9.u32);
	// stw r10,8(r14)
	PPC_STORE_U32(r14.u32 + 8, ctx.r10.u32);
	// stw r11,12(r14)
	PPC_STORE_U32(r14.u32 + 12, r11.u32);
	// lhz r11,50(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// stw r7,1416(r30)
	PPC_STORE_U32(r30.u32 + 1416, ctx.r7.u32);
	// lhz r10,18(r14)
	ctx.r10.u64 = PPC_LOAD_U16(r14.u32 + 18);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x826ea348
	if (!cr6.eq) goto loc_826EA348;
	// addi r11,r30,1386
	r11.s64 = r30.s64 + 1386;
	// stw r11,1416(r30)
	PPC_STORE_U32(r30.u32 + 1416, r11.u32);
loc_826EA348:
	// addi r11,r8,20
	r11.s64 = ctx.r8.s64 + 20;
	// addi r16,r16,1
	r16.s64 = r16.s64 + 1;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r16,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r16.u32);
	// cmplw cr6,r16,r11
	cr6.compare<uint32_t>(r16.u32, r11.u32, xer);
	// blt cr6,0x826e61e8
	if (cr6.lt) goto loc_826E61E8;
	// lwz r28,160(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// lwz r27,136(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
loc_826EA36C:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// addi r15,r15,1
	r15.s64 = r15.s64 + 1;
	// lhz r9,16(r14)
	ctx.r9.u64 = PPC_LOAD_U16(r14.u32 + 16);
	// lwz r10,0(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// lwz r9,192(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// stw r15,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r15.u32);
	// cmplw cr6,r15,r9
	cr6.compare<uint32_t>(r15.u32, ctx.r9.u32, xer);
	// stw r11,0(r14)
	PPC_STORE_U32(r14.u32 + 0, r11.u32);
	// sth r10,16(r14)
	PPC_STORE_U16(r14.u32 + 16, ctx.r10.u16);
	// lhz r11,74(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 74);
	// lhz r9,76(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 76);
	// rotlwi r10,r11,4
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 4);
	// rotlwi r11,r9,3
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// stw r28,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r28.u32);
	// stw r27,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r27.u32);
	// bge cr6,0x826ea3e0
	if (!cr6.lt) goto loc_826EA3E0;
	// lwz r25,436(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r16,0
	r16.s64 = 0;
	// b 0x826e5e6c
	goto loc_826E5E6C;
loc_826EA3D0:
	// li r31,4
	r31.s64 = 4;
loc_826EA3D4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// b 0x8239bd10
	return;
loc_826EA3E0:
	// lwz r11,436(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lwz r10,32(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 32);
	// li r7,-1
	ctx.r7.s64 = -1;
	// ori r8,r8,33684
	ctx.r8.u64 = ctx.r8.u64 | 33684;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,21572(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 21572);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// stwx r10,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, ctx.r10.u32);
	// lwz r10,32(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 32);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// lwz r9,84(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// ld r10,104(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 104);
	// std r10,0(r9)
	PPC_STORE_U64(ctx.r9.u32 + 0, ctx.r10.u64);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,112(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 112);
	// stw r9,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r9.u32);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,116(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 116);
	// stw r9,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r9.u32);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,120(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 120);
	// stw r9,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, ctx.r9.u32);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,124(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 124);
	// stw r9,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, ctx.r9.u32);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,128(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 128);
	// stw r9,24(r10)
	PPC_STORE_U32(ctx.r10.u32 + 24, ctx.r9.u32);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,132(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 132);
	// stw r9,28(r10)
	PPC_STORE_U32(ctx.r10.u32 + 28, ctx.r9.u32);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,136(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// stw r9,32(r10)
	PPC_STORE_U32(ctx.r10.u32 + 32, ctx.r9.u32);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,140(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// stw r9,36(r10)
	PPC_STORE_U32(ctx.r10.u32 + 36, ctx.r9.u32);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,144(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 144);
	// stw r9,40(r10)
	PPC_STORE_U32(ctx.r10.u32 + 40, ctx.r9.u32);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,148(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 148);
	// stw r9,44(r10)
	PPC_STORE_U32(ctx.r10.u32 + 44, ctx.r9.u32);
	// lwz r11,84(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r10,152(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826EA4A8"))) PPC_WEAK_FUNC(sub_826EA4A8);
PPC_FUNC_IMPL(__imp__sub_826EA4A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister r11{};
	// lwz r10,1516(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1516);
	// srawi r9,r4,8
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r4.s32 >> 8;
	// lwz r11,1616(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1616);
	// extsh r8,r4
	ctx.r8.s64 = ctx.r4.s16;
	// rlwinm r10,r10,9,0,22
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 9) & 0xFFFFFE00;
	// rlwinm r9,r9,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF00;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r8,r11
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// addi r10,r10,-256
	ctx.r10.s64 = ctx.r10.s64 + -256;
	// rlwinm r10,r10,0,0,22
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFE00;
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// srawi r10,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 8;
	// rlwinm r11,r11,0,0,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFE0000;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// or r3,r11,r10
	ctx.r3.u64 = r11.u64 | ctx.r10.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826EA4E8"))) PPC_WEAK_FUNC(sub_826EA4E8);
PPC_FUNC_IMPL(__imp__sub_826EA4E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister r11{};
	// srawi r8,r4,8
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r4.s32 >> 8;
	// lwz r11,1616(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1616);
	// lwz r9,1516(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1516);
	// extsh r10,r4
	ctx.r10.s64 = ctx.r4.s16;
	// rlwinm r8,r8,0,0,22
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFE00;
	// rlwinm r9,r9,17,0,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 17) & 0xFFFE0000;
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r8,r8,0,0,14
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFE0000;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// subf r10,r9,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r9.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// addis r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 65536;
	// or r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 | r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826EA524"))) PPC_WEAK_FUNC(sub_826EA524);
PPC_FUNC_IMPL(__imp__sub_826EA524) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826EA528"))) PPC_WEAK_FUNC(sub_826EA528);
PPC_FUNC_IMPL(__imp__sub_826EA528) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// extsh r10,r4
	ctx.r10.s64 = ctx.r4.s16;
	// srawi r7,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = ctx.r4.s32 >> 16;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r6,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 31;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// xor r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r6.u64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x826ea558
	if (!cr6.gt) goto loc_826EA558;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// b 0x826ea590
	goto loc_826EA590;
loc_826EA558:
	// lwz r6,1592(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 1592);
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// bge cr6,0x826ea574
	if (!cr6.lt) goto loc_826EA574;
	// lwz r9,1608(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 1608);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// srawi r3,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r3.s64 = ctx.r10.s32 >> 8;
	// b 0x826ea590
	goto loc_826EA590;
loc_826EA574:
	// lwz r8,1612(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 1612);
	// lwz r6,1600(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 1600);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// xor r8,r6,r9
	ctx.r8.u64 = ctx.r6.u64 ^ ctx.r9.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// subf r3,r9,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r9.s64;
loc_826EA590:
	// lwz r10,1516(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 1516);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826ea5c8
	if (!cr6.gt) goto loc_826EA5C8;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826ea60c
	goto loc_826EA60C;
loc_826EA5C8:
	// lwz r7,1596(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 1596);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826ea5e8
	if (!cr6.lt) goto loc_826EA5E8;
	// lwz r9,1608(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 1608);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 7;
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826ea60c
	goto loc_826EA60C;
loc_826EA5E8:
	// lwz r8,1604(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 1604);
	// lwz r7,1612(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 1612);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r10,r7,r10
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 7;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_826EA60C:
	// lhz r9,64(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// ble cr6,0x826ea624
	if (!cr6.gt) goto loc_826EA624;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// b 0x826ea634
	goto loc_826EA634;
loc_826EA624:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x826ea634
	if (!cr6.lt) goto loc_826EA634;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_826EA634:
	// lhz r11,62(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r3,r9
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r9.s32, xer);
	// ble cr6,0x826ea650
	if (!cr6.gt) goto loc_826EA650;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// rlwimi r3,r10,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// blr 
	return;
loc_826EA650:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bge cr6,0x826ea660
	if (!cr6.lt) goto loc_826EA660;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
loc_826EA660:
	// rlwimi r3,r10,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826EA668"))) PPC_WEAK_FUNC(sub_826EA668);
PPC_FUNC_IMPL(__imp__sub_826EA668) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// extsh r10,r4
	ctx.r10.s64 = ctx.r4.s16;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// lwz r9,1516(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 1516);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r6,r9,1
	xer.ca = ctx.r9.u32 <= 1;
	ctx.r6.s64 = 1 - ctx.r9.s64;
	// srawi r7,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = ctx.r4.s32 >> 16;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r5,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r5.u64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// bgt cr6,0x826ea6d4
	if (cr6.gt) goto loc_826EA6D4;
	// lwz r5,1592(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 1592);
	// cmpw cr6,r8,r5
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, xer);
	// bge cr6,0x826ea6b8
	if (!cr6.lt) goto loc_826EA6B8;
	// lwz r9,1608(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 1608);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// b 0x826ea6d4
	goto loc_826EA6D4;
loc_826EA6B8:
	// lwz r8,1612(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 1612);
	// lwz r5,1600(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 1600);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// xor r8,r5,r9
	ctx.r8.u64 = ctx.r5.u64 ^ ctx.r9.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
loc_826EA6D4:
	// srawi r9,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 1;
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// srawi r5,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// cmpwi cr6,r7,63
	cr6.compare<int32_t>(ctx.r7.s32, 63, xer);
	// ble cr6,0x826ea6fc
	if (!cr6.gt) goto loc_826EA6FC;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826ea740
	goto loc_826EA740;
loc_826EA6FC:
	// lwz r5,1596(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 1596);
	// cmpw cr6,r7,r5
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r5.s32, xer);
	// bge cr6,0x826ea71c
	if (!cr6.lt) goto loc_826EA71C;
	// lwz r8,1608(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 1608);
	// mullw r9,r8,r9
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// rlwinm r9,r9,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826ea740
	goto loc_826EA740;
loc_826EA71C:
	// lwz r7,1604(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 1604);
	// lwz r5,1612(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 1612);
	// xor r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r8.u64;
	// mullw r9,r5,r9
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r9.s32);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// srawi r7,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 7;
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r7,0,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFE;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
loc_826EA740:
	// lhz r8,64(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 64);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// addi r7,r8,-1
	ctx.r7.s64 = ctx.r8.s64 + -1;
	// cmpw cr6,r9,r7
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, xer);
	// ble cr6,0x826ea75c
	if (!cr6.gt) goto loc_826EA75C;
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// b 0x826ea76c
	goto loc_826EA76C;
loc_826EA75C:
	// subfic r8,r8,1
	xer.ca = ctx.r8.u32 <= 1;
	ctx.r8.s64 = 1 - ctx.r8.s64;
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// bge cr6,0x826ea76c
	if (!cr6.lt) goto loc_826EA76C;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
loc_826EA76C:
	// lhz r11,62(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 62);
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// ble cr6,0x826ea78c
	if (!cr6.gt) goto loc_826EA78C;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// rlwimi r3,r9,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// blr 
	return;
loc_826EA78C:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826ea79c
	if (!cr6.lt) goto loc_826EA79C;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826EA79C:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// rlwimi r3,r9,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826EA7A8"))) PPC_WEAK_FUNC(sub_826EA7A8);
PPC_FUNC_IMPL(__imp__sub_826EA7A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// extsh r10,r4
	ctx.r10.s64 = ctx.r4.s16;
	// srawi r7,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = ctx.r4.s32 >> 16;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r6,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 31;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// xor r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r6.u64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x826ea7d8
	if (!cr6.gt) goto loc_826EA7D8;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// b 0x826ea810
	goto loc_826EA810;
loc_826EA7D8:
	// lwz r6,1560(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 1560);
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// bge cr6,0x826ea7f4
	if (!cr6.lt) goto loc_826EA7F4;
	// lwz r9,1576(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 1576);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// srawi r3,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r3.s64 = ctx.r10.s32 >> 8;
	// b 0x826ea810
	goto loc_826EA810;
loc_826EA7F4:
	// lwz r8,1580(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 1580);
	// lwz r6,1568(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 1568);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// xor r8,r6,r9
	ctx.r8.u64 = ctx.r6.u64 ^ ctx.r9.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// subf r3,r9,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r9.s64;
loc_826EA810:
	// lhz r10,62(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 62);
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// cmpw cr6,r3,r9
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r9.s32, xer);
	// ble cr6,0x826ea828
	if (!cr6.gt) goto loc_826EA828;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// b 0x826ea838
	goto loc_826EA838;
loc_826EA828:
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// cmpw cr6,r3,r10
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r10.s32, xer);
	// bge cr6,0x826ea838
	if (!cr6.lt) goto loc_826EA838;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
loc_826EA838:
	// lwz r10,1516(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 1516);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826ea870
	if (!cr6.gt) goto loc_826EA870;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826ea8b4
	goto loc_826EA8B4;
loc_826EA870:
	// lwz r7,1564(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826ea890
	if (!cr6.lt) goto loc_826EA890;
	// lwz r9,1576(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 1576);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 7;
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826ea8b4
	goto loc_826EA8B4;
loc_826EA890:
	// lwz r8,1572(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 1572);
	// lwz r7,1580(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r10,r7,r10
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 7;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_826EA8B4:
	// lhz r11,64(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 64);
	// addi r9,r11,-2
	ctx.r9.s64 = r11.s64 + -2;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826ea8d0
	if (!cr6.gt) goto loc_826EA8D0;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// rlwimi r3,r10,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// blr 
	return;
loc_826EA8D0:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826ea8e0
	if (!cr6.lt) goto loc_826EA8E0;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826EA8E0:
	// rlwimi r3,r10,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826EA8E8"))) PPC_WEAK_FUNC(sub_826EA8E8);
PPC_FUNC_IMPL(__imp__sub_826EA8E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister r11{};
	// srawi r8,r4,8
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r4.s32 >> 8;
	// lwz r11,1584(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1584);
	// lwz r9,1516(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1516);
	// extsh r10,r4
	ctx.r10.s64 = ctx.r4.s16;
	// rlwinm r8,r8,0,0,22
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFE00;
	// rlwinm r9,r9,17,0,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 17) & 0xFFFE0000;
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r8,r8,0,0,14
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFE0000;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// subf r10,r9,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r9.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// addis r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 65536;
	// or r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 | r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826EA924"))) PPC_WEAK_FUNC(sub_826EA924);
PPC_FUNC_IMPL(__imp__sub_826EA924) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826EA928"))) PPC_WEAK_FUNC(sub_826EA928);
PPC_FUNC_IMPL(__imp__sub_826EA928) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r11,2928(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2928);
	// lwz r10,2088(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2088);
	// addi r8,r11,726
	ctx.r8.s64 = r11.s64 + 726;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r10,263
	ctx.r10.s64 = ctx.r10.s64 + 263;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwzx r8,r8,r31
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r8,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, ctx.r8.u32);
	// lwzx r8,r7,r31
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + r31.u32);
	// stw r8,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, ctx.r8.u32);
	// lwz r11,2100(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 2100);
	// stw r11,2092(r31)
	PPC_STORE_U32(r31.u32 + 2092, r11.u32);
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r9,456(r31)
	PPC_STORE_U32(r31.u32 + 456, ctx.r9.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(r31.u32 + 2096, r11.u32);
	// bl 0x82628f88
	sub_82628F88(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261ca78
	sub_8261CA78(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826EA9B0"))) PPC_WEAK_FUNC(sub_826EA9B0);
PPC_FUNC_IMPL(__imp__sub_826EA9B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r19,r4
	r19.u64 = ctx.r4.u64;
	// li r17,0
	r17.s64 = 0;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r22,r17
	r22.u64 = r17.u64;
	// lbz r11,29(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 29);
	// lwz r10,0(r19)
	ctx.r10.u64 = PPC_LOAD_U32(r19.u32 + 0);
	// lbz r24,34(r25)
	r24.u64 = PPC_LOAD_U8(r25.u32 + 34);
	// mr r28,r11
	r28.u64 = r11.u64;
	// lbz r16,5(r19)
	r16.u64 = PPC_LOAD_U8(r19.u32 + 5);
	// rlwinm r20,r10,12,30,31
	r20.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x3;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ea9f4
	if (cr6.eq) goto loc_826EA9F4;
	// rlwinm r24,r10,8,29,31
	r24.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0x7;
loc_826EA9F4:
	// lis r11,0
	r11.s64 = 0;
	// mr r18,r17
	r18.u64 = r17.u64;
	// ori r21,r11,32768
	r21.u64 = r11.u64 | 32768;
loc_826EAA00:
	// clrlwi r11,r16,31
	r11.u64 = r16.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826eae2c
	if (cr6.eq) goto loc_826EAE2C;
	// lwz r11,0(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eab74
	if (cr6.eq) goto loc_826EAB74;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x826eab74
	if (!cr6.eq) goto loc_826EAB74;
	// lwz r11,440(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 440);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826eab18
	if (cr6.lt) goto loc_826EAB18;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826eab10
	if (!cr6.lt) goto loc_826EAB10;
loc_826EAA78:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826eaaa4
	if (cr6.lt) goto loc_826EAAA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826eaa78
	if (cr6.eq) goto loc_826EAA78;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826eab54
	goto loc_826EAB54;
loc_826EAAA4:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EAB10:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826eab54
	goto loc_826EAB54;
loc_826EAB18:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826EAB20:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r21
	r11.u64 = r30.u64 + r21.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826eab20
	if (cr6.lt) goto loc_826EAB20;
loc_826EAB54:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826ead0c
	if (!cr6.eq) goto loc_826EAD0C;
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// add r10,r30,r25
	ctx.r10.u64 = r30.u64 + r25.u64;
	// lbz r24,516(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 516);
	// lbz r20,524(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + 524);
loc_826EAB74:
	// add r10,r18,r19
	ctx.r10.u64 = r18.u64 + r19.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// stb r24,6(r10)
	PPC_STORE_U8(ctx.r10.u32 + 6, r24.u8);
	// bne cr6,0x826eabc8
	if (!cr6.eq) goto loc_826EABC8;
	// lwz r31,20(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 20);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r4,236(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + 236);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// lbz r5,924(r25)
	ctx.r5.u64 = PPC_LOAD_U8(r25.u32 + 924);
	// bl 0x826fc940
	sub_826FC940(ctx, base);
	// rlwinm r11,r3,1,24,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFE;
	// lwz r10,24(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// ori r22,r22,1
	r22.u64 = r22.u64 | 1;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// mr r28,r17
	r28.u64 = r17.u64;
	// stw r11,20(r23)
	PPC_STORE_U32(r23.u32 + 20, r11.u32);
	// stb r3,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r3.u8);
	// lwz r11,24(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,24(r23)
	PPC_STORE_U32(r23.u32 + 24, r11.u32);
	// b 0x826eae34
	goto loc_826EAE34;
loc_826EABC8:
	// cmpwi cr6,r24,4
	cr6.compare<int32_t>(r24.s32, 4, xer);
	// bne cr6,0x826ead18
	if (!cr6.eq) goto loc_826EAD18;
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r11,444(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 444);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,6,58
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 6) & 0x3F;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826eacb8
	if (cr6.lt) goto loc_826EACB8;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826eacb0
	if (!cr6.lt) goto loc_826EACB0;
loc_826EAC18:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826eac44
	if (cr6.lt) goto loc_826EAC44;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826eac18
	if (cr6.eq) goto loc_826EAC18;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826eacf8
	goto loc_826EACF8;
loc_826EAC44:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EACB0:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826eacf8
	goto loc_826EACF8;
loc_826EACB8:
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826EACC4:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r21
	r11.u64 = r30.u64 + r21.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826eacc4
	if (cr6.lt) goto loc_826EACC4;
loc_826EACF8:
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826eada4
	if (cr6.eq) goto loc_826EADA4;
loc_826EAD0C:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd18
	return;
loc_826EAD18:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x826eada0
	if (!cr6.eq) goto loc_826EADA0;
	// lwz r11,0(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826eada0
	if (!cr6.eq) goto loc_826EADA0;
	// lwz r3,0(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826ead5c
	if (!cr0.lt) goto loc_826EAD5C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EAD5C:
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// bne cr6,0x826ead98
	if (!cr6.eq) goto loc_826EAD98;
	// lwz r3,0(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826ead90
	if (!cr0.lt) goto loc_826EAD90;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EAD90:
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// b 0x826eada4
	goto loc_826EADA4;
loc_826EAD98:
	// li r11,3
	r11.s64 = 3;
	// b 0x826eada4
	goto loc_826EADA4;
loc_826EADA0:
	// mr r11,r20
	r11.u64 = r20.u64;
loc_826EADA4:
	// add r10,r11,r25
	ctx.r10.u64 = r11.u64 + r25.u64;
	// lwz r28,236(r25)
	r28.u64 = PPC_LOAD_U32(r25.u32 + 236);
	// rlwinm r9,r24,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r26,24(r23)
	r26.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lwz r30,20(r23)
	r30.u64 = PPC_LOAD_U32(r23.u32 + 20);
	// mr r31,r17
	r31.u64 = r17.u64;
	// lbz r29,160(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 160);
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// add r9,r24,r25
	ctx.r9.u64 = r24.u64 + r25.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// or r22,r11,r22
	r22.u64 = r11.u64 | r22.u64;
	// lbz r27,924(r9)
	r27.u64 = PPC_LOAD_U8(ctx.r9.u32 + 924);
	// ble cr6,0x826eae0c
	if (!cr6.gt) goto loc_826EAE0C;
loc_826EADE0:
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x826fc940
	sub_826FC940(ctx, base);
	// stbx r3,r31,r26
	PPC_STORE_U8(r31.u32 + r26.u32, ctx.r3.u8);
	// rlwinm r11,r3,1,24,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFE;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// cmpw cr6,r31,r29
	cr6.compare<int32_t>(r31.s32, r29.s32, xer);
	// blt cr6,0x826eade0
	if (cr6.lt) goto loc_826EADE0;
loc_826EAE0C:
	// cmpwi cr6,r30,-1
	cr6.compare<int32_t>(r30.s32, -1, xer);
	// beq cr6,0x826ead0c
	if (cr6.eq) goto loc_826EAD0C;
	// lwz r11,24(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// mr r28,r17
	r28.u64 = r17.u64;
	// stw r30,20(r23)
	PPC_STORE_U32(r23.u32 + 20, r30.u32);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// stw r11,24(r23)
	PPC_STORE_U32(r23.u32 + 24, r11.u32);
	// b 0x826eae34
	goto loc_826EAE34;
loc_826EAE2C:
	// add r11,r18,r19
	r11.u64 = r18.u64 + r19.u64;
	// stb r17,6(r11)
	PPC_STORE_U8(r11.u32 + 6, r17.u8);
loc_826EAE34:
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// rlwinm r16,r16,31,1,31
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 31) & 0x7FFFFFFF;
	// rldicr r22,r22,8,55
	r22.u64 = __builtin_rotateleft64(r22.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// cmplwi cr6,r18,6
	cr6.compare<uint32_t>(r18.u32, 6, xer);
	// blt cr6,0x826eaa00
	if (cr6.lt) goto loc_826EAA00;
	// lwz r11,0(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 0);
	// rldicl r10,r22,56,8
	ctx.r10.u64 = __builtin_rotateleft64(r22.u64, 56) & 0xFFFFFFFFFFFFFF;
	// lbz r9,4(r19)
	ctx.r9.u64 = PPC_LOAD_U8(r19.u32 + 4);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// lbz r8,5(r19)
	ctx.r8.u64 = PPC_LOAD_U8(r19.u32 + 5);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lwz r7,4(r23)
	ctx.r7.u64 = PPC_LOAD_U32(r23.u32 + 4);
	// rlwinm r11,r11,0,24,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xC0;
	// lwz r6,1248(r25)
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + 1248);
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// rldicr r11,r11,48,15
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFF000000000000;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stdx r11,r7,r6
	PPC_STORE_U64(ctx.r7.u32 + ctx.r6.u32, r11.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_826EAE90"))) PPC_WEAK_FUNC(sub_826EAE90);
PPC_FUNC_IMPL(__imp__sub_826EAE90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826eaf94
	if (cr6.lt) goto loc_826EAF94;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826eaf8c
	if (!cr6.lt) goto loc_826EAF8C;
loc_826EAEF4:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826eaf20
	if (cr6.lt) goto loc_826EAF20;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826eaef4
	if (cr6.eq) goto loc_826EAEF4;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826eafd8
	goto loc_826EAFD8;
loc_826EAF20:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EAF8C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826eafd8
	goto loc_826EAFD8;
loc_826EAF94:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r29,r11,32768
	r29.u64 = r11.u64 | 32768;
loc_826EAFA4:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826eafa4
	if (cr6.lt) goto loc_826EAFA4;
loc_826EAFD8:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// li r29,0
	r29.s64 = 0;
	// bne cr6,0x826eb17c
	if (!cr6.eq) goto loc_826EB17C;
	// lwz r11,16(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826eb0c4
	if (cr6.eq) goto loc_826EB0C4;
	// li r30,2
	r30.s64 = 2;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x826eb060
	if (!cr6.lt) goto loc_826EB060;
loc_826EB008:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eb060
	if (cr6.eq) goto loc_826EB060;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826eb050
	if (!cr0.lt) goto loc_826EB050;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB050:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826eb008
	if (cr6.gt) goto loc_826EB008;
loc_826EB060:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eb09c
	if (!cr0.lt) goto loc_826EB09C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB09C:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// srawi r10,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r10.s64 = r30.s32 >> 1;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// xor r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 ^ r11.u64;
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// li r11,0
	r11.s64 = 0;
	// rlwimi r3,r11,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_826EB0C4:
	// li r30,1
	r30.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826eb128
	if (!cr6.lt) goto loc_826EB128;
loc_826EB0D0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eb128
	if (cr6.eq) goto loc_826EB128;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826eb118
	if (!cr0.lt) goto loc_826EB118;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB118:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826eb0d0
	if (cr6.gt) goto loc_826EB0D0;
loc_826EB128:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eb164
	if (!cr0.lt) goto loc_826EB164;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB164:
	// rlwinm r11,r30,1,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r3,r11,1
	xer.ca = r11.u32 <= 1;
	ctx.r3.s64 = 1 - r11.s64;
	// li r11,0
	r11.s64 = 0;
	// rlwimi r3,r11,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_826EB17C:
	// cmpwi cr6,r30,71
	cr6.compare<int32_t>(r30.s32, 71, xer);
	// bne cr6,0x826eb26c
	if (!cr6.eq) goto loc_826EB26C;
	// lhz r11,70(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 70);
	// lhz r10,72(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 72);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826eb1a8
	if (!cr6.eq) goto loc_826EB1A8;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x826eb248
	goto loc_826EB248;
loc_826EB1A8:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826eb208
	if (!cr6.gt) goto loc_826EB208;
loc_826EB1B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eb208
	if (cr6.eq) goto loc_826EB208;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826eb1f8
	if (!cr0.lt) goto loc_826EB1F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB1F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826eb1b0
	if (cr6.gt) goto loc_826EB1B0;
loc_826EB208:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eb244
	if (!cr0.lt) goto loc_826EB244;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB244:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_826EB248:
	// lhz r11,72(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 72);
	// li r9,1
	ctx.r9.s64 = 1;
	// slw r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// sraw r3,r10,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r3.s64 = ctx.r10.s32 >> temp.u32;
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// rlwimi r3,r11,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_826EB26C:
	// lwz r11,16(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// srawi r8,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r8.s64 = r11.s32 >> 4;
	// clrlwi r9,r11,28
	ctx.r9.u64 = r11.u32 & 0xF;
	// clrlwi r28,r8,28
	r28.u64 = ctx.r8.u32 & 0xF;
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// srawi r7,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r11.s32 >> 16;
	// srawi r11,r11,24
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFFF) != 0);
	r11.s64 = r11.s32 >> 24;
	// add r30,r9,r28
	r30.u64 = ctx.r9.u64 + r28.u64;
	// clrlwi r25,r11,24
	r25.u64 = r11.u32 & 0xFF;
	// clrlwi r27,r8,24
	r27.u64 = ctx.r8.u32 & 0xFF;
	// clrlwi r26,r7,24
	r26.u64 = ctx.r7.u32 & 0xFF;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826eb2b8
	if (!cr6.eq) goto loc_826EB2B8;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x826eb358
	goto loc_826EB358;
loc_826EB2B8:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826eb318
	if (!cr6.gt) goto loc_826EB318;
loc_826EB2C0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eb318
	if (cr6.eq) goto loc_826EB318;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826eb308
	if (!cr0.lt) goto loc_826EB308;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB308:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826eb2c0
	if (cr6.gt) goto loc_826EB2C0;
loc_826EB318:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eb354
	if (!cr0.lt) goto loc_826EB354;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB354:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_826EB358:
	// and r11,r10,r25
	r11.u64 = ctx.r10.u64 & r25.u64;
	// sraw r10,r10,r28
	temp.u32 = r28.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// clrlwi r8,r10,31
	ctx.r8.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// neg r8,r8
	ctx.r8.s64 = -ctx.r8.s64;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r3,r8,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r8.s64;
	// rlwimi r3,r11,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_826EB39C"))) PPC_WEAK_FUNC(sub_826EB39C);
PPC_FUNC_IMPL(__imp__sub_826EB39C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826EB3A0"))) PPC_WEAK_FUNC(sub_826EB3A0);
PPC_FUNC_IMPL(__imp__sub_826EB3A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826eb4a4
	if (cr6.lt) goto loc_826EB4A4;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826eb49c
	if (!cr6.lt) goto loc_826EB49C;
loc_826EB404:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826eb430
	if (cr6.lt) goto loc_826EB430;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826eb404
	if (cr6.eq) goto loc_826EB404;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826eb4e8
	goto loc_826EB4E8;
loc_826EB430:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EB49C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826eb4e8
	goto loc_826EB4E8;
loc_826EB4A4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r29,r11,32768
	r29.u64 = r11.u64 | 32768;
loc_826EB4B4:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826eb4b4
	if (cr6.lt) goto loc_826EB4B4;
loc_826EB4E8:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// li r29,0
	r29.s64 = 0;
	// bne cr6,0x826eb68c
	if (!cr6.eq) goto loc_826EB68C;
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826eb5d4
	if (cr6.eq) goto loc_826EB5D4;
	// li r30,2
	r30.s64 = 2;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x826eb570
	if (!cr6.lt) goto loc_826EB570;
loc_826EB518:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eb570
	if (cr6.eq) goto loc_826EB570;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826eb560
	if (!cr0.lt) goto loc_826EB560;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB560:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826eb518
	if (cr6.gt) goto loc_826EB518;
loc_826EB570:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eb5ac
	if (!cr0.lt) goto loc_826EB5AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB5AC:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// srawi r10,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r10.s64 = r30.s32 >> 1;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// xor r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 ^ r11.u64;
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// li r11,0
	r11.s64 = 0;
	// rlwimi r3,r11,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_826EB5D4:
	// li r30,1
	r30.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826eb638
	if (!cr6.lt) goto loc_826EB638;
loc_826EB5E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eb638
	if (cr6.eq) goto loc_826EB638;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826eb628
	if (!cr0.lt) goto loc_826EB628;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB628:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826eb5e0
	if (cr6.gt) goto loc_826EB5E0;
loc_826EB638:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eb674
	if (!cr0.lt) goto loc_826EB674;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB674:
	// rlwinm r11,r30,1,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r3,r11,1
	xer.ca = r11.u32 <= 1;
	ctx.r3.s64 = 1 - r11.s64;
	// li r11,0
	r11.s64 = 0;
	// rlwimi r3,r11,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_826EB68C:
	// cmpwi cr6,r30,125
	cr6.compare<int32_t>(r30.s32, 125, xer);
	// bne cr6,0x826eb77c
	if (!cr6.eq) goto loc_826EB77C;
	// lhz r11,70(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 70);
	// lhz r10,72(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 72);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826eb6b8
	if (!cr6.eq) goto loc_826EB6B8;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x826eb758
	goto loc_826EB758;
loc_826EB6B8:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826eb718
	if (!cr6.gt) goto loc_826EB718;
loc_826EB6C0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eb718
	if (cr6.eq) goto loc_826EB718;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826eb708
	if (!cr0.lt) goto loc_826EB708;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB708:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826eb6c0
	if (cr6.gt) goto loc_826EB6C0;
loc_826EB718:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eb754
	if (!cr0.lt) goto loc_826EB754;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB754:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_826EB758:
	// lhz r11,72(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 72);
	// li r9,1
	ctx.r9.s64 = 1;
	// slw r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// sraw r3,r10,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r3.s64 = ctx.r10.s32 >> temp.u32;
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// rlwimi r3,r11,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_826EB77C:
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// srawi r8,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r8.s64 = r11.s32 >> 4;
	// clrlwi r9,r11,28
	ctx.r9.u64 = r11.u32 & 0xF;
	// clrlwi r28,r8,28
	r28.u64 = ctx.r8.u32 & 0xF;
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// srawi r7,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r11.s32 >> 16;
	// srawi r11,r11,24
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFFF) != 0);
	r11.s64 = r11.s32 >> 24;
	// add r30,r9,r28
	r30.u64 = ctx.r9.u64 + r28.u64;
	// clrlwi r25,r11,24
	r25.u64 = r11.u32 & 0xFF;
	// clrlwi r27,r8,24
	r27.u64 = ctx.r8.u32 & 0xFF;
	// clrlwi r26,r7,24
	r26.u64 = ctx.r7.u32 & 0xFF;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826eb7c8
	if (!cr6.eq) goto loc_826EB7C8;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x826eb868
	goto loc_826EB868;
loc_826EB7C8:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826eb828
	if (!cr6.gt) goto loc_826EB828;
loc_826EB7D0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eb828
	if (cr6.eq) goto loc_826EB828;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826eb818
	if (!cr0.lt) goto loc_826EB818;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB818:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826eb7d0
	if (cr6.gt) goto loc_826EB7D0;
loc_826EB828:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eb864
	if (!cr0.lt) goto loc_826EB864;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EB864:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_826EB868:
	// li r11,1
	r11.s64 = 1;
	// slw r9,r11,r28
	ctx.r9.u64 = r28.u8 & 0x20 ? 0 : (r11.u32 << (r28.u8 & 0x3F));
	// sraw r11,r10,r28
	temp.u32 = r28.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	r11.s64 = ctx.r10.s32 >> temp.u32;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// srawi r7,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r7.s64 = r11.s32 >> 1;
	// and r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ctx.r10.u64;
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// clrlwi r8,r10,31
	ctx.r8.u64 = ctx.r10.u32 & 0x1;
	// srawi r11,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r11.s64 = ctx.r10.s32 >> 1;
	// neg r8,r8
	ctx.r8.s64 = -ctx.r8.s64;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// add r10,r7,r27
	ctx.r10.u64 = ctx.r7.u64 + r27.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r9,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// rlwimi r3,r11,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_826EB8C0"))) PPC_WEAK_FUNC(sub_826EB8C0);
PPC_FUNC_IMPL(__imp__sub_826EB8C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r25,0(r8)
	r25.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// li r22,0
	r22.s64 = 0;
	// lhz r28,50(r29)
	r28.u64 = PPC_LOAD_U16(r29.u32 + 50);
	// lwz r31,188(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 188);
	// srawi r27,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r27.s64 = r28.s32 >> 1;
	// beq cr6,0x826eb90c
	if (cr6.eq) goto loc_826EB90C;
	// lwz r11,1240(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 1240);
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r26,r22
	r26.u64 = r22.u64;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826eb910
	if (cr6.eq) goto loc_826EB910;
loc_826EB90C:
	// li r26,1
	r26.s64 = 1;
loc_826EB910:
	// mr r23,r22
	r23.u64 = r22.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x826eb92c
	if (cr6.eq) goto loc_826EB92C;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,176(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
loc_826EB92C:
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// stw r22,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r22.u32);
	// stw r22,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r22.u32);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// stw r22,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r22.u32);
	// beq cr6,0x826eb9dc
	if (cr6.eq) goto loc_826EB9DC;
	// lwz r10,-20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + -20);
	// addi r11,r25,-1
	r11.s64 = r25.s64 + -1;
	// rlwinm r9,r10,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826eb9dc
	if (cr6.eq) goto loc_826EB9DC;
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,512
	cr6.compare<uint32_t>(ctx.r10.u32, 512, xer);
	// bge cr6,0x826eb974
	if (!cr6.lt) goto loc_826EB974;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826eb9d0
	goto loc_826EB9D0;
loc_826EB974:
	// add r10,r11,r28
	ctx.r10.u64 = r11.u64 + r28.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lhz r11,90(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lhz r11,86(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lhz r11,84(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// sth r10,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r10.u16);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
loc_826EB9D0:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
loc_826EB9DC:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bne cr6,0x826ebb58
	if (!cr6.eq) goto loc_826EBB58;
	// rlwinm r10,r27,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r28,r25
	r11.s64 = r25.s64 - r28.s64;
	// add r10,r27,r10
	ctx.r10.u64 = r27.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r6,r10,r30
	ctx.r6.s64 = r30.s64 - ctx.r10.s64;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm r9,r10,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826eba94
	if (cr6.eq) goto loc_826EBA94;
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,512
	cr6.compare<uint32_t>(ctx.r10.u32, 512, xer);
	// bge cr6,0x826eba24
	if (!cr6.lt) goto loc_826EBA24;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// b 0x826eba80
	goto loc_826EBA80;
loc_826EBA24:
	// subf r10,r28,r11
	ctx.r10.s64 = r11.s64 - r28.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lhz r10,90(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r9,86(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// lhz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// lhz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// sth r9,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r9.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
loc_826EBA80:
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
loc_826EBA94:
	// cmpwi cr6,r27,1
	cr6.compare<int32_t>(r27.s32, 1, xer);
	// beq cr6,0x826ebb58
	if (cr6.eq) goto loc_826EBB58;
	// addi r10,r27,-1
	ctx.r10.s64 = r27.s64 + -1;
	// cmpw cr6,r24,r10
	cr6.compare<int32_t>(r24.s32, ctx.r10.s32, xer);
	// beq cr6,0x826ebab4
	if (cr6.eq) goto loc_826EBAB4;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r10,r6,20
	ctx.r10.s64 = ctx.r6.s64 + 20;
	// b 0x826ebabc
	goto loc_826EBABC;
loc_826EBAB4:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r6,-20
	ctx.r10.s64 = ctx.r6.s64 + -20;
loc_826EBABC:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r9,r10,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ebb58
	if (cr6.eq) goto loc_826EBB58;
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,512
	cr6.compare<uint32_t>(ctx.r10.u32, 512, xer);
	// bge cr6,0x826ebae8
	if (!cr6.lt) goto loc_826EBAE8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ebb44
	goto loc_826EBB44;
loc_826EBAE8:
	// subf r10,r28,r11
	ctx.r10.s64 = r11.s64 - r28.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lhz r11,86(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lwzx r11,r9,r31
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lhz r11,90(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// lhz r11,84(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// sth r10,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r10.u16);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
loc_826EBB44:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
loc_826EBB58:
	// cmpwi cr6,r5,2
	cr6.compare<int32_t>(ctx.r5.s32, 2, xer);
	// blt cr6,0x826ebc04
	if (cr6.lt) goto loc_826EBC04;
	// lhz r11,102(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// lhz r10,98(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// lhz r7,106(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 106);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 100);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 104);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r3,r7,r11
	ctx.r3.s64 = r11.s64 - ctx.r7.s64;
	// subf r30,r10,r7
	r30.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r27,r6,r9
	r27.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r26,r8,r6
	r26.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 ^ ctx.r5.u64;
	// xor r30,r30,r5
	r30.u64 = r30.u64 ^ ctx.r5.u64;
	// xor r27,r27,r4
	r27.u64 = r27.u64 ^ ctx.r4.u64;
	// srawi r5,r3,31
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r3.s32 >> 31;
	// xor r26,r26,r4
	r26.u64 = r26.u64 ^ ctx.r4.u64;
	// srawi r4,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r30.s32 >> 31;
	// srawi r3,r27,31
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r27.s32 >> 31;
	// srawi r30,r26,31
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r26.s32 >> 31;
	// or r27,r5,r4
	r27.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r26,r3,r30
	r26.u64 = ctx.r3.u64 | r30.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r27
	ctx.r7.u64 = ctx.r7.u64 & ~r27.u64;
	// andc r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 & ~r26.u64;
	// and r8,r30,r8
	ctx.r8.u64 = r30.u64 & ctx.r8.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r9,r3,r9
	ctx.r9.u64 = ctx.r3.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// b 0x826ebc1c
	goto loc_826EBC1C;
loc_826EBC04:
	// cmpwi cr6,r5,1
	cr6.compare<int32_t>(ctx.r5.s32, 1, xer);
	// bne cr6,0x826ebc18
	if (!cr6.eq) goto loc_826EBC18;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ebc1c
	goto loc_826EBC1C;
loc_826EBC18:
	// stw r22,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r22.u32);
loc_826EBC1C:
	// lhz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// add r8,r25,r28
	ctx.r8.u64 = r25.u64 + r28.u64;
	// lhz r11,62(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 62);
	// rlwinm r9,r25,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r7,82(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// extsh r5,r6
	ctx.r5.s64 = ctx.r6.s16;
	// lhz r10,64(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 64);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r3,68(r29)
	ctx.r3.u64 = PPC_LOAD_U16(r29.u32 + 68);
	// srawi r6,r23,16
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xFFFF) != 0);
	ctx.r6.s64 = r23.s32 >> 16;
	// lhz r4,66(r29)
	ctx.r4.u64 = PPC_LOAD_U16(r29.u32 + 66);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r5,r3
	ctx.r5.s64 = ctx.r3.s16;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// add r7,r7,r23
	ctx.r7.u64 = ctx.r7.u64 + r23.u64;
	// and r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 & ctx.r5.u64;
	// and r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 & ctx.r4.u64;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// subf r11,r11,r7
	r11.s64 = ctx.r7.s64 - r11.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// li r3,0
	ctx.r3.s64 = 0;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r11.u32);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_826EBCAC"))) PPC_WEAK_FUNC(sub_826EBCAC);
PPC_FUNC_IMPL(__imp__sub_826EBCAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826EBCB0"))) PPC_WEAK_FUNC(sub_826EBCB0);
PPC_FUNC_IMPL(__imp__sub_826EBCB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r17,r3
	r17.u64 = ctx.r3.u64;
	// lwz r18,0(r7)
	r18.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r15,r4
	r15.u64 = ctx.r4.u64;
	// mr r14,r5
	r14.u64 = ctx.r5.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// li r16,0
	r16.s64 = 0;
	// lhz r23,50(r17)
	r23.u64 = PPC_LOAD_U16(r17.u32 + 50);
	// srawi r22,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r22.s64 = r23.s32 >> 1;
	// beq cr6,0x826ebcf8
	if (cr6.eq) goto loc_826EBCF8;
	// lwz r11,1240(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 1240);
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r19,r16
	r19.u64 = r16.u64;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ebcfc
	if (cr6.eq) goto loc_826EBCFC;
loc_826EBCF8:
	// li r19,1
	r19.s64 = 1;
loc_826EBCFC:
	// lwz r11,180(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 180);
	// lwz r31,0(r17)
	r31.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// lwz r26,188(r17)
	r26.u64 = PPC_LOAD_U32(r17.u32 + 188);
	// lhz r25,62(r17)
	r25.u64 = PPC_LOAD_U16(r17.u32 + 62);
	// lhz r21,66(r17)
	r21.u64 = PPC_LOAD_U16(r17.u32 + 66);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lhz r24,64(r17)
	r24.u64 = PPC_LOAD_U16(r17.u32 + 64);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// lhz r20,68(r17)
	r20.u64 = PPC_LOAD_U16(r17.u32 + 68);
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r27,r11
	r27.s64 = r11.s16;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// blt cr6,0x826ebe04
	if (cr6.lt) goto loc_826EBE04;
	// clrlwi r11,r27,28
	r11.u64 = r27.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826ebdfc
	if (!cr6.lt) goto loc_826EBDFC;
loc_826EBD64:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826ebd90
	if (cr6.lt) goto loc_826EBD90;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826ebd64
	if (cr6.eq) goto loc_826EBD64;
	// srawi r27,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r27.s64 = r27.s32 >> 4;
	// b 0x826ebe48
	goto loc_826EBE48;
loc_826EBD90:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EBDFC:
	// srawi r27,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r27.s64 = r27.s32 >> 4;
	// b 0x826ebe48
	goto loc_826EBE48;
loc_826EBE04:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r28,r11,32768
	r28.u64 = r11.u64 | 32768;
loc_826EBE14:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r27
	r29.u64 = r11.u64 + r27.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r28
	r11.u64 = r29.u64 + r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r27,r11
	r27.s64 = r11.s16;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// blt cr6,0x826ebe14
	if (cr6.lt) goto loc_826EBE14;
loc_826EBE48:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ebe64
	if (cr6.eq) goto loc_826EBE64;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239bd10
	return;
loc_826EBE64:
	// rlwinm r11,r27,0,28,28
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x8;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ebe80
	if (cr6.eq) goto loc_826EBE80;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lwz r4,176(r17)
	ctx.r4.u64 = PPC_LOAD_U32(r17.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
loc_826EBE80:
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// stw r16,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r16.u32);
	// stw r16,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r16.u32);
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// stw r16,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r16.u32);
	// beq cr6,0x826ebf30
	if (cr6.eq) goto loc_826EBF30;
	// lwz r10,-20(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + -20);
	// addi r11,r18,-1
	r11.s64 = r18.s64 + -1;
	// rlwinm r9,r10,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ebf30
	if (cr6.eq) goto loc_826EBF30;
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,512
	cr6.compare<uint32_t>(ctx.r10.u32, 512, xer);
	// bge cr6,0x826ebec8
	if (!cr6.lt) goto loc_826EBEC8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ebf24
	goto loc_826EBF24;
loc_826EBEC8:
	// add r10,r11,r23
	ctx.r10.u64 = r11.u64 + r23.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lhz r11,90(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// lwzx r11,r10,r26
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r26.u32);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lhz r11,86(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lhz r11,84(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// sth r10,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r10.u16);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
loc_826EBF24:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
loc_826EBF30:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x826ec0ac
	if (!cr6.eq) goto loc_826EC0AC;
	// rlwinm r10,r22,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r23,r18
	r11.s64 = r18.s64 - r23.s64;
	// add r10,r22,r10
	ctx.r10.u64 = r22.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r6,r10,r15
	ctx.r6.s64 = r15.s64 - ctx.r10.s64;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm r9,r10,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ebfe8
	if (cr6.eq) goto loc_826EBFE8;
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,512
	cr6.compare<uint32_t>(ctx.r10.u32, 512, xer);
	// bge cr6,0x826ebf78
	if (!cr6.lt) goto loc_826EBF78;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r26
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r26.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// b 0x826ebfd4
	goto loc_826EBFD4;
loc_826EBF78:
	// subf r10,r23,r11
	ctx.r10.s64 = r11.s64 - r23.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r9,r26
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r26.u32);
	// lwzx r10,r10,r26
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r26.u32);
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lhz r10,90(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r9,86(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// lhz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// lhz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// sth r9,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r9.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
loc_826EBFD4:
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
loc_826EBFE8:
	// cmpwi cr6,r22,1
	cr6.compare<int32_t>(r22.s32, 1, xer);
	// beq cr6,0x826ec0ac
	if (cr6.eq) goto loc_826EC0AC;
	// addi r10,r22,-1
	ctx.r10.s64 = r22.s64 + -1;
	// cmpw cr6,r14,r10
	cr6.compare<int32_t>(r14.s32, ctx.r10.s32, xer);
	// beq cr6,0x826ec008
	if (cr6.eq) goto loc_826EC008;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r10,r6,20
	ctx.r10.s64 = ctx.r6.s64 + 20;
	// b 0x826ec010
	goto loc_826EC010;
loc_826EC008:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r6,-20
	ctx.r10.s64 = ctx.r6.s64 + -20;
loc_826EC010:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r9,r10,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ec0ac
	if (cr6.eq) goto loc_826EC0AC;
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,512
	cr6.compare<uint32_t>(ctx.r10.u32, 512, xer);
	// bge cr6,0x826ec03c
	if (!cr6.lt) goto loc_826EC03C;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ec098
	goto loc_826EC098;
loc_826EC03C:
	// subf r10,r23,r11
	ctx.r10.s64 = r11.s64 - r23.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lhz r11,86(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lwzx r11,r9,r26
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + r26.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lhz r11,90(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// lhz r11,84(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// sth r10,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r10.u16);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
loc_826EC098:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
loc_826EC0AC:
	// cmpwi cr6,r5,2
	cr6.compare<int32_t>(ctx.r5.s32, 2, xer);
	// blt cr6,0x826ec158
	if (cr6.lt) goto loc_826EC158;
	// lhz r11,102(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// lhz r10,98(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// lhz r7,106(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 106);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 100);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 104);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r31,r7,r11
	r31.s64 = r11.s64 - ctx.r7.s64;
	// subf r30,r10,r7
	r30.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r29,r6,r9
	r29.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r28,r8,r6
	r28.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// xor r30,r30,r5
	r30.u64 = r30.u64 ^ ctx.r5.u64;
	// xor r29,r29,r4
	r29.u64 = r29.u64 ^ ctx.r4.u64;
	// srawi r5,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r31.s32 >> 31;
	// xor r28,r28,r4
	r28.u64 = r28.u64 ^ ctx.r4.u64;
	// srawi r4,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r30.s32 >> 31;
	// srawi r31,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r29.s32 >> 31;
	// srawi r30,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r28.s32 >> 31;
	// or r29,r5,r4
	r29.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r28,r31,r30
	r28.u64 = r31.u64 | r30.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 & ~r29.u64;
	// andc r6,r6,r28
	ctx.r6.u64 = ctx.r6.u64 & ~r28.u64;
	// and r8,r30,r8
	ctx.r8.u64 = r30.u64 & ctx.r8.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r9,r31,r9
	ctx.r9.u64 = r31.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// b 0x826ec170
	goto loc_826EC170;
loc_826EC158:
	// cmpwi cr6,r5,1
	cr6.compare<int32_t>(ctx.r5.s32, 1, xer);
	// bne cr6,0x826ec16c
	if (!cr6.eq) goto loc_826EC16C;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ec170
	goto loc_826EC170;
loc_826EC16C:
	// stw r16,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r16.u32);
loc_826EC170:
	// rlwinm r11,r18,2,0,29
	r11.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// add r31,r11,r26
	r31.u64 = r11.u64 + r26.u64;
	// lhz r11,82(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r11,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	r11.s64 = ctx.r3.s32 >> 16;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// and r10,r10,r21
	ctx.r10.u64 = ctx.r10.u64 & r21.u64;
	// and r11,r11,r20
	r11.u64 = r11.u64 & r20.u64;
	// subf r10,r25,r10
	ctx.r10.s64 = ctx.r10.s64 - r25.s64;
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
	// rlwinm r9,r27,0,29,29
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x4;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// sth r10,2(r31)
	PPC_STORE_U16(r31.u32 + 2, ctx.r10.u16);
	// sth r11,0(r31)
	PPC_STORE_U16(r31.u32 + 0, r11.u16);
	// beq cr6,0x826ec1d0
	if (cr6.eq) goto loc_826EC1D0;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lwz r4,176(r17)
	ctx.r4.u64 = PPC_LOAD_U32(r17.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
loc_826EC1D0:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r7,1
	ctx.r7.s64 = 1;
	// stw r16,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r16.u32);
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// stw r16,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r16.u32);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// bne cr6,0x826ec410
	if (!cr6.eq) goto loc_826EC410;
	// rlwinm r10,r22,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r23,r18
	r11.s64 = r18.s64 - r23.s64;
	// add r10,r22,r10
	ctx.r10.u64 = r22.u64 + ctx.r10.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r6,r10,r15
	ctx.r6.s64 = r15.s64 - ctx.r10.s64;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm r9,r10,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ec298
	if (cr6.eq) goto loc_826EC298;
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,512
	cr6.compare<uint32_t>(ctx.r10.u32, 512, xer);
	// bge cr6,0x826ec230
	if (!cr6.lt) goto loc_826EC230;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r26
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r26.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// b 0x826ec28c
	goto loc_826EC28C;
loc_826EC230:
	// subf r10,r23,r11
	ctx.r10.s64 = r11.s64 - r23.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r9,r26
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r26.u32);
	// lwzx r10,r10,r26
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r26.u32);
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lhz r10,90(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r9,86(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// lhz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// lhz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// sth r9,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r9.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
loc_826EC28C:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r7,2
	ctx.r7.s64 = 2;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
loc_826EC298:
	// cmpwi cr6,r22,1
	cr6.compare<int32_t>(r22.s32, 1, xer);
	// beq cr6,0x826ec35c
	if (cr6.eq) goto loc_826EC35C;
	// addi r10,r22,-1
	ctx.r10.s64 = r22.s64 + -1;
	// cmpw cr6,r14,r10
	cr6.compare<int32_t>(r14.s32, ctx.r10.s32, xer);
	// beq cr6,0x826ec2b8
	if (cr6.eq) goto loc_826EC2B8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r6,20
	ctx.r10.s64 = ctx.r6.s64 + 20;
	// b 0x826ec2c0
	goto loc_826EC2C0;
loc_826EC2B8:
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// addi r10,r6,-20
	ctx.r10.s64 = ctx.r6.s64 + -20;
loc_826EC2C0:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r9,r10,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ec35c
	if (cr6.eq) goto loc_826EC35C;
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,512
	cr6.compare<uint32_t>(ctx.r10.u32, 512, xer);
	// bge cr6,0x826ec2ec
	if (!cr6.lt) goto loc_826EC2EC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ec348
	goto loc_826EC348;
loc_826EC2EC:
	// subf r10,r23,r11
	ctx.r10.s64 = r11.s64 - r23.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lhz r11,86(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lwzx r11,r9,r26
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + r26.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lhz r11,90(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// lhz r11,84(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// sth r10,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r10.u16);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
loc_826EC348:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
loc_826EC35C:
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// blt cr6,0x826ec408
	if (cr6.lt) goto loc_826EC408;
	// lhz r11,102(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// lhz r10,98(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// lhz r7,106(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 106);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 100);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 104);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r30,r7,r11
	r30.s64 = r11.s64 - ctx.r7.s64;
	// subf r29,r10,r7
	r29.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r28,r6,r9
	r28.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r22,r8,r6
	r22.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r30,r30,r5
	r30.u64 = r30.u64 ^ ctx.r5.u64;
	// xor r29,r29,r5
	r29.u64 = r29.u64 ^ ctx.r5.u64;
	// xor r28,r28,r4
	r28.u64 = r28.u64 ^ ctx.r4.u64;
	// srawi r5,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r30.s32 >> 31;
	// xor r22,r22,r4
	r22.u64 = r22.u64 ^ ctx.r4.u64;
	// srawi r4,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r29.s32 >> 31;
	// srawi r30,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r28.s32 >> 31;
	// srawi r29,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r22.s32 >> 31;
	// or r28,r5,r4
	r28.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r22,r30,r29
	r22.u64 = r30.u64 | r29.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 & ~r28.u64;
	// andc r6,r6,r22
	ctx.r6.u64 = ctx.r6.u64 & ~r22.u64;
	// and r8,r29,r8
	ctx.r8.u64 = r29.u64 & ctx.r8.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r9,r30,r9
	ctx.r9.u64 = r30.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// b 0x826ec418
	goto loc_826EC418;
loc_826EC408:
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// bne cr6,0x826ec418
	if (!cr6.eq) goto loc_826EC418;
loc_826EC410:
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_826EC418:
	// lhz r11,82(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// lhz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r11,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	r11.s64 = ctx.r3.s32 >> 16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// and r10,r10,r21
	ctx.r10.u64 = ctx.r10.u64 & r21.u64;
	// and r11,r11,r20
	r11.u64 = r11.u64 & r20.u64;
	// subf r10,r25,r10
	ctx.r10.s64 = ctx.r10.s64 - r25.s64;
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
	// rlwinm r9,r27,0,30,30
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x2;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// sth r10,6(r31)
	PPC_STORE_U16(r31.u32 + 6, ctx.r10.u16);
	// sth r11,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r11.u16);
	// beq cr6,0x826ec470
	if (cr6.eq) goto loc_826EC470;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lwz r4,176(r17)
	ctx.r4.u64 = PPC_LOAD_U32(r17.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
loc_826EC470:
	// mr r11,r16
	r11.u64 = r16.u64;
	// stw r16,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r16.u32);
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// beq cr6,0x826ec518
	if (cr6.eq) goto loc_826EC518;
	// lwz r9,-20(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + -20);
	// add r10,r18,r23
	ctx.r10.u64 = r18.u64 + r23.u64;
	// rlwinm r8,r9,0,14,14
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x826ec518
	if (cr6.eq) goto loc_826EC518;
	// rlwinm r11,r9,0,21,23
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r11,512
	cr6.compare<uint32_t>(r11.u32, 512, xer);
	// bge cr6,0x826ec4b0
	if (!cr6.lt) goto loc_826EC4B0;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// b 0x826ec510
	goto loc_826EC510;
loc_826EC4B0:
	// subf r11,r23,r10
	r11.s64 = ctx.r10.s64 - r23.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r26
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r26.u32);
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lhz r11,90(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r10,86(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// lhz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// lhz r11,84(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// sth r10,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r10.u16);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_826EC510:
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// li r11,1
	r11.s64 = 1;
loc_826EC518:
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r7,r1,100
	ctx.r7.s64 = ctx.r1.s64 + 100;
	// clrlwi r6,r27,31
	ctx.r6.u64 = r27.u32 & 0x1;
	// add r28,r18,r23
	r28.u64 = r18.u64 + r23.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// stwx r10,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, ctx.r10.u32);
	// rlwinm r5,r28,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r8,r11,r7
	PPC_STORE_U32(r11.u32 + ctx.r7.u32, ctx.r8.u32);
	// lhz r11,102(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// add r31,r5,r26
	r31.u64 = ctx.r5.u64 + r26.u64;
	// lhz r10,98(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// lhz r7,106(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 106);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 100);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 104);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r30,r7,r11
	r30.s64 = r11.s64 - ctx.r7.s64;
	// subf r29,r10,r7
	r29.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r27,r6,r9
	r27.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r22,r8,r6
	r22.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r30,r30,r5
	r30.u64 = r30.u64 ^ ctx.r5.u64;
	// xor r29,r29,r5
	r29.u64 = r29.u64 ^ ctx.r5.u64;
	// xor r27,r27,r4
	r27.u64 = r27.u64 ^ ctx.r4.u64;
	// srawi r5,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r30.s32 >> 31;
	// xor r22,r22,r4
	r22.u64 = r22.u64 ^ ctx.r4.u64;
	// srawi r4,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r29.s32 >> 31;
	// srawi r30,r27,31
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r27.s32 >> 31;
	// srawi r29,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r22.s32 >> 31;
	// or r22,r5,r4
	r22.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r19,r30,r29
	r19.u64 = r30.u64 | r29.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r22
	ctx.r7.u64 = ctx.r7.u64 & ~r22.u64;
	// andc r6,r6,r19
	ctx.r6.u64 = ctx.r6.u64 & ~r19.u64;
	// and r8,r29,r8
	ctx.r8.u64 = r29.u64 & ctx.r8.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r9,r30,r9
	ctx.r9.u64 = r30.u64 & ctx.r9.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// srawi r27,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	r27.s64 = ctx.r3.s32 >> 16;
	// add r10,r9,r3
	ctx.r10.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r11,r27,r11
	r11.u64 = r27.u64 + r11.u64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// and r10,r10,r21
	ctx.r10.u64 = ctx.r10.u64 & r21.u64;
	// and r11,r11,r20
	r11.u64 = r11.u64 & r20.u64;
	// subf r10,r25,r10
	ctx.r10.s64 = ctx.r10.s64 - r25.s64;
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// sth r10,2(r31)
	PPC_STORE_U16(r31.u32 + 2, ctx.r10.u16);
	// sth r11,0(r31)
	PPC_STORE_U16(r31.u32 + 0, r11.u16);
	// beq cr6,0x826ec62c
	if (cr6.eq) goto loc_826EC62C;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lwz r4,176(r17)
	ctx.r4.u64 = PPC_LOAD_U32(r17.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
loc_826EC62C:
	// subf r11,r23,r28
	r11.s64 = r28.s64 - r23.s64;
	// lhz r10,2(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 2);
	// lhz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lhz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r7
	r11.s64 = ctx.r7.s16;
	// extsh r7,r6
	ctx.r7.s64 = ctx.r6.s16;
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// subf r4,r11,r8
	ctx.r4.s64 = ctx.r8.s64 - r11.s64;
	// subf r29,r10,r8
	r29.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r28,r11,r10
	r28.s64 = ctx.r10.s64 - r11.s64;
	// subf r30,r6,r7
	r30.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r27,r9,r7
	r27.s64 = ctx.r7.s64 - ctx.r9.s64;
	// subf r26,r6,r9
	r26.s64 = ctx.r9.s64 - ctx.r6.s64;
	// xor r29,r29,r4
	r29.u64 = r29.u64 ^ ctx.r4.u64;
	// xor r28,r28,r4
	r28.u64 = r28.u64 ^ ctx.r4.u64;
	// xor r27,r27,r30
	r27.u64 = r27.u64 ^ r30.u64;
	// srawi r4,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r29.s32 >> 31;
	// xor r26,r26,r30
	r26.u64 = r26.u64 ^ r30.u64;
	// srawi r30,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r28.s32 >> 31;
	// srawi r29,r27,31
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r27.s32 >> 31;
	// srawi r28,r26,31
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x7FFFFFFF) != 0);
	r28.s64 = r26.s32 >> 31;
	// or r26,r4,r30
	r26.u64 = ctx.r4.u64 | r30.u64;
	// or r23,r29,r28
	r23.u64 = r29.u64 | r28.u64;
	// andc r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 & ~r26.u64;
	// and r11,r30,r11
	r11.u64 = r30.u64 & r11.u64;
	// andc r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 & ~r23.u64;
	// and r6,r28,r6
	ctx.r6.u64 = r28.u64 & ctx.r6.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// and r10,r4,r8
	ctx.r10.u64 = ctx.r4.u64 & ctx.r8.u64;
	// or r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 | ctx.r6.u64;
	// and r8,r29,r7
	ctx.r8.u64 = r29.u64 & ctx.r7.u64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// or r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 | ctx.r8.u64;
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// srawi r27,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	r27.s64 = ctx.r5.s32 >> 16;
	// add r10,r9,r5
	ctx.r10.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r11,r27,r11
	r11.u64 = r27.u64 + r11.u64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// and r10,r10,r21
	ctx.r10.u64 = ctx.r10.u64 & r21.u64;
	// and r11,r11,r20
	r11.u64 = r11.u64 & r20.u64;
	// subf r10,r25,r10
	ctx.r10.s64 = ctx.r10.s64 - r25.s64;
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
	// sth r10,6(r31)
	PPC_STORE_U16(r31.u32 + 6, ctx.r10.u16);
	// sth r11,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r11.u16);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826EC710"))) PPC_WEAK_FUNC(sub_826EC710);
PPC_FUNC_IMPL(__imp__sub_826EC710) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r22,0(r7)
	r22.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r21,r4
	r21.u64 = ctx.r4.u64;
	// mr r19,r5
	r19.u64 = ctx.r5.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// li r17,0
	r17.s64 = 0;
	// lhz r20,50(r27)
	r20.u64 = PPC_LOAD_U16(r27.u32 + 50);
	// lwz r25,188(r27)
	r25.u64 = PPC_LOAD_U32(r27.u32 + 188);
	// srawi r26,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r26.s64 = r20.s32 >> 1;
	// beq cr6,0x826ec75c
	if (cr6.eq) goto loc_826EC75C;
	// lwz r11,1240(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1240);
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r18,r17
	r18.u64 = r17.u64;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ec760
	if (cr6.eq) goto loc_826EC760;
loc_826EC75C:
	// li r18,1
	r18.s64 = 1;
loc_826EC760:
	// lwz r11,184(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 184);
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lhz r24,62(r27)
	r24.u64 = PPC_LOAD_U16(r27.u32 + 62);
	// lhz r16,66(r27)
	r16.u64 = PPC_LOAD_U16(r27.u32 + 66);
	// lhz r23,64(r27)
	r23.u64 = PPC_LOAD_U16(r27.u32 + 64);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lhz r15,68(r27)
	r15.u64 = PPC_LOAD_U16(r27.u32 + 68);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826ec864
	if (cr6.lt) goto loc_826EC864;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826ec85c
	if (!cr6.lt) goto loc_826EC85C;
loc_826EC7C4:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826ec7f0
	if (cr6.lt) goto loc_826EC7F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826ec7c4
	if (cr6.eq) goto loc_826EC7C4;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826ec8a8
	goto loc_826EC8A8;
loc_826EC7F0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EC85C:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826ec8a8
	goto loc_826EC8A8;
loc_826EC864:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r28,r11,32768
	r28.u64 = r11.u64 | 32768;
loc_826EC874:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r28
	r11.u64 = r29.u64 + r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826ec874
	if (cr6.lt) goto loc_826EC874;
loc_826EC8A8:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ec8c4
	if (cr6.eq) goto loc_826EC8C4;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
loc_826EC8C4:
	// rlwinm r11,r29,0,30,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x2;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ec8e0
	if (cr6.eq) goto loc_826EC8E0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r4,176(r27)
	ctx.r4.u64 = PPC_LOAD_U32(r27.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
loc_826EC8E0:
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// stw r17,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r17.u32);
	// stw r17,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r17.u32);
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// stw r17,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r17.u32);
	// beq cr6,0x826ec91c
	if (cr6.eq) goto loc_826EC91C;
	// lwz r11,-20(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + -20);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ec91c
	if (cr6.eq) goto loc_826EC91C;
	// rlwinm r11,r22,2,0,29
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,1
	ctx.r10.s64 = 1;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
loc_826EC91C:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x826ec9d8
	if (!cr6.eq) goto loc_826EC9D8;
	// rlwinm r9,r26,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r20,r22
	r11.s64 = r22.s64 - r20.s64;
	// add r9,r26,r9
	ctx.r9.u64 = r26.u64 + ctx.r9.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r9,r21
	ctx.r8.s64 = r21.s64 - ctx.r9.s64;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// rlwinm r7,r9,0,14,14
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x826ec978
	if (cr6.eq) goto loc_826EC978;
	// rlwinm r9,r9,0,21,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x700;
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r9,512
	cr6.compare<uint32_t>(ctx.r9.u32, 512, xer);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bge cr6,0x826ec968
	if (!cr6.lt) goto loc_826EC968;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x826ec970
	goto loc_826EC970;
loc_826EC968:
	// subf r9,r20,r11
	ctx.r9.s64 = r11.s64 - r20.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_826EC970:
	// lwzx r9,r9,r25
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r25.u32);
	// stwx r9,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r9.u32);
loc_826EC978:
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// beq cr6,0x826ec9d8
	if (cr6.eq) goto loc_826EC9D8;
	// addi r9,r26,-1
	ctx.r9.s64 = r26.s64 + -1;
	// cmpw cr6,r19,r9
	cr6.compare<int32_t>(r19.s32, ctx.r9.s32, xer);
	// beq cr6,0x826ec998
	if (cr6.eq) goto loc_826EC998;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r9,r8,20
	ctx.r9.s64 = ctx.r8.s64 + 20;
	// b 0x826ec9a0
	goto loc_826EC9A0;
loc_826EC998:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r8,-20
	ctx.r9.s64 = ctx.r8.s64 + -20;
loc_826EC9A0:
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r8,r9,0,14,14
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x826ec9d8
	if (cr6.eq) goto loc_826EC9D8;
	// rlwinm r9,r9,0,21,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x700;
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// cmplwi cr6,r9,512
	cr6.compare<uint32_t>(ctx.r9.u32, 512, xer);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x826ec9cc
	if (cr6.lt) goto loc_826EC9CC;
	// subf r11,r20,r11
	r11.s64 = r11.s64 - r20.s64;
loc_826EC9CC:
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r25
	r11.u64 = PPC_LOAD_U32(r11.u32 + r25.u32);
	// stwx r11,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, r11.u32);
loc_826EC9D8:
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826ecb14
	if (!cr6.gt) goto loc_826ECB14;
	// addi r9,r1,104
	ctx.r9.s64 = ctx.r1.s64 + 104;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
loc_826EC9F8:
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r4,r4,0,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x826eca1c
	if (cr6.eq) goto loc_826ECA1C;
	// stw r4,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r4.u32);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// b 0x826eca28
	goto loc_826ECA28;
loc_826ECA1C:
	// stw r4,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
loc_826ECA28:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x826ec9f8
	if (!cr6.eq) goto loc_826EC9F8;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826ecb14
	if (!cr6.gt) goto loc_826ECB14;
	// cmpwi cr6,r6,3
	cr6.compare<int32_t>(ctx.r6.s32, 3, xer);
	// beq cr6,0x826eca70
	if (cr6.eq) goto loc_826ECA70;
	// cmpwi cr6,r5,3
	cr6.compare<int32_t>(ctx.r5.s32, 3, xer);
	// beq cr6,0x826eca70
	if (cr6.eq) goto loc_826ECA70;
	// cmpw cr6,r6,r5
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, xer);
	// blt cr6,0x826eca64
	if (cr6.lt) goto loc_826ECA64;
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ecb18
	goto loc_826ECB18;
loc_826ECA64:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ecb18
	goto loc_826ECB18;
loc_826ECA70:
	// lhz r11,94(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 94);
	// lhz r10,90(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r7,98(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 92);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r31,r7,r11
	r31.s64 = r11.s64 - ctx.r7.s64;
	// subf r30,r10,r7
	r30.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r28,r6,r9
	r28.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r14,r8,r6
	r14.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// xor r30,r30,r5
	r30.u64 = r30.u64 ^ ctx.r5.u64;
	// xor r28,r28,r4
	r28.u64 = r28.u64 ^ ctx.r4.u64;
	// srawi r5,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r31.s32 >> 31;
	// xor r14,r14,r4
	r14.u64 = r14.u64 ^ ctx.r4.u64;
	// srawi r4,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r30.s32 >> 31;
	// srawi r31,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r28.s32 >> 31;
	// srawi r30,r14,31
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r14.s32 >> 31;
	// or r28,r5,r4
	r28.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r14,r31,r30
	r14.u64 = r31.u64 | r30.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 & ~r28.u64;
	// andc r6,r6,r14
	ctx.r6.u64 = ctx.r6.u64 & ~r14.u64;
	// and r8,r30,r8
	ctx.r8.u64 = r30.u64 & ctx.r8.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r9,r31,r9
	ctx.r9.u64 = r31.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// b 0x826ecb18
	goto loc_826ECB18;
loc_826ECB14:
	// stw r17,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r17.u32);
loc_826ECB18:
	// lhz r10,82(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// rlwinm r11,r22,2,0,29
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// srawi r10,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r3.s32 >> 16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r9,r9,r24
	ctx.r9.u64 = ctx.r9.u64 + r24.u64;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// and r9,r9,r16
	ctx.r9.u64 = ctx.r9.u64 & r16.u64;
	// and r10,r10,r15
	ctx.r10.u64 = ctx.r10.u64 & r15.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// subf r10,r23,r10
	ctx.r10.s64 = ctx.r10.s64 - r23.s64;
	// subf r9,r24,r9
	ctx.r9.s64 = ctx.r9.s64 - r24.s64;
	// clrlwi r8,r29,31
	ctx.r8.u64 = r29.u32 & 0x1;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// sth r9,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r9.u16);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// beq cr6,0x826ecb80
	if (cr6.eq) goto loc_826ECB80;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r4,176(r27)
	ctx.r4.u64 = PPC_LOAD_U32(r27.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
loc_826ECB80:
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// stw r17,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r17.u32);
	// stw r17,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r17.u32);
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// stw r17,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r17.u32);
	// beq cr6,0x826ecbc0
	if (cr6.eq) goto loc_826ECBC0;
	// lwz r11,-20(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + -20);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ecbc0
	if (cr6.eq) goto loc_826ECBC0;
	// add r11,r22,r20
	r11.u64 = r22.u64 + r20.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
loc_826ECBC0:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x826ecc54
	if (!cr6.eq) goto loc_826ECC54;
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r20,r22
	ctx.r10.s64 = r22.s64 - r20.s64;
	// add r11,r26,r11
	r11.u64 = r26.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r21
	r11.s64 = r21.s64 - r11.s64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r9,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ecc04
	if (cr6.eq) goto loc_826ECC04;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwzx r9,r9,r25
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r25.u32);
	// stwx r9,r8,r7
	PPC_STORE_U32(ctx.r8.u32 + ctx.r7.u32, ctx.r9.u32);
loc_826ECC04:
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// beq cr6,0x826ecc54
	if (cr6.eq) goto loc_826ECC54;
	// addi r9,r26,-1
	ctx.r9.s64 = r26.s64 + -1;
	// cmpw cr6,r19,r9
	cr6.compare<int32_t>(r19.s32, ctx.r9.s32, xer);
	// beq cr6,0x826ecc24
	if (cr6.eq) goto loc_826ECC24;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// b 0x826ecc2c
	goto loc_826ECC2C;
loc_826ECC24:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,-20
	r11.s64 = r11.s64 + -20;
loc_826ECC2C:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ecc54
	if (cr6.eq) goto loc_826ECC54;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwzx r11,r11,r25
	r11.u64 = PPC_LOAD_U32(r11.u32 + r25.u32);
	// stwx r11,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, r11.u32);
loc_826ECC54:
	// mr r7,r17
	ctx.r7.u64 = r17.u64;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826ecd90
	if (!cr6.gt) goto loc_826ECD90;
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// addi r9,r1,104
	ctx.r9.s64 = ctx.r1.s64 + 104;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
loc_826ECC74:
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r4,r4,0,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x826ecc98
	if (cr6.eq) goto loc_826ECC98;
	// stw r4,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x826ecca4
	goto loc_826ECCA4;
loc_826ECC98:
	// stw r4,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_826ECCA4:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x826ecc74
	if (!cr6.eq) goto loc_826ECC74;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826ecd90
	if (!cr6.gt) goto loc_826ECD90;
	// cmpwi cr6,r7,3
	cr6.compare<int32_t>(ctx.r7.s32, 3, xer);
	// beq cr6,0x826eccec
	if (cr6.eq) goto loc_826ECCEC;
	// cmpwi cr6,r6,3
	cr6.compare<int32_t>(ctx.r6.s32, 3, xer);
	// beq cr6,0x826eccec
	if (cr6.eq) goto loc_826ECCEC;
	// cmpw cr6,r7,r6
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r6.s32, xer);
	// blt cr6,0x826ecce0
	if (cr6.lt) goto loc_826ECCE0;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ecd94
	goto loc_826ECD94;
loc_826ECCE0:
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ecd94
	goto loc_826ECD94;
loc_826ECCEC:
	// lhz r11,94(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 94);
	// lhz r10,90(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r7,98(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 92);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r31,r7,r11
	r31.s64 = r11.s64 - ctx.r7.s64;
	// subf r30,r10,r7
	r30.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r29,r6,r9
	r29.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r28,r8,r6
	r28.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// xor r30,r30,r5
	r30.u64 = r30.u64 ^ ctx.r5.u64;
	// xor r29,r29,r4
	r29.u64 = r29.u64 ^ ctx.r4.u64;
	// srawi r5,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r31.s32 >> 31;
	// xor r28,r28,r4
	r28.u64 = r28.u64 ^ ctx.r4.u64;
	// srawi r4,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r30.s32 >> 31;
	// srawi r31,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r29.s32 >> 31;
	// srawi r30,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r28.s32 >> 31;
	// or r29,r5,r4
	r29.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r28,r31,r30
	r28.u64 = r31.u64 | r30.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 & ~r29.u64;
	// andc r6,r6,r28
	ctx.r6.u64 = ctx.r6.u64 & ~r28.u64;
	// and r8,r30,r8
	ctx.r8.u64 = r30.u64 & ctx.r8.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r9,r31,r9
	ctx.r9.u64 = r31.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// b 0x826ecd94
	goto loc_826ECD94;
loc_826ECD90:
	// stw r17,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r17.u32);
loc_826ECD94:
	// lhz r10,82(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// add r11,r22,r20
	r11.u64 = r22.u64 + r20.u64;
	// lhz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// srawi r10,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r3.s32 >> 16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r9,r9,r24
	ctx.r9.u64 = ctx.r9.u64 + r24.u64;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// and r9,r9,r16
	ctx.r9.u64 = ctx.r9.u64 & r16.u64;
	// and r10,r10,r15
	ctx.r10.u64 = ctx.r10.u64 & r15.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r23,r10
	ctx.r10.s64 = ctx.r10.s64 - r23.s64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// subf r9,r24,r9
	ctx.r9.s64 = ctx.r9.s64 - r24.s64;
	// li r3,0
	ctx.r3.s64 = 0;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// sth r9,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r9.u16);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826ECDF0"))) PPC_WEAK_FUNC(sub_826ECDF0);
PPC_FUNC_IMPL(__imp__sub_826ECDF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// lwz r25,0(r7)
	r25.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r14,r5
	r14.u64 = ctx.r5.u64;
	// mr r16,r4
	r16.u64 = ctx.r4.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// li r19,0
	r19.s64 = 0;
	// lhz r24,50(r23)
	r24.u64 = PPC_LOAD_U16(r23.u32 + 50);
	// lwz r26,188(r23)
	r26.u64 = PPC_LOAD_U32(r23.u32 + 188);
	// stw r14,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, r14.u32);
	// srawi r20,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	r20.s64 = r24.s32 >> 1;
	// beq cr6,0x826ece40
	if (cr6.eq) goto loc_826ECE40;
	// lwz r11,1240(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 1240);
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r15,r19
	r15.u64 = r19.u64;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ece44
	if (cr6.eq) goto loc_826ECE44;
loc_826ECE40:
	// li r15,1
	r15.s64 = 1;
loc_826ECE44:
	// lwz r11,180(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 180);
	// lwz r31,0(r23)
	r31.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// lhz r22,62(r23)
	r22.u64 = PPC_LOAD_U16(r23.u32 + 62);
	// lhz r18,66(r23)
	r18.u64 = PPC_LOAD_U16(r23.u32 + 66);
	// lhz r21,64(r23)
	r21.u64 = PPC_LOAD_U16(r23.u32 + 64);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lhz r17,68(r23)
	r17.u64 = PPC_LOAD_U16(r23.u32 + 68);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r27,r11
	r27.s64 = r11.s16;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// blt cr6,0x826ecf48
	if (cr6.lt) goto loc_826ECF48;
	// clrlwi r11,r27,28
	r11.u64 = r27.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826ecf40
	if (!cr6.lt) goto loc_826ECF40;
loc_826ECEA8:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826eced4
	if (cr6.lt) goto loc_826ECED4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826ecea8
	if (cr6.eq) goto loc_826ECEA8;
	// srawi r27,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r27.s64 = r27.s32 >> 4;
	// b 0x826ecf8c
	goto loc_826ECF8C;
loc_826ECED4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826ECF40:
	// srawi r27,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r27.s64 = r27.s32 >> 4;
	// b 0x826ecf8c
	goto loc_826ECF8C;
loc_826ECF48:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r28,r11,32768
	r28.u64 = r11.u64 | 32768;
loc_826ECF58:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r27
	r29.u64 = r11.u64 + r27.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r28
	r11.u64 = r29.u64 + r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r27,r11
	r27.s64 = r11.s16;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// blt cr6,0x826ecf58
	if (cr6.lt) goto loc_826ECF58;
loc_826ECF8C:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ecfa8
	if (cr6.eq) goto loc_826ECFA8;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
loc_826ECFA8:
	// rlwinm r11,r27,0,28,28
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x8;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ecfc4
	if (cr6.eq) goto loc_826ECFC4;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// lwz r4,176(r23)
	ctx.r4.u64 = PPC_LOAD_U32(r23.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
loc_826ECFC4:
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
	// stw r19,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r19.u32);
	// stw r19,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r19.u32);
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// stw r19,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r19.u32);
	// beq cr6,0x826ed000
	if (cr6.eq) goto loc_826ED000;
	// lwz r11,-20(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + -20);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ed000
	if (cr6.eq) goto loc_826ED000;
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,1
	ctx.r10.s64 = 1;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
loc_826ED000:
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// bne cr6,0x826ed0bc
	if (!cr6.eq) goto loc_826ED0BC;
	// rlwinm r9,r20,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r24,r25
	r11.s64 = r25.s64 - r24.s64;
	// add r9,r20,r9
	ctx.r9.u64 = r20.u64 + ctx.r9.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r9,r16
	ctx.r8.s64 = r16.s64 - ctx.r9.s64;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// rlwinm r7,r9,0,14,14
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x826ed05c
	if (cr6.eq) goto loc_826ED05C;
	// rlwinm r9,r9,0,21,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x700;
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r9,512
	cr6.compare<uint32_t>(ctx.r9.u32, 512, xer);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bge cr6,0x826ed04c
	if (!cr6.lt) goto loc_826ED04C;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x826ed054
	goto loc_826ED054;
loc_826ED04C:
	// subf r9,r24,r11
	ctx.r9.s64 = r11.s64 - r24.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_826ED054:
	// lwzx r9,r9,r26
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r26.u32);
	// stwx r9,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r9.u32);
loc_826ED05C:
	// cmpwi cr6,r20,1
	cr6.compare<int32_t>(r20.s32, 1, xer);
	// beq cr6,0x826ed0bc
	if (cr6.eq) goto loc_826ED0BC;
	// addi r9,r20,-1
	ctx.r9.s64 = r20.s64 + -1;
	// cmpw cr6,r14,r9
	cr6.compare<int32_t>(r14.s32, ctx.r9.s32, xer);
	// beq cr6,0x826ed07c
	if (cr6.eq) goto loc_826ED07C;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r9,r8,20
	ctx.r9.s64 = ctx.r8.s64 + 20;
	// b 0x826ed084
	goto loc_826ED084;
loc_826ED07C:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r8,-20
	ctx.r9.s64 = ctx.r8.s64 + -20;
loc_826ED084:
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r8,r9,0,14,14
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x826ed0bc
	if (cr6.eq) goto loc_826ED0BC;
	// rlwinm r9,r9,0,21,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x700;
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// cmplwi cr6,r9,512
	cr6.compare<uint32_t>(ctx.r9.u32, 512, xer);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x826ed0b0
	if (cr6.lt) goto loc_826ED0B0;
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
loc_826ED0B0:
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stwx r11,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, r11.u32);
loc_826ED0BC:
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826ed1f8
	if (!cr6.gt) goto loc_826ED1F8;
	// addi r9,r1,104
	ctx.r9.s64 = ctx.r1.s64 + 104;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
loc_826ED0DC:
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r4,r4,0,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x826ed100
	if (cr6.eq) goto loc_826ED100;
	// stw r4,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r4.u32);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// b 0x826ed10c
	goto loc_826ED10C;
loc_826ED100:
	// stw r4,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
loc_826ED10C:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x826ed0dc
	if (!cr6.eq) goto loc_826ED0DC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826ed1f8
	if (!cr6.gt) goto loc_826ED1F8;
	// cmpwi cr6,r6,3
	cr6.compare<int32_t>(ctx.r6.s32, 3, xer);
	// beq cr6,0x826ed154
	if (cr6.eq) goto loc_826ED154;
	// cmpwi cr6,r5,3
	cr6.compare<int32_t>(ctx.r5.s32, 3, xer);
	// beq cr6,0x826ed154
	if (cr6.eq) goto loc_826ED154;
	// cmpw cr6,r6,r5
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, xer);
	// blt cr6,0x826ed148
	if (cr6.lt) goto loc_826ED148;
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ed1fc
	goto loc_826ED1FC;
loc_826ED148:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ed1fc
	goto loc_826ED1FC;
loc_826ED154:
	// lhz r11,94(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 94);
	// lhz r10,90(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r7,98(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 92);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r31,r7,r11
	r31.s64 = r11.s64 - ctx.r7.s64;
	// subf r30,r10,r7
	r30.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r29,r6,r9
	r29.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r28,r8,r6
	r28.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// xor r30,r30,r5
	r30.u64 = r30.u64 ^ ctx.r5.u64;
	// xor r29,r29,r4
	r29.u64 = r29.u64 ^ ctx.r4.u64;
	// srawi r5,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r31.s32 >> 31;
	// xor r28,r28,r4
	r28.u64 = r28.u64 ^ ctx.r4.u64;
	// srawi r4,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r30.s32 >> 31;
	// srawi r31,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r29.s32 >> 31;
	// srawi r30,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r28.s32 >> 31;
	// or r29,r5,r4
	r29.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r28,r31,r30
	r28.u64 = r31.u64 | r30.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 & ~r29.u64;
	// andc r6,r6,r28
	ctx.r6.u64 = ctx.r6.u64 & ~r28.u64;
	// and r8,r30,r8
	ctx.r8.u64 = r30.u64 & ctx.r8.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r9,r31,r9
	ctx.r9.u64 = r31.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// b 0x826ed1fc
	goto loc_826ED1FC;
loc_826ED1F8:
	// stw r19,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r19.u32);
loc_826ED1FC:
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// add r29,r11,r26
	r29.u64 = r11.u64 + r26.u64;
	// lhz r11,82(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r11,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	r11.s64 = ctx.r3.s32 >> 16;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// add r11,r11,r21
	r11.u64 = r11.u64 + r21.u64;
	// and r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 & r18.u64;
	// and r11,r11,r17
	r11.u64 = r11.u64 & r17.u64;
	// subf r10,r22,r10
	ctx.r10.s64 = ctx.r10.s64 - r22.s64;
	// subf r11,r21,r11
	r11.s64 = r11.s64 - r21.s64;
	// rlwinm r9,r27,0,29,29
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x4;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// sth r10,2(r29)
	PPC_STORE_U16(r29.u32 + 2, ctx.r10.u16);
	// sth r11,0(r29)
	PPC_STORE_U16(r29.u32 + 0, r11.u16);
	// beq cr6,0x826ed25c
	if (cr6.eq) goto loc_826ED25C;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// lwz r4,176(r23)
	ctx.r4.u64 = PPC_LOAD_U32(r23.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
loc_826ED25C:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// li r8,1
	ctx.r8.s64 = 1;
	// stw r19,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r19.u32);
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// stw r19,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r19.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// bne cr6,0x826ed328
	if (!cr6.eq) goto loc_826ED328;
	// rlwinm r10,r20,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r24,r25
	r11.s64 = r25.s64 - r24.s64;
	// add r10,r20,r10
	ctx.r10.u64 = r20.u64 + ctx.r10.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r10,r16
	ctx.r9.s64 = r16.s64 - ctx.r10.s64;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r7,r10,0,14,14
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x826ed2c8
	if (cr6.eq) goto loc_826ED2C8;
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// li r8,2
	ctx.r8.s64 = 2;
	// cmplwi cr6,r10,512
	cr6.compare<uint32_t>(ctx.r10.u32, 512, xer);
	// bge cr6,0x826ed2b8
	if (!cr6.lt) goto loc_826ED2B8;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x826ed2c0
	goto loc_826ED2C0;
loc_826ED2B8:
	// subf r10,r24,r11
	ctx.r10.s64 = r11.s64 - r24.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_826ED2C0:
	// lwzx r10,r10,r26
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r26.u32);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
loc_826ED2C8:
	// cmpwi cr6,r20,1
	cr6.compare<int32_t>(r20.s32, 1, xer);
	// beq cr6,0x826ed328
	if (cr6.eq) goto loc_826ED328;
	// addi r10,r20,-1
	ctx.r10.s64 = r20.s64 + -1;
	// cmpw cr6,r14,r10
	cr6.compare<int32_t>(r14.s32, ctx.r10.s32, xer);
	// beq cr6,0x826ed2e8
	if (cr6.eq) goto loc_826ED2E8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r9,20
	ctx.r10.s64 = ctx.r9.s64 + 20;
	// b 0x826ed2f0
	goto loc_826ED2F0;
loc_826ED2E8:
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// addi r10,r9,-20
	ctx.r10.s64 = ctx.r9.s64 + -20;
loc_826ED2F0:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r9,r10,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ed328
	if (cr6.eq) goto loc_826ED328;
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// cmplwi cr6,r10,512
	cr6.compare<uint32_t>(ctx.r10.u32, 512, xer);
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// blt cr6,0x826ed31c
	if (cr6.lt) goto loc_826ED31C;
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
loc_826ED31C:
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stwx r11,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, r11.u32);
loc_826ED328:
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x826ed468
	if (!cr6.gt) goto loc_826ED468;
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// addi r9,r1,104
	ctx.r9.s64 = ctx.r1.s64 + 104;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
loc_826ED348:
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r4,r4,0,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x826ed36c
	if (cr6.eq) goto loc_826ED36C;
	// stw r4,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x826ed378
	goto loc_826ED378;
loc_826ED36C:
	// stw r4,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_826ED378:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x826ed348
	if (!cr6.eq) goto loc_826ED348;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x826ed468
	if (!cr6.gt) goto loc_826ED468;
	// cmpwi cr6,r6,3
	cr6.compare<int32_t>(ctx.r6.s32, 3, xer);
	// beq cr6,0x826ed3c0
	if (cr6.eq) goto loc_826ED3C0;
	// cmpwi cr6,r5,3
	cr6.compare<int32_t>(ctx.r5.s32, 3, xer);
	// beq cr6,0x826ed3c0
	if (cr6.eq) goto loc_826ED3C0;
	// cmpw cr6,r6,r5
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, xer);
	// blt cr6,0x826ed3b4
	if (cr6.lt) goto loc_826ED3B4;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ed46c
	goto loc_826ED46C;
loc_826ED3B4:
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ed46c
	goto loc_826ED46C;
loc_826ED3C0:
	// lhz r11,94(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 94);
	// lhz r10,90(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r7,98(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 92);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r31,r7,r11
	r31.s64 = r11.s64 - ctx.r7.s64;
	// subf r30,r10,r7
	r30.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r28,r6,r9
	r28.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r14,r8,r6
	r14.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// xor r30,r30,r5
	r30.u64 = r30.u64 ^ ctx.r5.u64;
	// xor r28,r28,r4
	r28.u64 = r28.u64 ^ ctx.r4.u64;
	// srawi r5,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r31.s32 >> 31;
	// xor r14,r14,r4
	r14.u64 = r14.u64 ^ ctx.r4.u64;
	// srawi r4,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r30.s32 >> 31;
	// srawi r31,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r28.s32 >> 31;
	// srawi r30,r14,31
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r14.s32 >> 31;
	// or r28,r5,r4
	r28.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r14,r31,r30
	r14.u64 = r31.u64 | r30.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 & ~r28.u64;
	// andc r6,r6,r14
	ctx.r6.u64 = ctx.r6.u64 & ~r14.u64;
	// lwz r14,324(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// and r8,r30,r8
	ctx.r8.u64 = r30.u64 & ctx.r8.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r9,r31,r9
	ctx.r9.u64 = r31.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// b 0x826ed46c
	goto loc_826ED46C;
loc_826ED468:
	// stw r19,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r19.u32);
loc_826ED46C:
	// lhz r11,82(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// lhz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r11,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	r11.s64 = ctx.r3.s32 >> 16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// add r11,r11,r21
	r11.u64 = r11.u64 + r21.u64;
	// and r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 & r18.u64;
	// and r11,r11,r17
	r11.u64 = r11.u64 & r17.u64;
	// subf r10,r22,r10
	ctx.r10.s64 = ctx.r10.s64 - r22.s64;
	// subf r11,r21,r11
	r11.s64 = r11.s64 - r21.s64;
	// rlwinm r9,r27,0,30,30
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x2;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// sth r10,6(r29)
	PPC_STORE_U16(r29.u32 + 6, ctx.r10.u16);
	// sth r11,4(r29)
	PPC_STORE_U16(r29.u32 + 4, r11.u16);
	// beq cr6,0x826ed4c4
	if (cr6.eq) goto loc_826ED4C4;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// lwz r4,176(r23)
	ctx.r4.u64 = PPC_LOAD_U32(r23.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
loc_826ED4C4:
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// stw r19,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r19.u32);
	// stw r19,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r19.u32);
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// stw r19,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r19.u32);
	// beq cr6,0x826ed504
	if (cr6.eq) goto loc_826ED504;
	// lwz r11,-20(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + -20);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ed504
	if (cr6.eq) goto loc_826ED504;
	// add r11,r25,r24
	r11.u64 = r25.u64 + r24.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
loc_826ED504:
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// bne cr6,0x826ed598
	if (!cr6.eq) goto loc_826ED598;
	// rlwinm r11,r20,2,0,29
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r24,r25
	ctx.r10.s64 = r25.s64 - r24.s64;
	// add r11,r20,r11
	r11.u64 = r20.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r16
	r11.s64 = r16.s64 - r11.s64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r9,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ed548
	if (cr6.eq) goto loc_826ED548;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwzx r9,r9,r26
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r26.u32);
	// stwx r9,r8,r7
	PPC_STORE_U32(ctx.r8.u32 + ctx.r7.u32, ctx.r9.u32);
loc_826ED548:
	// cmpwi cr6,r20,1
	cr6.compare<int32_t>(r20.s32, 1, xer);
	// beq cr6,0x826ed598
	if (cr6.eq) goto loc_826ED598;
	// addi r9,r20,-1
	ctx.r9.s64 = r20.s64 + -1;
	// cmpw cr6,r14,r9
	cr6.compare<int32_t>(r14.s32, ctx.r9.s32, xer);
	// beq cr6,0x826ed568
	if (cr6.eq) goto loc_826ED568;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// b 0x826ed570
	goto loc_826ED570;
loc_826ED568:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,-20
	r11.s64 = r11.s64 + -20;
loc_826ED570:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ed598
	if (cr6.eq) goto loc_826ED598;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stwx r11,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, r11.u32);
loc_826ED598:
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826ed6d4
	if (!cr6.gt) goto loc_826ED6D4;
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// addi r9,r1,104
	ctx.r9.s64 = ctx.r1.s64 + 104;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
loc_826ED5B8:
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r4,r4,0,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x826ed5dc
	if (cr6.eq) goto loc_826ED5DC;
	// stw r4,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x826ed5e8
	goto loc_826ED5E8;
loc_826ED5DC:
	// stw r4,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_826ED5E8:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x826ed5b8
	if (!cr6.eq) goto loc_826ED5B8;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826ed6d4
	if (!cr6.gt) goto loc_826ED6D4;
	// cmpwi cr6,r7,3
	cr6.compare<int32_t>(ctx.r7.s32, 3, xer);
	// beq cr6,0x826ed630
	if (cr6.eq) goto loc_826ED630;
	// cmpwi cr6,r6,3
	cr6.compare<int32_t>(ctx.r6.s32, 3, xer);
	// beq cr6,0x826ed630
	if (cr6.eq) goto loc_826ED630;
	// cmpw cr6,r7,r6
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r6.s32, xer);
	// blt cr6,0x826ed624
	if (cr6.lt) goto loc_826ED624;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ed6d8
	goto loc_826ED6D8;
loc_826ED624:
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ed6d8
	goto loc_826ED6D8;
loc_826ED630:
	// lhz r11,94(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 94);
	// lhz r10,90(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r7,98(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 92);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r31,r7,r11
	r31.s64 = r11.s64 - ctx.r7.s64;
	// subf r30,r10,r7
	r30.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r29,r6,r9
	r29.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r28,r8,r6
	r28.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// xor r30,r30,r5
	r30.u64 = r30.u64 ^ ctx.r5.u64;
	// xor r29,r29,r4
	r29.u64 = r29.u64 ^ ctx.r4.u64;
	// srawi r5,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r31.s32 >> 31;
	// xor r28,r28,r4
	r28.u64 = r28.u64 ^ ctx.r4.u64;
	// srawi r4,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r30.s32 >> 31;
	// srawi r31,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r29.s32 >> 31;
	// srawi r30,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r28.s32 >> 31;
	// or r29,r5,r4
	r29.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r28,r31,r30
	r28.u64 = r31.u64 | r30.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 & ~r29.u64;
	// andc r6,r6,r28
	ctx.r6.u64 = ctx.r6.u64 & ~r28.u64;
	// and r8,r30,r8
	ctx.r8.u64 = r30.u64 & ctx.r8.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r9,r31,r9
	ctx.r9.u64 = r31.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// b 0x826ed6d8
	goto loc_826ED6D8;
loc_826ED6D4:
	// stw r19,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r19.u32);
loc_826ED6D8:
	// lhz r11,82(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// add r31,r25,r24
	r31.u64 = r25.u64 + r24.u64;
	// lhz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r11,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	r11.s64 = ctx.r3.s32 >> 16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// add r11,r11,r21
	r11.u64 = r11.u64 + r21.u64;
	// and r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 & r18.u64;
	// and r11,r11,r17
	r11.u64 = r11.u64 & r17.u64;
	// subf r10,r22,r10
	ctx.r10.s64 = ctx.r10.s64 - r22.s64;
	// subf r9,r21,r11
	ctx.r9.s64 = r11.s64 - r21.s64;
	// clrlwi r11,r27,31
	r11.u64 = r27.u32 & 0x1;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r11,r26
	r29.u64 = r11.u64 + r26.u64;
	// sth r10,2(r29)
	PPC_STORE_U16(r29.u32 + 2, ctx.r10.u16);
	// sth r9,0(r29)
	PPC_STORE_U16(r29.u32 + 0, ctx.r9.u16);
	// beq cr6,0x826ed73c
	if (cr6.eq) goto loc_826ED73C;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// lwz r4,176(r23)
	ctx.r4.u64 = PPC_LOAD_U32(r23.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
loc_826ED73C:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r19,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r19.u32);
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// stw r19,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r19.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// bne cr6,0x826ed7e4
	if (!cr6.eq) goto loc_826ED7E4;
	// rlwinm r10,r20,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r24,1,0,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r20,r10
	ctx.r10.u64 = r20.u64 + ctx.r10.u64;
	// subf r11,r11,r31
	r11.s64 = r31.s64 - r11.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// subf r10,r10,r16
	ctx.r10.s64 = r16.s64 - ctx.r10.s64;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r9,r9,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ed794
	if (cr6.eq) goto loc_826ED794;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r5,2
	ctx.r5.s64 = 2;
	// lwzx r9,r9,r26
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r26.u32);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
loc_826ED794:
	// cmpwi cr6,r20,1
	cr6.compare<int32_t>(r20.s32, 1, xer);
	// beq cr6,0x826ed7e4
	if (cr6.eq) goto loc_826ED7E4;
	// addi r9,r20,-1
	ctx.r9.s64 = r20.s64 + -1;
	// cmpw cr6,r14,r9
	cr6.compare<int32_t>(r14.s32, ctx.r9.s32, xer);
	// beq cr6,0x826ed7b4
	if (cr6.eq) goto loc_826ED7B4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// b 0x826ed7bc
	goto loc_826ED7BC;
loc_826ED7B4:
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// addi r10,r10,-20
	ctx.r10.s64 = ctx.r10.s64 + -20;
loc_826ED7BC:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,0,14,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826ed7e4
	if (cr6.eq) goto loc_826ED7E4;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stwx r11,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, r11.u32);
loc_826ED7E4:
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826ed920
	if (!cr6.gt) goto loc_826ED920;
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// addi r9,r1,104
	ctx.r9.s64 = ctx.r1.s64 + 104;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
loc_826ED804:
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r4,r4,0,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x826ed828
	if (cr6.eq) goto loc_826ED828;
	// stw r4,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x826ed834
	goto loc_826ED834;
loc_826ED828:
	// stw r4,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_826ED834:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x826ed804
	if (!cr6.eq) goto loc_826ED804;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826ed920
	if (!cr6.gt) goto loc_826ED920;
	// cmpwi cr6,r7,3
	cr6.compare<int32_t>(ctx.r7.s32, 3, xer);
	// beq cr6,0x826ed87c
	if (cr6.eq) goto loc_826ED87C;
	// cmpwi cr6,r6,3
	cr6.compare<int32_t>(ctx.r6.s32, 3, xer);
	// beq cr6,0x826ed87c
	if (cr6.eq) goto loc_826ED87C;
	// cmpw cr6,r7,r6
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r6.s32, xer);
	// blt cr6,0x826ed870
	if (cr6.lt) goto loc_826ED870;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ed924
	goto loc_826ED924;
loc_826ED870:
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x826ed924
	goto loc_826ED924;
loc_826ED87C:
	// lhz r11,94(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 94);
	// lhz r10,90(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// lhz r7,98(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 92);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r31,r7,r11
	r31.s64 = r11.s64 - ctx.r7.s64;
	// subf r30,r10,r7
	r30.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r28,r6,r9
	r28.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r27,r8,r6
	r27.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// xor r30,r30,r5
	r30.u64 = r30.u64 ^ ctx.r5.u64;
	// xor r28,r28,r4
	r28.u64 = r28.u64 ^ ctx.r4.u64;
	// srawi r5,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r31.s32 >> 31;
	// xor r27,r27,r4
	r27.u64 = r27.u64 ^ ctx.r4.u64;
	// srawi r4,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r30.s32 >> 31;
	// srawi r31,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r28.s32 >> 31;
	// srawi r30,r27,31
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r27.s32 >> 31;
	// or r28,r5,r4
	r28.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r27,r31,r30
	r27.u64 = r31.u64 | r30.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 & ~r28.u64;
	// andc r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 & ~r27.u64;
	// and r8,r30,r8
	ctx.r8.u64 = r30.u64 & ctx.r8.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r9,r31,r9
	ctx.r9.u64 = r31.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// b 0x826ed924
	goto loc_826ED924;
loc_826ED920:
	// stw r19,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r19.u32);
loc_826ED924:
	// lhz r11,82(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// lhz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r11,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	r11.s64 = ctx.r3.s32 >> 16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// add r11,r11,r21
	r11.u64 = r11.u64 + r21.u64;
	// and r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 & r18.u64;
	// and r11,r11,r17
	r11.u64 = r11.u64 & r17.u64;
	// subf r10,r22,r10
	ctx.r10.s64 = ctx.r10.s64 - r22.s64;
	// subf r11,r21,r11
	r11.s64 = r11.s64 - r21.s64;
	// li r3,0
	ctx.r3.s64 = 0;
	// sth r10,6(r29)
	PPC_STORE_U16(r29.u32 + 6, ctx.r10.u16);
	// sth r11,4(r29)
	PPC_STORE_U16(r29.u32 + 4, r11.u16);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826ED96C"))) PPC_WEAK_FUNC(sub_826ED96C);
PPC_FUNC_IMPL(__imp__sub_826ED96C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826ED970"))) PPC_WEAK_FUNC(sub_826ED970);
PPC_FUNC_IMPL(__imp__sub_826ED970) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// mr r26,r9
	r26.u64 = ctx.r9.u64;
	// rlwinm r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ed9c4
	if (cr6.eq) goto loc_826ED9C4;
	// li r11,0
	r11.s64 = 0;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// stb r11,5(r23)
	PPC_STORE_U8(r23.u32 + 5, r11.u8);
	// bl 0x826eb8c0
	sub_826EB8C0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_826ED9C4:
	// lis r11,0
	r11.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ori r21,r11,32768
	r21.u64 = r11.u64 | 32768;
	// beq cr6,0x826edb24
	if (cr6.eq) goto loc_826EDB24;
	// lwz r11,1176(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1176);
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826edac8
	if (cr6.lt) goto loc_826EDAC8;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826edac0
	if (!cr6.lt) goto loc_826EDAC0;
loc_826EDA28:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826eda54
	if (cr6.lt) goto loc_826EDA54;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826eda28
	if (cr6.eq) goto loc_826EDA28;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826edb04
	goto loc_826EDB04;
loc_826EDA54:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EDAC0:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826edb04
	goto loc_826EDB04;
loc_826EDAC8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826EDAD0:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r21
	r11.u64 = r30.u64 + r21.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826edad0
	if (cr6.lt) goto loc_826EDAD0;
loc_826EDB04:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// addi r10,r30,1
	ctx.r10.s64 = r30.s64 + 1;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826edb28
	if (cr6.eq) goto loc_826EDB28;
loc_826EDB18:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_826EDB24:
	// li r10,0
	ctx.r10.s64 = 0;
loc_826EDB28:
	// lwz r9,1200(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 1200);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// rlwinm r11,r11,24,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0x7;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lbzx r22,r9,r10
	r22.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r22,5(r23)
	PPC_STORE_U8(r23.u32 + 5, r22.u8);
	// bne cr6,0x826edb64
	if (!cr6.eq) goto loc_826EDB64;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// bl 0x826eb8c0
	sub_826EB8C0(ctx, base);
	// b 0x826edb8c
	goto loc_826EDB8C;
loc_826EDB64:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// bne cr6,0x826edb78
	if (!cr6.eq) goto loc_826EDB78;
	// bl 0x826ebcb0
	sub_826EBCB0(ctx, base);
	// b 0x826edb8c
	goto loc_826EDB8C;
loc_826EDB78:
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x826edb88
	if (!cr6.eq) goto loc_826EDB88;
	// bl 0x826ec710
	sub_826EC710(ctx, base);
	// b 0x826edb8c
	goto loc_826EDB8C;
loc_826EDB88:
	// bl 0x826ecdf0
	sub_826ECDF0(ctx, base);
loc_826EDB8C:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826ee008
	if (!cr6.eq) goto loc_826EE008;
	// lbz r11,27(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 27);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ede60
	if (cr6.eq) goto loc_826EDE60;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// beq cr6,0x826ede60
	if (cr6.eq) goto loc_826EDE60;
	// lbz r11,1181(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1181);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826edbec
	if (cr6.eq) goto loc_826EDBEC;
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm r10,r10,20,12,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFFFFF;
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826edbd8
	if (cr6.eq) goto loc_826EDBD8;
	// lbz r11,1182(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// b 0x826ede44
	goto loc_826EDE44;
loc_826EDBD8:
	// lbz r11,1180(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1180);
	// lbz r10,1185(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826ede44
	goto loc_826EDE44;
loc_826EDBEC:
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// li r29,0
	r29.s64 = 0;
	// lbz r11,1186(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826edcd8
	if (cr6.eq) goto loc_826EDCD8;
	// li r30,1
	r30.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826edc6c
	if (!cr6.lt) goto loc_826EDC6C;
loc_826EDC14:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826edc6c
	if (cr6.eq) goto loc_826EDC6C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826edc5c
	if (!cr0.lt) goto loc_826EDC5C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EDC5C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826edc14
	if (cr6.gt) goto loc_826EDC14;
loc_826EDC6C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826edca8
	if (!cr0.lt) goto loc_826EDCA8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EDCA8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826edcc0
	if (cr6.eq) goto loc_826EDCC0;
	// lbz r11,1182(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826ede48
	goto loc_826EDE48;
loc_826EDCC0:
	// lbz r11,1180(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1180);
	// lbz r10,1185(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826ede48
	goto loc_826EDE48;
loc_826EDCD8:
	// li r30,3
	r30.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826edd3c
	if (!cr6.lt) goto loc_826EDD3C;
loc_826EDCE4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826edd3c
	if (cr6.eq) goto loc_826EDD3C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826edd2c
	if (!cr0.lt) goto loc_826EDD2C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EDD2C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826edce4
	if (cr6.gt) goto loc_826EDCE4;
loc_826EDD3C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826edd78
	if (!cr0.lt) goto loc_826EDD78;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EDD78:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x826ede38
	if (!cr6.eq) goto loc_826EDE38;
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// li r30,5
	r30.s64 = 5;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826eddf4
	if (!cr6.lt) goto loc_826EDDF4;
loc_826EDD9C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eddf4
	if (cr6.eq) goto loc_826EDDF4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826edde4
	if (!cr0.lt) goto loc_826EDDE4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EDDE4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826edd9c
	if (cr6.gt) goto loc_826EDD9C;
loc_826EDDF4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826ede30
	if (!cr0.lt) goto loc_826EDE30;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EDE30:
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x826ede40
	goto loc_826EDE40;
loc_826EDE38:
	// lbz r11,1180(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1180);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
loc_826EDE40:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826EDE44:
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
loc_826EDE48:
	// stb r11,4(r23)
	PPC_STORE_U8(r23.u32 + 4, r11.u8);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x826edb18
	if (cr6.lt) goto loc_826EDB18;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// bgt cr6,0x826edb18
	if (cr6.gt) goto loc_826EDB18;
loc_826EDE60:
	// lbz r11,29(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 29);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ee004
	if (cr6.eq) goto loc_826EE004;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// beq cr6,0x826ee004
	if (cr6.eq) goto loc_826EE004;
	// lwz r11,200(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 200);
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826edf68
	if (cr6.lt) goto loc_826EDF68;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826edf60
	if (!cr6.lt) goto loc_826EDF60;
loc_826EDEC8:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826edef4
	if (cr6.lt) goto loc_826EDEF4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826edec8
	if (cr6.eq) goto loc_826EDEC8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826edfa4
	goto loc_826EDFA4;
loc_826EDEF4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EDF60:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826edfa4
	goto loc_826EDFA4;
loc_826EDF68:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826EDF70:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r21
	r11.u64 = r30.u64 + r21.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826edf70
	if (cr6.lt) goto loc_826EDF70;
loc_826EDFA4:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826edb18
	if (!cr6.eq) goto loc_826EDB18;
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// li r9,1
	ctx.r9.s64 = 1;
	// blt cr6,0x826edfc4
	if (cr6.lt) goto loc_826EDFC4;
	// li r9,0
	ctx.r9.s64 = 0;
loc_826EDFC4:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwimi r11,r9,28,3,3
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 28) & 0x10000000) | (r11.u64 & 0xFFFFFFFFEFFFFFFF);
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// addi r9,r9,14080
	ctx.r9.s64 = ctx.r9.s64 + 14080;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// addi r8,r9,88
	ctx.r8.s64 = ctx.r9.s64 + 88;
	// lwzx r11,r10,r9
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// rlwimi r7,r11,24,5,7
	ctx.r7.u64 = (__builtin_rotateleft32(r11.u32, 24) & 0x7000000) | (ctx.r7.u64 & 0xFFFFFFFFF8FFFFFF);
	// stw r7,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r7.u32);
	// lwzx r11,r10,r8
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// rotlwi r10,r7,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// rlwimi r10,r11,20,10,11
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 20) & 0x300000) | (ctx.r10.u64 & 0xFFFFFFFFFFCFFFFF);
	// rlwinm r11,r10,0,5,3
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFF7FFFFFF;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
loc_826EE004:
	// li r3,0
	ctx.r3.s64 = 0;
loc_826EE008:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_826EE010"))) PPC_WEAK_FUNC(sub_826EE010);
PPC_FUNC_IMPL(__imp__sub_826EE010) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r16,0
	r16.s64 = 0;
	// mr r15,r3
	r15.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r17,r5
	r17.u64 = ctx.r5.u64;
	// mr r31,r16
	r31.u64 = r16.u64;
	// bl 0x826ea928
	sub_826EA928(ctx, base);
	// lwz r30,268(r15)
	r30.u64 = PPC_LOAD_U32(r15.u32 + 268);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x82615470
	sub_82615470(ctx, base);
	// lwz r11,21556(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 21556);
	// mr r23,r16
	r23.u64 = r16.u64;
	// stw r11,20(r17)
	PPC_STORE_U32(r17.u32 + 20, r11.u32);
	// lwz r11,21568(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 21568);
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// stw r11,24(r17)
	PPC_STORE_U32(r17.u32 + 24, r11.u32);
	// lwz r11,21560(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 21560);
	// stw r11,28(r17)
	PPC_STORE_U32(r17.u32 + 28, r11.u32);
	// lwz r11,21572(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 21572);
	// stw r16,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r16.u32);
	// stw r16,4(r17)
	PPC_STORE_U32(r17.u32 + 4, r16.u32);
	// sth r16,16(r17)
	PPC_STORE_U16(r17.u32 + 16, r16.u16);
	// stw r11,32(r17)
	PPC_STORE_U32(r17.u32 + 32, r11.u32);
	// lhz r11,52(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 52);
	// lhz r10,50(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 50);
	// lwz r27,188(r25)
	r27.u64 = PPC_LOAD_U32(r25.u32 + 188);
	// rlwinm r28,r11,31,1,31
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r26,r10,31,1,31
	r26.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// stw r28,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r28.u32);
	// stw r27,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r27.u32);
	// stw r26,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r26.u32);
	// beq cr6,0x826efa00
	if (cr6.eq) goto loc_826EFA00;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r22,2
	r22.s64 = 131072;
	// addi r11,r11,14144
	r11.s64 = r11.s64 + 14144;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r18,r11,29840
	r18.s64 = r11.s64 + 29840;
	// lis r11,0
	r11.s64 = 0;
	// ori r14,r11,32768
	r14.u64 = r11.u64 | 32768;
loc_826EE0C4:
	// addi r11,r25,1376
	r11.s64 = r25.s64 + 1376;
	// sth r16,18(r17)
	PPC_STORE_U16(r17.u32 + 18, r16.u16);
	// stw r11,1416(r25)
	PPC_STORE_U32(r25.u32 + 1416, r11.u32);
	// lwz r11,21236(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ee2cc
	if (cr6.eq) goto loc_826EE2CC;
	// lwz r11,1240(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1240);
	// rlwinm r10,r23,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ee2cc
	if (cr6.eq) goto loc_826EE2CC;
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// ld r10,104(r25)
	ctx.r10.u64 = PPC_LOAD_U64(r25.u32 + 104);
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,112(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,116(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,120(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,124(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,128(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,132(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,140(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,144(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,148(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,152(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r31,84(r15)
	r31.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ee20c
	if (cr6.eq) goto loc_826EE20C;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r29,1
	r29.s64 = 1;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826ee1e4
	if (!cr6.lt) goto loc_826EE1E4;
loc_826EE1A4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ee1e4
	if (cr6.eq) goto loc_826EE1E4;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x826ee1d4
	if (!cr0.lt) goto loc_826EE1D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EE1D4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826ee1a4
	if (cr6.gt) goto loc_826EE1A4;
loc_826EE1E4:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r29,32
	ctx.r10.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x826ee20c
	if (!cr0.lt) goto loc_826EE20C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EE20C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// bl 0x82639b10
	sub_82639B10(ctx, base);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,104(r25)
	PPC_STORE_U64(r25.u32 + 104, r11.u64);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,112(r25)
	PPC_STORE_U32(r25.u32 + 112, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,116(r25)
	PPC_STORE_U32(r25.u32 + 116, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,120(r25)
	PPC_STORE_U32(r25.u32 + 120, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,124(r25)
	PPC_STORE_U32(r25.u32 + 124, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,128(r25)
	PPC_STORE_U32(r25.u32 + 128, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,132(r25)
	PPC_STORE_U32(r25.u32 + 132, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,136(r25)
	PPC_STORE_U32(r25.u32 + 136, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,140(r25)
	PPC_STORE_U32(r25.u32 + 140, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,144(r25)
	PPC_STORE_U32(r25.u32 + 144, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,148(r25)
	PPC_STORE_U32(r25.u32 + 148, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stb r10,1187(r25)
	PPC_STORE_U8(r25.u32 + 1187, ctx.r10.u8);
	// stw r11,152(r25)
	PPC_STORE_U32(r25.u32 + 152, r11.u32);
	// bne cr6,0x826efac4
	if (!cr6.eq) goto loc_826EFAC4;
loc_826EE2CC:
	// lwz r11,3932(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 3932);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826ee410
	if (cr6.eq) goto loc_826EE410;
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// ld r10,104(r25)
	ctx.r10.u64 = PPC_LOAD_U64(r25.u32 + 104);
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,112(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,116(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,120(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,124(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,128(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,132(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,140(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,144(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,148(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,152(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// bl 0x82639ef0
	sub_82639EF0(ctx, base);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,104(r25)
	PPC_STORE_U64(r25.u32 + 104, r11.u64);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,112(r25)
	PPC_STORE_U32(r25.u32 + 112, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,116(r25)
	PPC_STORE_U32(r25.u32 + 116, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,120(r25)
	PPC_STORE_U32(r25.u32 + 120, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,124(r25)
	PPC_STORE_U32(r25.u32 + 124, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,128(r25)
	PPC_STORE_U32(r25.u32 + 128, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,132(r25)
	PPC_STORE_U32(r25.u32 + 132, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,136(r25)
	PPC_STORE_U32(r25.u32 + 136, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,140(r25)
	PPC_STORE_U32(r25.u32 + 140, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,144(r25)
	PPC_STORE_U32(r25.u32 + 144, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,148(r25)
	PPC_STORE_U32(r25.u32 + 148, r11.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stw r11,152(r25)
	PPC_STORE_U32(r25.u32 + 152, r11.u32);
	// bne cr6,0x826efac4
	if (!cr6.eq) goto loc_826EFAC4;
loc_826EE410:
	// lwz r10,1240(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1240);
	// rlwinm r9,r23,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r11,r23
	r11.s64 = -r23.s64;
	// mr r24,r16
	r24.u64 = r16.u64;
	// srawi r11,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 31;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r24,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r24.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// beq cr6,0x826ef9d4
	if (cr6.eq) goto loc_826EF9D4;
loc_826EE444:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r10,128
	ctx.r10.s64 = 128;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// dcbt r10,r11
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lbz r9,24(r25)
	ctx.r9.u64 = PPC_LOAD_U8(r25.u32 + 24);
	// rlwimi r11,r10,17,27,28
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 17) & 0x18) | (r11.u64 & 0xFFFFFFFFFFFFFFE7);
	// rlwimi r11,r10,17,14,15
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 17) & 0x30000) | (r11.u64 & 0xFFFFFFFFFFFCFFFF);
	// rlwimi r11,r10,17,3,3
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 17) & 0x10000000) | (r11.u64 & 0xFFFFFFFFEFFFFFFF);
	// stb r9,4(r30)
	PPC_STORE_U8(r30.u32 + 4, ctx.r9.u8);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lbz r11,26(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 26);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826ee4b8
	if (!cr6.eq) goto loc_826EE4B8;
	// lwz r3,0(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826ee4ac
	if (!cr0.lt) goto loc_826EE4AC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EE4AC:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwimi r11,r31,31,0,0
	r11.u64 = (__builtin_rotateleft32(r31.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
loc_826EE4B8:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lis r10,-32768
	ctx.r10.s64 = -2147483648;
	// rlwinm r11,r11,0,21,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFF7FF;
	// rlwinm r9,r11,0,0,0
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// bne cr6,0x826ee4e8
	if (!cr6.eq) goto loc_826EE4E8;
	// rlwinm r11,r11,0,24,20
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFF8FF;
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// rlwinm r11,r11,0,16,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFEFFFF;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// b 0x826ee68c
	goto loc_826EE68C;
loc_826EE4E8:
	// lwz r11,1316(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1316);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826ee5dc
	if (cr6.lt) goto loc_826EE5DC;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826ee5d4
	if (!cr6.lt) goto loc_826EE5D4;
loc_826EE53C:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826ee568
	if (cr6.lt) goto loc_826EE568;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826ee53c
	if (cr6.eq) goto loc_826EE53C;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826ee618
	goto loc_826EE618;
loc_826EE568:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EE5D4:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826ee618
	goto loc_826EE618;
loc_826EE5DC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826EE5E4:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r14
	r11.u64 = r29.u64 + r14.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826ee5e4
	if (cr6.lt) goto loc_826EE5E4;
loc_826EE618:
	// mr r11,r29
	r11.u64 = r29.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826efad0
	if (cr6.lt) goto loc_826EFAD0;
	// cmpwi cr6,r29,14
	cr6.compare<int32_t>(r29.s32, 14, xer);
	// bgt cr6,0x826efad0
	if (cr6.gt) goto loc_826EFAD0;
	// lwz r10,1320(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1320);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x826ee644
	if (!cr6.eq) goto loc_826EE644;
	// cmpwi cr6,r29,8
	cr6.compare<int32_t>(r29.s32, 8, xer);
	// bne cr6,0x826ee644
	if (!cr6.eq) goto loc_826EE644;
	// li r11,14
	r11.s64 = 14;
loc_826EE644:
	// lwz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm r10,r10,0,24,20
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFF8FF;
	// lbzx r11,r11,r9
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// rlwinm r10,r10,0,16,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFEFFFF;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// rlwinm r9,r11,0,28,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	// clrlwi r8,r11,29
	ctx.r8.u64 = r11.u32 & 0x7;
	// srawi r9,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 3;
	// srawi r7,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r7.s64 = r11.s32 >> 4;
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// clrlwi r8,r7,31
	ctx.r8.u64 = ctx.r7.u32 & 0x1;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// clrlwi r7,r11,31
	ctx.r7.u64 = r11.u32 & 0x1;
	// or r11,r9,r10
	r11.u64 = ctx.r9.u64 | ctx.r10.u64;
loc_826EE68C:
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// rlwinm r11,r11,0,21,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r11,1024
	cr6.compare<uint32_t>(r11.u32, 1024, xer);
	// bne cr6,0x826ef8ac
	if (!cr6.eq) goto loc_826EF8AC;
	// lbz r11,33(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 33);
	// li r29,1
	r29.s64 = 1;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r28,r16
	r28.u64 = r16.u64;
	// rlwimi r10,r11,11,20,20
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 11) & 0x800) | (ctx.r10.u64 & 0xFFFFFFFFFFFFF7FF);
	// rlwinm r11,r10,0,15,13
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826ee728
	if (!cr6.lt) goto loc_826EE728;
loc_826EE6D0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ee728
	if (cr6.eq) goto loc_826EE728;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826ee718
	if (!cr0.lt) goto loc_826EE718;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EE718:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826ee6d0
	if (cr6.gt) goto loc_826EE6D0;
loc_826EE728:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826ee764
	if (!cr0.lt) goto loc_826EE764;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EE764:
	// mr r11,r29
	r11.u64 = r29.u64;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r29,1
	r29.s64 = 1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r28,r16
	r28.u64 = r16.u64;
	// rlwimi r10,r11,16,15,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0x10000) | (ctx.r10.u64 & 0xFFFFFFFFFFFEFFFF);
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826ee7ec
	if (!cr6.lt) goto loc_826EE7EC;
loc_826EE794:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ee7ec
	if (cr6.eq) goto loc_826EE7EC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826ee7dc
	if (!cr0.lt) goto loc_826EE7DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EE7DC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826ee794
	if (cr6.gt) goto loc_826EE794;
loc_826EE7EC:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826ee828
	if (!cr0.lt) goto loc_826EE828;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EE828:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x826ee9fc
	if (cr6.eq) goto loc_826EE9FC;
	// lwz r11,1172(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1172);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826ee924
	if (cr6.lt) goto loc_826EE924;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826ee91c
	if (!cr6.lt) goto loc_826EE91C;
loc_826EE884:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826ee8b0
	if (cr6.lt) goto loc_826EE8B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826ee884
	if (cr6.eq) goto loc_826EE884;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826ee960
	goto loc_826EE960;
loc_826EE8B0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EE91C:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826ee960
	goto loc_826EE960;
loc_826EE924:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826EE92C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r14
	r11.u64 = r29.u64 + r14.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826ee92c
	if (cr6.lt) goto loc_826EE92C;
loc_826EE960:
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// cmplwi cr6,r11,63
	cr6.compare<uint32_t>(r11.u32, 63, xer);
	// bgt cr6,0x826eec78
	if (cr6.gt) goto loc_826EEC78;
loc_826EE96C:
	// lwz r3,0(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826eec78
	if (!cr6.eq) goto loc_826EEC78;
	// lwz r10,1200(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1200);
	// lbzx r31,r10,r11
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826ee9ac
	if (!cr0.lt) goto loc_826EE9AC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EE9AC:
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// stb r31,5(r30)
	PPC_STORE_U8(r30.u32 + 5, r31.u8);
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// lbz r11,27(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 27);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eec78
	if (cr6.eq) goto loc_826EEC78;
	// lbz r11,1181(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1181);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eea18
	if (cr6.eq) goto loc_826EEA18;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// rlwinm r10,r10,20,12,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFFFFF;
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eea04
	if (cr6.eq) goto loc_826EEA04;
	// lbz r11,1182(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// b 0x826eec70
	goto loc_826EEC70;
loc_826EE9FC:
	// mr r11,r16
	r11.u64 = r16.u64;
	// b 0x826ee96c
	goto loc_826EE96C;
loc_826EEA04:
	// lbz r11,1180(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1180);
	// lbz r10,1185(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826eec70
	goto loc_826EEC70;
loc_826EEA18:
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// mr r28,r16
	r28.u64 = r16.u64;
	// lbz r11,1186(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826eeb04
	if (cr6.eq) goto loc_826EEB04;
	// li r29,1
	r29.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826eea98
	if (!cr6.lt) goto loc_826EEA98;
loc_826EEA40:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eea98
	if (cr6.eq) goto loc_826EEA98;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826eea88
	if (!cr0.lt) goto loc_826EEA88;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EEA88:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826eea40
	if (cr6.gt) goto loc_826EEA40;
loc_826EEA98:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eead4
	if (!cr0.lt) goto loc_826EEAD4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EEAD4:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826eeaec
	if (cr6.eq) goto loc_826EEAEC;
	// lbz r11,1182(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826eec74
	goto loc_826EEC74;
loc_826EEAEC:
	// lbz r11,1180(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1180);
	// lbz r10,1185(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826eec74
	goto loc_826EEC74;
loc_826EEB04:
	// li r29,3
	r29.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826eeb68
	if (!cr6.lt) goto loc_826EEB68;
loc_826EEB10:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eeb68
	if (cr6.eq) goto loc_826EEB68;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826eeb58
	if (!cr0.lt) goto loc_826EEB58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EEB58:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826eeb10
	if (cr6.gt) goto loc_826EEB10;
loc_826EEB68:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eeba4
	if (!cr0.lt) goto loc_826EEBA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EEBA4:
	// cmpwi cr6,r29,7
	cr6.compare<int32_t>(r29.s32, 7, xer);
	// bne cr6,0x826eec64
	if (!cr6.eq) goto loc_826EEC64;
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r29,5
	r29.s64 = 5;
	// mr r28,r16
	r28.u64 = r16.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826eec20
	if (!cr6.lt) goto loc_826EEC20;
loc_826EEBC8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826eec20
	if (cr6.eq) goto loc_826EEC20;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826eec10
	if (!cr0.lt) goto loc_826EEC10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EEC10:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x826eebc8
	if (cr6.gt) goto loc_826EEBC8;
loc_826EEC20:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826eec5c
	if (!cr0.lt) goto loc_826EEC5C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EEC5C:
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x826eec6c
	goto loc_826EEC6C;
loc_826EEC64:
	// lbz r11,1180(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1180);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
loc_826EEC6C:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826EEC70:
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
loc_826EEC74:
	// stb r11,4(r30)
	PPC_STORE_U8(r30.u32 + 4, r11.u8);
loc_826EEC78:
	// lbz r11,4(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// mr r26,r16
	r26.u64 = r16.u64;
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// rotlwi r9,r11,2
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 2);
	// lbz r21,5(r30)
	r21.u64 = PPC_LOAD_U8(r30.u32 + 5);
	// neg r20,r10
	r20.s64 = -ctx.r10.s64;
	// lwz r10,220(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r16,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, r16.u64);
	// stw r26,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r26.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r19,r11,r10
	r19.u64 = r11.u64 + ctx.r10.u64;
loc_826EECA8:
	// clrlwi r9,r21,31
	ctx.r9.u64 = r21.u32 & 0x1;
	// lhz r11,50(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 50);
	// rlwinm r10,r26,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xFFFFFFFC;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// mr r31,r16
	r31.u64 = r16.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r9,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r9.u32);
	// rlwinm r6,r11,31,1,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r9,r16
	ctx.r9.u64 = r16.u64;
	// bne cr6,0x826eed88
	if (!cr6.eq) goto loc_826EED88;
	// addi r10,r25,248
	ctx.r10.s64 = r25.s64 + 248;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// beq cr6,0x826eecec
	if (cr6.eq) goto loc_826EECEC;
	// lwz r10,-20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + -20);
	// not r10,r10
	ctx.r10.u64 = ~ctx.r10.u64;
	// rlwinm r4,r10,15,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0x1;
loc_826EECEC:
	// clrlwi r5,r26,31
	ctx.r5.u64 = r26.u32 & 0x1;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// add r4,r5,r4
	ctx.r4.u64 = ctx.r5.u64 + ctx.r4.u64;
	// beq cr6,0x826eed18
	if (cr6.eq) goto loc_826EED18;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r30
	ctx.r10.s64 = r30.s64 - ctx.r10.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// not r10,r10
	ctx.r10.u64 = ~ctx.r10.u64;
	// rlwinm r9,r10,15,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0x1;
loc_826EED18:
	// srawi r10,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	ctx.r10.s64 = r26.s32 >> 1;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// beq cr6,0x826eed58
	if (cr6.eq) goto loc_826EED58;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826eed58
	if (cr6.eq) goto loc_826EED58;
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// mullw r9,r9,r6
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// lwz r9,-20(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + -20);
	// not r9,r9
	ctx.r9.u64 = ~ctx.r9.u64;
	// rlwinm r31,r9,15,31,31
	r31.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 15) & 0x1;
loc_826EED58:
	// rlwinm r7,r23,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r29,1160(r25)
	r29.u64 = PPC_LOAD_U32(r25.u32 + 1160);
	// rlwinm r8,r24,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,264(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 264);
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// li r7,119
	ctx.r7.s64 = 119;
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// stw r7,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r7.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// b 0x826eee28
	goto loc_826EEE28;
loc_826EED88:
	// lwz r11,1164(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1164);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// addi r11,r25,236
	r11.s64 = r25.s64 + 236;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// li r11,119
	r11.s64 = 119;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// beq cr6,0x826eedb8
	if (cr6.eq) goto loc_826EEDB8;
	// lwz r11,-20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + -20);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r4,r11,15,31,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
loc_826EEDB8:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x826eede0
	if (cr6.eq) goto loc_826EEDE0;
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,15,17,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x7FFF;
	// andc r11,r20,r11
	r11.u64 = r20.u64 & ~r11.u64;
	// clrlwi r3,r11,31
	ctx.r3.u64 = r11.u32 & 0x1;
loc_826EEDE0:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x826eee10
	if (cr6.eq) goto loc_826EEE10;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826eee10
	if (cr6.eq) goto loc_826EEE10;
	// addi r11,r6,1
	r11.s64 = ctx.r6.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r31,r11,15,31,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
loc_826EEE10:
	// addi r10,r26,63
	ctx.r10.s64 = r26.s64 + 63;
	// mullw r11,r6,r23
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(r23.s32);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r24
	ctx.r10.u64 = r11.u64 + r24.u64;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// lwzx r9,r9,r25
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r25.u32);
loc_826EEE28:
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r8,r11,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r11,16(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 16);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r29,r16
	r29.u64 = r16.u64;
	// rlwinm r9,r9,29,30,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x3;
	// addi r27,r10,-16
	r27.s64 = ctx.r10.s64 + -16;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// subf r28,r8,r10
	r28.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// mr r11,r16
	r11.u64 = r16.u64;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// stw r9,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r9.u32);
	// beq cr6,0x826ef078
	if (cr6.eq) goto loc_826EF078;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826ef06c
	if (cr6.eq) goto loc_826EF06C;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x826eee80
	if (cr6.eq) goto loc_826EEE80;
	// lhz r11,-32(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + -32);
	// extsh r7,r11
	ctx.r7.s64 = r11.s16;
	// b 0x826eee84
	goto loc_826EEE84;
loc_826EEE80:
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
loc_826EEE84:
	// lhz r11,0(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 0);
	// lhz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 0);
	// lbz r9,27(r25)
	ctx.r9.u64 = PPC_LOAD_U8(r25.u32 + 27);
	// extsh r5,r11
	ctx.r5.s64 = r11.s16;
	// extsh r4,r10
	ctx.r4.s64 = ctx.r10.s16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826ef044
	if (cr6.eq) goto loc_826EF044;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826eef90
	if (cr6.eq) goto loc_826EEF90;
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// beq cr6,0x826eef90
	if (cr6.eq) goto loc_826EEF90;
	// cmpwi cr6,r26,5
	cr6.compare<int32_t>(r26.s32, 5, xer);
	// beq cr6,0x826eef90
	if (cr6.eq) goto loc_826EEF90;
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// bne cr6,0x826eef2c
	if (!cr6.eq) goto loc_826EEF2C;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r11,4(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// lwz r10,220(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rotlwi r8,r11,2
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 2);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// subf r9,r9,r30
	ctx.r9.s64 = r30.s64 - ctx.r9.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lbz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// rotlwi r8,r9,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwzx r11,r11,r18
	r11.u64 = PPC_LOAD_U32(r11.u32 + r18.u32);
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// mullw r10,r11,r7
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// add r11,r11,r22
	r11.u64 = r11.u64 + r22.u64;
	// srawi r7,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 18;
	// srawi r5,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	ctx.r5.s64 = r11.s32 >> 18;
	// b 0x826ef044
	goto loc_826EF044;
loc_826EEF2C:
	// cmpwi cr6,r26,2
	cr6.compare<int32_t>(r26.s32, 2, xer);
	// bne cr6,0x826ef044
	if (!cr6.eq) goto loc_826EF044;
	// lbz r11,4(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// lbz r9,-16(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + -16);
	// rotlwi r8,r11,2
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 2);
	// lwz r10,220(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rotlwi r8,r9,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r8,r11,r10
	ctx.r8.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,16(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// lwzx r11,r11,r18
	r11.u64 = PPC_LOAD_U32(r11.u32 + r18.u32);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// mullw r10,r11,r7
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// mullw r11,r11,r4
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// add r11,r11,r22
	r11.u64 = r11.u64 + r22.u64;
	// srawi r7,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 18;
	// srawi r4,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = r11.s32 >> 18;
	// b 0x826ef044
	goto loc_826EF044;
loc_826EEF90:
	// rlwinm r8,r6,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// lbz r9,-16(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + -16);
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lwz r11,220(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// rotlwi r3,r10,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// subf r8,r8,r30
	ctx.r8.s64 = r30.s64 - ctx.r8.s64;
	// rotlwi r3,r9,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r9,r3
	ctx.r3.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// lbz r10,-16(r8)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + -16);
	// lbz r9,4(r8)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r8.u32 + 4);
	// rlwinm r8,r3,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r8,r11
	ctx.r3.u64 = ctx.r8.u64 + r11.u64;
	// lwz r8,16(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rotlwi r31,r10,2
	r31.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r29,16(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// rotlwi r3,r9,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r8,r18
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + r18.u32);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,16(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// mullw r9,r9,r7
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// mullw r10,r29,r10
	ctx.r10.s64 = int64_t(r29.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + r22.u64;
	// add r11,r11,r22
	r11.u64 = r11.u64 + r22.u64;
	// mullw r10,r10,r4
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// srawi r7,r9,18
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 18;
	// srawi r5,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	ctx.r5.s64 = r11.s32 >> 18;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// lis r11,0
	r11.s64 = 0;
	// srawi r4,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 18;
	// ori r14,r11,32768
	r14.u64 = r11.u64 | 32768;
loc_826EF044:
	// subf r11,r4,r7
	r11.s64 = ctx.r7.s64 - ctx.r4.s64;
	// subf r10,r5,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r5.s64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x826ef080
	if (cr6.lt) goto loc_826EF080;
loc_826EF06C:
	// mr r11,r27
	r11.u64 = r27.u64;
	// li r29,8
	r29.s64 = 8;
	// b 0x826ef088
	goto loc_826EF088;
loc_826EF078:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826ef284
	if (cr6.eq) goto loc_826EF284;
loc_826EF080:
	// li r29,1
	r29.s64 = 1;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_826EF088:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ef284
	if (cr6.eq) goto loc_826EF284;
	// lbz r10,27(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 27);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826ef284
	if (cr6.eq) goto loc_826EF284;
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bne cr6,0x826ef188
	if (!cr6.eq) goto loc_826EF188;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826ef0e8
	if (cr6.eq) goto loc_826EF0E8;
	// cmpwi cr6,r26,2
	cr6.compare<int32_t>(r26.s32, 2, xer);
	// beq cr6,0x826ef0e8
	if (cr6.eq) goto loc_826EF0E8;
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// beq cr6,0x826ef0e8
	if (cr6.eq) goto loc_826EF0E8;
	// cmpwi cr6,r26,5
	cr6.compare<int32_t>(r26.s32, 5, xer);
	// beq cr6,0x826ef0e8
	if (cr6.eq) goto loc_826EF0E8;
	// addi r10,r1,176
	ctx.r10.s64 = ctx.r1.s64 + 176;
	// li r9,16
	ctx.r9.s64 = 16;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_826EF0D0:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x826ef0d0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826EF0D0;
	// b 0x826ef280
	goto loc_826EF280;
loc_826EF0E8:
	// lbz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// addi r7,r1,178
	ctx.r7.s64 = ctx.r1.s64 + 178;
	// lbz r9,-16(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + -16);
	// rotlwi r5,r10,2
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// lwz r6,220(r25)
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// rotlwi r4,r9,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r5,r10,r5
	ctx.r5.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r4,r9,r4
	ctx.r4.u64 = ctx.r9.u64 + ctx.r4.u64;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// extsh r3,r8
	ctx.r3.s64 = ctx.r8.s16;
	// add r31,r5,r6
	r31.u64 = ctx.r5.u64 + ctx.r6.u64;
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// add r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 + ctx.r6.u64;
	// addi r8,r11,2
	ctx.r8.s64 = r11.s64 + 2;
	// lwz r5,16(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// li r11,15
	r11.s64 = 15;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r18
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r18.u32);
	// lwz r6,16(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// lwzx r5,r5,r18
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + r18.u32);
	// mullw r6,r5,r6
	ctx.r6.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r6.s32);
	// mullw r6,r6,r3
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r3.s32);
	// add r6,r6,r22
	ctx.r6.u64 = ctx.r6.u64 + r22.u64;
	// srawi r6,r6,18
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 18;
	// sth r6,176(r1)
	PPC_STORE_U16(ctx.r1.u32 + 176, ctx.r6.u16);
loc_826EF154:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mullw r6,r6,r10
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r10.s32);
	// mullw r6,r6,r9
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r9.s32);
	// add r6,r6,r22
	ctx.r6.u64 = ctx.r6.u64 + r22.u64;
	// srawi r6,r6,18
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 18;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bne cr6,0x826ef154
	if (!cr6.eq) goto loc_826EF154;
	// b 0x826ef278
	goto loc_826EF278;
loc_826EF188:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x826ef1cc
	if (cr6.eq) goto loc_826EF1CC;
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// beq cr6,0x826ef1cc
	if (cr6.eq) goto loc_826EF1CC;
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// beq cr6,0x826ef1cc
	if (cr6.eq) goto loc_826EF1CC;
	// cmpwi cr6,r26,5
	cr6.compare<int32_t>(r26.s32, 5, xer);
	// beq cr6,0x826ef1cc
	if (cr6.eq) goto loc_826EF1CC;
	// addi r10,r1,176
	ctx.r10.s64 = ctx.r1.s64 + 176;
	// li r9,16
	ctx.r9.s64 = 16;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_826EF1B4:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x826ef1b4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826EF1B4;
	// b 0x826ef280
	goto loc_826EF280;
loc_826EF1CC:
	// lbz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,220(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// rotlwi r5,r10,2
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rotlwi r3,r10,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r30
	ctx.r9.s64 = r30.s64 - ctx.r9.s64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwzx r5,r3,r18
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + r18.u32);
	// extsh r4,r7
	ctx.r4.s64 = ctx.r7.s16;
	// addi r7,r11,2
	ctx.r7.s64 = r11.s64 + 2;
	// addi r6,r1,178
	ctx.r6.s64 = ctx.r1.s64 + 178;
	// lbz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// li r11,15
	r11.s64 = 15;
	// lwz r3,16(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// rotlwi r10,r9,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mullw r9,r5,r9
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r9.s32);
	// lwzx r5,r3,r18
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + r18.u32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// mullw r10,r5,r10
	ctx.r10.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// mullw r10,r10,r4
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// srawi r10,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 18;
	// sth r10,176(r1)
	PPC_STORE_U16(ctx.r1.u32 + 176, ctx.r10.u16);
loc_826EF24C:
	// lhz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// srawi r10,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 18;
	// sth r10,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, ctx.r10.u16);
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// bne cr6,0x826ef24c
	if (!cr6.eq) goto loc_826EF24C;
loc_826EF278:
	// lhz r11,176(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 176);
	// sth r11,192(r1)
	PPC_STORE_U16(ctx.r1.u32 + 192, r11.u16);
loc_826EF280:
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
loc_826EF284:
	// lwz r10,28(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + 28);
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// addi r11,r10,-128
	r11.s64 = ctx.r10.s64 + -128;
	// stw r29,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r29.u32);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// stw r11,28(r17)
	PPC_STORE_U32(r17.u32 + 28, r11.u32);
	// dcbzl r0,r11
	memset(base + ((r11.u32) & ~127), 0, 128);
	// lwz r26,124(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// dcbt r0,r26
	// lwz r24,128(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// dcbt r0,r24
	// lwz r23,88(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// dcbt r0,r23
	// mr r28,r16
	r28.u64 = r16.u64;
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lbz r4,8(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// lwz r29,0(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r29.u32);
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826ef3b0
	if (cr6.lt) goto loc_826EF3B0;
	// clrlwi r10,r30,28
	ctx.r10.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x826ef3a8
	if (!cr6.lt) goto loc_826EF3A8;
loc_826EF310:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826ef33c
	if (cr6.lt) goto loc_826EF33C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826ef310
	if (cr6.eq) goto loc_826EF310;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826ef3ec
	goto loc_826EF3EC;
loc_826EF33C:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826EF3A8:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826ef3ec
	goto loc_826EF3EC;
loc_826EF3B0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826EF3B8:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r14
	r11.u64 = r30.u64 + r14.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826ef3b8
	if (cr6.lt) goto loc_826EF3B8;
loc_826EF3EC:
	// clrlwi r30,r30,16
	r30.u64 = r30.u32 & 0xFFFF;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// mr r27,r30
	r27.u64 = r30.u64;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// beq cr6,0x826ef53c
	if (cr6.eq) goto loc_826EF53C;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x826ef65c
	if (cr6.eq) goto loc_826EF65C;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x826ef454
	if (!cr6.eq) goto loc_826EF454;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x826ef440
	if (!cr0.lt) goto loc_826EF440;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EF440:
	// rlwinm r11,r27,1,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826ef518
	goto loc_826EF518;
loc_826EF454:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x826ef51c
	if (!cr6.eq) goto loc_826EF51C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r16
	r29.u64 = r16.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x826ef4cc
	if (!cr6.lt) goto loc_826EF4CC;
loc_826EF474:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ef4cc
	if (cr6.eq) goto loc_826EF4CC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826ef4bc
	if (!cr0.lt) goto loc_826EF4BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EF4BC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826ef474
	if (cr6.gt) goto loc_826EF474;
loc_826EF4CC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826ef508
	if (!cr0.lt) goto loc_826EF508;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EF508:
	// rlwinm r11,r27,2,0,29
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-3
	r11.s64 = r11.s64 + -3;
loc_826EF518:
	// clrlwi r30,r11,16
	r30.u64 = r11.u32 & 0xFFFF;
loc_826EF51C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x826ef638
	goto loc_826EF638;
loc_826EF53C:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bgt cr6,0x826ef554
	if (cr6.gt) goto loc_826EF554;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// subfic r11,r11,3
	xer.ca = r11.u32 <= 3;
	r11.s64 = 3 - r11.s64;
	// b 0x826ef558
	goto loc_826EF558;
loc_826EF554:
	// mr r11,r16
	r11.u64 = r16.u64;
loc_826EF558:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r30,r11,8
	r30.s64 = r11.s64 + 8;
	// mr r29,r16
	r29.u64 = r16.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x826ef578
	if (!cr6.eq) goto loc_826EF578;
	// mr r11,r16
	r11.u64 = r16.u64;
	// b 0x826ef618
	goto loc_826EF618;
loc_826EF578:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826ef5d8
	if (!cr6.gt) goto loc_826EF5D8;
loc_826EF580:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ef5d8
	if (cr6.eq) goto loc_826EF5D8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826ef5c8
	if (!cr0.lt) goto loc_826EF5C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EF5C8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826ef580
	if (cr6.gt) goto loc_826EF580;
loc_826EF5D8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826ef614
	if (!cr0.lt) goto loc_826EF614;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EF614:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_826EF618:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrlwi r30,r11,16
	r30.u64 = r11.u32 & 0xFFFF;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
loc_826EF638:
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// bge 0x826ef648
	if (!cr0.lt) goto loc_826EF648;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EF648:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r10,r30
	ctx.r10.s64 = r30.s16;
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// extsh r28,r11
	r28.s64 = r11.s16;
loc_826EF65C:
	// sth r28,0(r26)
	PPC_STORE_U16(r26.u32 + 0, r28.u16);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826efad0
	if (!cr6.eq) goto loc_826EFAD0;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826ef688
	if (!cr6.eq) goto loc_826EF688;
	// lwz r6,1204(r25)
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + 1204);
	// mr r31,r16
	r31.u64 = r16.u64;
	// b 0x826ef6bc
	goto loc_826EF6BC;
loc_826EF688:
	// lwz r31,136(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lbz r10,1188(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 1188);
	// addi r11,r31,301
	r11.s64 = r31.s64 + 301;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r11,r25
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + r25.u32);
	// beq cr6,0x826ef6bc
	if (cr6.eq) goto loc_826EF6BC;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x826ef6bc
	if (cr6.eq) goto loc_826EF6BC;
	// cmpwi cr6,r31,8
	cr6.compare<int32_t>(r31.s32, 8, xer);
	// li r31,8
	r31.s64 = 8;
	// bne cr6,0x826ef6bc
	if (!cr6.eq) goto loc_826EF6BC;
	// li r31,1
	r31.s64 = 1;
loc_826EF6BC:
	// lwz r29,140(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x826ef6e4
	if (cr6.eq) goto loc_826EF6E4;
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x826fcd00
	sub_826FCD00(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x826efad0
	if (cr6.lt) goto loc_826EFAD0;
loc_826EF6E4:
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x826ef754
	if (cr6.eq) goto loc_826EF754;
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// beq cr6,0x826ef744
	if (cr6.eq) goto loc_826EF744;
	// cmpwi cr6,r31,8
	cr6.compare<int32_t>(r31.s32, 8, xer);
	// beq cr6,0x826ef710
	if (cr6.eq) goto loc_826EF710;
	// lhz r11,0(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// lhz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U16(r24.u32 + 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sth r11,0(r26)
	PPC_STORE_U16(r26.u32 + 0, r11.u16);
	// b 0x826ef754
	goto loc_826EF754;
loc_826EF710:
	// mr r11,r26
	r11.u64 = r26.u64;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// li r10,8
	ctx.r10.s64 = 8;
loc_826EF71C:
	// lhz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bne cr6,0x826ef71c
	if (!cr6.eq) goto loc_826EF71C;
	// b 0x826ef754
	goto loc_826EF754;
loc_826EF744:
	// lvx128 v0,r0,r24
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r26
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v0,r0,r26
	_mm_store_si128((__m128i*)(base + ((r26.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_826EF754:
	// lbz r11,1188(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1188);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826ef7b0
	if (cr6.eq) goto loc_826EF7B0;
	// lhz r11,0(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// addi r10,r23,16
	ctx.r10.s64 = r23.s64 + 16;
	// sth r11,0(r23)
	PPC_STORE_U16(r23.u32 + 0, r11.u16);
	// lhz r11,16(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 16);
	// sth r11,2(r23)
	PPC_STORE_U16(r23.u32 + 2, r11.u16);
	// lhz r11,32(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 32);
	// sth r11,4(r23)
	PPC_STORE_U16(r23.u32 + 4, r11.u16);
	// lhz r11,48(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 48);
	// sth r11,6(r23)
	PPC_STORE_U16(r23.u32 + 6, r11.u16);
	// lhz r11,64(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 64);
	// sth r11,8(r23)
	PPC_STORE_U16(r23.u32 + 8, r11.u16);
	// lhz r11,80(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 80);
	// sth r11,10(r23)
	PPC_STORE_U16(r23.u32 + 10, r11.u16);
	// lhz r11,96(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 96);
	// sth r11,12(r23)
	PPC_STORE_U16(r23.u32 + 12, r11.u16);
	// lhz r11,112(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 112);
	// sth r11,14(r23)
	PPC_STORE_U16(r23.u32 + 14, r11.u16);
	// lvx128 v0,r0,r26
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x826ef7f8
	goto loc_826EF7F8;
loc_826EF7B0:
	// lvx128 v0,r0,r26
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r23
	_mm_store_si128((__m128i*)(base + ((r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lhz r11,0(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// sth r11,16(r23)
	PPC_STORE_U16(r23.u32 + 16, r11.u16);
	// lhz r11,16(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 16);
	// sth r11,18(r23)
	PPC_STORE_U16(r23.u32 + 18, r11.u16);
	// lhz r11,32(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 32);
	// sth r11,20(r23)
	PPC_STORE_U16(r23.u32 + 20, r11.u16);
	// lhz r11,48(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 48);
	// sth r11,22(r23)
	PPC_STORE_U16(r23.u32 + 22, r11.u16);
	// lhz r11,64(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 64);
	// sth r11,24(r23)
	PPC_STORE_U16(r23.u32 + 24, r11.u16);
	// lhz r11,80(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 80);
	// sth r11,26(r23)
	PPC_STORE_U16(r23.u32 + 26, r11.u16);
	// lhz r11,96(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 96);
	// sth r11,28(r23)
	PPC_STORE_U16(r23.u32 + 28, r11.u16);
	// lhz r11,112(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 112);
	// sth r11,30(r23)
	PPC_STORE_U16(r23.u32 + 30, r11.u16);
loc_826EF7F8:
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// extsw r9,r29
	ctx.r9.s64 = r29.s32;
	// lwz r30,112(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// srawi r21,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	r21.s64 = r21.s32 >> 1;
	// addi r26,r11,1
	r26.s64 = r11.s64 + 1;
	// lwz r23,84(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// ld r11,160(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// ori r9,r9,128
	ctx.r9.u64 = ctx.r9.u64 | 128;
	// lwz r24,80(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r26,6
	cr6.compare<int32_t>(r26.s32, 6, xer);
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// stw r26,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r26.u32);
	// stb r16,6(r10)
	PPC_STORE_U8(ctx.r10.u32 + 6, r16.u8);
	// std r11,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, r11.u64);
	// blt cr6,0x826eeca8
	if (cr6.lt) goto loc_826EECA8;
	// lhz r10,16(r17)
	ctx.r10.u64 = PPC_LOAD_U16(r17.u32 + 16);
	// rldicl r7,r11,56,8
	ctx.r7.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// lhz r9,18(r17)
	ctx.r9.u64 = PPC_LOAD_U16(r17.u32 + 18);
	// mr r31,r16
	r31.u64 = r16.u64;
	// rlwinm r8,r10,15,0,15
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0xFFFF0000;
	// lwz r6,32(r17)
	ctx.r6.u64 = PPC_LOAD_U32(r17.u32 + 32);
	// rlwinm r11,r9,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r10,4(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + 4);
	// or r11,r8,r11
	r11.u64 = ctx.r8.u64 | r11.u64;
	// lwz r9,1248(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1248);
	// rlwinm r8,r5,30,23,25
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x1C0;
	// lwz r26,144(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r27,148(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r11.u32);
	// lwz r11,32(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r11,32(r17)
	PPC_STORE_U32(r17.u32 + 32, r11.u32);
	// lbz r11,4(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// lbz r6,5(r30)
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + 5);
	// rldicr r5,r11,8,63
	ctx.r5.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// or r11,r6,r8
	r11.u64 = ctx.r6.u64 | ctx.r8.u64;
	// clrldi r11,r11,56
	r11.u64 = r11.u64 & 0xFF;
	// or r11,r5,r11
	r11.u64 = ctx.r5.u64 | r11.u64;
	// rldicr r11,r11,48,15
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFF000000000000;
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// stdx r11,r10,r9
	PPC_STORE_U64(ctx.r10.u32 + ctx.r9.u32, r11.u64);
	// b 0x826ef8e8
	goto loc_826EF8E8;
loc_826EF8AC:
	// mr r9,r17
	ctx.r9.u64 = r17.u64;
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x826ed970
	sub_826ED970(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826efabc
	if (!cr6.eq) goto loc_826EFABC;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x826ea9b0
	sub_826EA9B0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x826efac4
	if (!cr6.eq) goto loc_826EFAC4;
loc_826EF8E8:
	// lwz r11,14772(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826ef994
	if (!cr6.gt) goto loc_826EF994;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,136(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 136);
	// rlwinm r10,r10,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,1024
	cr6.compare<uint32_t>(ctx.r10.u32, 1024, xer);
	// mullw r10,r11,r23
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(r23.s32);
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r9,r24
	r11.u64 = ctx.r9.u64 + r24.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bne cr6,0x826ef954
	if (!cr6.eq) goto loc_826EF954;
	// lwz r9,216(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// stwx r16,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, r16.u32);
	// lwz r9,216(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// stw r16,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r16.u32);
	// lwz r11,216(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// stwx r16,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, r16.u32);
	// lwz r11,216(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r16,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r16.u32);
	// b 0x826ef994
	goto loc_826EF994;
loc_826EF954:
	// add r9,r11,r27
	ctx.r9.u64 = r11.u64 + r27.u64;
	// lwz r7,216(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// add r8,r10,r27
	ctx.r8.u64 = ctx.r10.u64 + r27.u64;
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stwx r6,r7,r11
	PPC_STORE_U32(ctx.r7.u32 + r11.u32, ctx.r6.u32);
	// lwz r7,216(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// lwz r11,216(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// lwz r11,216(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// lwz r9,4(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
loc_826EF994:
	// lhz r9,18(r17)
	ctx.r9.u64 = PPC_LOAD_U16(r17.u32 + 18);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// lwz r10,0(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// addi r30,r30,20
	r30.s64 = r30.s64 + 20;
	// lwz r11,4(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 4);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r24,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r24.u32);
	// cmplw cr6,r24,r26
	cr6.compare<uint32_t>(r24.u32, r26.u32, xer);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// sth r9,18(r17)
	PPC_STORE_U16(r17.u32 + 18, ctx.r9.u16);
	// stw r10,0(r17)
	PPC_STORE_U32(r17.u32 + 0, ctx.r10.u32);
	// stw r11,4(r17)
	PPC_STORE_U32(r17.u32 + 4, r11.u32);
	// blt cr6,0x826ee444
	if (cr6.lt) goto loc_826EE444;
	// lwz r28,152(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
loc_826EF9D4:
	// lhz r9,16(r17)
	ctx.r9.u64 = PPC_LOAD_U16(r17.u32 + 16);
	// rlwinm r11,r26,1,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,0(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r23,r28
	cr6.compare<uint32_t>(r23.u32, r28.u32, xer);
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// sth r9,16(r17)
	PPC_STORE_U16(r17.u32 + 16, ctx.r9.u16);
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// blt cr6,0x826ee0c4
	if (cr6.lt) goto loc_826EE0C4;
loc_826EFA00:
	// lwz r11,32(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 32);
	// lis r9,1
	ctx.r9.s64 = 65536;
	// lwz r10,21572(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 21572);
	// li r8,-1
	ctx.r8.s64 = -1;
	// ori r9,r9,33684
	ctx.r9.u64 = ctx.r9.u64 | 33684;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// stwx r11,r15,r9
	PPC_STORE_U32(r15.u32 + ctx.r9.u32, r11.u32);
	// lwz r11,32(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 32);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// ld r11,104(r25)
	r11.u64 = PPC_LOAD_U64(r25.u32 + 104);
	// lwz r10,84(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, r11.u64);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,112(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,116(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,120(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,124(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,128(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,132(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,140(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,144(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,148(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 84);
	// lwz r10,152(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
loc_826EFABC:
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// b 0x8239bd10
	return;
loc_826EFAC4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// b 0x8239bd10
	return;
loc_826EFAD0:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826EFADC"))) PPC_WEAK_FUNC(sub_826EFADC);
PPC_FUNC_IMPL(__imp__sub_826EFADC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826EFAE0"))) PPC_WEAK_FUNC(sub_826EFAE0);
PPC_FUNC_IMPL(__imp__sub_826EFAE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,14776(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14776);
	// lwz r10,3392(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3392);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826efb18
	if (cr6.eq) goto loc_826EFB18;
	// li r11,3
	r11.s64 = 3;
	// li r10,4
	ctx.r10.s64 = 4;
	// b 0x826efb20
	goto loc_826EFB20;
loc_826EFB18:
	// li r11,4
	r11.s64 = 4;
	// li r10,3
	ctx.r10.s64 = 3;
loc_826EFB20:
	// stw r11,14780(r31)
	PPC_STORE_U32(r31.u32 + 14780, r11.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r11,2928(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2928);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,14784(r31)
	PPC_STORE_U32(r31.u32 + 14784, ctx.r10.u32);
	// addi r8,r11,726
	ctx.r8.s64 = r11.s64 + 726;
	// lwz r10,2088(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2088);
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r10,263
	ctx.r10.s64 = ctx.r10.s64 + 263;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwzx r8,r8,r31
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r8,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, ctx.r8.u32);
	// lwzx r8,r7,r31
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + r31.u32);
	// stw r8,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, ctx.r8.u32);
	// lwz r11,2100(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 2100);
	// stw r11,2092(r31)
	PPC_STORE_U32(r31.u32 + 2092, r11.u32);
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r9,456(r31)
	PPC_STORE_U32(r31.u32 + 456, ctx.r9.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(r31.u32 + 2096, r11.u32);
	// bl 0x82628f88
	sub_82628F88(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261ca78
	sub_8261CA78(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826EFBA0"))) PPC_WEAK_FUNC(sub_826EFBA0);
PPC_FUNC_IMPL(__imp__sub_826EFBA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r15,0
	r15.s64 = 0;
	// mr r22,r4
	r22.u64 = ctx.r4.u64;
	// mr r14,r5
	r14.u64 = ctx.r5.u64;
	// mr r31,r15
	r31.u64 = r15.u64;
	// stw r29,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, r29.u32);
	// bl 0x826efae0
	sub_826EFAE0(ctx, base);
	// lwz r23,268(r29)
	r23.u64 = PPC_LOAD_U32(r29.u32 + 268);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r23,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r23.u32);
	// bl 0x82615470
	sub_82615470(ctx, base);
	// lwz r11,21556(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21556);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,188(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 188);
	// stw r11,20(r14)
	PPC_STORE_U32(r14.u32 + 20, r11.u32);
	// lwz r11,21568(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21568);
	// stw r11,24(r14)
	PPC_STORE_U32(r14.u32 + 24, r11.u32);
	// lwz r11,21560(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21560);
	// stw r11,28(r14)
	PPC_STORE_U32(r14.u32 + 28, r11.u32);
	// lwz r11,21572(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21572);
	// stw r11,32(r14)
	PPC_STORE_U32(r14.u32 + 32, r11.u32);
	// lhz r11,52(r22)
	r11.u64 = PPC_LOAD_U16(r22.u32 + 52);
	// lhz r10,50(r22)
	ctx.r10.u64 = PPC_LOAD_U16(r22.u32 + 50);
	// rlwinm r28,r11,31,1,31
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r15,4(r14)
	PPC_STORE_U32(r14.u32 + 4, r15.u32);
	// rlwinm r21,r10,31,1,31
	r21.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r15,0(r14)
	PPC_STORE_U32(r14.u32 + 0, r15.u32);
	// sth r15,16(r14)
	PPC_STORE_U16(r14.u32 + 16, r15.u16);
	// mullw r11,r28,r21
	r11.s64 = int64_t(r28.s32) * int64_t(r21.s32);
	// stw r28,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r28.u32);
	// stw r21,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r21.u32);
	// rlwinm r5,r11,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r25,r15
	r25.u64 = r15.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// beq cr6,0x826f23ec
	if (cr6.eq) goto loc_826F23EC;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lis r27,-32768
	r27.s64 = -2147483648;
	// addi r11,r11,14080
	r11.s64 = r11.s64 + 14080;
	// li r24,119
	r24.s64 = 119;
	// lis r18,2
	r18.s64 = 131072;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r17,r11,29840
	r17.s64 = r11.s64 + 29840;
	// lis r11,0
	r11.s64 = 0;
	// ori r16,r11,32768
	r16.u64 = r11.u64 | 32768;
loc_826EFC6C:
	// sth r15,18(r14)
	PPC_STORE_U16(r14.u32 + 18, r15.u16);
	// lwz r11,21236(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826efe6c
	if (cr6.eq) goto loc_826EFE6C;
	// lwz r11,1240(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1240);
	// rlwinm r10,r25,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826efe6c
	if (cr6.eq) goto loc_826EFE6C;
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// ld r10,104(r22)
	ctx.r10.u64 = PPC_LOAD_U64(r22.u32 + 104);
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,112(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,116(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,120(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,124(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,128(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,132(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,136(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,140(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,144(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,148(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,152(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r31,84(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826efdac
	if (cr6.eq) goto loc_826EFDAC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,1
	r30.s64 = 1;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826efd84
	if (!cr6.lt) goto loc_826EFD84;
loc_826EFD44:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826efd84
	if (cr6.eq) goto loc_826EFD84;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x826efd74
	if (!cr0.lt) goto loc_826EFD74;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EFD74:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826efd44
	if (cr6.gt) goto loc_826EFD44;
loc_826EFD84:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x826efdac
	if (!cr0.lt) goto loc_826EFDAC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826EFDAC:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82639b10
	sub_82639B10(ctx, base);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,104(r22)
	PPC_STORE_U64(r22.u32 + 104, r11.u64);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,112(r22)
	PPC_STORE_U32(r22.u32 + 112, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,116(r22)
	PPC_STORE_U32(r22.u32 + 116, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,120(r22)
	PPC_STORE_U32(r22.u32 + 120, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,124(r22)
	PPC_STORE_U32(r22.u32 + 124, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,128(r22)
	PPC_STORE_U32(r22.u32 + 128, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,132(r22)
	PPC_STORE_U32(r22.u32 + 132, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,136(r22)
	PPC_STORE_U32(r22.u32 + 136, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,140(r22)
	PPC_STORE_U32(r22.u32 + 140, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,144(r22)
	PPC_STORE_U32(r22.u32 + 144, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,148(r22)
	PPC_STORE_U32(r22.u32 + 148, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stb r10,1187(r22)
	PPC_STORE_U8(r22.u32 + 1187, ctx.r10.u8);
	// stw r11,152(r22)
	PPC_STORE_U32(r22.u32 + 152, r11.u32);
	// bne cr6,0x826f24b0
	if (!cr6.eq) goto loc_826F24B0;
loc_826EFE6C:
	// lwz r11,3932(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3932);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826effb0
	if (cr6.eq) goto loc_826EFFB0;
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// ld r10,104(r22)
	ctx.r10.u64 = PPC_LOAD_U64(r22.u32 + 104);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,112(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,116(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,120(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,124(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,128(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,132(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,136(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,140(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,144(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,148(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,152(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// bl 0x82639ef0
	sub_82639EF0(ctx, base);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,104(r22)
	PPC_STORE_U64(r22.u32 + 104, r11.u64);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,112(r22)
	PPC_STORE_U32(r22.u32 + 112, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,116(r22)
	PPC_STORE_U32(r22.u32 + 116, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,120(r22)
	PPC_STORE_U32(r22.u32 + 120, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,124(r22)
	PPC_STORE_U32(r22.u32 + 124, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,128(r22)
	PPC_STORE_U32(r22.u32 + 128, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,132(r22)
	PPC_STORE_U32(r22.u32 + 132, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,136(r22)
	PPC_STORE_U32(r22.u32 + 136, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,140(r22)
	PPC_STORE_U32(r22.u32 + 140, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,144(r22)
	PPC_STORE_U32(r22.u32 + 144, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,148(r22)
	PPC_STORE_U32(r22.u32 + 148, r11.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stw r11,152(r22)
	PPC_STORE_U32(r22.u32 + 152, r11.u32);
	// bne cr6,0x826f24b0
	if (!cr6.eq) goto loc_826F24B0;
loc_826EFFB0:
	// lwz r10,1240(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 1240);
	// rlwinm r9,r25,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r11,r25
	r11.s64 = -r25.s64;
	// mr r26,r15
	r26.u64 = r15.u64;
	// srawi r11,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 31;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// beq cr6,0x826f23c0
	if (cr6.eq) goto loc_826F23C0;
loc_826EFFE4:
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// li r10,128
	ctx.r10.s64 = 128;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// dcbt r10,r11
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// lbz r9,24(r22)
	ctx.r9.u64 = PPC_LOAD_U8(r22.u32 + 24);
	// rlwimi r11,r10,17,27,28
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 17) & 0x18) | (r11.u64 & 0xFFFFFFFFFFFFFFE7);
	// rlwimi r11,r10,17,14,15
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 17) & 0x30000) | (r11.u64 & 0xFFFFFFFFFFFCFFFF);
	// rlwimi r11,r10,17,3,3
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 17) & 0x10000000) | (r11.u64 & 0xFFFFFFFFEFFFFFFF);
	// stb r9,4(r23)
	PPC_STORE_U8(r23.u32 + 4, ctx.r9.u8);
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// lbz r11,26(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 26);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826f0058
	if (!cr6.eq) goto loc_826F0058;
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826f004c
	if (!cr0.lt) goto loc_826F004C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F004C:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwimi r11,r31,31,0,0
	r11.u64 = (__builtin_rotateleft32(r31.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
loc_826F0058:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm r11,r11,0,21,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFF7FF;
	// rlwinm r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplw cr6,r10,r27
	cr6.compare<uint32_t>(ctx.r10.u32, r27.u32, xer);
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// bne cr6,0x826f0084
	if (!cr6.eq) goto loc_826F0084;
	// rlwinm r11,r11,0,24,20
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFF8FF;
	// mr r27,r15
	r27.u64 = r15.u64;
	// rlwinm r11,r11,0,16,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFEFFFF;
	// mr r28,r15
	r28.u64 = r15.u64;
	// b 0x826f022c
	goto loc_826F022C;
loc_826F0084:
	// lwz r11,1316(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1316);
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f0178
	if (cr6.lt) goto loc_826F0178;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f0170
	if (!cr6.lt) goto loc_826F0170;
loc_826F00D8:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f0104
	if (cr6.lt) goto loc_826F0104;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f00d8
	if (cr6.eq) goto loc_826F00D8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f01b4
	goto loc_826F01B4;
loc_826F0104:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F0170:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f01b4
	goto loc_826F01B4;
loc_826F0178:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F0180:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f0180
	if (cr6.lt) goto loc_826F0180;
loc_826F01B4:
	// mr r11,r30
	r11.u64 = r30.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f19f4
	if (cr6.lt) goto loc_826F19F4;
	// cmpwi cr6,r30,14
	cr6.compare<int32_t>(r30.s32, 14, xer);
	// bgt cr6,0x826f19f4
	if (cr6.gt) goto loc_826F19F4;
	// lwz r10,1320(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 1320);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x826f01e0
	if (!cr6.eq) goto loc_826F01E0;
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// bne cr6,0x826f01e0
	if (!cr6.eq) goto loc_826F01E0;
	// li r11,14
	r11.s64 = 14;
loc_826F01E0:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r9,0(r23)
	ctx.r9.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// rlwinm r9,r9,0,24,20
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFF8FF;
	// rlwinm r9,r9,0,16,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFEFFFF;
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// rlwinm r10,r11,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	// clrlwi r8,r11,29
	ctx.r8.u64 = r11.u32 & 0x7;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// srawi r7,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r7.s64 = r11.s32 >> 4;
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// clrlwi r28,r11,31
	r28.u64 = r11.u32 & 0x1;
	// or r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 | ctx.r8.u64;
	// clrlwi r27,r7,31
	r27.u64 = ctx.r7.u32 & 0x1;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// or r11,r10,r9
	r11.u64 = ctx.r10.u64 | ctx.r9.u64;
loc_826F022C:
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// rlwinm r11,r11,0,21,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r11,1024
	cr6.compare<uint32_t>(r11.u32, 1024, xer);
	// bne cr6,0x826f1440
	if (!cr6.eq) goto loc_826F1440;
	// lbz r11,33(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 33);
	// li r30,1
	r30.s64 = 1;
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// mr r29,r15
	r29.u64 = r15.u64;
	// rlwimi r10,r11,11,20,20
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 11) & 0x800) | (ctx.r10.u64 & 0xFFFFFFFFFFFFF7FF);
	// rlwinm r11,r10,0,15,13
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f02c8
	if (!cr6.lt) goto loc_826F02C8;
loc_826F0270:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f02c8
	if (cr6.eq) goto loc_826F02C8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f02b8
	if (!cr0.lt) goto loc_826F02B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F02B8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f0270
	if (cr6.gt) goto loc_826F0270;
loc_826F02C8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f0304
	if (!cr0.lt) goto loc_826F0304;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F0304:
	// mr r11,r30
	r11.u64 = r30.u64;
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r29,r15
	r29.u64 = r15.u64;
	// rlwimi r10,r11,16,15,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0x10000) | (ctx.r10.u64 & 0xFFFFFFFFFFFEFFFF);
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f038c
	if (!cr6.lt) goto loc_826F038C;
loc_826F0334:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f038c
	if (cr6.eq) goto loc_826F038C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f037c
	if (!cr0.lt) goto loc_826F037C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F037C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f0334
	if (cr6.gt) goto loc_826F0334;
loc_826F038C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f03c8
	if (!cr0.lt) goto loc_826F03C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F03C8:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x826f059c
	if (cr6.eq) goto loc_826F059C;
	// lwz r11,1172(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1172);
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f04c4
	if (cr6.lt) goto loc_826F04C4;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f04bc
	if (!cr6.lt) goto loc_826F04BC;
loc_826F0424:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f0450
	if (cr6.lt) goto loc_826F0450;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f0424
	if (cr6.eq) goto loc_826F0424;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f0500
	goto loc_826F0500;
loc_826F0450:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F04BC:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f0500
	goto loc_826F0500;
loc_826F04C4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F04CC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f04cc
	if (cr6.lt) goto loc_826F04CC;
loc_826F0500:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// cmplwi cr6,r11,63
	cr6.compare<uint32_t>(r11.u32, 63, xer);
	// bgt cr6,0x826f0818
	if (cr6.gt) goto loc_826F0818;
loc_826F050C:
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826f0818
	if (!cr6.eq) goto loc_826F0818;
	// lwz r10,1200(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 1200);
	// lbzx r31,r10,r11
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826f054c
	if (!cr0.lt) goto loc_826F054C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F054C:
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// stb r31,5(r23)
	PPC_STORE_U8(r23.u32 + 5, r31.u8);
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
	// lbz r11,27(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 27);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f0818
	if (cr6.eq) goto loc_826F0818;
	// lbz r11,1181(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1181);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f05b8
	if (cr6.eq) goto loc_826F05B8;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// rlwinm r10,r10,20,12,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFFFFF;
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f05a4
	if (cr6.eq) goto loc_826F05A4;
	// lbz r11,1182(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// b 0x826f0810
	goto loc_826F0810;
loc_826F059C:
	// mr r11,r15
	r11.u64 = r15.u64;
	// b 0x826f050c
	goto loc_826F050C;
loc_826F05A4:
	// lbz r11,1180(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1180);
	// lbz r10,1185(r22)
	ctx.r10.u64 = PPC_LOAD_U8(r22.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826f0810
	goto loc_826F0810;
loc_826F05B8:
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// mr r29,r15
	r29.u64 = r15.u64;
	// lbz r11,1186(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826f06a4
	if (cr6.eq) goto loc_826F06A4;
	// li r30,1
	r30.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f0638
	if (!cr6.lt) goto loc_826F0638;
loc_826F05E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f0638
	if (cr6.eq) goto loc_826F0638;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f0628
	if (!cr0.lt) goto loc_826F0628;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F0628:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f05e0
	if (cr6.gt) goto loc_826F05E0;
loc_826F0638:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f0674
	if (!cr0.lt) goto loc_826F0674;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F0674:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826f068c
	if (cr6.eq) goto loc_826F068C;
	// lbz r11,1182(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f0814
	goto loc_826F0814;
loc_826F068C:
	// lbz r11,1180(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1180);
	// lbz r10,1185(r22)
	ctx.r10.u64 = PPC_LOAD_U8(r22.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f0814
	goto loc_826F0814;
loc_826F06A4:
	// li r30,3
	r30.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826f0708
	if (!cr6.lt) goto loc_826F0708;
loc_826F06B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f0708
	if (cr6.eq) goto loc_826F0708;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f06f8
	if (!cr0.lt) goto loc_826F06F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F06F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f06b0
	if (cr6.gt) goto loc_826F06B0;
loc_826F0708:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f0744
	if (!cr0.lt) goto loc_826F0744;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F0744:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x826f0804
	if (!cr6.eq) goto loc_826F0804;
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// li r30,5
	r30.s64 = 5;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826f07c0
	if (!cr6.lt) goto loc_826F07C0;
loc_826F0768:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f07c0
	if (cr6.eq) goto loc_826F07C0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f07b0
	if (!cr0.lt) goto loc_826F07B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F07B0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f0768
	if (cr6.gt) goto loc_826F0768;
loc_826F07C0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f07fc
	if (!cr0.lt) goto loc_826F07FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F07FC:
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x826f080c
	goto loc_826F080C;
loc_826F0804:
	// lbz r11,1180(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1180);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
loc_826F080C:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826F0810:
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
loc_826F0814:
	// stb r11,4(r23)
	PPC_STORE_U8(r23.u32 + 4, r11.u8);
loc_826F0818:
	// lbz r11,4(r23)
	r11.u64 = PPC_LOAD_U8(r23.u32 + 4);
	// mr r27,r15
	r27.u64 = r15.u64;
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// rotlwi r9,r11,2
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 2);
	// lbz r21,5(r23)
	r21.u64 = PPC_LOAD_U8(r23.u32 + 5);
	// neg r20,r10
	r20.s64 = -ctx.r10.s64;
	// lwz r10,220(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 220);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r15,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r15.u64);
	// stw r27,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r27.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r19,r11,r10
	r19.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826f0858
	goto loc_826F0858;
loc_826F084C:
	// lwz r26,84(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r24,119
	r24.s64 = 119;
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_826F0858:
	// clrlwi r9,r21,31
	ctx.r9.u64 = r21.u32 & 0x1;
	// lhz r11,50(r22)
	r11.u64 = PPC_LOAD_U16(r22.u32 + 50);
	// rlwinm r10,r27,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFC;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r31,r15
	r31.u64 = r15.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r9,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r9.u32);
	// rlwinm r6,r11,31,1,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r9,r15
	ctx.r9.u64 = r15.u64;
	// bne cr6,0x826f0934
	if (!cr6.eq) goto loc_826F0934;
	// addi r10,r22,248
	ctx.r10.s64 = r22.s64 + 248;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// beq cr6,0x826f089c
	if (cr6.eq) goto loc_826F089C;
	// lwz r10,-20(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + -20);
	// not r10,r10
	ctx.r10.u64 = ~ctx.r10.u64;
	// rlwinm r4,r10,15,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0x1;
loc_826F089C:
	// clrlwi r5,r27,31
	ctx.r5.u64 = r27.u32 & 0x1;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// add r4,r4,r5
	ctx.r4.u64 = ctx.r4.u64 + ctx.r5.u64;
	// beq cr6,0x826f08c8
	if (cr6.eq) goto loc_826F08C8;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r23
	ctx.r10.s64 = r23.s64 - ctx.r10.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// not r10,r10
	ctx.r10.u64 = ~ctx.r10.u64;
	// rlwinm r9,r10,15,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0x1;
loc_826F08C8:
	// srawi r10,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r10.s64 = r27.s32 >> 1;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// add r3,r9,r10
	ctx.r3.u64 = ctx.r9.u64 + ctx.r10.u64;
	// beq cr6,0x826f0908
	if (cr6.eq) goto loc_826F0908;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826f0908
	if (cr6.eq) goto loc_826F0908;
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// mullw r9,r9,r6
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// lwz r9,-20(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + -20);
	// not r9,r9
	ctx.r9.u64 = ~ctx.r9.u64;
	// rlwinm r31,r9,15,31,31
	r31.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 15) & 0x1;
loc_826F0908:
	// rlwinm r7,r25,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r30,1160(r22)
	r30.u64 = PPC_LOAD_U32(r22.u32 + 1160);
	// rlwinm r8,r26,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,264(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 264);
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r24,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r24.u32);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// b 0x826f09d0
	goto loc_826F09D0;
loc_826F0934:
	// lwz r11,1164(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1164);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// stw r24,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r24.u32);
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// addi r11,r22,236
	r11.s64 = r22.s64 + 236;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// beq cr6,0x826f0960
	if (cr6.eq) goto loc_826F0960;
	// lwz r11,-20(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + -20);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r4,r11,15,31,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
loc_826F0960:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x826f0988
	if (cr6.eq) goto loc_826F0988;
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r23
	r11.s64 = r23.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,15,17,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x7FFF;
	// andc r11,r20,r11
	r11.u64 = r20.u64 & ~r11.u64;
	// clrlwi r3,r11,31
	ctx.r3.u64 = r11.u32 & 0x1;
loc_826F0988:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x826f09b8
	if (cr6.eq) goto loc_826F09B8;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826f09b8
	if (cr6.eq) goto loc_826F09B8;
	// addi r11,r6,1
	r11.s64 = ctx.r6.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r23
	r11.s64 = r23.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r31,r11,15,31,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 15) & 0x1;
loc_826F09B8:
	// addi r10,r27,63
	ctx.r10.s64 = r27.s64 + 63;
	// mullw r11,r6,r25
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(r25.s32);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r26
	ctx.r10.u64 = r11.u64 + r26.u64;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// lwzx r9,r9,r22
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r22.u32);
loc_826F09D0:
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r8,r11,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r11,16(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 16);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,0(r23)
	ctx.r9.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// mr r30,r15
	r30.u64 = r15.u64;
	// rlwinm r9,r9,29,30,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x3;
	// addi r28,r10,-16
	r28.s64 = ctx.r10.s64 + -16;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// subf r29,r8,r10
	r29.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// mr r11,r15
	r11.u64 = r15.u64;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// stw r9,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r9.u32);
	// beq cr6,0x826f0c18
	if (cr6.eq) goto loc_826F0C18;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826f0c0c
	if (cr6.eq) goto loc_826F0C0C;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x826f0a28
	if (cr6.eq) goto loc_826F0A28;
	// lhz r11,-32(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + -32);
	// extsh r7,r11
	ctx.r7.s64 = r11.s16;
	// b 0x826f0a2c
	goto loc_826F0A2C;
loc_826F0A28:
	// mr r7,r15
	ctx.r7.u64 = r15.u64;
loc_826F0A2C:
	// lhz r11,0(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 0);
	// lhz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 0);
	// lbz r9,27(r22)
	ctx.r9.u64 = PPC_LOAD_U8(r22.u32 + 27);
	// extsh r5,r11
	ctx.r5.s64 = r11.s16;
	// extsh r4,r10
	ctx.r4.s64 = ctx.r10.s16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826f0be4
	if (cr6.eq) goto loc_826F0BE4;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x826f0b38
	if (cr6.eq) goto loc_826F0B38;
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// beq cr6,0x826f0b38
	if (cr6.eq) goto loc_826F0B38;
	// cmpwi cr6,r27,5
	cr6.compare<int32_t>(r27.s32, 5, xer);
	// beq cr6,0x826f0b38
	if (cr6.eq) goto loc_826F0B38;
	// cmpwi cr6,r27,1
	cr6.compare<int32_t>(r27.s32, 1, xer);
	// bne cr6,0x826f0ad4
	if (!cr6.eq) goto loc_826F0AD4;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r11,4(r23)
	r11.u64 = PPC_LOAD_U8(r23.u32 + 4);
	// lwz r10,220(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 220);
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rotlwi r8,r11,2
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 2);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// subf r9,r9,r23
	ctx.r9.s64 = r23.s64 - ctx.r9.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lbz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// rotlwi r8,r9,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwzx r11,r11,r17
	r11.u64 = PPC_LOAD_U32(r11.u32 + r17.u32);
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// mullw r10,r7,r11
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// mullw r11,r5,r11
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// add r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 + r18.u64;
	// add r11,r11,r18
	r11.u64 = r11.u64 + r18.u64;
	// srawi r7,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 18;
	// srawi r5,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	ctx.r5.s64 = r11.s32 >> 18;
	// b 0x826f0be4
	goto loc_826F0BE4;
loc_826F0AD4:
	// cmpwi cr6,r27,2
	cr6.compare<int32_t>(r27.s32, 2, xer);
	// bne cr6,0x826f0be4
	if (!cr6.eq) goto loc_826F0BE4;
	// lbz r11,4(r23)
	r11.u64 = PPC_LOAD_U8(r23.u32 + 4);
	// lbz r9,-16(r23)
	ctx.r9.u64 = PPC_LOAD_U8(r23.u32 + -16);
	// rotlwi r8,r11,2
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 2);
	// lwz r10,220(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 220);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rotlwi r8,r9,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r8,r11,r10
	ctx.r8.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,16(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// lwzx r11,r11,r17
	r11.u64 = PPC_LOAD_U32(r11.u32 + r17.u32);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// mullw r10,r7,r11
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// mullw r11,r4,r11
	r11.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// add r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 + r18.u64;
	// add r11,r11,r18
	r11.u64 = r11.u64 + r18.u64;
	// srawi r7,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 18;
	// srawi r4,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = r11.s32 >> 18;
	// b 0x826f0be4
	goto loc_826F0BE4;
loc_826F0B38:
	// rlwinm r8,r6,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r10,4(r23)
	ctx.r10.u64 = PPC_LOAD_U8(r23.u32 + 4);
	// lbz r9,-16(r23)
	ctx.r9.u64 = PPC_LOAD_U8(r23.u32 + -16);
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lwz r11,220(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 220);
	// rotlwi r3,r10,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// subf r8,r8,r23
	ctx.r8.s64 = r23.s64 - ctx.r8.s64;
	// rotlwi r3,r9,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r9,r3
	ctx.r3.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// lbz r10,-16(r8)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + -16);
	// lbz r9,4(r8)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r8.u32 + 4);
	// rlwinm r8,r3,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r8,r11
	ctx.r3.u64 = ctx.r8.u64 + r11.u64;
	// lwz r8,16(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,16(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// mullw r31,r3,r4
	r31.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r4.s32);
	// rotlwi r3,r10,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// rotlwi r4,r9,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + r11.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwzx r10,r8,r17
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + r17.u32);
	// lwz r9,16(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mullw r9,r9,r7
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// mullw r10,r31,r10
	ctx.r10.s64 = int64_t(r31.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + r18.u64;
	// add r11,r11,r18
	r11.u64 = r11.u64 + r18.u64;
	// add r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 + r18.u64;
	// srawi r7,r9,18
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 18;
	// srawi r5,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	ctx.r5.s64 = r11.s32 >> 18;
	// srawi r4,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 18;
loc_826F0BE4:
	// subf r11,r4,r7
	r11.s64 = ctx.r7.s64 - ctx.r4.s64;
	// subf r10,r5,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r5.s64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x826f0c20
	if (cr6.lt) goto loc_826F0C20;
loc_826F0C0C:
	// mr r11,r28
	r11.u64 = r28.u64;
	// li r30,8
	r30.s64 = 8;
	// b 0x826f0c28
	goto loc_826F0C28;
loc_826F0C18:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826f0e24
	if (cr6.eq) goto loc_826F0E24;
loc_826F0C20:
	// li r30,1
	r30.s64 = 1;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_826F0C28:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f0e24
	if (cr6.eq) goto loc_826F0E24;
	// lbz r10,27(r22)
	ctx.r10.u64 = PPC_LOAD_U8(r22.u32 + 27);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826f0e24
	if (cr6.eq) goto loc_826F0E24;
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// bne cr6,0x826f0d28
	if (!cr6.eq) goto loc_826F0D28;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x826f0c88
	if (cr6.eq) goto loc_826F0C88;
	// cmpwi cr6,r27,2
	cr6.compare<int32_t>(r27.s32, 2, xer);
	// beq cr6,0x826f0c88
	if (cr6.eq) goto loc_826F0C88;
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// beq cr6,0x826f0c88
	if (cr6.eq) goto loc_826F0C88;
	// cmpwi cr6,r27,5
	cr6.compare<int32_t>(r27.s32, 5, xer);
	// beq cr6,0x826f0c88
	if (cr6.eq) goto loc_826F0C88;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// li r9,16
	ctx.r9.s64 = 16;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_826F0C70:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x826f0c70
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826F0C70;
	// b 0x826f0e20
	goto loc_826F0E20;
loc_826F0C88:
	// lbz r10,4(r23)
	ctx.r10.u64 = PPC_LOAD_U8(r23.u32 + 4);
	// addi r7,r1,162
	ctx.r7.s64 = ctx.r1.s64 + 162;
	// lbz r9,-16(r23)
	ctx.r9.u64 = PPC_LOAD_U8(r23.u32 + -16);
	// rotlwi r5,r10,2
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// lwz r6,220(r22)
	ctx.r6.u64 = PPC_LOAD_U32(r22.u32 + 220);
	// rotlwi r4,r9,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r5,r10,r5
	ctx.r5.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r4,r9,r4
	ctx.r4.u64 = ctx.r9.u64 + ctx.r4.u64;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// extsh r3,r8
	ctx.r3.s64 = ctx.r8.s16;
	// add r31,r5,r6
	r31.u64 = ctx.r5.u64 + ctx.r6.u64;
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// add r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 + ctx.r6.u64;
	// addi r8,r11,2
	ctx.r8.s64 = r11.s64 + 2;
	// lwz r5,16(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// li r11,15
	r11.s64 = 15;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r17
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r17.u32);
	// lwz r6,16(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// lwzx r5,r5,r17
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + r17.u32);
	// mullw r6,r5,r6
	ctx.r6.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r6.s32);
	// mullw r6,r6,r3
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r3.s32);
	// add r6,r6,r18
	ctx.r6.u64 = ctx.r6.u64 + r18.u64;
	// srawi r6,r6,18
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 18;
	// sth r6,160(r1)
	PPC_STORE_U16(ctx.r1.u32 + 160, ctx.r6.u16);
loc_826F0CF4:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mullw r6,r6,r10
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r10.s32);
	// mullw r6,r6,r9
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r9.s32);
	// add r6,r6,r18
	ctx.r6.u64 = ctx.r6.u64 + r18.u64;
	// srawi r6,r6,18
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 18;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bne cr6,0x826f0cf4
	if (!cr6.eq) goto loc_826F0CF4;
	// b 0x826f0e18
	goto loc_826F0E18;
loc_826F0D28:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x826f0d6c
	if (cr6.eq) goto loc_826F0D6C;
	// cmpwi cr6,r27,1
	cr6.compare<int32_t>(r27.s32, 1, xer);
	// beq cr6,0x826f0d6c
	if (cr6.eq) goto loc_826F0D6C;
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// beq cr6,0x826f0d6c
	if (cr6.eq) goto loc_826F0D6C;
	// cmpwi cr6,r27,5
	cr6.compare<int32_t>(r27.s32, 5, xer);
	// beq cr6,0x826f0d6c
	if (cr6.eq) goto loc_826F0D6C;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// li r9,16
	ctx.r9.s64 = 16;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_826F0D54:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x826f0d54
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826F0D54;
	// b 0x826f0e20
	goto loc_826F0E20;
loc_826F0D6C:
	// lbz r10,4(r23)
	ctx.r10.u64 = PPC_LOAD_U8(r23.u32 + 4);
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,220(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + 220);
	// rotlwi r5,r10,2
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rotlwi r3,r10,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r23
	ctx.r9.s64 = r23.s64 - ctx.r9.s64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwzx r5,r3,r17
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + r17.u32);
	// extsh r4,r7
	ctx.r4.s64 = ctx.r7.s16;
	// addi r7,r11,2
	ctx.r7.s64 = r11.s64 + 2;
	// addi r6,r1,162
	ctx.r6.s64 = ctx.r1.s64 + 162;
	// lbz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// li r11,15
	r11.s64 = 15;
	// lwz r3,16(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// rotlwi r10,r9,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mullw r9,r5,r9
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r9.s32);
	// lwzx r5,r3,r17
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + r17.u32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// mullw r10,r5,r10
	ctx.r10.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// mullw r10,r10,r4
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 + r18.u64;
	// srawi r10,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 18;
	// sth r10,160(r1)
	PPC_STORE_U16(ctx.r1.u32 + 160, ctx.r10.u16);
loc_826F0DEC:
	// lhz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 + r18.u64;
	// srawi r10,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 18;
	// sth r10,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, ctx.r10.u16);
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// bne cr6,0x826f0dec
	if (!cr6.eq) goto loc_826F0DEC;
loc_826F0E18:
	// lhz r11,160(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 160);
	// sth r11,176(r1)
	PPC_STORE_U16(ctx.r1.u32 + 176, r11.u16);
loc_826F0E20:
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
loc_826F0E24:
	// lwz r10,28(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 28);
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// addi r11,r10,-128
	r11.s64 = ctx.r10.s64 + -128;
	// stw r30,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r30.u32);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// stw r11,28(r14)
	PPC_STORE_U32(r14.u32 + 28, r11.u32);
	// dcbzl r0,r11
	memset(base + ((r11.u32) & ~127), 0, 128);
	// lwz r26,124(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// dcbt r0,r26
	// lwz r25,128(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// dcbt r0,r25
	// lwz r24,88(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// dcbt r0,r24
	// mr r28,r15
	r28.u64 = r15.u64;
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lbz r4,8(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// lwz r29,0(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r29.u32);
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f0f50
	if (cr6.lt) goto loc_826F0F50;
	// clrlwi r10,r30,28
	ctx.r10.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x826f0f48
	if (!cr6.lt) goto loc_826F0F48;
loc_826F0EB0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f0edc
	if (cr6.lt) goto loc_826F0EDC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f0eb0
	if (cr6.eq) goto loc_826F0EB0;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f0f8c
	goto loc_826F0F8C;
loc_826F0EDC:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F0F48:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f0f8c
	goto loc_826F0F8C;
loc_826F0F50:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F0F58:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f0f58
	if (cr6.lt) goto loc_826F0F58;
loc_826F0F8C:
	// clrlwi r30,r30,16
	r30.u64 = r30.u32 & 0xFFFF;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// mr r27,r30
	r27.u64 = r30.u64;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// beq cr6,0x826f10dc
	if (cr6.eq) goto loc_826F10DC;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x826f11fc
	if (cr6.eq) goto loc_826F11FC;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x826f0ff4
	if (!cr6.eq) goto loc_826F0FF4;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x826f0fe0
	if (!cr0.lt) goto loc_826F0FE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F0FE0:
	// rlwinm r11,r27,1,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f10b8
	goto loc_826F10B8;
loc_826F0FF4:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x826f10bc
	if (!cr6.eq) goto loc_826F10BC;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r15
	r29.u64 = r15.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x826f106c
	if (!cr6.lt) goto loc_826F106C;
loc_826F1014:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f106c
	if (cr6.eq) goto loc_826F106C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f105c
	if (!cr0.lt) goto loc_826F105C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F105C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f1014
	if (cr6.gt) goto loc_826F1014;
loc_826F106C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f10a8
	if (!cr0.lt) goto loc_826F10A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F10A8:
	// rlwinm r11,r27,2,0,29
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-3
	r11.s64 = r11.s64 + -3;
loc_826F10B8:
	// clrlwi r30,r11,16
	r30.u64 = r11.u32 & 0xFFFF;
loc_826F10BC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x826f11d8
	goto loc_826F11D8;
loc_826F10DC:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bgt cr6,0x826f10f4
	if (cr6.gt) goto loc_826F10F4;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// subfic r11,r11,3
	xer.ca = r11.u32 <= 3;
	r11.s64 = 3 - r11.s64;
	// b 0x826f10f8
	goto loc_826F10F8;
loc_826F10F4:
	// mr r11,r15
	r11.u64 = r15.u64;
loc_826F10F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r30,r11,8
	r30.s64 = r11.s64 + 8;
	// mr r29,r15
	r29.u64 = r15.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x826f1118
	if (!cr6.eq) goto loc_826F1118;
	// mr r11,r15
	r11.u64 = r15.u64;
	// b 0x826f11b8
	goto loc_826F11B8;
loc_826F1118:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826f1178
	if (!cr6.gt) goto loc_826F1178;
loc_826F1120:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f1178
	if (cr6.eq) goto loc_826F1178;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f1168
	if (!cr0.lt) goto loc_826F1168;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F1168:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f1120
	if (cr6.gt) goto loc_826F1120;
loc_826F1178:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f11b4
	if (!cr0.lt) goto loc_826F11B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F11B4:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_826F11B8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrlwi r30,r11,16
	r30.u64 = r11.u32 & 0xFFFF;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
loc_826F11D8:
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// bge 0x826f11e8
	if (!cr0.lt) goto loc_826F11E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F11E8:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r10,r30
	ctx.r10.s64 = r30.s16;
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// extsh r28,r11
	r28.s64 = r11.s16;
loc_826F11FC:
	// sth r28,0(r26)
	PPC_STORE_U16(r26.u32 + 0, r28.u16);
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f19f4
	if (!cr6.eq) goto loc_826F19F4;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f1228
	if (!cr6.eq) goto loc_826F1228;
	// lwz r6,1204(r22)
	ctx.r6.u64 = PPC_LOAD_U32(r22.u32 + 1204);
	// mr r31,r15
	r31.u64 = r15.u64;
	// b 0x826f125c
	goto loc_826F125C;
loc_826F1228:
	// lwz r31,136(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lbz r10,1188(r22)
	ctx.r10.u64 = PPC_LOAD_U8(r22.u32 + 1188);
	// addi r11,r31,301
	r11.s64 = r31.s64 + 301;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r11,r22
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// beq cr6,0x826f125c
	if (cr6.eq) goto loc_826F125C;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x826f125c
	if (cr6.eq) goto loc_826F125C;
	// cmpwi cr6,r31,8
	cr6.compare<int32_t>(r31.s32, 8, xer);
	// li r31,8
	r31.s64 = 8;
	// bne cr6,0x826f125c
	if (!cr6.eq) goto loc_826F125C;
	// li r31,1
	r31.s64 = 1;
loc_826F125C:
	// lwz r30,140(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x826f1284
	if (cr6.eq) goto loc_826F1284;
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x826fcd00
	sub_826FCD00(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x826f19f4
	if (cr6.lt) goto loc_826F19F4;
loc_826F1284:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x826f12f4
	if (cr6.eq) goto loc_826F12F4;
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// beq cr6,0x826f12e4
	if (cr6.eq) goto loc_826F12E4;
	// cmpwi cr6,r31,8
	cr6.compare<int32_t>(r31.s32, 8, xer);
	// beq cr6,0x826f12b0
	if (cr6.eq) goto loc_826F12B0;
	// lhz r11,0(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 0);
	// lhz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sth r11,0(r26)
	PPC_STORE_U16(r26.u32 + 0, r11.u16);
	// b 0x826f12f4
	goto loc_826F12F4;
loc_826F12B0:
	// mr r11,r26
	r11.u64 = r26.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// li r10,8
	ctx.r10.s64 = 8;
loc_826F12BC:
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bne cr6,0x826f12bc
	if (!cr6.eq) goto loc_826F12BC;
	// b 0x826f12f4
	goto loc_826F12F4;
loc_826F12E4:
	// lvx128 v0,r0,r25
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r26
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v0,r0,r26
	_mm_store_si128((__m128i*)(base + ((r26.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_826F12F4:
	// lbz r11,1188(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1188);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f1350
	if (cr6.eq) goto loc_826F1350;
	// lhz r11,0(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// addi r10,r24,16
	ctx.r10.s64 = r24.s64 + 16;
	// sth r11,0(r24)
	PPC_STORE_U16(r24.u32 + 0, r11.u16);
	// lhz r11,16(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 16);
	// sth r11,2(r24)
	PPC_STORE_U16(r24.u32 + 2, r11.u16);
	// lhz r11,32(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 32);
	// sth r11,4(r24)
	PPC_STORE_U16(r24.u32 + 4, r11.u16);
	// lhz r11,48(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 48);
	// sth r11,6(r24)
	PPC_STORE_U16(r24.u32 + 6, r11.u16);
	// lhz r11,64(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 64);
	// sth r11,8(r24)
	PPC_STORE_U16(r24.u32 + 8, r11.u16);
	// lhz r11,80(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 80);
	// sth r11,10(r24)
	PPC_STORE_U16(r24.u32 + 10, r11.u16);
	// lhz r11,96(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 96);
	// sth r11,12(r24)
	PPC_STORE_U16(r24.u32 + 12, r11.u16);
	// lhz r11,112(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 112);
	// sth r11,14(r24)
	PPC_STORE_U16(r24.u32 + 14, r11.u16);
	// lvx128 v0,r0,r26
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x826f1398
	goto loc_826F1398;
loc_826F1350:
	// lvx128 v0,r0,r26
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r24
	_mm_store_si128((__m128i*)(base + ((r24.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lhz r11,0(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// sth r11,16(r24)
	PPC_STORE_U16(r24.u32 + 16, r11.u16);
	// lhz r11,16(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 16);
	// sth r11,18(r24)
	PPC_STORE_U16(r24.u32 + 18, r11.u16);
	// lhz r11,32(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 32);
	// sth r11,20(r24)
	PPC_STORE_U16(r24.u32 + 20, r11.u16);
	// lhz r11,48(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 48);
	// sth r11,22(r24)
	PPC_STORE_U16(r24.u32 + 22, r11.u16);
	// lhz r11,64(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 64);
	// sth r11,24(r24)
	PPC_STORE_U16(r24.u32 + 24, r11.u16);
	// lhz r11,80(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 80);
	// sth r11,26(r24)
	PPC_STORE_U16(r24.u32 + 26, r11.u16);
	// lhz r11,96(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 96);
	// sth r11,28(r24)
	PPC_STORE_U16(r24.u32 + 28, r11.u16);
	// lhz r11,112(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 112);
	// sth r11,30(r24)
	PPC_STORE_U16(r24.u32 + 30, r11.u16);
loc_826F1398:
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// extsw r9,r30
	ctx.r9.s64 = r30.s32;
	// lwz r23,116(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// srawi r21,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	r21.s64 = r21.s32 >> 1;
	// addi r27,r11,1
	r27.s64 = r11.s64 + 1;
	// add r10,r11,r23
	ctx.r10.u64 = r11.u64 + r23.u64;
	// ld r11,152(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// ori r9,r9,128
	ctx.r9.u64 = ctx.r9.u64 | 128;
	// cmpwi cr6,r27,6
	cr6.compare<int32_t>(r27.s32, 6, xer);
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// stw r27,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r27.u32);
	// stb r15,6(r10)
	PPC_STORE_U8(ctx.r10.u32 + 6, r15.u8);
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// blt cr6,0x826f084c
	if (cr6.lt) goto loc_826F084C;
	// lhz r10,16(r14)
	ctx.r10.u64 = PPC_LOAD_U16(r14.u32 + 16);
	// rldicl r7,r11,56,8
	ctx.r7.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// lhz r9,18(r14)
	ctx.r9.u64 = PPC_LOAD_U16(r14.u32 + 18);
	// mr r31,r15
	r31.u64 = r15.u64;
	// rlwinm r8,r10,15,0,15
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0xFFFF0000;
	// lwz r6,32(r14)
	ctx.r6.u64 = PPC_LOAD_U32(r14.u32 + 32);
	// rlwinm r11,r9,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r5,0(r23)
	ctx.r5.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// lwz r10,4(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 4);
	// or r11,r8,r11
	r11.u64 = ctx.r8.u64 | r11.u64;
	// lwz r9,1248(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 1248);
	// rlwinm r8,r5,30,23,25
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x1C0;
	// lwz r21,144(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r11.u32);
	// lwz r11,32(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r11,32(r14)
	PPC_STORE_U32(r14.u32 + 32, r11.u32);
	// lbz r11,4(r23)
	r11.u64 = PPC_LOAD_U8(r23.u32 + 4);
	// lbz r6,5(r23)
	ctx.r6.u64 = PPC_LOAD_U8(r23.u32 + 5);
	// rldicr r5,r11,8,63
	ctx.r5.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// or r11,r6,r8
	r11.u64 = ctx.r6.u64 | ctx.r8.u64;
	// clrldi r11,r11,56
	r11.u64 = r11.u64 & 0xFF;
	// or r11,r5,r11
	r11.u64 = ctx.r5.u64 | r11.u64;
	// rldicr r11,r11,48,15
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFF000000000000;
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// stdx r11,r10,r9
	PPC_STORE_U64(ctx.r10.u32 + ctx.r9.u32, r11.u64);
	// b 0x826f236c
	goto loc_826F236C;
loc_826F1440:
	// lwz r11,1460(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1460);
	// li r20,1
	r20.s64 = 1;
	// lwz r25,188(r22)
	r25.u64 = PPC_LOAD_U32(r22.u32 + 188);
	// lwz r24,0(r14)
	r24.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lhz r26,50(r22)
	r26.u64 = PPC_LOAD_U16(r22.u32 + 50);
	// bne cr6,0x826f1518
	if (!cr6.eq) goto loc_826F1518;
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// mr r30,r20
	r30.u64 = r20.u64;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f14d0
	if (!cr6.lt) goto loc_826F14D0;
loc_826F1478:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f14d0
	if (cr6.eq) goto loc_826F14D0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f14c0
	if (!cr0.lt) goto loc_826F14C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F14C0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f1478
	if (cr6.gt) goto loc_826F1478;
loc_826F14D0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f150c
	if (!cr0.lt) goto loc_826F150C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F150C:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwimi r11,r30,5,24,26
	r11.u64 = (__builtin_rotateleft32(r30.u32, 5) & 0xE0) | (r11.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
loc_826F1518:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm r10,r11,0,24,26
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826f1890
	if (!cr6.eq) goto loc_826F1890;
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// rlwinm r11,r11,0,21,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// mr r29,r15
	r29.u64 = r15.u64;
	// cmplwi cr6,r11,512
	cr6.compare<uint32_t>(r11.u32, 512, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x826f16fc
	if (!cr6.eq) goto loc_826F16FC;
	// li r30,2
	r30.s64 = 2;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x826f15a8
	if (!cr6.lt) goto loc_826F15A8;
loc_826F1550:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f15a8
	if (cr6.eq) goto loc_826F15A8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f1598
	if (!cr0.lt) goto loc_826F1598;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F1598:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f1550
	if (cr6.gt) goto loc_826F1550;
loc_826F15A8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f15e4
	if (!cr0.lt) goto loc_826F15E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F15E4:
	// cmplwi cr6,r30,3
	cr6.compare<uint32_t>(r30.u32, 3, xer);
	// bgt cr6,0x826f1890
	if (cr6.gt) goto loc_826F1890;
	// lis r12,-32145
	r12.s64 = -2106654720;
	// addi r12,r12,5636
	r12.s64 = r12.s64 + 5636;
	// rlwinm r0,r30,2,0,29
	r0.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r30.u64) {
	case 0:
		goto loc_826F17A4;
	case 1:
		goto loc_826F1614;
	case 2:
		goto loc_826F162C;
	case 3:
		goto loc_826F1884;
	default:
		__builtin_unreachable();
	}
	// lwz r19,6052(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + 6052);
	// lwz r19,5652(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + 5652);
	// lwz r19,5676(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + 5676);
	// lwz r19,6276(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + 6276);
loc_826F1614:
	// lwz r11,1464(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1464);
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// subfic r11,r11,2
	xer.ca = r11.u32 <= 2;
	r11.s64 = 2 - r11.s64;
	// rlwimi r10,r11,5,24,26
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 5) & 0xE0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
	// b 0x826f1890
	goto loc_826F1890;
loc_826F162C:
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// mr r30,r20
	r30.u64 = r20.u64;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f16a0
	if (!cr6.lt) goto loc_826F16A0;
loc_826F1648:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f16a0
	if (cr6.eq) goto loc_826F16A0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f1690
	if (!cr0.lt) goto loc_826F1690;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F1690:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f1648
	if (cr6.gt) goto loc_826F1648;
loc_826F16A0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f16dc
	if (!cr0.lt) goto loc_826F16DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F16DC:
	// lwz r11,1468(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1468);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// beq cr6,0x826f1878
	if (cr6.eq) goto loc_826F1878;
	// subfic r11,r11,2
	xer.ca = r11.u32 <= 2;
	r11.s64 = 2 - r11.s64;
	// rlwimi r10,r11,5,24,26
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 5) & 0xE0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
	// b 0x826f1890
	goto loc_826F1890;
loc_826F16FC:
	// mr r30,r20
	r30.u64 = r20.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f1760
	if (!cr6.lt) goto loc_826F1760;
loc_826F1708:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f1760
	if (cr6.eq) goto loc_826F1760;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f1750
	if (!cr0.lt) goto loc_826F1750;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F1750:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f1708
	if (cr6.gt) goto loc_826F1708;
loc_826F1760:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f179c
	if (!cr0.lt) goto loc_826F179C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F179C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826f17b8
	if (!cr6.eq) goto loc_826F17B8;
loc_826F17A4:
	// lwz r11,1464(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1464);
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwimi r10,r11,5,24,26
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 5) & 0xE0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
	// b 0x826f1890
	goto loc_826F1890;
loc_826F17B8:
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// mr r30,r20
	r30.u64 = r20.u64;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f182c
	if (!cr6.lt) goto loc_826F182C;
loc_826F17D4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f182c
	if (cr6.eq) goto loc_826F182C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f181c
	if (!cr0.lt) goto loc_826F181C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F181C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f17d4
	if (cr6.gt) goto loc_826F17D4;
loc_826F182C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f1868
	if (!cr0.lt) goto loc_826F1868;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F1868:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826f1884
	if (!cr6.eq) goto loc_826F1884;
	// lwz r11,1468(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1468);
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
loc_826F1878:
	// rlwimi r10,r11,5,24,26
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 5) & 0xE0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
	// b 0x826f1890
	goto loc_826F1890;
loc_826F1884:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwimi r11,r20,6,24,26
	r11.u64 = (__builtin_rotateleft32(r20.u32, 6) & 0xE0) | (r11.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
loc_826F1890:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f18a8
	if (cr6.eq) goto loc_826F18A8;
	// stb r15,5(r23)
	PPC_STORE_U8(r23.u32 + 5, r15.u8);
	// b 0x826f2350
	goto loc_826F2350;
loc_826F18A8:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826f1a00
	if (cr6.eq) goto loc_826F1A00;
	// lwz r11,1176(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1176);
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f19a4
	if (cr6.lt) goto loc_826F19A4;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f199c
	if (!cr6.lt) goto loc_826F199C;
loc_826F1904:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f1930
	if (cr6.lt) goto loc_826F1930;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f1904
	if (cr6.eq) goto loc_826F1904;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f19e0
	goto loc_826F19E0;
loc_826F1930:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F199C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f19e0
	goto loc_826F19E0;
loc_826F19A4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F19AC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f19ac
	if (cr6.lt) goto loc_826F19AC;
loc_826F19E0:
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// addi r10,r30,1
	ctx.r10.s64 = r30.s64 + 1;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f1a04
	if (cr6.eq) goto loc_826F1A04;
loc_826F19F4:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// b 0x8239bd10
	return;
loc_826F1A00:
	// mr r10,r15
	ctx.r10.u64 = r15.u64;
loc_826F1A04:
	// lwz r9,1200(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 1200);
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// lbzx r28,r9,r10
	r28.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// rlwinm r10,r11,27,29,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// stb r28,5(r23)
	PPC_STORE_U8(r23.u32 + 5, r28.u8);
	// beq cr6,0x826f1ee8
	if (cr6.eq) goto loc_826F1EE8;
	// rlwinm r11,r11,0,21,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826f1bdc
	if (!cr6.eq) goto loc_826F1BDC;
	// cmplwi cr6,r10,2
	cr6.compare<uint32_t>(ctx.r10.u32, 2, xer);
	// beq cr6,0x826f1a54
	if (cr6.eq) goto loc_826F1A54;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x826f1ee8
	if (cr6.eq) goto loc_826F1EE8;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r4,176(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// rlwinm r11,r24,2,0,29
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r25
	PPC_STORE_U32(r11.u32 + r25.u32, ctx.r3.u32);
	// b 0x826f1ee8
	goto loc_826F1EE8;
loc_826F1A54:
	// lwz r11,184(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 184);
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f1b48
	if (cr6.lt) goto loc_826F1B48;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f1b40
	if (!cr6.lt) goto loc_826F1B40;
loc_826F1AA8:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f1ad4
	if (cr6.lt) goto loc_826F1AD4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f1aa8
	if (cr6.eq) goto loc_826F1AA8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f1b84
	goto loc_826F1B84;
loc_826F1AD4:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F1B40:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f1b84
	goto loc_826F1B84;
loc_826F1B48:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F1B50:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f1b50
	if (cr6.lt) goto loc_826F1B50;
loc_826F1B84:
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f19f4
	if (!cr6.eq) goto loc_826F19F4;
	// rlwinm r11,r30,0,30,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f1bb4
	if (cr6.eq) goto loc_826F1BB4;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r4,176(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// rlwinm r11,r24,2,0,29
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r25
	PPC_STORE_U32(r11.u32 + r25.u32, ctx.r3.u32);
loc_826F1BB4:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f1ee8
	if (cr6.eq) goto loc_826F1EE8;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r4,176(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// addi r11,r24,1
	r11.s64 = r24.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r25
	PPC_STORE_U32(r11.u32 + r25.u32, ctx.r3.u32);
	// b 0x826f1ee8
	goto loc_826F1EE8;
loc_826F1BDC:
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// cmplwi cr6,r10,2
	cr6.compare<uint32_t>(ctx.r10.u32, 2, xer);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// beq cr6,0x826f1d6c
	if (cr6.eq) goto loc_826F1D6C;
	// lwz r11,184(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 184);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f1cd8
	if (cr6.lt) goto loc_826F1CD8;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f1cd0
	if (!cr6.lt) goto loc_826F1CD0;
loc_826F1C38:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f1c64
	if (cr6.lt) goto loc_826F1C64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f1c38
	if (cr6.eq) goto loc_826F1C38;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f1d14
	goto loc_826F1D14;
loc_826F1C64:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F1CD0:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f1d14
	goto loc_826F1D14;
loc_826F1CD8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F1CE0:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f1ce0
	if (cr6.lt) goto loc_826F1CE0;
loc_826F1D14:
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f19f4
	if (!cr6.eq) goto loc_826F19F4;
	// rlwinm r11,r30,0,30,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f1d44
	if (cr6.eq) goto loc_826F1D44;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r4,176(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// rlwinm r11,r24,2,0,29
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r25
	PPC_STORE_U32(r11.u32 + r25.u32, ctx.r3.u32);
loc_826F1D44:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f1ee8
	if (cr6.eq) goto loc_826F1EE8;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r4,176(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// add r11,r26,r24
	r11.u64 = r26.u64 + r24.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r25
	PPC_STORE_U32(r11.u32 + r25.u32, ctx.r3.u32);
	// b 0x826f1ee8
	goto loc_826F1EE8;
loc_826F1D6C:
	// lwz r11,180(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 180);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f1e58
	if (cr6.lt) goto loc_826F1E58;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f1e50
	if (!cr6.lt) goto loc_826F1E50;
loc_826F1DB8:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f1de4
	if (cr6.lt) goto loc_826F1DE4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f1db8
	if (cr6.eq) goto loc_826F1DB8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f1e94
	goto loc_826F1E94;
loc_826F1DE4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F1E50:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f1e94
	goto loc_826F1E94;
loc_826F1E58:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F1E60:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f1e60
	if (cr6.lt) goto loc_826F1E60;
loc_826F1E94:
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f19f4
	if (!cr6.eq) goto loc_826F19F4;
	// li r29,3
	r29.s64 = 3;
	// addi r31,r22,36
	r31.s64 = r22.s64 + 36;
loc_826F1EAC:
	// slw r11,r20,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (r20.u32 << (r29.u8 & 0x3F));
	// and r11,r11,r30
	r11.u64 = r11.u64 & r30.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f1ed8
	if (cr6.eq) goto loc_826F1ED8;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r4,176(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 176);
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// lhz r11,0(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r25
	PPC_STORE_U32(r11.u32 + r25.u32, ctx.r3.u32);
loc_826F1ED8:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// cmpwi cr6,r29,-1
	cr6.compare<int32_t>(r29.s32, -1, xer);
	// bgt cr6,0x826f1eac
	if (cr6.gt) goto loc_826F1EAC;
loc_826F1EE8:
	// lbz r11,27(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 27);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f21b0
	if (cr6.eq) goto loc_826F21B0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826f21b0
	if (cr6.eq) goto loc_826F21B0;
	// lbz r11,1181(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1181);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f1f3c
	if (cr6.eq) goto loc_826F1F3C;
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm r10,r10,20,28,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xF;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f1f28
	if (cr6.eq) goto loc_826F1F28;
	// lbz r11,1182(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// b 0x826f2194
	goto loc_826F2194;
loc_826F1F28:
	// lbz r11,1180(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1180);
	// lbz r10,1185(r22)
	ctx.r10.u64 = PPC_LOAD_U8(r22.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826f2194
	goto loc_826F2194;
loc_826F1F3C:
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// mr r29,r15
	r29.u64 = r15.u64;
	// lbz r11,1186(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826f2028
	if (cr6.eq) goto loc_826F2028;
	// mr r30,r20
	r30.u64 = r20.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f1fbc
	if (!cr6.lt) goto loc_826F1FBC;
loc_826F1F64:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f1fbc
	if (cr6.eq) goto loc_826F1FBC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f1fac
	if (!cr0.lt) goto loc_826F1FAC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F1FAC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f1f64
	if (cr6.gt) goto loc_826F1F64;
loc_826F1FBC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f1ff8
	if (!cr0.lt) goto loc_826F1FF8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F1FF8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826f2010
	if (cr6.eq) goto loc_826F2010;
	// lbz r11,1182(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f2198
	goto loc_826F2198;
loc_826F2010:
	// lbz r11,1180(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1180);
	// lbz r10,1185(r22)
	ctx.r10.u64 = PPC_LOAD_U8(r22.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f2198
	goto loc_826F2198;
loc_826F2028:
	// li r30,3
	r30.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826f208c
	if (!cr6.lt) goto loc_826F208C;
loc_826F2034:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f208c
	if (cr6.eq) goto loc_826F208C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f207c
	if (!cr0.lt) goto loc_826F207C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F207C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f2034
	if (cr6.gt) goto loc_826F2034;
loc_826F208C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f20c8
	if (!cr0.lt) goto loc_826F20C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F20C8:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x826f2188
	if (!cr6.eq) goto loc_826F2188;
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// li r30,5
	r30.s64 = 5;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826f2144
	if (!cr6.lt) goto loc_826F2144;
loc_826F20EC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f2144
	if (cr6.eq) goto loc_826F2144;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f2134
	if (!cr0.lt) goto loc_826F2134;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F2134:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f20ec
	if (cr6.gt) goto loc_826F20EC;
loc_826F2144:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f2180
	if (!cr0.lt) goto loc_826F2180;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F2180:
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x826f2190
	goto loc_826F2190;
loc_826F2188:
	// lbz r11,1180(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 1180);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
loc_826F2190:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826F2194:
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
loc_826F2198:
	// stb r11,4(r23)
	PPC_STORE_U8(r23.u32 + 4, r11.u8);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x826f19f4
	if (cr6.lt) goto loc_826F19F4;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// bgt cr6,0x826f19f4
	if (cr6.gt) goto loc_826F19F4;
loc_826F21B0:
	// lbz r11,29(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 29);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f2350
	if (cr6.eq) goto loc_826F2350;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826f2350
	if (cr6.eq) goto loc_826F2350;
	// lwz r11,200(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 200);
	// lwz r31,0(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f22b8
	if (cr6.lt) goto loc_826F22B8;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f22b0
	if (!cr6.lt) goto loc_826F22B0;
loc_826F2218:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f2244
	if (cr6.lt) goto loc_826F2244;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f2218
	if (cr6.eq) goto loc_826F2218;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f22f4
	goto loc_826F22F4;
loc_826F2244:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F22B0:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f22f4
	goto loc_826F22F4;
loc_826F22B8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F22C0:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f22c0
	if (cr6.lt) goto loc_826F22C0;
loc_826F22F4:
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f19f4
	if (!cr6.eq) goto loc_826F19F4;
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// blt cr6,0x826f2314
	if (cr6.lt) goto loc_826F2314;
	// mr r9,r15
	ctx.r9.u64 = r15.u64;
loc_826F2314:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwimi r11,r9,28,3,3
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 28) & 0x10000000) | (r11.u64 & 0xFFFFFFFFEFFFFFFF);
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// addi r8,r9,88
	ctx.r8.s64 = ctx.r9.s64 + 88;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// lwzx r11,r10,r9
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// rlwimi r7,r11,24,5,7
	ctx.r7.u64 = (__builtin_rotateleft32(r11.u32, 24) & 0x7000000) | (ctx.r7.u64 & 0xFFFFFFFFF8FFFFFF);
	// stw r7,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r7.u32);
	// lwzx r11,r10,r8
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// rotlwi r10,r7,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// rlwimi r10,r11,20,10,11
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 20) & 0x300000) | (ctx.r10.u64 & 0xFFFFFFFFFFCFFFFF);
	// rlwinm r11,r10,0,5,3
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFF7FFFFFF;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
loc_826F2350:
	// mr r5,r14
	ctx.r5.u64 = r14.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x826ea9b0
	sub_826EA9B0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x826f24b0
	if (!cr6.eq) goto loc_826F24B0;
loc_826F236C:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r23,r23,20
	r23.s64 = r23.s64 + 20;
	// lhz r9,18(r14)
	ctx.r9.u64 = PPC_LOAD_U16(r14.u32 + 18);
	// lis r27,-32768
	r27.s64 = -2147483648;
	// addi r26,r11,1
	r26.s64 = r11.s64 + 1;
	// lwz r10,0(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// lwz r11,4(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 4);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r23,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r23.u32);
	// cmplw cr6,r26,r21
	cr6.compare<uint32_t>(r26.u32, r21.u32, xer);
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// li r24,119
	r24.s64 = 119;
	// sth r9,18(r14)
	PPC_STORE_U16(r14.u32 + 18, ctx.r9.u16);
	// stw r10,0(r14)
	PPC_STORE_U32(r14.u32 + 0, ctx.r10.u32);
	// stw r11,4(r14)
	PPC_STORE_U32(r14.u32 + 4, r11.u32);
	// blt cr6,0x826effe4
	if (cr6.lt) goto loc_826EFFE4;
	// lwz r28,148(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r29,372(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
loc_826F23C0:
	// lhz r9,16(r14)
	ctx.r9.u64 = PPC_LOAD_U16(r14.u32 + 16);
	// rlwinm r11,r21,1,0,30
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,0(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r25,r28
	cr6.compare<uint32_t>(r25.u32, r28.u32, xer);
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// sth r9,16(r14)
	PPC_STORE_U16(r14.u32 + 16, ctx.r9.u16);
	// stw r11,0(r14)
	PPC_STORE_U32(r14.u32 + 0, r11.u32);
	// blt cr6,0x826efc6c
	if (cr6.lt) goto loc_826EFC6C;
loc_826F23EC:
	// lwz r11,32(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 32);
	// lis r9,1
	ctx.r9.s64 = 65536;
	// lwz r10,21572(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 21572);
	// li r8,-1
	ctx.r8.s64 = -1;
	// ori r9,r9,33684
	ctx.r9.u64 = ctx.r9.u64 | 33684;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// stwx r11,r29,r9
	PPC_STORE_U32(r29.u32 + ctx.r9.u32, r11.u32);
	// lwz r11,32(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 32);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// ld r11,104(r22)
	r11.u64 = PPC_LOAD_U64(r22.u32 + 104);
	// lwz r10,84(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, r11.u64);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,112(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,116(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,120(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,124(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,128(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,132(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,136(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,140(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,144(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,148(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,152(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// b 0x8239bd10
	return;
loc_826F24B0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826F24BC"))) PPC_WEAK_FUNC(sub_826F24BC);
PPC_FUNC_IMPL(__imp__sub_826F24BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826F24C0"))) PPC_WEAK_FUNC(sub_826F24C0);
PPC_FUNC_IMPL(__imp__sub_826F24C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r10,3960(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// beq cr6,0x826f24f4
	if (cr6.eq) goto loc_826F24F4;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// li r9,0
	ctx.r9.s64 = 0;
	// bne cr6,0x826f24f8
	if (!cr6.eq) goto loc_826F24F8;
loc_826F24F4:
	// li r9,1
	ctx.r9.s64 = 1;
loc_826F24F8:
	// lwz r11,2928(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2928);
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// lwz r10,2088(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2088);
	// addi r8,r11,726
	ctx.r8.s64 = r11.s64 + 726;
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r10,263
	ctx.r10.s64 = ctx.r10.s64 + 263;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwzx r8,r8,r31
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r8,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, ctx.r8.u32);
	// lwzx r8,r7,r31
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + r31.u32);
	// stw r8,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, ctx.r8.u32);
	// lwz r11,2100(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 2100);
	// stw r11,2092(r31)
	PPC_STORE_U32(r31.u32 + 2092, r11.u32);
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(r31.u32 + 2096, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// beq cr6,0x826f2550
	if (cr6.eq) goto loc_826F2550;
	// li r11,1
	r11.s64 = 1;
loc_826F2550:
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
	// lwz r11,21000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21000);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f2574
	if (!cr6.eq) goto loc_826F2574;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r10,21268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826f2578
	goto loc_826F2578;
loc_826F2574:
	// lwz r11,21268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21268);
loc_826F2578:
	// stw r11,21264(r31)
	PPC_STORE_U32(r31.u32 + 21264, r11.u32);
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f2590
	if (!cr6.eq) goto loc_826F2590;
	// stw r9,21472(r31)
	PPC_STORE_U32(r31.u32 + 21472, ctx.r9.u32);
	// b 0x826f2594
	goto loc_826F2594;
loc_826F2590:
	// stw r9,21476(r31)
	PPC_STORE_U32(r31.u32 + 21476, ctx.r9.u32);
loc_826F2594:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// bl 0x825ebc08
	sub_825EBC08(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// bl 0x82628f88
	sub_82628F88(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261ca78
	sub_8261CA78(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261cba0
	sub_8261CBA0(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82615470
	sub_82615470(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826F25E0"))) PPC_WEAK_FUNC(sub_826F25E0);
PPC_FUNC_IMPL(__imp__sub_826F25E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// li r30,9
	r30.s64 = 9;
	// li r29,0
	r29.s64 = 0;
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r26,1524(r27)
	r26.u64 = PPC_LOAD_U32(r27.u32 + 1524);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,9
	cr6.compare<uint32_t>(r11.u32, 9, xer);
	// bge cr6,0x826f266c
	if (!cr6.lt) goto loc_826F266C;
loc_826F2614:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f266c
	if (cr6.eq) goto loc_826F266C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f265c
	if (!cr0.lt) goto loc_826F265C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F265C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f2614
	if (cr6.gt) goto loc_826F2614;
loc_826F266C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f26a8
	if (!cr0.lt) goto loc_826F26A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F26A8:
	// lwz r11,1528(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1528);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f26e0
	if (!cr6.eq) goto loc_826F26E0;
	// lwz r11,1520(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1520);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f26e0
	if (!cr6.eq) goto loc_826F26E0;
	// lhz r11,52(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 52);
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x826f26f4
	if (cr6.eq) goto loc_826F26F4;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_826F26E0:
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// beq cr6,0x826f26f4
	if (cr6.eq) goto loc_826F26F4;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_826F26F4:
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f2768
	if (!cr6.lt) goto loc_826F2768;
loc_826F2710:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f2768
	if (cr6.eq) goto loc_826F2768;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f2758
	if (!cr0.lt) goto loc_826F2758;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F2758:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f2710
	if (cr6.gt) goto loc_826F2710;
loc_826F2768:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f27a4
	if (!cr0.lt) goto loc_826F27A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F27A4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x826f28dc
	if (cr6.eq) goto loc_826F28DC;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x826f28dc
	if (!cr6.gt) goto loc_826F28DC;
loc_826F27B4:
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r26,32
	cr6.compare<int32_t>(r26.s32, 32, xer);
	// ble cr6,0x826f2858
	if (!cr6.gt) goto loc_826F2858;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r26,r26,-32
	r26.s64 = r26.s64 + -32;
	// li r30,32
	r30.s64 = 32;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// bge cr6,0x826f281c
	if (!cr6.lt) goto loc_826F281C;
loc_826F27D8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f281c
	if (cr6.eq) goto loc_826F281C;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x826f280c
	if (!cr0.lt) goto loc_826F280C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F280C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f27d8
	if (cr6.gt) goto loc_826F27D8;
loc_826F281C:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x826f2844
	if (!cr0.lt) goto loc_826F2844;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F2844:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bgt cr6,0x826f27b4
	if (cr6.gt) goto loc_826F27B4;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_826F2858:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r30,r26
	r30.u64 = r26.u64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// beq cr6,0x826f28dc
	if (cr6.eq) goto loc_826F28DC;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// ble cr6,0x826f28b4
	if (!cr6.gt) goto loc_826F28B4;
loc_826F2874:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f28b4
	if (cr6.eq) goto loc_826F28B4;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x826f28a4
	if (!cr0.lt) goto loc_826F28A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F28A4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f2874
	if (cr6.gt) goto loc_826F2874;
loc_826F28B4:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x826f28dc
	if (!cr0.lt) goto loc_826F28DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F28DC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_826F28E8"))) PPC_WEAK_FUNC(sub_826F28E8);
PPC_FUNC_IMPL(__imp__sub_826F28E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-752(r1)
	ea = -752 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r19,0
	r19.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lhz r11,52(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 52);
	// mr r29,r19
	r29.u64 = r19.u64;
	// lhz r10,50(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 50);
	// rlwinm r26,r11,31,1,31
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r8,1516(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1516);
	// rlwinm r28,r10,31,1,31
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r9,21556(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 21556);
	// stw r27,772(r1)
	PPC_STORE_U32(ctx.r1.u32 + 772, r27.u32);
	// mullw r11,r26,r28
	r11.s64 = int64_t(r26.s32) * int64_t(r28.s32);
	// stw r25,780(r1)
	PPC_STORE_U32(ctx.r1.u32 + 780, r25.u32);
	// stw r23,788(r1)
	PPC_STORE_U32(ctx.r1.u32 + 788, r23.u32);
	// stw r26,496(r1)
	PPC_STORE_U32(ctx.r1.u32 + 496, r26.u32);
	// stw r28,384(r1)
	PPC_STORE_U32(ctx.r1.u32 + 384, r28.u32);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r10,r11,r8
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// rlwinm r10,r10,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// rlwinm r8,r11,7,0,24
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,20(r23)
	PPC_STORE_U32(r23.u32 + 20, ctx.r10.u32);
	// lwz r10,1516(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1516);
	// lwz r9,21568(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 21568);
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r7,20(r23)
	ctx.r7.u64 = PPC_LOAD_U32(r23.u32 + 20);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r8,28(r23)
	PPC_STORE_U32(r23.u32 + 28, ctx.r8.u32);
	// stw r10,24(r23)
	PPC_STORE_U32(r23.u32 + 24, ctx.r10.u32);
	// lwz r9,1516(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1516);
	// lwz r10,21572(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 21572);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// stw r31,32(r23)
	PPC_STORE_U32(r23.u32 + 32, r31.u32);
	// lwz r11,188(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 188);
	// lwz r3,192(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// stw r31,504(r1)
	PPC_STORE_U32(ctx.r1.u32 + 504, r31.u32);
	// stw r11,488(r1)
	PPC_STORE_U32(ctx.r1.u32 + 488, r11.u32);
	// bl 0x826a8e48
	sub_826A8E48(ctx, base);
	// stw r19,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r19.u32);
	// mullw r11,r26,r28
	r11.s64 = int64_t(r26.s32) * int64_t(r28.s32);
	// stw r19,4(r23)
	PPC_STORE_U32(r23.u32 + 4, r19.u32);
	// sth r19,16(r23)
	PPC_STORE_U16(r23.u32 + 16, r19.u16);
	// lwz r9,1516(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1516);
	// lwz r10,268(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r24,r19
	r24.u64 = r19.u64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r17,r11,r10
	r17.u64 = r11.u64 + ctx.r10.u64;
	// stw r24,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r24.u32);
	// stw r17,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, r17.u32);
	// beq cr6,0x826f97cc
	if (cr6.eq) goto loc_826F97CC;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// li r22,16384
	r22.s64 = 16384;
	// addi r11,r11,14080
	r11.s64 = r11.s64 + 14080;
	// li r21,119
	r21.s64 = 119;
	// lis r14,2
	r14.s64 = 131072;
	// stw r11,376(r1)
	PPC_STORE_U32(ctx.r1.u32 + 376, r11.u32);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r11,r11,29840
	r11.s64 = r11.s64 + 29840;
	// stw r11,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r11.u32);
loc_826F2A14:
	// addi r11,r25,1376
	r11.s64 = r25.s64 + 1376;
	// sth r19,18(r23)
	PPC_STORE_U16(r23.u32 + 18, r19.u16);
	// stw r11,1416(r25)
	PPC_STORE_U32(r25.u32 + 1416, r11.u32);
	// lwz r11,21236(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f2ce8
	if (cr6.eq) goto loc_826F2CE8;
	// lwz r11,1240(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1240);
	// rlwinm r10,r24,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f2ce8
	if (cr6.eq) goto loc_826F2CE8;
	// lwz r11,16(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f2b0c
	if (cr6.eq) goto loc_826F2B0C;
	// lis r11,1
	r11.s64 = 65536;
	// ori r11,r11,33712
	r11.u64 = r11.u64 | 33712;
	// lwzx r11,r27,r11
	r11.u64 = PPC_LOAD_U32(r27.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f2b0c
	if (!cr6.eq) goto loc_826F2B0C;
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f2aec
	if (cr6.eq) goto loc_826F2AEC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,1
	r30.s64 = 1;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f2ac4
	if (!cr6.lt) goto loc_826F2AC4;
loc_826F2A84:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f2ac4
	if (cr6.eq) goto loc_826F2AC4;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x826f2ab4
	if (!cr0.lt) goto loc_826F2AB4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F2AB4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f2a84
	if (cr6.gt) goto loc_826F2A84;
loc_826F2AC4:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x826f2aec
	if (!cr0.lt) goto loc_826F2AEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F2AEC:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x826f25e0
	sub_826F25E0(ctx, base);
	// b 0x826f2cd8
	goto loc_826F2CD8;
loc_826F2B0C:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r10,104(r25)
	ctx.r10.u64 = PPC_LOAD_U64(r25.u32 + 104);
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,112(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,116(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,120(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,124(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,128(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,132(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,140(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,144(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,148(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,152(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f2c28
	if (cr6.eq) goto loc_826F2C28;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,1
	r30.s64 = 1;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f2c00
	if (!cr6.lt) goto loc_826F2C00;
loc_826F2BC0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f2c00
	if (cr6.eq) goto loc_826F2C00;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x826f2bf0
	if (!cr0.lt) goto loc_826F2BF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F2BF0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f2bc0
	if (cr6.gt) goto loc_826F2BC0;
loc_826F2C00:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x826f2c28
	if (!cr0.lt) goto loc_826F2C28;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F2C28:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82639b10
	sub_82639B10(ctx, base);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,104(r25)
	PPC_STORE_U64(r25.u32 + 104, r11.u64);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,112(r25)
	PPC_STORE_U32(r25.u32 + 112, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,116(r25)
	PPC_STORE_U32(r25.u32 + 116, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,120(r25)
	PPC_STORE_U32(r25.u32 + 120, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,124(r25)
	PPC_STORE_U32(r25.u32 + 124, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,128(r25)
	PPC_STORE_U32(r25.u32 + 128, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,132(r25)
	PPC_STORE_U32(r25.u32 + 132, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,136(r25)
	PPC_STORE_U32(r25.u32 + 136, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,140(r25)
	PPC_STORE_U32(r25.u32 + 140, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,144(r25)
	PPC_STORE_U32(r25.u32 + 144, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,148(r25)
	PPC_STORE_U32(r25.u32 + 148, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stw r11,152(r25)
	PPC_STORE_U32(r25.u32 + 152, r11.u32);
loc_826F2CD8:
	// li r11,1
	r11.s64 = 1;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// stb r11,1187(r25)
	PPC_STORE_U8(r25.u32 + 1187, r11.u8);
	// bne cr6,0x826f9884
	if (!cr6.eq) goto loc_826F9884;
loc_826F2CE8:
	// lwz r11,3932(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3932);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f2e2c
	if (cr6.eq) goto loc_826F2E2C;
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// ld r10,104(r25)
	ctx.r10.u64 = PPC_LOAD_U64(r25.u32 + 104);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,112(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,116(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,120(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,124(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,128(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,132(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,140(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,144(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,148(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,152(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// bl 0x82639ef0
	sub_82639EF0(ctx, base);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,104(r25)
	PPC_STORE_U64(r25.u32 + 104, r11.u64);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,112(r25)
	PPC_STORE_U32(r25.u32 + 112, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,116(r25)
	PPC_STORE_U32(r25.u32 + 116, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,120(r25)
	PPC_STORE_U32(r25.u32 + 120, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,124(r25)
	PPC_STORE_U32(r25.u32 + 124, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,128(r25)
	PPC_STORE_U32(r25.u32 + 128, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,132(r25)
	PPC_STORE_U32(r25.u32 + 132, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,136(r25)
	PPC_STORE_U32(r25.u32 + 136, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,140(r25)
	PPC_STORE_U32(r25.u32 + 140, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,144(r25)
	PPC_STORE_U32(r25.u32 + 144, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,148(r25)
	PPC_STORE_U32(r25.u32 + 148, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stw r11,152(r25)
	PPC_STORE_U32(r25.u32 + 152, r11.u32);
	// bne cr6,0x826f9884
	if (!cr6.eq) goto loc_826F9884;
loc_826F2E2C:
	// mr r15,r19
	r15.u64 = r19.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// stw r15,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r15.u32);
	// beq cr6,0x826f979c
	if (cr6.eq) goto loc_826F979C;
loc_826F2E3C:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r10,128
	ctx.r10.s64 = 128;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// dcbt r10,r11
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// lbz r10,24(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 24);
	// rlwinm r11,r11,0,29,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFE7;
	// rlwinm r11,r11,0,4,2
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFEFFFFFFF;
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// stb r10,4(r17)
	PPC_STORE_U8(r17.u32 + 4, ctx.r10.u8);
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// lwz r11,1324(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1324);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f2fd8
	if (cr6.lt) goto loc_826F2FD8;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f2f50
	if (!cr6.lt) goto loc_826F2F50;
loc_826F2EBC:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f2ee4
	if (cr6.lt) goto loc_826F2EE4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f2ebc
	if (cr6.eq) goto loc_826F2EBC;
	// b 0x826f2f50
	goto loc_826F2F50;
loc_826F2EE4:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F2F50:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f8d24
	if (cr6.lt) goto loc_826F8D24;
loc_826F2F5C:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bgt cr6,0x826f8d24
	if (cr6.gt) goto loc_826F8D24;
	// lwz r11,376(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	// addi r10,r11,80
	ctx.r10.s64 = r11.s64 + 80;
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// lbzx r10,r30,r10
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + ctx.r10.u32);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// rlwimi r11,r10,8,21,23
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 8) & 0x700) | (r11.u64 & 0xFFFFFFFFFFFFF8FF);
	// srawi r9,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 3;
	// srawi r10,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 4;
	// clrlwi r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// rlwinm r10,r11,24,29,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0x7;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// stw r28,480(r1)
	PPC_STORE_U32(ctx.r1.u32 + 480, r28.u32);
	// bne cr6,0x826f4284
	if (!cr6.eq) goto loc_826F4284;
	// lbz r10,27(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 27);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826f32a4
	if (cr6.eq) goto loc_826F32A4;
	// lbz r10,1181(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 1181);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826f3030
	if (cr6.eq) goto loc_826F3030;
	// rlwinm r11,r11,20,12,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 20) & 0xFFFFF;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f301c
	if (cr6.eq) goto loc_826F301C;
	// lbz r11,1182(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// b 0x826f3288
	goto loc_826F3288;
loc_826F2FD8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F2FE0:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addis r11,r30,1
	r11.s64 = r30.s64 + 65536;
	// addi r11,r11,-32768
	r11.s64 = r11.s64 + -32768;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f2fe0
	if (cr6.lt) goto loc_826F2FE0;
	// b 0x826f2f5c
	goto loc_826F2F5C;
loc_826F301C:
	// lbz r11,1180(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1180);
	// lbz r10,1185(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826f3288
	goto loc_826F3288;
loc_826F3030:
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// mr r29,r19
	r29.u64 = r19.u64;
	// lbz r11,1186(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826f311c
	if (cr6.eq) goto loc_826F311C;
	// li r30,1
	r30.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f30b0
	if (!cr6.lt) goto loc_826F30B0;
loc_826F3058:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f30b0
	if (cr6.eq) goto loc_826F30B0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f30a0
	if (!cr0.lt) goto loc_826F30A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F30A0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f3058
	if (cr6.gt) goto loc_826F3058;
loc_826F30B0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f30ec
	if (!cr0.lt) goto loc_826F30EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F30EC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826f3104
	if (cr6.eq) goto loc_826F3104;
	// lbz r11,1182(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f328c
	goto loc_826F328C;
loc_826F3104:
	// lbz r11,1180(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1180);
	// lbz r10,1185(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f328c
	goto loc_826F328C;
loc_826F311C:
	// li r30,3
	r30.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826f3180
	if (!cr6.lt) goto loc_826F3180;
loc_826F3128:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f3180
	if (cr6.eq) goto loc_826F3180;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f3170
	if (!cr0.lt) goto loc_826F3170;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F3170:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f3128
	if (cr6.gt) goto loc_826F3128;
loc_826F3180:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f31bc
	if (!cr0.lt) goto loc_826F31BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F31BC:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x826f327c
	if (!cr6.eq) goto loc_826F327C;
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r30,5
	r30.s64 = 5;
	// mr r29,r19
	r29.u64 = r19.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826f3238
	if (!cr6.lt) goto loc_826F3238;
loc_826F31E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f3238
	if (cr6.eq) goto loc_826F3238;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f3228
	if (!cr0.lt) goto loc_826F3228;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F3228:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f31e0
	if (cr6.gt) goto loc_826F31E0;
loc_826F3238:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f3274
	if (!cr0.lt) goto loc_826F3274;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F3274:
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x826f3284
	goto loc_826F3284;
loc_826F327C:
	// lbz r11,1180(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1180);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
loc_826F3284:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826F3288:
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
loc_826F328C:
	// stb r11,4(r17)
	PPC_STORE_U8(r17.u32 + 4, r11.u8);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x826f8d24
	if (cr6.lt) goto loc_826F8D24;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// bgt cr6,0x826f8d24
	if (cr6.gt) goto loc_826F8D24;
loc_826F32A4:
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// mr r29,r19
	r29.u64 = r19.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f3318
	if (!cr6.lt) goto loc_826F3318;
loc_826F32C0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f3318
	if (cr6.eq) goto loc_826F3318;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f3308
	if (!cr0.lt) goto loc_826F3308;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F3308:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f32c0
	if (cr6.gt) goto loc_826F32C0;
loc_826F3318:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f3354
	if (!cr0.lt) goto loc_826F3354;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F3354:
	// lwz r10,0(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r17)
	PPC_STORE_U32(r17.u32 + 0, ctx.r10.u32);
	// beq cr6,0x826f34c4
	if (cr6.eq) goto loc_826F34C4;
	// lwz r11,1172(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1172);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f3460
	if (cr6.lt) goto loc_826F3460;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f3458
	if (!cr6.lt) goto loc_826F3458;
loc_826F33C0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f33ec
	if (cr6.lt) goto loc_826F33EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f33c0
	if (cr6.eq) goto loc_826F33C0;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f34a0
	goto loc_826F34A0;
loc_826F33EC:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F3458:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f34a0
	goto loc_826F34A0;
loc_826F3460:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F3468:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addis r11,r30,1
	r11.s64 = r30.s64 + 65536;
	// addi r11,r11,-32768
	r11.s64 = r11.s64 + -32768;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f3468
	if (cr6.lt) goto loc_826F3468;
loc_826F34A0:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f8d24
	if (!cr6.eq) goto loc_826F8D24;
	// lwz r11,1200(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1200);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lbz r11,1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// stb r11,5(r17)
	PPC_STORE_U8(r17.u32 + 5, r11.u8);
	// b 0x826f34c8
	goto loc_826F34C8;
loc_826F34C4:
	// stb r19,5(r17)
	PPC_STORE_U8(r17.u32 + 5, r19.u8);
loc_826F34C8:
	// lbz r11,4(r17)
	r11.u64 = PPC_LOAD_U8(r17.u32 + 4);
	// clrlwi r8,r24,31
	ctx.r8.u64 = r24.u32 & 0x1;
	// lwz r9,0(r17)
	ctx.r9.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rotlwi r5,r11,2
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 2);
	// lwz r7,4(r23)
	ctx.r7.u64 = PPC_LOAD_U32(r23.u32 + 4);
	// rlwinm r3,r9,0,27,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x18;
	// lwz r31,220(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// lwz r4,1248(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + 1248);
	// cntlzw r28,r3
	r28.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// stw r11,536(r1)
	PPC_STORE_U32(ctx.r1.u32 + 536, r11.u32);
	// rlwinm r3,r5,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r6,28(r25)
	ctx.r6.u64 = PPC_LOAD_U8(r25.u32 + 28);
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// lhz r10,50(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 50);
	// rlwinm r28,r28,27,31,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 27) & 0x1;
	// std r19,552(r1)
	PPC_STORE_U64(ctx.r1.u32 + 552, r19.u64);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r16,5(r17)
	r16.u64 = PPC_LOAD_U8(r17.u32 + 5);
	// add r11,r5,r4
	r11.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lwz r30,192(r25)
	r30.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// neg r29,r8
	r29.s64 = -ctx.r8.s64;
	// lwz r8,0(r23)
	ctx.r8.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// lwz r6,188(r25)
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + 188);
	// stb r28,176(r1)
	PPC_STORE_U8(ctx.r1.u32 + 176, r28.u8);
	// and r19,r29,r10
	r19.u64 = r29.u64 & ctx.r10.u64;
	// stw r3,452(r1)
	PPC_STORE_U32(ctx.r1.u32 + 452, ctx.r3.u32);
	// stw r11,544(r1)
	PPC_STORE_U32(ctx.r1.u32 + 544, r11.u32);
	// beq cr6,0x826f355c
	if (cr6.eq) goto loc_826F355C;
	// lwz r5,228(r25)
	ctx.r5.u64 = PPC_LOAD_U32(r25.u32 + 228);
	// rlwinm r11,r9,12,28,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xC;
	// lwz r9,232(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 232);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// stw r5,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, ctx.r5.u32);
	// b 0x826f3568
	goto loc_826F3568;
loc_826F355C:
	// addi r11,r25,236
	r11.s64 = r25.s64 + 236;
	// stw r11,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, r11.u32);
	// addi r11,r25,248
	r11.s64 = r25.s64 + 248;
loc_826F3568:
	// stw r11,424(r1)
	PPC_STORE_U32(ctx.r1.u32 + 424, r11.u32);
	// add r11,r8,r10
	r11.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,1240(r25)
	ctx.r5.u64 = PPC_LOAD_U32(r25.u32 + 1240);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r8,r11,r6
	ctx.r8.u64 = r11.u64 + ctx.r6.u64;
	// rlwinm r11,r24,2,0,29
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r20,r10,r24
	r20.s64 = int64_t(ctx.r10.s32) * int64_t(r24.s32);
	// lwzx r11,r5,r11
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + r11.u32);
	// stw r22,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r22.u32);
	// stw r22,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r22.u32);
	// stw r22,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r22.u32);
	// stw r22,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r22.u32);
	// stwx r22,r7,r30
	PPC_STORE_U32(ctx.r7.u32 + r30.u32, r22.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// and r18,r11,r24
	r18.u64 = r11.u64 & r24.u64;
	// li r11,0
	r11.s64 = 0;
	// stw r11,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, r11.u32);
	// b 0x826f35c4
	goto loc_826F35C4;
loc_826F35BC:
	// lwz r23,788(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	// li r21,119
	r21.s64 = 119;
loc_826F35C4:
	// clrlwi r9,r16,31
	ctx.r9.u64 = r16.u32 & 0x1;
	// stw r18,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, r18.u32);
	// rlwinm r10,r11,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// stw r15,420(r1)
	PPC_STORE_U32(ctx.r1.u32 + 420, r15.u32);
	// stw r21,408(r1)
	PPC_STORE_U32(ctx.r1.u32 + 408, r21.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r9,528(r1)
	PPC_STORE_U32(ctx.r1.u32 + 528, ctx.r9.u32);
	// bne cr6,0x826f3670
	if (!cr6.eq) goto loc_826F3670;
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// lwz r6,1160(r25)
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + 1160);
	// addi r9,r11,18
	ctx.r9.s64 = r11.s64 + 18;
	// lwz r8,264(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 264);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// lwz r7,188(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 188);
	// rlwinm r4,r24,1,30,30
	ctx.r4.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0x2;
	// add r11,r11,r15
	r11.u64 = r11.u64 + r15.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r6,400(r1)
	PPC_STORE_U32(ctx.r1.u32 + 400, ctx.r6.u32);
	// add r6,r20,r15
	ctx.r6.u64 = r20.u64 + r15.u64;
	// add r3,r19,r15
	ctx.r3.u64 = r19.u64 + r15.u64;
	// rlwinm r5,r6,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,420(r1)
	PPC_STORE_U32(ctx.r1.u32 + 420, r11.u32);
	// or r11,r4,r10
	r11.u64 = ctx.r4.u64 | ctx.r10.u64;
	// add r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 + r18.u64;
	// lhzx r9,r9,r25
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + r25.u32);
	// addi r11,r11,104
	r11.s64 = r11.s64 + 104;
	// rlwinm r6,r3,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r3,424(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, ctx.r10.u32);
	// add r10,r5,r9
	ctx.r10.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r3,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, ctx.r3.u32);
	// lhzx r6,r11,r25
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + r25.u32);
	// rlwinm r11,r9,5,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// stw r10,392(r1)
	PPC_STORE_U32(ctx.r1.u32 + 392, ctx.r10.u32);
	// stw r11,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, r11.u32);
	// extsh r11,r6
	r11.s64 = ctx.r6.s16;
	// stw r11,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, r11.u32);
	// b 0x826f36d0
	goto loc_826F36D0;
loc_826F3670:
	// lwz r9,1164(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1164);
	// clrlwi r10,r24,31
	ctx.r10.u64 = r24.u32 & 0x1;
	// lwz r8,192(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// addi r7,r10,102
	ctx.r7.s64 = ctx.r10.s64 + 102;
	// srawi r10,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	ctx.r10.s64 = r20.s32 >> 1;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r9,400(r1)
	PPC_STORE_U32(ctx.r1.u32 + 400, ctx.r9.u32);
	// addi r9,r11,63
	ctx.r9.s64 = r11.s64 + 63;
	// srawi r11,r19,1
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x1) != 0);
	r11.s64 = r19.s32 >> 1;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r15
	ctx.r10.u64 = ctx.r10.u64 + r15.u64;
	// add r11,r11,r15
	r11.u64 = r11.u64 + r15.u64;
	// lhzx r7,r7,r25
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + r25.u32);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// lwzx r10,r6,r25
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + r25.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,392(r1)
	PPC_STORE_U32(ctx.r1.u32 + 392, ctx.r9.u32);
	// stw r11,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, r11.u32);
	// extsh r11,r7
	r11.s64 = ctx.r7.s16;
	// stw r11,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, r11.u32);
	// lwz r11,404(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// stw r11,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, r11.u32);
loc_826F36D0:
	// lwz r10,452(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// lwz r11,28(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 28);
	// addi r11,r11,-128
	r11.s64 = r11.s64 + -128;
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// stw r11,464(r1)
	PPC_STORE_U32(ctx.r1.u32 + 464, r11.u32);
	// stw r11,28(r23)
	PPC_STORE_U32(r23.u32 + 28, r11.u32);
	// stw r10,416(r1)
	PPC_STORE_U32(ctx.r1.u32 + 416, ctx.r10.u32);
	// dcbzl r0,r11
	memset(base + ((r11.u32) & ~127), 0, 128);
	// li r28,0
	r28.s64 = 0;
	// lwz r10,400(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lbz r4,8(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// lwz r29,0(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r29.u32);
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f37e8
	if (cr6.lt) goto loc_826F37E8;
	// clrlwi r10,r30,28
	ctx.r10.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x826f37e0
	if (!cr6.lt) goto loc_826F37E0;
loc_826F3748:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f3774
	if (cr6.lt) goto loc_826F3774;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f3748
	if (cr6.eq) goto loc_826F3748;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f3828
	goto loc_826F3828;
loc_826F3774:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F37E0:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f3828
	goto loc_826F3828;
loc_826F37E8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F37F0:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addis r11,r30,1
	r11.s64 = r30.s64 + 65536;
	// addi r11,r11,-32768
	r11.s64 = r11.s64 + -32768;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f37f0
	if (cr6.lt) goto loc_826F37F0;
loc_826F3828:
	// clrlwi r30,r30,16
	r30.u64 = r30.u32 & 0xFFFF;
	// lwz r11,408(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	// mr r27,r30
	r27.u64 = r30.u64;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// beq cr6,0x826f3978
	if (cr6.eq) goto loc_826F3978;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x826f3a98
	if (cr6.eq) goto loc_826F3A98;
	// lwz r11,416(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x826f3890
	if (!cr6.eq) goto loc_826F3890;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x826f387c
	if (!cr0.lt) goto loc_826F387C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F387C:
	// rlwinm r11,r27,1,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f3954
	goto loc_826F3954;
loc_826F3890:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x826f3958
	if (!cr6.eq) goto loc_826F3958;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x826f3908
	if (!cr6.lt) goto loc_826F3908;
loc_826F38B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f3908
	if (cr6.eq) goto loc_826F3908;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f38f8
	if (!cr0.lt) goto loc_826F38F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F38F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f38b0
	if (cr6.gt) goto loc_826F38B0;
loc_826F3908:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f3944
	if (!cr0.lt) goto loc_826F3944;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F3944:
	// rlwinm r11,r27,2,0,29
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-3
	r11.s64 = r11.s64 + -3;
loc_826F3954:
	// clrlwi r30,r11,16
	r30.u64 = r11.u32 & 0xFFFF;
loc_826F3958:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x826f3a74
	goto loc_826F3A74;
loc_826F3978:
	// lwz r11,416(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bgt cr6,0x826f3990
	if (cr6.gt) goto loc_826F3990;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// subfic r11,r11,3
	xer.ca = r11.u32 <= 3;
	r11.s64 = 3 - r11.s64;
	// b 0x826f3994
	goto loc_826F3994;
loc_826F3990:
	// li r11,0
	r11.s64 = 0;
loc_826F3994:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r30,r11,8
	r30.s64 = r11.s64 + 8;
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x826f39b4
	if (!cr6.eq) goto loc_826F39B4;
	// li r11,0
	r11.s64 = 0;
	// b 0x826f3a54
	goto loc_826F3A54;
loc_826F39B4:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826f3a14
	if (!cr6.gt) goto loc_826F3A14;
loc_826F39BC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f3a14
	if (cr6.eq) goto loc_826F3A14;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f3a04
	if (!cr0.lt) goto loc_826F3A04;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F3A04:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f39bc
	if (cr6.gt) goto loc_826F39BC;
loc_826F3A14:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f3a50
	if (!cr0.lt) goto loc_826F3A50;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F3A50:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_826F3A54:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrlwi r30,r11,16
	r30.u64 = r11.u32 & 0xFFFF;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
loc_826F3A74:
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// bge 0x826f3a84
	if (!cr0.lt) goto loc_826F3A84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F3A84:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r10,r30
	ctx.r10.s64 = r30.s16;
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// extsh r28,r11
	r28.s64 = r11.s16;
loc_826F3A98:
	// lwz r5,464(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	// sth r28,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, r28.u16);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f8d24
	if (!cr6.eq) goto loc_826F8D24;
	// lwz r21,528(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x826f3ad8
	if (cr6.eq) goto loc_826F3AD8;
	// lwz r11,340(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r6,276(r25)
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + 276);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x826fcd00
	sub_826FCD00(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x826f8d24
	if (cr6.lt) goto loc_826F8D24;
loc_826F3AD8:
	// lwz r22,364(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r23,1
	r23.s64 = 1;
	// lhz r27,50(r25)
	r27.u64 = PPC_LOAD_U16(r25.u32 + 50);
	// li r26,0
	r26.s64 = 0;
	// srawi r11,r22,2
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3) != 0);
	r11.s64 = r22.s32 >> 2;
	// lwz r10,348(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	// lwz r7,392(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	// li r24,0
	r24.s64 = 0;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,388(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// li r9,0
	ctx.r9.s64 = 0;
	// srw r8,r27,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (r27.u32 >> (r11.u8 & 0x3F));
	// beq cr6,0x826f3b38
	if (cr6.eq) goto loc_826F3B38;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r7
	r11.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// bne cr6,0x826f3b38
	if (!cr6.eq) goto loc_826F3B38;
	// lwz r11,356(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// li r23,8
	r23.s64 = 8;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r26,r11,r10
	r26.s64 = ctx.r10.s64 - r11.s64;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
loc_826F3B38:
	// lwz r11,420(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r31,544(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,536(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	// beq cr6,0x826f3d98
	if (cr6.eq) goto loc_826F3D98;
	// lwz r6,-4(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + -4);
	// cmpwi cr6,r6,16384
	cr6.compare<int32_t>(ctx.r6.s32, 16384, xer);
	// bne cr6,0x826f3d98
	if (!cr6.eq) goto loc_826F3D98;
	// addi r24,r10,-32
	r24.s64 = ctx.r10.s64 + -32;
	// li r23,1
	r23.s64 = 1;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x826f3fc0
	if (cr6.eq) goto loc_826F3FC0;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x826f3d98
	if (cr6.eq) goto loc_826F3D98;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r8,16384
	cr6.compare<int32_t>(ctx.r8.s32, 16384, xer);
	// bne cr6,0x826f3b98
	if (!cr6.eq) goto loc_826F3B98;
	// lhz r8,-16(r26)
	ctx.r8.u64 = PPC_LOAD_U16(r26.u32 + -16);
	// extsh r3,r8
	ctx.r3.s64 = ctx.r8.s16;
loc_826F3B98:
	// lhz r8,16(r26)
	ctx.r8.u64 = PPC_LOAD_U16(r26.u32 + 16);
	// lhz r7,0(r24)
	ctx.r7.u64 = PPC_LOAD_U16(r24.u32 + 0);
	// lbz r6,27(r25)
	ctx.r6.u64 = PPC_LOAD_U8(r25.u32 + 27);
	// extsh r30,r8
	r30.s64 = ctx.r8.s16;
	// extsh r29,r7
	r29.s64 = ctx.r7.s16;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x826f3d60
	if (cr6.eq) goto loc_826F3D60;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// beq cr6,0x826f3ca8
	if (cr6.eq) goto loc_826F3CA8;
	// cmpwi cr6,r22,4
	cr6.compare<int32_t>(r22.s32, 4, xer);
	// beq cr6,0x826f3ca8
	if (cr6.eq) goto loc_826F3CA8;
	// cmpwi cr6,r22,5
	cr6.compare<int32_t>(r22.s32, 5, xer);
	// beq cr6,0x826f3ca8
	if (cr6.eq) goto loc_826F3CA8;
	// lwz r28,184(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmpwi cr6,r22,1
	cr6.compare<int32_t>(r22.s32, 1, xer);
	// bne cr6,0x826f3c40
	if (!cr6.eq) goto loc_826F3C40;
	// rlwinm r7,r27,2,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFF8;
	// lwz r8,220(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r7,r31
	ctx.r7.s64 = r31.s64 - ctx.r7.s64;
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// clrlwi r7,r7,26
	ctx.r7.u64 = ctx.r7.u32 & 0x3F;
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,16(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// rlwinm r4,r6,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lwzx r7,r4,r28
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + r28.u32);
	// lwz r8,16(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// mullw r6,r8,r30
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// mullw r8,r7,r8
	ctx.r8.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// mullw r8,r8,r3
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r3.s32);
	// mullw r7,r7,r6
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// add r8,r8,r14
	ctx.r8.u64 = ctx.r8.u64 + r14.u64;
	// add r7,r7,r14
	ctx.r7.u64 = ctx.r7.u64 + r14.u64;
	// srawi r3,r8,18
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 18;
	// srawi r30,r7,18
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFFF) != 0);
	r30.s64 = ctx.r7.s32 >> 18;
	// b 0x826f3d64
	goto loc_826F3D64;
loc_826F3C40:
	// cmpwi cr6,r22,2
	cr6.compare<int32_t>(r22.s32, 2, xer);
	// bne cr6,0x826f3d64
	if (!cr6.eq) goto loc_826F3D64;
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r7,-8(r31)
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + -8);
	// lwz r8,220(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// clrlwi r7,r7,26
	ctx.r7.u64 = ctx.r7.u32 & 0x3F;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r7,16(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r7,r28
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + r28.u32);
	// lwz r7,16(r6)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// mullw r6,r7,r29
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(r29.s32);
	// mullw r7,r8,r7
	ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// mullw r7,r7,r3
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r3.s32);
	// mullw r8,r8,r6
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// add r7,r7,r14
	ctx.r7.u64 = ctx.r7.u64 + r14.u64;
	// add r8,r8,r14
	ctx.r8.u64 = ctx.r8.u64 + r14.u64;
	// srawi r3,r7,18
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 18;
	// srawi r29,r8,18
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFFF) != 0);
	r29.s64 = ctx.r8.s32 >> 18;
	// b 0x826f3d64
	goto loc_826F3D64;
loc_826F3CA8:
	// rlwinm r7,r27,2,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFF8;
	// lbz r6,-8(r31)
	ctx.r6.u64 = PPC_LOAD_U8(r31.u32 + -8);
	// lwz r8,220(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r7,r31
	ctx.r7.s64 = r31.s64 - ctx.r7.s64;
	// clrlwi r6,r6,26
	ctx.r6.u64 = ctx.r6.u32 & 0x3F;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// rlwinm r28,r4,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r5,-8(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + -8);
	// lbz r17,0(r7)
	r17.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r5,r5,26
	ctx.r5.u64 = ctx.r5.u32 & 0x3F;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r28,r28,r8
	r28.u64 = r28.u64 + ctx.r8.u64;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r4,r17,26
	ctx.r4.u64 = r17.u32 & 0x3F;
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lwz r7,16(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// lwz r28,184(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,16(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// mullw r6,r6,r29
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r29.s32);
	// lwzx r7,r7,r28
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + r28.u32);
	// rlwinm r29,r5,2,0,29
	r29.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r29
	ctx.r5.u64 = ctx.r5.u64 + r29.u64;
	// rlwinm r29,r4,2,0,29
	r29.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r29
	ctx.r4.u64 = ctx.r4.u64 + r29.u64;
	// add r29,r5,r8
	r29.u64 = ctx.r5.u64 + ctx.r8.u64;
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// lwz r8,16(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// mullw r4,r7,r8
	ctx.r4.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// lwz r8,16(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// mullw r8,r7,r8
	ctx.r8.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// mullw r5,r4,r3
	ctx.r5.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r3.s32);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// mullw r7,r7,r6
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// add r6,r5,r14
	ctx.r6.u64 = ctx.r5.u64 + r14.u64;
	// add r8,r8,r14
	ctx.r8.u64 = ctx.r8.u64 + r14.u64;
	// add r7,r7,r14
	ctx.r7.u64 = ctx.r7.u64 + r14.u64;
	// srawi r3,r6,18
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = ctx.r6.s32 >> 18;
	// srawi r30,r8,18
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFFF) != 0);
	r30.s64 = ctx.r8.s32 >> 18;
	// srawi r29,r7,18
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFFF) != 0);
	r29.s64 = ctx.r7.s32 >> 18;
	// lis r14,2
	r14.s64 = 131072;
	// b 0x826f3d64
	goto loc_826F3D64;
loc_826F3D60:
	// lwz r28,184(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
loc_826F3D64:
	// subf r7,r30,r3
	ctx.r7.s64 = ctx.r3.s64 - r30.s64;
	// subf r8,r29,r3
	ctx.r8.s64 = ctx.r3.s64 - r29.s64;
	// srawi r6,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// srawi r5,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 31;
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// xor r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r5.u64;
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f3d9c
	if (!cr6.lt) goto loc_826F3D9C;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// li r23,8
	r23.s64 = 8;
	// b 0x826f3d9c
	goto loc_826F3D9C;
loc_826F3D98:
	// lwz r28,184(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
loc_826F3D9C:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826f3fc0
	if (cr6.eq) goto loc_826F3FC0;
	// lbz r8,176(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 176);
	// lbz r7,27(r25)
	ctx.r7.u64 = PPC_LOAD_U8(r25.u32 + 27);
	// neg r8,r8
	ctx.r8.s64 = -ctx.r8.s64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// or r23,r8,r23
	r23.u64 = ctx.r8.u64 | r23.u64;
	// beq cr6,0x826f3fb4
	if (cr6.eq) goto loc_826F3FB4;
	// cmplw cr6,r9,r24
	cr6.compare<uint32_t>(ctx.r9.u32, r24.u32, xer);
	// bne cr6,0x826f3eb8
	if (!cr6.eq) goto loc_826F3EB8;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// beq cr6,0x826f3e0c
	if (cr6.eq) goto loc_826F3E0C;
	// cmpwi cr6,r22,2
	cr6.compare<int32_t>(r22.s32, 2, xer);
	// beq cr6,0x826f3e0c
	if (cr6.eq) goto loc_826F3E0C;
	// cmpwi cr6,r22,4
	cr6.compare<int32_t>(r22.s32, 4, xer);
	// beq cr6,0x826f3e0c
	if (cr6.eq) goto loc_826F3E0C;
	// cmpwi cr6,r22,5
	cr6.compare<int32_t>(r22.s32, 5, xer);
	// beq cr6,0x826f3e0c
	if (cr6.eq) goto loc_826F3E0C;
	// addi r11,r1,560
	r11.s64 = ctx.r1.s64 + 560;
	// li r8,16
	ctx.r8.s64 = 16;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_826F3DF0:
	// lhz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x826f3df0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826F3DF0;
	// addi r9,r1,560
	ctx.r9.s64 = ctx.r1.s64 + 560;
	// b 0x826f3fc0
	goto loc_826F3FC0;
loc_826F3E0C:
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r8,-8(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + -8);
	// lwz r5,220(r25)
	ctx.r5.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// addi r6,r1,562
	ctx.r6.s64 = ctx.r1.s64 + 562;
	// clrlwi r8,r8,26
	ctx.r8.u64 = ctx.r8.u32 & 0x3F;
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r31,r7
	r31.s64 = ctx.r7.s16;
	// lwzx r3,r4,r28
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + r28.u32);
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r9,2
	ctx.r7.s64 = ctx.r9.s64 + 2;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// li r9,15
	ctx.r9.s64 = 15;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r5
	ctx.r4.u64 = ctx.r4.u64 + ctx.r5.u64;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// lwz r4,16(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwzx r5,r5,r28
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + r28.u32);
	// mullw r11,r11,r31
	r11.s64 = int64_t(r11.s32) * int64_t(r31.s32);
	// mullw r11,r5,r11
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// add r11,r11,r14
	r11.u64 = r11.u64 + r14.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// sth r11,560(r1)
	PPC_STORE_U16(ctx.r1.u32 + 560, r11.u16);
loc_826F3E78:
	// lhz r11,0(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// mullw r11,r3,r11
	r11.s64 = int64_t(ctx.r3.s32) * int64_t(r11.s32);
	// add r11,r11,r14
	r11.u64 = r11.u64 + r14.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// sth r11,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, r11.u16);
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// bne cr6,0x826f3e78
	if (!cr6.eq) goto loc_826F3E78;
	// lhz r11,560(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 560);
	// addi r9,r1,560
	ctx.r9.s64 = ctx.r1.s64 + 560;
	// sth r11,576(r1)
	PPC_STORE_U16(ctx.r1.u32 + 576, r11.u16);
	// b 0x826f3fc0
	goto loc_826F3FC0;
loc_826F3EB8:
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// beq cr6,0x826f3f00
	if (cr6.eq) goto loc_826F3F00;
	// cmpwi cr6,r22,1
	cr6.compare<int32_t>(r22.s32, 1, xer);
	// beq cr6,0x826f3f00
	if (cr6.eq) goto loc_826F3F00;
	// cmpwi cr6,r22,4
	cr6.compare<int32_t>(r22.s32, 4, xer);
	// beq cr6,0x826f3f00
	if (cr6.eq) goto loc_826F3F00;
	// cmpwi cr6,r22,5
	cr6.compare<int32_t>(r22.s32, 5, xer);
	// beq cr6,0x826f3f00
	if (cr6.eq) goto loc_826F3F00;
	// addi r11,r1,560
	r11.s64 = ctx.r1.s64 + 560;
	// li r8,16
	ctx.r8.s64 = 16;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_826F3EE4:
	// lhz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x826f3ee4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826F3EE4;
	// addi r9,r1,576
	ctx.r9.s64 = ctx.r1.s64 + 576;
	// b 0x826f3fc0
	goto loc_826F3FC0;
loc_826F3F00:
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,220(r25)
	ctx.r5.u64 = PPC_LOAD_U32(r25.u32 + 220);
	// rlwinm r8,r27,2,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFF8;
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r6,r1,562
	ctx.r6.s64 = ctx.r1.s64 + 562;
	// subf r8,r8,r31
	ctx.r8.s64 = r31.s64 - ctx.r8.s64;
	// extsh r31,r7
	r31.s64 = ctx.r7.s16;
	// lwzx r3,r4,r28
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + r28.u32);
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r9,2
	ctx.r7.s64 = ctx.r9.s64 + 2;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lbz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// li r9,15
	ctx.r9.s64 = 15;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r8,r8,26
	ctx.r8.u64 = ctx.r8.u32 & 0x3F;
	// add r4,r11,r5
	ctx.r4.u64 = r11.u64 + ctx.r5.u64;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,16(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwzx r5,r4,r28
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + r28.u32);
	// mullw r11,r11,r31
	r11.s64 = int64_t(r11.s32) * int64_t(r31.s32);
	// mullw r11,r5,r11
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// add r11,r11,r14
	r11.u64 = r11.u64 + r14.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// sth r11,560(r1)
	PPC_STORE_U16(ctx.r1.u32 + 560, r11.u16);
loc_826F3F74:
	// lhz r11,0(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// mullw r11,r3,r11
	r11.s64 = int64_t(ctx.r3.s32) * int64_t(r11.s32);
	// add r11,r11,r14
	r11.u64 = r11.u64 + r14.u64;
	// srawi r11,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r11.s64 = r11.s32 >> 18;
	// sth r11,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, r11.u16);
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// bne cr6,0x826f3f74
	if (!cr6.eq) goto loc_826F3F74;
	// lhz r11,560(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 560);
	// addi r9,r1,576
	ctx.r9.s64 = ctx.r1.s64 + 576;
	// sth r11,576(r1)
	PPC_STORE_U16(ctx.r1.u32 + 576, r11.u16);
	// b 0x826f3fc0
	goto loc_826F3FC0;
loc_826F3FB4:
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x826f3fc0
	if (!cr6.eq) goto loc_826F3FC0;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
loc_826F3FC0:
	// lwz r6,788(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r11,28(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	// beq cr6,0x826f416c
	if (cr6.eq) goto loc_826F416C;
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmpwi cr6,r23,1
	cr6.compare<int32_t>(r23.s32, 1, xer);
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// sth r8,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r8.u16);
	// sth r8,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r8.u16);
	// bne cr6,0x826f40a0
	if (!cr6.eq) goto loc_826F40A0;
	// lhz r8,2(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lhz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r8.u16);
	// sth r8,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r8.u16);
	// lhz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lhz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,4(r11)
	PPC_STORE_U16(r11.u32 + 4, ctx.r8.u16);
	// sth r8,4(r10)
	PPC_STORE_U16(ctx.r10.u32 + 4, ctx.r8.u16);
	// lhz r8,6(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// lhz r7,6(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,6(r11)
	PPC_STORE_U16(r11.u32 + 6, ctx.r8.u16);
	// sth r8,6(r10)
	PPC_STORE_U16(ctx.r10.u32 + 6, ctx.r8.u16);
	// lhz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 8);
	// lhz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,8(r11)
	PPC_STORE_U16(r11.u32 + 8, ctx.r8.u16);
	// sth r8,8(r10)
	PPC_STORE_U16(ctx.r10.u32 + 8, ctx.r8.u16);
	// lhz r8,10(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 10);
	// lhz r7,10(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,10(r11)
	PPC_STORE_U16(r11.u32 + 10, ctx.r8.u16);
	// sth r8,10(r10)
	PPC_STORE_U16(ctx.r10.u32 + 10, ctx.r8.u16);
	// lhz r8,12(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 12);
	// lhz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 12);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,12(r11)
	PPC_STORE_U16(r11.u32 + 12, ctx.r8.u16);
	// sth r8,12(r10)
	PPC_STORE_U16(ctx.r10.u32 + 12, ctx.r8.u16);
	// lhz r9,14(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 14);
	// lhz r8,14(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// sth r9,14(r11)
	PPC_STORE_U16(r11.u32 + 14, ctx.r9.u16);
	// sth r9,14(r10)
	PPC_STORE_U16(ctx.r10.u32 + 14, ctx.r9.u16);
	// b 0x826f4190
	goto loc_826F4190;
loc_826F40A0:
	// cmpwi cr6,r23,8
	cr6.compare<int32_t>(r23.s32, 8, xer);
	// bne cr6,0x826f4178
	if (!cr6.eq) goto loc_826F4178;
	// lhz r8,2(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// sth r8,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r8.u16);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// ld r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// std r8,8(r10)
	PPC_STORE_U64(ctx.r10.u32 + 8, ctx.r8.u64);
	// lhz r8,2(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lhz r7,16(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 16);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,16(r11)
	PPC_STORE_U16(r11.u32 + 16, ctx.r8.u16);
	// sth r8,18(r10)
	PPC_STORE_U16(ctx.r10.u32 + 18, ctx.r8.u16);
	// lhz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lhz r7,32(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,32(r11)
	PPC_STORE_U16(r11.u32 + 32, ctx.r8.u16);
	// sth r8,20(r10)
	PPC_STORE_U16(ctx.r10.u32 + 20, ctx.r8.u16);
	// lhz r8,6(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// lhz r7,48(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 48);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,48(r11)
	PPC_STORE_U16(r11.u32 + 48, ctx.r8.u16);
	// sth r8,22(r10)
	PPC_STORE_U16(ctx.r10.u32 + 22, ctx.r8.u16);
	// lhz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 8);
	// lhz r7,64(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 64);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,64(r11)
	PPC_STORE_U16(r11.u32 + 64, ctx.r8.u16);
	// sth r8,24(r10)
	PPC_STORE_U16(ctx.r10.u32 + 24, ctx.r8.u16);
	// lhz r8,10(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 10);
	// lhz r7,80(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 80);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,80(r11)
	PPC_STORE_U16(r11.u32 + 80, ctx.r8.u16);
	// sth r8,26(r10)
	PPC_STORE_U16(ctx.r10.u32 + 26, ctx.r8.u16);
	// lhz r8,12(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 12);
	// lhz r7,96(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 96);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,96(r11)
	PPC_STORE_U16(r11.u32 + 96, ctx.r8.u16);
	// sth r8,28(r10)
	PPC_STORE_U16(ctx.r10.u32 + 28, ctx.r8.u16);
	// lhz r9,14(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 14);
	// lhz r8,112(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 112);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// sth r9,112(r11)
	PPC_STORE_U16(r11.u32 + 112, ctx.r9.u16);
	// sth r9,30(r10)
	PPC_STORE_U16(ctx.r10.u32 + 30, ctx.r9.u16);
	// b 0x826f41c8
	goto loc_826F41C8;
loc_826F416C:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// sth r9,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r9.u16);
loc_826F4178:
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// sth r9,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r9.u16);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r9,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r9.u32);
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// std r9,8(r10)
	PPC_STORE_U64(ctx.r10.u32 + 8, ctx.r9.u64);
loc_826F4190:
	// lhz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 16);
	// sth r9,18(r10)
	PPC_STORE_U16(ctx.r10.u32 + 18, ctx.r9.u16);
	// lhz r9,32(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 32);
	// sth r9,20(r10)
	PPC_STORE_U16(ctx.r10.u32 + 20, ctx.r9.u16);
	// lhz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 48);
	// sth r9,22(r10)
	PPC_STORE_U16(ctx.r10.u32 + 22, ctx.r9.u16);
	// lhz r9,64(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 64);
	// sth r9,24(r10)
	PPC_STORE_U16(ctx.r10.u32 + 24, ctx.r9.u16);
	// lhz r9,80(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 80);
	// sth r9,26(r10)
	PPC_STORE_U16(ctx.r10.u32 + 26, ctx.r9.u16);
	// lhz r9,96(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 96);
	// sth r9,28(r10)
	PPC_STORE_U16(ctx.r10.u32 + 28, ctx.r9.u16);
	// lhz r11,112(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 112);
	// sth r11,30(r10)
	PPC_STORE_U16(ctx.r10.u32 + 30, r11.u16);
loc_826F41C8:
	// rlwinm r11,r21,3,0,28
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r24,316(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// extsw r10,r21
	ctx.r10.s64 = r21.s32;
	// ld r9,552(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 552);
	// or r11,r11,r22
	r11.u64 = r11.u64 | r22.u64;
	// lwz r15,212(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// ori r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 128;
	// lwz r8,32(r6)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + 32);
	// rlwinm r11,r11,12,0,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFFFF000;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// or r11,r11,r24
	r11.u64 = r11.u64 | r24.u64;
	// rldicr r10,r10,8,55
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// rlwinm r9,r11,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// addi r11,r22,1
	r11.s64 = r22.s64 + 1;
	// or r9,r9,r15
	ctx.r9.u64 = ctx.r9.u64 | r15.u64;
	// srawi r16,r16,1
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x1) != 0);
	r16.s64 = r16.s32 >> 1;
	// std r10,552(r1)
	PPC_STORE_U64(ctx.r1.u32 + 552, ctx.r10.u64);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// stw r11,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, r11.u32);
	// stw r9,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
	// lwz r9,32(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stw r9,32(r6)
	PPC_STORE_U32(ctx.r6.u32 + 32, ctx.r9.u32);
	// blt cr6,0x826f35bc
	if (cr6.lt) goto loc_826F35BC;
	// lwz r17,264(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// rldicl r8,r10,56,8
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u64, 56) & 0xFFFFFFFFFFFFFF;
	// lbz r9,1260(r25)
	ctx.r9.u64 = PPC_LOAD_U8(r25.u32 + 1260);
	// li r29,0
	r29.s64 = 0;
	// lwz r6,788(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// lbz r7,5(r17)
	ctx.r7.u64 = PPC_LOAD_U8(r17.u32 + 5);
	// rlwinm r10,r11,24,29,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0x7;
	// lbz r11,4(r17)
	r11.u64 = PPC_LOAD_U8(r17.u32 + 4);
	// rldicr r5,r11,8,63
	ctx.r5.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r10,4(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// neg r4,r11
	ctx.r4.s64 = -r11.s64;
	// xor r11,r4,r11
	r11.u64 = ctx.r4.u64 ^ r11.u64;
	// srawi r11,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 31;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// or r11,r7,r11
	r11.u64 = ctx.r7.u64 | r11.u64;
	// clrldi r11,r11,56
	r11.u64 = r11.u64 & 0xFF;
	// or r11,r5,r11
	r11.u64 = ctx.r5.u64 | r11.u64;
	// rldicr r11,r11,48,15
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFF000000000000;
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// b 0x826f96a0
	goto loc_826F96A0;
loc_826F4284:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826f51b4
	if (!cr6.eq) goto loc_826F51B4;
	// lhz r21,50(r25)
	r21.u64 = PPC_LOAD_U16(r25.u32 + 50);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// lwz r22,0(r23)
	r22.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// srawi r29,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	r29.s64 = r21.s32 >> 1;
	// stw r21,500(r1)
	PPC_STORE_U32(ctx.r1.u32 + 500, r21.u32);
	// stw r22,444(r1)
	PPC_STORE_U32(ctx.r1.u32 + 444, r22.u32);
	// beq cr6,0x826f42c0
	if (cr6.eq) goto loc_826F42C0;
	// lwz r11,1240(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1240);
	// rlwinm r10,r24,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r30,r19
	r30.u64 = r19.u64;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f42c4
	if (cr6.eq) goto loc_826F42C4;
loc_826F42C0:
	// li r30,1
	r30.s64 = 1;
loc_826F42C4:
	// li r19,0
	r19.s64 = 0;
	// lwz r24,188(r25)
	r24.u64 = PPC_LOAD_U32(r25.u32 + 188);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// mr r26,r19
	r26.u64 = r19.u64;
	// mr r27,r19
	r27.u64 = r19.u64;
	// mr r23,r19
	r23.u64 = r19.u64;
	// stw r24,524(r1)
	PPC_STORE_U32(ctx.r1.u32 + 524, r24.u32);
	// mr r31,r19
	r31.u64 = r19.u64;
	// stw r26,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r26.u32);
	// stw r27,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r27.u32);
	// stw r23,336(r1)
	PPC_STORE_U32(ctx.r1.u32 + 336, r23.u32);
	// beq cr6,0x826f4328
	if (cr6.eq) goto loc_826F4328;
	// lwz r11,1508(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1508);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r4,176(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + 176);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f4314
	if (cr6.eq) goto loc_826F4314;
	// bl 0x826eb3a0
	sub_826EB3A0(ctx, base);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// b 0x826f4324
	goto loc_826F4324;
loc_826F4314:
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// rlwinm r11,r3,1,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFE0000;
	// clrlwi r10,r3,16
	ctx.r10.u64 = ctx.r3.u32 & 0xFFFF;
	// or r23,r11,r10
	r23.u64 = r11.u64 | ctx.r10.u64;
loc_826F4324:
	// stw r23,336(r1)
	PPC_STORE_U32(ctx.r1.u32 + 336, r23.u32);
loc_826F4328:
	// lwz r3,1516(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 1516);
	// lis r20,1
	r20.s64 = 65536;
	// stw r19,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r19.u32);
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// rlwinm r4,r3,17,0,14
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 17) & 0xFFFE0000;
	// stw r19,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r19.u32);
	// stw r19,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r19.u32);
	// subf r28,r4,r20
	r28.s64 = r20.s64 - ctx.r4.s64;
	// stw r19,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r19.u32);
	// stw r19,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r19.u32);
	// stw r19,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, r19.u32);
	// stw r19,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, r19.u32);
	// stw r19,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, r19.u32);
	// stw r28,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, r28.u32);
	// beq cr6,0x826f4520
	if (cr6.eq) goto loc_826F4520;
	// addi r11,r22,-1
	r11.s64 = r22.s64 + -1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r24
	r11.u64 = PPC_LOAD_U32(r11.u32 + r24.u32);
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826f4520
	if (cr6.eq) goto loc_826F4520;
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f44d4
	if (cr6.eq) goto loc_826F44D4;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// stw r11,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r11.u32);
	// srawi r8,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 16;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r6,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// xor r11,r7,r6
	r11.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// bgt cr6,0x826f43e8
	if (cr6.gt) goto loc_826F43E8;
	// lwz r7,1560(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1560);
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f43cc
	if (!cr6.lt) goto loc_826F43CC;
	// lwz r11,1576(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f43e8
	goto loc_826F43E8;
loc_826F43CC:
	// lwz r11,1580(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// lwz r7,1568(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F43E8:
	// lhz r11,62(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f4400
	if (!cr6.gt) goto loc_826F4400;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f4410
	goto loc_826F4410;
loc_826F4400:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f4410
	if (!cr6.lt) goto loc_826F4410;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F4410:
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f4444
	if (!cr6.gt) goto loc_826F4444;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f4488
	goto loc_826F4488;
loc_826F4444:
	// lwz r7,1564(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f4464
	if (!cr6.lt) goto loc_826F4464;
	// lwz r9,1576(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f4488
	goto loc_826F4488;
loc_826F4464:
	// lwz r8,1572(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1572);
	// lwz r7,1580(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F4488:
	// lhz r9,64(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f44b0
	if (!cr6.gt) goto loc_826F44B0;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// li r27,1
	r27.s64 = 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r27,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r27.u32);
	// stw r10,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r10.u32);
	// b 0x826f4514
	goto loc_826F4514;
loc_826F44B0:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f44c0
	if (!cr6.lt) goto loc_826F44C0;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F44C0:
	// li r27,1
	r27.s64 = 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r27,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r27.u32);
	// stw r10,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r10.u32);
	// b 0x826f4514
	goto loc_826F4514;
loc_826F44D4:
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// lwz r10,1584(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1584);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// rlwinm r8,r8,0,0,22
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFE00;
	// li r26,1
	r26.s64 = 1;
	// mullw r11,r8,r10
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// stw r26,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r26.u32);
	// rlwinm r11,r11,0,0,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFE0000;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// add r11,r11,r20
	r11.u64 = r11.u64 + r20.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r11.u32);
loc_826F4514:
	// li r11,1
	r11.s64 = 1;
	// li r31,1
	r31.s64 = 1;
	// stw r11,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, r11.u32);
loc_826F4520:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x826f48d0
	if (!cr6.eq) goto loc_826F48D0;
	// subf r5,r21,r22
	ctx.r5.s64 = r22.s64 - r21.s64;
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r24
	r11.u64 = PPC_LOAD_U32(r11.u32 + r24.u32);
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826f46f0
	if (cr6.eq) goto loc_826F46F0;
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f4694
	if (cr6.eq) goto loc_826F4694;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r7,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r11.s32 >> 16;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r30,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = ctx.r8.s32 >> 31;
	// rlwinm r6,r31,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// xor r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 ^ r30.u64;
	// addi r18,r1,192
	r18.s64 = ctx.r1.s64 + 192;
	// subf r8,r30,r8
	ctx.r8.s64 = ctx.r8.s64 - r30.s64;
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// stwx r11,r6,r18
	PPC_STORE_U32(ctx.r6.u32 + r18.u32, r11.u32);
	// bgt cr6,0x826f45b4
	if (cr6.gt) goto loc_826F45B4;
	// lwz r11,1560(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1560);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x826f4598
	if (!cr6.lt) goto loc_826F4598;
	// lwz r11,1576(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f45b4
	goto loc_826F45B4;
loc_826F4598:
	// lwz r11,1580(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// lwz r8,1568(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F45B4:
	// lhz r11,62(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f45cc
	if (!cr6.gt) goto loc_826F45CC;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f45dc
	goto loc_826F45DC;
loc_826F45CC:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f45dc
	if (!cr6.lt) goto loc_826F45DC;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F45DC:
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f4610
	if (!cr6.gt) goto loc_826F4610;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f4654
	goto loc_826F4654;
loc_826F4610:
	// lwz r7,1564(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f4630
	if (!cr6.lt) goto loc_826F4630;
	// lwz r9,1576(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f4654
	goto loc_826F4654;
loc_826F4630:
	// lwz r8,1572(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1572);
	// lwz r7,1580(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F4654:
	// lhz r9,64(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f466c
	if (!cr6.gt) goto loc_826F466C;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x826f467c
	goto loc_826F467C;
loc_826F466C:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f467c
	if (!cr6.lt) goto loc_826F467C;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F467C:
	// addi r9,r1,224
	ctx.r9.s64 = ctx.r1.s64 + 224;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// stwx r10,r6,r9
	PPC_STORE_U32(ctx.r6.u32 + ctx.r9.u32, ctx.r10.u32);
	// stw r27,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r27.u32);
	// b 0x826f46e0
	goto loc_826F46E0;
loc_826F4694:
	// srawi r7,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r7.s64 = r11.s32 >> 8;
	// lwz r10,1584(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1584);
	// rlwinm r9,r31,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,224
	ctx.r6.s64 = ctx.r1.s64 + 224;
	// rlwinm r7,r7,0,0,22
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFE00;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// mullw r7,r7,r10
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// stwx r11,r9,r6
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, r11.u32);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r7,0,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFE0000;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// add r11,r11,r20
	r11.u64 = r11.u64 + r20.u64;
	// addi r30,r1,192
	r30.s64 = ctx.r1.s64 + 192;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r26,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r26.u32);
	// stwx r11,r9,r30
	PPC_STORE_U32(ctx.r9.u32 + r30.u32, r11.u32);
loc_826F46E0:
	// li r11,1
	r11.s64 = 1;
	// stw r31,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, r31.u32);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stw r11,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, r11.u32);
loc_826F46F0:
	// cmpwi cr6,r29,1
	cr6.compare<int32_t>(r29.s32, 1, xer);
	// ble cr6,0x826f48d0
	if (!cr6.gt) goto loc_826F48D0;
	// addi r11,r29,-1
	r11.s64 = r29.s64 + -1;
	// cmpw cr6,r15,r11
	cr6.compare<int32_t>(r15.s32, r11.s32, xer);
	// li r11,1
	r11.s64 = 1;
	// blt cr6,0x826f470c
	if (cr6.lt) goto loc_826F470C;
	// mr r11,r19
	r11.u64 = r19.u64;
loc_826F470C:
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r24
	r11.u64 = PPC_LOAD_U32(r11.u32 + r24.u32);
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826f48d0
	if (cr6.eq) goto loc_826F48D0;
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f4880
	if (cr6.eq) goto loc_826F4880;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r7,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r11.s32 >> 16;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r5,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 31;
	// rlwinm r6,r31,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// xor r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r5.u64;
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// stwx r11,r6,r4
	PPC_STORE_U32(ctx.r6.u32 + ctx.r4.u32, r11.u32);
	// bgt cr6,0x826f47a0
	if (cr6.gt) goto loc_826F47A0;
	// lwz r11,1560(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1560);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x826f4784
	if (!cr6.lt) goto loc_826F4784;
	// lwz r11,1576(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f47a0
	goto loc_826F47A0;
loc_826F4784:
	// lwz r11,1580(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// lwz r8,1568(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F47A0:
	// lhz r11,62(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f47b8
	if (!cr6.gt) goto loc_826F47B8;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f47c8
	goto loc_826F47C8;
loc_826F47B8:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f47c8
	if (!cr6.lt) goto loc_826F47C8;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F47C8:
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f47fc
	if (!cr6.gt) goto loc_826F47FC;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f4840
	goto loc_826F4840;
loc_826F47FC:
	// lwz r7,1564(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f481c
	if (!cr6.lt) goto loc_826F481C;
	// lwz r9,1576(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f4840
	goto loc_826F4840;
loc_826F481C:
	// lwz r8,1572(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1572);
	// lwz r7,1580(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F4840:
	// lhz r9,64(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f4858
	if (!cr6.gt) goto loc_826F4858;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x826f4868
	goto loc_826F4868;
loc_826F4858:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f4868
	if (!cr6.lt) goto loc_826F4868;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F4868:
	// addi r9,r1,224
	ctx.r9.s64 = ctx.r1.s64 + 224;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// stwx r10,r6,r9
	PPC_STORE_U32(ctx.r6.u32 + ctx.r9.u32, ctx.r10.u32);
	// stw r27,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r27.u32);
	// b 0x826f48cc
	goto loc_826F48CC;
loc_826F4880:
	// srawi r7,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r7.s64 = r11.s32 >> 8;
	// lwz r10,1584(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1584);
	// rlwinm r9,r31,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,224
	ctx.r6.s64 = ctx.r1.s64 + 224;
	// rlwinm r7,r7,0,0,22
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFE00;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// mullw r7,r7,r10
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// stwx r11,r9,r6
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, r11.u32);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r7,0,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFE0000;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// add r11,r11,r20
	r11.u64 = r11.u64 + r20.u64;
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r26,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r26.u32);
	// stwx r11,r9,r5
	PPC_STORE_U32(ctx.r9.u32 + ctx.r5.u32, r11.u32);
loc_826F48CC:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
loc_826F48D0:
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// lwz r11,1320(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1320);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// stw r3,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, ctx.r3.u32);
	// beq cr6,0x826f48f4
	if (cr6.eq) goto loc_826F48F4;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x826f48f4
	if (cr6.eq) goto loc_826F48F4;
	// stw r19,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, r19.u32);
	// b 0x826f48fc
	goto loc_826F48FC;
loc_826F48F4:
	// li r11,1
	r11.s64 = 1;
	// stw r11,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, r11.u32);
loc_826F48FC:
	// li r4,0
	ctx.r4.s64 = 0;
	// lhz r16,226(r1)
	r16.u64 = PPC_LOAD_U16(ctx.r1.u32 + 226);
	// li r5,0
	ctx.r5.s64 = 0;
	// lhz r15,224(r1)
	r15.u64 = PPC_LOAD_U16(ctx.r1.u32 + 224);
	// lhz r14,194(r1)
	r14.u64 = PPC_LOAD_U16(ctx.r1.u32 + 194);
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// stw r4,516(r1)
	PPC_STORE_U32(ctx.r1.u32 + 516, ctx.r4.u32);
	// stw r5,428(r1)
	PPC_STORE_U32(ctx.r1.u32 + 428, ctx.r5.u32);
	// ble cr6,0x826f4a8c
	if (!cr6.gt) goto loc_826F4A8C;
	// lhz r11,234(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 234);
	// extsh r9,r16
	ctx.r9.s64 = r16.s16;
	// lhz r10,230(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 230);
	// extsh r6,r15
	ctx.r6.s64 = r15.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r8,232(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 232);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r7,228(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 228);
	// lhz r5,202(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 202);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r28,r9,r10
	r28.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lhz r4,198(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 198);
	// subf r27,r11,r10
	r27.s64 = ctx.r10.s64 - r11.s64;
	// lhz r31,200(r1)
	r31.u64 = PPC_LOAD_U16(ctx.r1.u32 + 200);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r30,196(r1)
	r30.u64 = PPC_LOAD_U16(ctx.r1.u32 + 196);
	// subf r26,r9,r11
	r26.s64 = r11.s64 - ctx.r9.s64;
	// lhz r29,192(r1)
	r29.u64 = PPC_LOAD_U16(ctx.r1.u32 + 192);
	// xor r24,r27,r28
	r24.u64 = r27.u64 ^ r28.u64;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// extsh r3,r14
	ctx.r3.s64 = r14.s16;
	// subf r27,r6,r7
	r27.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r25,r8,r7
	r25.s64 = ctx.r7.s64 - ctx.r8.s64;
	// subf r23,r6,r8
	r23.s64 = ctx.r8.s64 - ctx.r6.s64;
	// xor r22,r26,r28
	r22.u64 = r26.u64 ^ r28.u64;
	// subf r28,r3,r4
	r28.s64 = ctx.r4.s64 - ctx.r3.s64;
	// subf r21,r5,r4
	r21.s64 = ctx.r4.s64 - ctx.r5.s64;
	// extsh r31,r31
	r31.s64 = r31.s16;
	// extsh r30,r30
	r30.s64 = r30.s16;
	// extsh r29,r29
	r29.s64 = r29.s16;
	// xor r20,r25,r27
	r20.u64 = r25.u64 ^ r27.u64;
	// srawi r26,r24,31
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7FFFFFFF) != 0);
	r26.s64 = r24.s32 >> 31;
	// xor r23,r23,r27
	r23.u64 = r23.u64 ^ r27.u64;
	// subf r19,r3,r5
	r19.s64 = ctx.r5.s64 - ctx.r3.s64;
	// srawi r27,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r27.s64 = r22.s32 >> 31;
	// xor r22,r21,r28
	r22.u64 = r21.u64 ^ r28.u64;
	// subf r25,r29,r30
	r25.s64 = r30.s64 - r29.s64;
	// subf r18,r31,r30
	r18.s64 = r30.s64 - r31.s64;
	// srawi r24,r20,31
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x7FFFFFFF) != 0);
	r24.s64 = r20.s32 >> 31;
	// subf r21,r29,r31
	r21.s64 = r31.s64 - r29.s64;
	// xor r20,r19,r28
	r20.u64 = r19.u64 ^ r28.u64;
	// xor r19,r18,r25
	r19.u64 = r18.u64 ^ r25.u64;
	// srawi r28,r23,31
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7FFFFFFF) != 0);
	r28.s64 = r23.s32 >> 31;
	// srawi r23,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r23.s64 = r22.s32 >> 31;
	// xor r21,r21,r25
	r21.u64 = r21.u64 ^ r25.u64;
	// srawi r25,r20,31
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x7FFFFFFF) != 0);
	r25.s64 = r20.s32 >> 31;
	// srawi r22,r19,31
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x7FFFFFFF) != 0);
	r22.s64 = r19.s32 >> 31;
	// and r9,r27,r9
	ctx.r9.u64 = r27.u64 & ctx.r9.u64;
	// srawi r21,r21,31
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x7FFFFFFF) != 0);
	r21.s64 = r21.s32 >> 31;
	// and r10,r26,r10
	ctx.r10.u64 = r26.u64 & ctx.r10.u64;
	// and r6,r28,r6
	ctx.r6.u64 = r28.u64 & ctx.r6.u64;
	// and r7,r24,r7
	ctx.r7.u64 = r24.u64 & ctx.r7.u64;
	// nor r20,r26,r27
	r20.u64 = ~(r26.u64 | r27.u64);
	// and r4,r23,r4
	ctx.r4.u64 = r23.u64 & ctx.r4.u64;
	// and r3,r25,r3
	ctx.r3.u64 = r25.u64 & ctx.r3.u64;
	// nor r19,r24,r28
	r19.u64 = ~(r24.u64 | r28.u64);
	// nor r18,r23,r25
	r18.u64 = ~(r23.u64 | r25.u64);
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// nor r17,r22,r21
	r17.u64 = ~(r22.u64 | r21.u64);
	// and r29,r21,r29
	r29.u64 = r21.u64 & r29.u64;
	// and r30,r22,r30
	r30.u64 = r22.u64 & r30.u64;
	// or r9,r6,r7
	ctx.r9.u64 = ctx.r6.u64 | ctx.r7.u64;
	// and r11,r20,r11
	r11.u64 = r20.u64 & r11.u64;
	// or r7,r3,r4
	ctx.r7.u64 = ctx.r3.u64 | ctx.r4.u64;
	// lwz r3,436(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// and r6,r18,r5
	ctx.r6.u64 = r18.u64 & ctx.r5.u64;
	// and r8,r19,r8
	ctx.r8.u64 = r19.u64 & ctx.r8.u64;
	// and r4,r17,r31
	ctx.r4.u64 = r17.u64 & r31.u64;
	// or r5,r29,r30
	ctx.r5.u64 = r29.u64 | r30.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 | ctx.r8.u64;
	// or r9,r7,r6
	ctx.r9.u64 = ctx.r7.u64 | ctx.r6.u64;
	// or r8,r5,r4
	ctx.r8.u64 = ctx.r5.u64 | ctx.r4.u64;
	// lwz r4,516(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	// sth r11,162(r1)
	PPC_STORE_U16(ctx.r1.u32 + 162, r11.u16);
	// sth r10,160(r1)
	PPC_STORE_U16(ctx.r1.u32 + 160, ctx.r10.u16);
	// sth r9,150(r1)
	PPC_STORE_U16(ctx.r1.u32 + 150, ctx.r9.u16);
	// sth r8,148(r1)
	PPC_STORE_U16(ctx.r1.u32 + 148, ctx.r8.u16);
	// lwz r7,160(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// lwz r6,148(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r25,780(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	// lwz r23,336(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	// lwz r27,292(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r24,524(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	// lwz r22,444(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// lwz r21,500(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	// lwz r17,264(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// lwz r5,428(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r26,272(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// b 0x826f4aac
	goto loc_826F4AAC;
loc_826F4A8C:
	// bne cr6,0x826f4a9c
	if (!cr6.eq) goto loc_826F4A9C;
	// lwz r7,224(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// lwz r6,192(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// b 0x826f4aa4
	goto loc_826F4AA4;
loc_826F4A9C:
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
loc_826F4AA4:
	// stw r6,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r6.u32);
	// stw r7,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r7.u32);
loc_826F4AAC:
	// lwz r11,396(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f4d40
	if (cr6.eq) goto loc_826F4D40;
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f4d40
	if (cr6.eq) goto loc_826F4D40;
	// lwz r11,412(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// addi r9,r1,192
	ctx.r9.s64 = ctx.r1.s64 + 192;
	// lwz r28,224(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r20,192(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r8,380(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r28,460(r1)
	PPC_STORE_U32(ctx.r1.u32 + 460, r28.u32);
	// lwzx r19,r11,r10
	r19.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r18,r11,r9
	r18.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// stw r20,540(r1)
	PPC_STORE_U32(ctx.r1.u32 + 540, r20.u32);
	// stw r19,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, r19.u32);
	// stw r18,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r18.u32);
	// bne cr6,0x826f4c20
	if (!cr6.eq) goto loc_826F4C20;
	// lhz r11,160(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 160);
	// extsh r9,r15
	ctx.r9.s64 = r15.s16;
	// lhz r10,162(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 162);
	// extsh r8,r16
	ctx.r8.s64 = r16.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// srawi r31,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r9.s32 >> 31;
	// srawi r30,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = ctx.r8.s32 >> 31;
	// xor r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 ^ r31.u64;
	// xor r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 ^ r30.u64;
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - r31.s64;
	// subf r8,r30,r8
	ctx.r8.s64 = ctx.r8.s64 - r30.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x826f4b88
	if (cr6.gt) goto loc_826F4B88;
	// lhz r9,252(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 252);
	// lhz r8,254(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 254);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f4b8c
	if (!cr6.gt) goto loc_826F4B8C;
loc_826F4B88:
	// li r4,1
	ctx.r4.s64 = 1;
loc_826F4B8C:
	// lhz r11,148(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 148);
	// extsh r8,r14
	ctx.r8.s64 = r14.s16;
	// lhz r9,192(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 192);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r10,150(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 150);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// srawi r31,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r9.s32 >> 31;
	// srawi r30,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = ctx.r8.s32 >> 31;
	// xor r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 ^ r31.u64;
	// xor r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 ^ r30.u64;
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - r31.s64;
	// subf r8,r30,r8
	ctx.r8.s64 = ctx.r8.s64 - r30.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x826f4c18
	if (cr6.gt) goto loc_826F4C18;
	// lhz r9,236(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 236);
	// lhz r8,238(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 238);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f4d50
	if (!cr6.gt) goto loc_826F4D50;
loc_826F4C18:
	// li r5,1
	ctx.r5.s64 = 1;
	// b 0x826f4d50
	goto loc_826F4D50;
loc_826F4C20:
	// lhz r11,162(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 162);
	// extsh r9,r16
	ctx.r9.s64 = r16.s16;
	// lhz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 160);
	// extsh r8,r15
	ctx.r8.s64 = r15.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r31,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r9.s32 >> 31;
	// srawi r30,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = ctx.r8.s32 >> 31;
	// xor r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 ^ r31.u64;
	// xor r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 ^ r30.u64;
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - r31.s64;
	// subf r8,r30,r8
	ctx.r8.s64 = ctx.r8.s64 - r30.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x826f4ca8
	if (cr6.gt) goto loc_826F4CA8;
	// lhz r9,254(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 254);
	// lhz r8,252(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 252);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f4cac
	if (!cr6.gt) goto loc_826F4CAC;
loc_826F4CA8:
	// li r4,1
	ctx.r4.s64 = 1;
loc_826F4CAC:
	// lhz r11,150(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 150);
	// extsh r9,r14
	ctx.r9.s64 = r14.s16;
	// lhz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 148);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r8,192(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 192);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r31,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r9.s32 >> 31;
	// srawi r30,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = ctx.r8.s32 >> 31;
	// xor r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 ^ r31.u64;
	// xor r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 ^ r30.u64;
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - r31.s64;
	// subf r8,r30,r8
	ctx.r8.s64 = ctx.r8.s64 - r30.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x826f4d38
	if (cr6.gt) goto loc_826F4D38;
	// lhz r9,238(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 238);
	// lhz r8,236(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 236);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f4d50
	if (!cr6.gt) goto loc_826F4D50;
loc_826F4D38:
	// li r5,1
	ctx.r5.s64 = 1;
	// b 0x826f4d50
	goto loc_826F4D50;
loc_826F4D40:
	// lwz r18,236(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// lwz r20,540(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	// lwz r19,252(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r28,460(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
loc_826F4D50:
	// lwz r11,1508(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1508);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f4d6c
	if (cr6.eq) goto loc_826F4D6C;
	// cmpw cr6,r26,r27
	cr6.compare<int32_t>(r26.s32, r27.s32, xer);
	// ble cr6,0x826f4d78
	if (!cr6.gt) goto loc_826F4D78;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f4d7c
	if (!cr6.eq) goto loc_826F4D7C;
loc_826F4D6C:
	// lwz r11,1512(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1512);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f4d7c
	if (cr6.eq) goto loc_826F4D7C;
loc_826F4D78:
	// li r3,1
	ctx.r3.s64 = 1;
loc_826F4D7C:
	// rlwinm r11,r23,0,15,15
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f4f68
	if (cr6.eq) goto loc_826F4F68;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826f4e7c
	if (!cr6.eq) goto loc_826F4E7C;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x826f4e70
	if (cr6.eq) goto loc_826F4E70;
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f4e0c
	if (!cr6.lt) goto loc_826F4E0C;
loc_826F4DB4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f4e0c
	if (cr6.eq) goto loc_826F4E0C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f4dfc
	if (!cr0.lt) goto loc_826F4DFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F4DFC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f4db4
	if (cr6.gt) goto loc_826F4DB4;
loc_826F4E0C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f4e48
	if (!cr0.lt) goto loc_826F4E48;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F4E48:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f4e64
	if (!cr6.eq) goto loc_826F4E64;
	// mr r11,r20
	r11.u64 = r20.u64;
	// addis r7,r11,1
	ctx.r7.s64 = r11.s64 + 65536;
	// b 0x826f5124
	goto loc_826F5124;
loc_826F4E64:
	// mr r11,r18
	r11.u64 = r18.u64;
	// addis r7,r11,1
	ctx.r7.s64 = r11.s64 + 65536;
	// b 0x826f5124
	goto loc_826F5124;
loc_826F4E70:
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// addis r7,r11,1
	ctx.r7.s64 = r11.s64 + 65536;
	// b 0x826f5124
	goto loc_826F5124;
loc_826F4E7C:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x826f4f5c
	if (cr6.eq) goto loc_826F4F5C;
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f4ef8
	if (!cr6.lt) goto loc_826F4EF8;
loc_826F4EA0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f4ef8
	if (cr6.eq) goto loc_826F4EF8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f4ee8
	if (!cr0.lt) goto loc_826F4EE8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F4EE8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f4ea0
	if (cr6.gt) goto loc_826F4EA0;
loc_826F4EF8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f4f34
	if (!cr0.lt) goto loc_826F4F34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F4F34:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f4f50
	if (!cr6.eq) goto loc_826F4F50;
	// mr r11,r28
	r11.u64 = r28.u64;
	// addis r7,r11,1
	ctx.r7.s64 = r11.s64 + 65536;
	// b 0x826f5124
	goto loc_826F5124;
loc_826F4F50:
	// mr r11,r19
	r11.u64 = r19.u64;
	// addis r7,r11,1
	ctx.r7.s64 = r11.s64 + 65536;
	// b 0x826f5124
	goto loc_826F5124;
loc_826F4F5C:
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// addis r7,r11,1
	ctx.r7.s64 = r11.s64 + 65536;
	// b 0x826f5124
	goto loc_826F5124;
loc_826F4F68:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826f5048
	if (!cr6.eq) goto loc_826F5048;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x826f5124
	if (cr6.eq) goto loc_826F5124;
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f4fec
	if (!cr6.lt) goto loc_826F4FEC;
loc_826F4F94:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f4fec
	if (cr6.eq) goto loc_826F4FEC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f4fdc
	if (!cr0.lt) goto loc_826F4FDC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F4FDC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f4f94
	if (cr6.gt) goto loc_826F4F94;
loc_826F4FEC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f5028
	if (!cr0.lt) goto loc_826F5028;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F5028:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f5040
	if (!cr6.eq) goto loc_826F5040;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// b 0x826f5124
	goto loc_826F5124;
loc_826F5040:
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// b 0x826f5124
	goto loc_826F5124;
loc_826F5048:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x826f5120
	if (cr6.eq) goto loc_826F5120;
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f50c4
	if (!cr6.lt) goto loc_826F50C4;
loc_826F506C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f50c4
	if (cr6.eq) goto loc_826F50C4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f50b4
	if (!cr0.lt) goto loc_826F50B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F50B4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f506c
	if (cr6.gt) goto loc_826F506C;
loc_826F50C4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f5100
	if (!cr0.lt) goto loc_826F5100;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F5100:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f5118
	if (!cr6.eq) goto loc_826F5118;
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// b 0x826f5124
	goto loc_826F5124;
loc_826F5118:
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// b 0x826f5124
	goto loc_826F5124;
loc_826F5120:
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
loc_826F5124:
	// stw r7,280(r1)
	PPC_STORE_U32(ctx.r1.u32 + 280, ctx.r7.u32);
	// add r8,r22,r21
	ctx.r8.u64 = r22.u64 + r21.u64;
	// stw r7,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r7.u32);
	// rlwinm r9,r22,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r7,280(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 280);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r11,62(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 62);
	// add r9,r9,r24
	ctx.r9.u64 = ctx.r9.u64 + r24.u64;
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// lhz r7,166(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 166);
	// lhz r10,64(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 64);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// lhz r4,66(r25)
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + 66);
	// srawi r7,r23,16
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r23.s32 >> 16;
	// lhz r3,68(r25)
	ctx.r3.u64 = PPC_LOAD_U16(r25.u32 + 68);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r6,r6,r23
	ctx.r6.u64 = ctx.r6.u64 + r23.u64;
	// extsh r5,r4
	ctx.r5.s64 = ctx.r4.s16;
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// extsh r4,r3
	ctx.r4.s64 = ctx.r3.s16;
	// and r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 & ctx.r5.u64;
	// and r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 & ctx.r4.u64;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// add r8,r8,r24
	ctx.r8.u64 = ctx.r8.u64 + r24.u64;
	// sth r11,282(r1)
	PPC_STORE_U16(ctx.r1.u32 + 282, r11.u16);
	// sth r10,280(r1)
	PPC_STORE_U16(ctx.r1.u32 + 280, ctx.r10.u16);
	// lwz r11,280(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	// stw r11,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r11.u32);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// b 0x826f8bbc
	goto loc_826F8BBC;
loc_826F51B4:
	// lhz r26,50(r25)
	r26.u64 = PPC_LOAD_U16(r25.u32 + 50);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// lwz r27,0(r23)
	r27.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// srawi r20,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	r20.s64 = r26.s32 >> 1;
	// stw r26,328(r1)
	PPC_STORE_U32(ctx.r1.u32 + 328, r26.u32);
	// stw r27,320(r1)
	PPC_STORE_U32(ctx.r1.u32 + 320, r27.u32);
	// stw r20,360(r1)
	PPC_STORE_U32(ctx.r1.u32 + 360, r20.u32);
	// beq cr6,0x826f51ec
	if (cr6.eq) goto loc_826F51EC;
	// lwz r11,1240(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1240);
	// rlwinm r10,r24,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r24,r19
	r24.u64 = r19.u64;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f51f0
	if (cr6.eq) goto loc_826F51F0;
loc_826F51EC:
	// li r24,1
	r24.s64 = 1;
loc_826F51F0:
	// lwz r11,180(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 180);
	// li r18,0
	r18.s64 = 0;
	// lhz r10,66(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 66);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// mr r23,r18
	r23.u64 = r18.u64;
	// lwz r22,188(r25)
	r22.u64 = PPC_LOAD_U32(r25.u32 + 188);
	// mr r21,r18
	r21.u64 = r18.u64;
	// stw r24,344(r1)
	PPC_STORE_U32(ctx.r1.u32 + 344, r24.u32);
	// mr r28,r18
	r28.u64 = r18.u64;
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lhz r11,62(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 62);
	// stw r10,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r10.u32);
	// lhz r10,68(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 68);
	// stw r22,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, r22.u32);
	// stw r23,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r23.u32);
	// stw r11,296(r1)
	PPC_STORE_U32(ctx.r1.u32 + 296, r11.u32);
	// lhz r11,64(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 64);
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// stw r21,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r21.u32);
	// stw r11,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, r11.u32);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826f5328
	if (cr6.lt) goto loc_826F5328;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f5320
	if (!cr6.lt) goto loc_826F5320;
loc_826F5288:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f52b4
	if (cr6.lt) goto loc_826F52B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f5288
	if (cr6.eq) goto loc_826F5288;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826f536c
	goto loc_826F536C;
loc_826F52B4:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F5320:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826f536c
	goto loc_826F536C;
loc_826F5328:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r19,r11,32768
	r19.u64 = r11.u64 | 32768;
loc_826F5338:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r19
	r11.u64 = r29.u64 + r19.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826f5338
	if (cr6.lt) goto loc_826F5338;
loc_826F536C:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// stw r29,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r29.u32);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f8d24
	if (!cr6.eq) goto loc_826F8D24;
	// rlwinm r11,r29,0,28,28
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x8;
	// stw r18,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r18.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f53c4
	if (cr6.eq) goto loc_826F53C4;
	// lwz r11,1508(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1508);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r4,176(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + 176);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f53b0
	if (cr6.eq) goto loc_826F53B0;
	// bl 0x826eb3a0
	sub_826EB3A0(ctx, base);
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// b 0x826f53c4
	goto loc_826F53C4;
loc_826F53B0:
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// rlwinm r11,r3,1,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFE0000;
	// clrlwi r10,r3,16
	ctx.r10.u64 = ctx.r3.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
loc_826F53C4:
	// lwz r3,1516(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 1516);
	// lis r30,1
	r30.s64 = 65536;
	// stw r18,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r18.u32);
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// rlwinm r4,r3,17,0,14
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 17) & 0xFFFE0000;
	// stw r18,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r18.u32);
	// stw r18,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r18.u32);
	// subf r31,r4,r30
	r31.s64 = r30.s64 - ctx.r4.s64;
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// stw r18,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r18.u32);
	// stw r18,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r18.u32);
	// stw r18,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r18.u32);
	// stw r18,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r18.u32);
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// beq cr6,0x826f55bc
	if (cr6.eq) goto loc_826F55BC;
	// addi r11,r27,-1
	r11.s64 = r27.s64 + -1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r22
	r11.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826f55bc
	if (cr6.eq) goto loc_826F55BC;
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f5570
	if (cr6.eq) goto loc_826F5570;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// srawi r8,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 16;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r6,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// xor r11,r7,r6
	r11.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// bgt cr6,0x826f5484
	if (cr6.gt) goto loc_826F5484;
	// lwz r7,1560(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1560);
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f5468
	if (!cr6.lt) goto loc_826F5468;
	// lwz r11,1576(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f5484
	goto loc_826F5484;
loc_826F5468:
	// lwz r11,1580(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// lwz r7,1568(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F5484:
	// lhz r11,62(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f549c
	if (!cr6.gt) goto loc_826F549C;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f54ac
	goto loc_826F54AC;
loc_826F549C:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f54ac
	if (!cr6.lt) goto loc_826F54AC;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F54AC:
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f54e0
	if (!cr6.gt) goto loc_826F54E0;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f5524
	goto loc_826F5524;
loc_826F54E0:
	// lwz r7,1564(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f5500
	if (!cr6.lt) goto loc_826F5500;
	// lwz r9,1576(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f5524
	goto loc_826F5524;
loc_826F5500:
	// lwz r8,1572(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1572);
	// lwz r7,1580(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F5524:
	// lhz r9,64(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f554c
	if (!cr6.gt) goto loc_826F554C;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// li r21,1
	r21.s64 = 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r21,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r21.u32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// b 0x826f55b0
	goto loc_826F55B0;
loc_826F554C:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f555c
	if (!cr6.lt) goto loc_826F555C;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F555C:
	// li r21,1
	r21.s64 = 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r21,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r21.u32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// b 0x826f55b0
	goto loc_826F55B0;
loc_826F5570:
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// lwz r10,1584(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1584);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// rlwinm r8,r8,0,0,22
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFE00;
	// li r23,1
	r23.s64 = 1;
	// mullw r11,r8,r10
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// stw r23,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r23.u32);
	// rlwinm r11,r11,0,0,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFE0000;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_826F55B0:
	// li r11,1
	r11.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// stw r11,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r11.u32);
loc_826F55BC:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x826f595c
	if (!cr6.eq) goto loc_826F595C;
	// subf r5,r26,r27
	ctx.r5.s64 = r27.s64 - r26.s64;
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r22
	r11.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826f578c
	if (cr6.eq) goto loc_826F578C;
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f5730
	if (cr6.eq) goto loc_826F5730;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r7,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r11.s32 >> 16;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r29,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = ctx.r8.s32 >> 31;
	// rlwinm r6,r28,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// xor r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 ^ r29.u64;
	// addi r27,r1,80
	r27.s64 = ctx.r1.s64 + 80;
	// subf r8,r29,r8
	ctx.r8.s64 = ctx.r8.s64 - r29.s64;
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// stwx r11,r6,r27
	PPC_STORE_U32(ctx.r6.u32 + r27.u32, r11.u32);
	// bgt cr6,0x826f5650
	if (cr6.gt) goto loc_826F5650;
	// lwz r11,1560(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1560);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x826f5634
	if (!cr6.lt) goto loc_826F5634;
	// lwz r11,1576(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f5650
	goto loc_826F5650;
loc_826F5634:
	// lwz r11,1580(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// lwz r8,1568(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F5650:
	// lhz r11,62(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f5668
	if (!cr6.gt) goto loc_826F5668;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f5678
	goto loc_826F5678;
loc_826F5668:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f5678
	if (!cr6.lt) goto loc_826F5678;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F5678:
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f56ac
	if (!cr6.gt) goto loc_826F56AC;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f56f0
	goto loc_826F56F0;
loc_826F56AC:
	// lwz r7,1564(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f56cc
	if (!cr6.lt) goto loc_826F56CC;
	// lwz r9,1576(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f56f0
	goto loc_826F56F0;
loc_826F56CC:
	// lwz r8,1572(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1572);
	// lwz r7,1580(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F56F0:
	// lhz r9,64(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f5708
	if (!cr6.gt) goto loc_826F5708;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x826f5718
	goto loc_826F5718;
loc_826F5708:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f5718
	if (!cr6.lt) goto loc_826F5718;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F5718:
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// stwx r10,r6,r9
	PPC_STORE_U32(ctx.r6.u32 + ctx.r9.u32, ctx.r10.u32);
	// stw r21,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r21.u32);
	// b 0x826f577c
	goto loc_826F577C;
loc_826F5730:
	// srawi r7,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r7.s64 = r11.s32 >> 8;
	// lwz r10,1584(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1584);
	// rlwinm r9,r28,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// rlwinm r7,r7,0,0,22
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFE00;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// mullw r7,r7,r10
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// stwx r11,r9,r6
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, r11.u32);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r7,0,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFE0000;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r29,r1,80
	r29.s64 = ctx.r1.s64 + 80;
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r23,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r23.u32);
	// stwx r11,r9,r29
	PPC_STORE_U32(ctx.r9.u32 + r29.u32, r11.u32);
loc_826F577C:
	// li r11,1
	r11.s64 = 1;
	// stw r28,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r28.u32);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// stw r11,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r11.u32);
loc_826F578C:
	// cmpwi cr6,r20,1
	cr6.compare<int32_t>(r20.s32, 1, xer);
	// ble cr6,0x826f595c
	if (!cr6.gt) goto loc_826F595C;
	// cntlzw r11,r15
	r11.u64 = r15.u32 == 0 ? 32 : __builtin_clz(r15.u32);
	// rlwinm r11,r11,28,30,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x2;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r22
	r11.u64 = PPC_LOAD_U32(r11.u32 + r22.u32);
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826f595c
	if (cr6.eq) goto loc_826F595C;
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f590c
	if (cr6.eq) goto loc_826F590C;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r7,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r11.s32 >> 16;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r5,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 31;
	// rlwinm r6,r28,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// xor r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r5.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// stwx r11,r6,r4
	PPC_STORE_U32(ctx.r6.u32 + ctx.r4.u32, r11.u32);
	// bgt cr6,0x826f582c
	if (cr6.gt) goto loc_826F582C;
	// lwz r11,1560(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1560);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x826f5810
	if (!cr6.lt) goto loc_826F5810;
	// lwz r11,1576(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f582c
	goto loc_826F582C;
loc_826F5810:
	// lwz r11,1580(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// lwz r8,1568(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F582C:
	// lhz r11,62(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f5844
	if (!cr6.gt) goto loc_826F5844;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f5854
	goto loc_826F5854;
loc_826F5844:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f5854
	if (!cr6.lt) goto loc_826F5854;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F5854:
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f5888
	if (!cr6.gt) goto loc_826F5888;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f58cc
	goto loc_826F58CC;
loc_826F5888:
	// lwz r7,1564(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f58a8
	if (!cr6.lt) goto loc_826F58A8;
	// lwz r9,1576(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f58cc
	goto loc_826F58CC;
loc_826F58A8:
	// lwz r8,1572(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 1572);
	// lwz r7,1580(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F58CC:
	// lhz r9,64(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f58e4
	if (!cr6.gt) goto loc_826F58E4;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x826f58f4
	goto loc_826F58F4;
loc_826F58E4:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f58f4
	if (!cr6.lt) goto loc_826F58F4;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F58F4:
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r11,r21,1
	r11.s64 = r21.s64 + 1;
	// stwx r10,r6,r9
	PPC_STORE_U32(ctx.r6.u32 + ctx.r9.u32, ctx.r10.u32);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// b 0x826f5958
	goto loc_826F5958;
loc_826F590C:
	// srawi r7,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r7.s64 = r11.s32 >> 8;
	// lwz r10,1584(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1584);
	// rlwinm r9,r28,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// rlwinm r7,r7,0,0,22
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFE00;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// mullw r7,r7,r10
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// stwx r11,r9,r6
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, r11.u32);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r7,0,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFE0000;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r3,r23,1
	ctx.r3.s64 = r23.s64 + 1;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// stwx r11,r9,r5
	PPC_STORE_U32(ctx.r9.u32 + ctx.r5.u32, r11.u32);
loc_826F5958:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
loc_826F595C:
	// lwz r11,1320(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1320);
	// stw r18,520(r1)
	PPC_STORE_U32(ctx.r1.u32 + 520, r18.u32);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826f597c
	if (cr6.eq) goto loc_826F597C;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x826f597c
	if (cr6.eq) goto loc_826F597C;
	// stw r18,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r18.u32);
	// b 0x826f5984
	goto loc_826F5984;
loc_826F597C:
	// li r11,1
	r11.s64 = 1;
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r11.u32);
loc_826F5984:
	// li r22,0
	r22.s64 = 0;
	// lhz r16,98(r1)
	r16.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// lhz r15,96(r1)
	r15.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// lhz r14,82(r1)
	r14.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// stw r4,468(r1)
	PPC_STORE_U32(ctx.r1.u32 + 468, ctx.r4.u32);
	// stw r5,508(r1)
	PPC_STORE_U32(ctx.r1.u32 + 508, ctx.r5.u32);
	// ble cr6,0x826f5af8
	if (!cr6.gt) goto loc_826F5AF8;
	// lhz r11,106(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 106);
	// extsh r9,r16
	ctx.r9.s64 = r16.s16;
	// lhz r10,102(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// extsh r6,r15
	ctx.r6.s64 = r15.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 104);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r7,100(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 100);
	// lhz r5,90(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r28,r9,r10
	r28.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lhz r4,86(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// subf r27,r11,r10
	r27.s64 = ctx.r10.s64 - r11.s64;
	// lhz r31,88(r1)
	r31.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r30,84(r1)
	r30.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// subf r26,r9,r11
	r26.s64 = r11.s64 - ctx.r9.s64;
	// lhz r29,80(r1)
	r29.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// xor r24,r27,r28
	r24.u64 = r27.u64 ^ r28.u64;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r3,r14
	ctx.r3.s64 = r14.s16;
	// subf r27,r6,r7
	r27.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r25,r8,r7
	r25.s64 = ctx.r7.s64 - ctx.r8.s64;
	// subf r23,r6,r8
	r23.s64 = ctx.r8.s64 - ctx.r6.s64;
	// xor r22,r26,r28
	r22.u64 = r26.u64 ^ r28.u64;
	// subf r21,r5,r4
	r21.s64 = ctx.r4.s64 - ctx.r5.s64;
	// subf r28,r3,r4
	r28.s64 = ctx.r4.s64 - ctx.r3.s64;
	// extsh r31,r31
	r31.s64 = r31.s16;
	// extsh r30,r30
	r30.s64 = r30.s16;
	// extsh r29,r29
	r29.s64 = r29.s16;
	// xor r20,r25,r27
	r20.u64 = r25.u64 ^ r27.u64;
	// srawi r26,r24,31
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7FFFFFFF) != 0);
	r26.s64 = r24.s32 >> 31;
	// xor r23,r23,r27
	r23.u64 = r23.u64 ^ r27.u64;
	// subf r19,r3,r5
	r19.s64 = ctx.r5.s64 - ctx.r3.s64;
	// srawi r27,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r27.s64 = r22.s32 >> 31;
	// xor r22,r21,r28
	r22.u64 = r21.u64 ^ r28.u64;
	// subf r25,r29,r30
	r25.s64 = r30.s64 - r29.s64;
	// subf r18,r31,r30
	r18.s64 = r30.s64 - r31.s64;
	// srawi r24,r20,31
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x7FFFFFFF) != 0);
	r24.s64 = r20.s32 >> 31;
	// subf r21,r29,r31
	r21.s64 = r31.s64 - r29.s64;
	// xor r20,r19,r28
	r20.u64 = r19.u64 ^ r28.u64;
	// xor r19,r18,r25
	r19.u64 = r18.u64 ^ r25.u64;
	// srawi r28,r23,31
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7FFFFFFF) != 0);
	r28.s64 = r23.s32 >> 31;
	// srawi r23,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r23.s64 = r22.s32 >> 31;
	// xor r21,r21,r25
	r21.u64 = r21.u64 ^ r25.u64;
	// srawi r25,r20,31
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x7FFFFFFF) != 0);
	r25.s64 = r20.s32 >> 31;
	// srawi r22,r19,31
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x7FFFFFFF) != 0);
	r22.s64 = r19.s32 >> 31;
	// and r9,r27,r9
	ctx.r9.u64 = r27.u64 & ctx.r9.u64;
	// and r10,r26,r10
	ctx.r10.u64 = r26.u64 & ctx.r10.u64;
	// srawi r21,r21,31
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x7FFFFFFF) != 0);
	r21.s64 = r21.s32 >> 31;
	// and r6,r28,r6
	ctx.r6.u64 = r28.u64 & ctx.r6.u64;
	// and r7,r24,r7
	ctx.r7.u64 = r24.u64 & ctx.r7.u64;
	// nor r20,r26,r27
	r20.u64 = ~(r26.u64 | r27.u64);
	// and r4,r23,r4
	ctx.r4.u64 = r23.u64 & ctx.r4.u64;
	// nor r19,r24,r28
	r19.u64 = ~(r24.u64 | r28.u64);
	// nor r18,r23,r25
	r18.u64 = ~(r23.u64 | r25.u64);
	// and r3,r25,r3
	ctx.r3.u64 = r25.u64 & ctx.r3.u64;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// nor r17,r22,r21
	r17.u64 = ~(r22.u64 | r21.u64);
	// and r29,r21,r29
	r29.u64 = r21.u64 & r29.u64;
	// and r30,r22,r30
	r30.u64 = r22.u64 & r30.u64;
	// or r9,r6,r7
	ctx.r9.u64 = ctx.r6.u64 | ctx.r7.u64;
	// and r11,r20,r11
	r11.u64 = r20.u64 & r11.u64;
	// and r6,r18,r5
	ctx.r6.u64 = r18.u64 & ctx.r5.u64;
	// or r7,r3,r4
	ctx.r7.u64 = ctx.r3.u64 | ctx.r4.u64;
	// and r8,r19,r8
	ctx.r8.u64 = r19.u64 & ctx.r8.u64;
	// or r5,r29,r30
	ctx.r5.u64 = r29.u64 | r30.u64;
	// and r4,r17,r31
	ctx.r4.u64 = r17.u64 & r31.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 | ctx.r8.u64;
	// or r8,r5,r4
	ctx.r8.u64 = ctx.r5.u64 | ctx.r4.u64;
	// lwz r5,508(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	// or r9,r7,r6
	ctx.r9.u64 = ctx.r7.u64 | ctx.r6.u64;
	// lwz r4,468(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// sth r11,146(r1)
	PPC_STORE_U16(ctx.r1.u32 + 146, r11.u16);
	// sth r10,144(r1)
	PPC_STORE_U16(ctx.r1.u32 + 144, ctx.r10.u16);
	// sth r8,152(r1)
	PPC_STORE_U16(ctx.r1.u32 + 152, ctx.r8.u16);
	// sth r9,154(r1)
	PPC_STORE_U16(ctx.r1.u32 + 154, ctx.r9.u16);
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r6,152(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// li r22,0
	r22.s64 = 0;
	// b 0x826f5b18
	goto loc_826F5B18;
loc_826F5AF8:
	// bne cr6,0x826f5b08
	if (!cr6.eq) goto loc_826F5B08;
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x826f5b10
	goto loc_826F5B10;
loc_826F5B08:
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
loc_826F5B10:
	// stw r6,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r6.u32);
	// stw r10,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r10.u32);
loc_826F5B18:
	// lwz r11,204(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f5dac
	if (cr6.eq) goto loc_826F5DAC;
	// lwz r11,220(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f5dac
	if (cr6.eq) goto loc_826F5DAC;
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lwz r27,96(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r26,80(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r7,244(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r27,512(r1)
	PPC_STORE_U32(ctx.r1.u32 + 512, r27.u32);
	// lwzx r25,r9,r11
	r25.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// lwzx r24,r8,r11
	r24.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// stw r26,476(r1)
	PPC_STORE_U32(ctx.r1.u32 + 476, r26.u32);
	// stw r25,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, r25.u32);
	// stw r24,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r24.u32);
	// bne cr6,0x826f5c8c
	if (!cr6.eq) goto loc_826F5C8C;
	// lhz r11,144(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 144);
	// extsh r8,r15
	ctx.r8.s64 = r15.s16;
	// lhz r9,146(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 146);
	// extsh r7,r16
	ctx.r7.s64 = r16.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// subf r8,r8,r11
	ctx.r8.s64 = r11.s64 - ctx.r8.s64;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// srawi r3,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 31;
	// srawi r31,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r7.s32 >> 31;
	// xor r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r3.u64;
	// xor r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 ^ r31.u64;
	// subf r8,r3,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r3.s64;
	// subf r7,r31,r7
	ctx.r7.s64 = ctx.r7.s64 - r31.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmpwi cr6,r8,32
	cr6.compare<int32_t>(ctx.r8.s32, 32, xer);
	// bgt cr6,0x826f5bf4
	if (cr6.gt) goto loc_826F5BF4;
	// lhz r8,260(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 260);
	// lhz r7,262(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 262);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f5bf8
	if (!cr6.gt) goto loc_826F5BF8;
loc_826F5BF4:
	// li r4,1
	ctx.r4.s64 = 1;
loc_826F5BF8:
	// lhz r11,152(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 152);
	// extsh r7,r14
	ctx.r7.s64 = r14.s16;
	// lhz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,154(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 154);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// subf r8,r8,r11
	ctx.r8.s64 = r11.s64 - ctx.r8.s64;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// srawi r3,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 31;
	// srawi r31,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r7.s32 >> 31;
	// xor r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r3.u64;
	// xor r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 ^ r31.u64;
	// subf r8,r3,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r3.s64;
	// subf r7,r31,r7
	ctx.r7.s64 = ctx.r7.s64 - r31.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmpwi cr6,r8,32
	cr6.compare<int32_t>(ctx.r8.s32, 32, xer);
	// bgt cr6,0x826f5c84
	if (cr6.gt) goto loc_826F5C84;
	// lhz r8,240(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 240);
	// lhz r7,242(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 242);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f5dbc
	if (!cr6.gt) goto loc_826F5DBC;
loc_826F5C84:
	// li r5,1
	ctx.r5.s64 = 1;
	// b 0x826f5dbc
	goto loc_826F5DBC;
loc_826F5C8C:
	// lhz r11,146(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 146);
	// extsh r8,r16
	ctx.r8.s64 = r16.s16;
	// lhz r9,144(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 144);
	// extsh r7,r15
	ctx.r7.s64 = r15.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// subf r8,r8,r11
	ctx.r8.s64 = r11.s64 - ctx.r8.s64;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r3,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 31;
	// srawi r31,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r7.s32 >> 31;
	// xor r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r3.u64;
	// xor r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 ^ r31.u64;
	// subf r8,r3,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r3.s64;
	// subf r7,r31,r7
	ctx.r7.s64 = ctx.r7.s64 - r31.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmpwi cr6,r8,32
	cr6.compare<int32_t>(ctx.r8.s32, 32, xer);
	// bgt cr6,0x826f5d14
	if (cr6.gt) goto loc_826F5D14;
	// lhz r8,262(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 262);
	// lhz r7,260(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 260);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f5d18
	if (!cr6.gt) goto loc_826F5D18;
loc_826F5D14:
	// li r4,1
	ctx.r4.s64 = 1;
loc_826F5D18:
	// lhz r11,154(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 154);
	// extsh r8,r14
	ctx.r8.s64 = r14.s16;
	// lhz r9,152(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 152);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// subf r8,r8,r11
	ctx.r8.s64 = r11.s64 - ctx.r8.s64;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// srawi r3,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 31;
	// srawi r31,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r7.s32 >> 31;
	// xor r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r3.u64;
	// xor r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 ^ r31.u64;
	// subf r8,r3,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r3.s64;
	// subf r7,r31,r7
	ctx.r7.s64 = ctx.r7.s64 - r31.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmpwi cr6,r8,32
	cr6.compare<int32_t>(ctx.r8.s32, 32, xer);
	// bgt cr6,0x826f5da4
	if (cr6.gt) goto loc_826F5DA4;
	// lhz r8,242(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 242);
	// lhz r7,240(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 240);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f5dbc
	if (!cr6.gt) goto loc_826F5DBC;
loc_826F5DA4:
	// li r5,1
	ctx.r5.s64 = 1;
	// b 0x826f5dbc
	goto loc_826F5DBC;
loc_826F5DAC:
	// lwz r27,512(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	// lwz r25,260(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r26,476(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// lwz r24,240(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 240);
loc_826F5DBC:
	// lwz r28,780(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	// lwz r11,1508(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1508);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f5de4
	if (cr6.eq) goto loc_826F5DE4;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r8,108(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f5df0
	if (!cr6.gt) goto loc_826F5DF0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f5dfc
	if (!cr6.eq) goto loc_826F5DFC;
loc_826F5DE4:
	// lwz r11,1512(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1512);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f5dfc
	if (cr6.eq) goto loc_826F5DFC;
loc_826F5DF0:
	// li r21,1
	r21.s64 = 1;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// b 0x826f5e04
	goto loc_826F5E04;
loc_826F5DFC:
	// lwz r9,520(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	// li r21,1
	r21.s64 = 1;
loc_826F5E04:
	// lwz r23,116(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// rlwinm r11,r23,0,15,15
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f5ff4
	if (cr6.eq) goto loc_826F5FF4;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826f5f08
	if (!cr6.eq) goto loc_826F5F08;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x826f5efc
	if (cr6.eq) goto loc_826F5EFC;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f5e98
	if (!cr6.lt) goto loc_826F5E98;
loc_826F5E40:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f5e98
	if (cr6.eq) goto loc_826F5E98;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f5e88
	if (!cr0.lt) goto loc_826F5E88;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F5E88:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f5e40
	if (cr6.gt) goto loc_826F5E40;
loc_826F5E98:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f5ed4
	if (!cr0.lt) goto loc_826F5ED4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F5ED4:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f5ef0
	if (!cr6.eq) goto loc_826F5EF0;
	// mr r11,r26
	r11.u64 = r26.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f61b0
	goto loc_826F61B0;
loc_826F5EF0:
	// mr r11,r24
	r11.u64 = r24.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f61b0
	goto loc_826F61B0;
loc_826F5EFC:
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f61b0
	goto loc_826F61B0;
loc_826F5F08:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x826f5fe8
	if (cr6.eq) goto loc_826F5FE8;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f5f84
	if (!cr6.lt) goto loc_826F5F84;
loc_826F5F2C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f5f84
	if (cr6.eq) goto loc_826F5F84;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f5f74
	if (!cr0.lt) goto loc_826F5F74;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F5F74:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f5f2c
	if (cr6.gt) goto loc_826F5F2C;
loc_826F5F84:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f5fc0
	if (!cr0.lt) goto loc_826F5FC0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F5FC0:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f5fdc
	if (!cr6.eq) goto loc_826F5FDC;
	// mr r11,r27
	r11.u64 = r27.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f61b0
	goto loc_826F61B0;
loc_826F5FDC:
	// mr r11,r25
	r11.u64 = r25.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f61b0
	goto loc_826F61B0;
loc_826F5FE8:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f61b0
	goto loc_826F61B0;
loc_826F5FF4:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826f60d4
	if (!cr6.eq) goto loc_826F60D4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x826f61b0
	if (cr6.eq) goto loc_826F61B0;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f6078
	if (!cr6.lt) goto loc_826F6078;
loc_826F6020:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f6078
	if (cr6.eq) goto loc_826F6078;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f6068
	if (!cr0.lt) goto loc_826F6068;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F6068:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f6020
	if (cr6.gt) goto loc_826F6020;
loc_826F6078:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f60b4
	if (!cr0.lt) goto loc_826F60B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F60B4:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f60cc
	if (!cr6.eq) goto loc_826F60CC;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// b 0x826f61b0
	goto loc_826F61B0;
loc_826F60CC:
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// b 0x826f61b0
	goto loc_826F61B0;
loc_826F60D4:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x826f61ac
	if (cr6.eq) goto loc_826F61AC;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f6150
	if (!cr6.lt) goto loc_826F6150;
loc_826F60F8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f6150
	if (cr6.eq) goto loc_826F6150;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f6140
	if (!cr0.lt) goto loc_826F6140;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F6140:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f60f8
	if (cr6.gt) goto loc_826F60F8;
loc_826F6150:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f618c
	if (!cr0.lt) goto loc_826F618C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F618C:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f61a4
	if (!cr6.eq) goto loc_826F61A4;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// b 0x826f61b0
	goto loc_826F61B0;
loc_826F61A4:
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// b 0x826f61b0
	goto loc_826F61B0;
loc_826F61AC:
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
loc_826F61B0:
	// lwz r30,320(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// lwz r29,300(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// stw r10,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r10.u32);
	// add r8,r11,r29
	ctx.r8.u64 = r11.u64 + r29.u64;
	// lhz r11,120(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 120);
	// lwz r6,268(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// lhz r11,174(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 174);
	// stw r22,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r22.u32);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r11,r23,16
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xFFFF) != 0);
	r11.s64 = r23.s32 >> 16;
	// stw r8,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r8.u32);
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,296(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// add r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// and r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 & ctx.r6.u64;
	// lwz r6,276(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// sth r9,122(r1)
	PPC_STORE_U16(ctx.r1.u32 + 122, ctx.r9.u16);
	// sth r11,120(r1)
	PPC_STORE_U16(ctx.r1.u32 + 120, r11.u16);
	// lwz r31,120(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r31,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r31.u32);
	// beq cr6,0x826f6268
	if (cr6.eq) goto loc_826F6268;
	// lwz r11,1508(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1508);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r4,176(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 176);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f6254
	if (cr6.eq) goto loc_826F6254;
	// bl 0x826eb3a0
	sub_826EB3A0(ctx, base);
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// b 0x826f6268
	goto loc_826F6268;
loc_826F6254:
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// rlwinm r11,r3,1,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFE0000;
	// clrlwi r10,r3,16
	ctx.r10.u64 = ctx.r3.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
loc_826F6268:
	// lwz r27,1516(r28)
	r27.u64 = PPC_LOAD_U32(r28.u32 + 1516);
	// lis r25,1
	r25.s64 = 65536;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// stw r22,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r22.u32);
	// rlwinm r6,r27,17,0,14
	ctx.r6.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 17) & 0xFFFE0000;
	// stw r22,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r22.u32);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
	// subf r26,r6,r25
	r26.s64 = r25.s64 - ctx.r6.s64;
	// stw r22,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r22.u32);
	// rlwinm r11,r31,0,15,15
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x10000;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// stw r3,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r3.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// stw r26,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r26.u32);
	// beq cr6,0x826f6400
	if (cr6.eq) goto loc_826F6400;
	// extsh r11,r31
	r11.s64 = r31.s16;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// srawi r7,r31,16
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r31.s32 >> 16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x826f62dc
	if (!cr6.gt) goto loc_826F62DC;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// b 0x826f6314
	goto loc_826F6314;
loc_826F62DC:
	// lwz r8,1560(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// bge cr6,0x826f62f8
	if (!cr6.lt) goto loc_826F62F8;
	// lwz r10,1576(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// b 0x826f6314
	goto loc_826F6314;
loc_826F62F8:
	// lwz r9,1580(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// lwz r8,1568(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1568);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// xor r9,r8,r10
	ctx.r9.u64 = ctx.r8.u64 ^ ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// subf r8,r10,r11
	ctx.r8.s64 = r11.s64 - ctx.r10.s64;
loc_826F6314:
	// lhz r11,62(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 62);
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// ble cr6,0x826f632c
	if (!cr6.gt) goto loc_826F632C;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// b 0x826f633c
	goto loc_826F633C;
loc_826F632C:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x826f633c
	if (!cr6.lt) goto loc_826F633C;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_826F633C:
	// rlwinm r11,r27,1,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// cmpwi cr6,r9,63
	cr6.compare<int32_t>(ctx.r9.s32, 63, xer);
	// ble cr6,0x826f6370
	if (!cr6.gt) goto loc_826F6370;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f63b4
	goto loc_826F63B4;
loc_826F6370:
	// lwz r7,1564(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1564);
	// cmpw cr6,r9,r7
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f6390
	if (!cr6.lt) goto loc_826F6390;
	// lwz r10,1576(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f63b4
	goto loc_826F63B4;
loc_826F6390:
	// lwz r9,1572(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1572);
	// lwz r7,1580(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// xor r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r10.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// srawi r9,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r9.s64 = r11.s32 >> 7;
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r9,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_826F63B4:
	// lhz r10,64(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 64);
	// addi r9,r10,-2
	ctx.r9.s64 = ctx.r10.s64 + -2;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f63dc
	if (!cr6.gt) goto loc_826F63DC;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// rlwimi r8,r11,16,0,15
	ctx.r8.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r8.u64 & 0xFFFFFFFF0000FFFF);
	// stw r3,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r3.u32);
	// stw r8,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r8.u32);
	// b 0x826f6440
	goto loc_826F6440;
loc_826F63DC:
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826f63ec
	if (!cr6.lt) goto loc_826F63EC;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_826F63EC:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// rlwimi r8,r11,16,0,15
	ctx.r8.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r8.u64 & 0xFFFFFFFF0000FFFF);
	// stw r3,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r3.u32);
	// stw r8,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r8.u32);
	// b 0x826f6440
	goto loc_826F6440;
loc_826F6400:
	// srawi r9,r31,8
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFF) != 0);
	ctx.r9.s64 = r31.s32 >> 8;
	// lwz r11,1584(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1584);
	// extsh r10,r31
	ctx.r10.s64 = r31.s16;
	// stw r31,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r31.u32);
	// rlwinm r9,r9,0,0,22
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFE00;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r9,r9,0,0,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFE0000;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// subf r10,r6,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r6.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_826F6440:
	// lwz r11,344(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f67ec
	if (!cr6.eq) goto loc_826F67EC;
	// lwz r11,328(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r29
	r11.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826f6610
	if (cr6.eq) goto loc_826F6610;
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f65c4
	if (cr6.eq) goto loc_826F65C4;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// srawi r8,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 16;
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r31,r5,31
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r5.s32 >> 31;
	// xor r11,r5,r31
	r11.u64 = ctx.r5.u64 ^ r31.u64;
	// subf r11,r31,r11
	r11.s64 = r11.s64 - r31.s64;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// bgt cr6,0x826f64d8
	if (cr6.gt) goto loc_826F64D8;
	// lwz r5,1560(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x826f64bc
	if (!cr6.lt) goto loc_826F64BC;
	// lwz r11,1576(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f64d8
	goto loc_826F64D8;
loc_826F64BC:
	// lwz r11,1580(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// lwz r5,1568(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r5,r9
	ctx.r10.u64 = ctx.r5.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F64D8:
	// lhz r11,62(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f64f0
	if (!cr6.gt) goto loc_826F64F0;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f6500
	goto loc_826F6500;
loc_826F64F0:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f6500
	if (!cr6.lt) goto loc_826F6500;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F6500:
	// rlwinm r11,r27,1,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r5,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r5.u64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f6534
	if (!cr6.gt) goto loc_826F6534;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f6578
	goto loc_826F6578;
loc_826F6534:
	// lwz r5,1564(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 1564);
	// cmpw cr6,r8,r5
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, xer);
	// bge cr6,0x826f6554
	if (!cr6.lt) goto loc_826F6554;
	// lwz r9,1576(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f6578
	goto loc_826F6578;
loc_826F6554:
	// lwz r8,1572(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1572);
	// lwz r5,1580(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r5,r11
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F6578:
	// lhz r9,64(r28)
	ctx.r9.u64 = PPC_LOAD_U16(r28.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f65a0
	if (!cr6.gt) goto loc_826F65A0;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r3,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r3.u32);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// b 0x826f6604
	goto loc_826F6604;
loc_826F65A0:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f65b0
	if (!cr6.lt) goto loc_826F65B0;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F65B0:
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r3,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r3.u32);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// b 0x826f6604
	goto loc_826F6604;
loc_826F65C4:
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// lwz r10,1584(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1584);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// rlwinm r8,r8,0,0,22
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFE00;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// mullw r11,r8,r10
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// rlwinm r11,r11,0,0,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFE0000;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_826F6604:
	// stw r21,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r21.u32);
	// li r5,2
	ctx.r5.s64 = 2;
	// stw r21,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r21.u32);
loc_826F6610:
	// lwz r11,360(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	// lwz r10,212(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// mr r11,r21
	r11.u64 = r21.u64;
	// blt cr6,0x826f662c
	if (cr6.lt) goto loc_826F662C;
	// mr r11,r22
	r11.u64 = r22.u64;
loc_826F662C:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r29
	r11.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826f67ec
	if (cr6.eq) goto loc_826F67EC;
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// addi r31,r1,80
	r31.s64 = ctx.r1.s64 + 80;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f67a0
	if (cr6.eq) goto loc_826F67A0;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r7,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r11.s32 >> 16;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r4,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = ctx.r8.s32 >> 31;
	// rlwinm r6,r5,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// xor r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r4.u64;
	// subf r8,r4,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r4.s64;
	// stwx r11,r6,r31
	PPC_STORE_U32(ctx.r6.u32 + r31.u32, r11.u32);
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// bgt cr6,0x826f66c0
	if (cr6.gt) goto loc_826F66C0;
	// lwz r11,1560(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x826f66a4
	if (!cr6.lt) goto loc_826F66A4;
	// lwz r11,1576(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f66c0
	goto loc_826F66C0;
loc_826F66A4:
	// lwz r11,1580(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// lwz r8,1568(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F66C0:
	// lhz r11,62(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f66d8
	if (!cr6.gt) goto loc_826F66D8;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f66e8
	goto loc_826F66E8;
loc_826F66D8:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f66e8
	if (!cr6.lt) goto loc_826F66E8;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F66E8:
	// rlwinm r11,r27,1,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f671c
	if (!cr6.gt) goto loc_826F671C;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f6760
	goto loc_826F6760;
loc_826F671C:
	// lwz r7,1564(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f673c
	if (!cr6.lt) goto loc_826F673C;
	// lwz r9,1576(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f6760
	goto loc_826F6760;
loc_826F673C:
	// lwz r8,1572(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1572);
	// lwz r7,1580(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F6760:
	// lhz r9,64(r28)
	ctx.r9.u64 = PPC_LOAD_U16(r28.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f6778
	if (!cr6.gt) goto loc_826F6778;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x826f6788
	goto loc_826F6788;
loc_826F6778:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f6788
	if (!cr6.lt) goto loc_826F6788;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F6788:
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r11,r3,1
	r11.s64 = ctx.r3.s64 + 1;
	// stwx r10,r6,r9
	PPC_STORE_U32(ctx.r6.u32 + ctx.r9.u32, ctx.r10.u32);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// b 0x826f67e8
	goto loc_826F67E8;
loc_826F67A0:
	// srawi r7,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r7.s64 = r11.s32 >> 8;
	// lwz r10,1584(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1584);
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// rlwinm r7,r7,0,0,22
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFE00;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// mullw r7,r7,r10
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// stwx r11,r9,r3
	PPC_STORE_U32(ctx.r9.u32 + ctx.r3.u32, r11.u32);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r7,0,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFE0000;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// stwx r11,r9,r31
	PPC_STORE_U32(ctx.r9.u32 + r31.u32, r11.u32);
loc_826F67E8:
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
loc_826F67EC:
	// lwz r11,1320(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1320);
	// stw r22,440(r1)
	PPC_STORE_U32(ctx.r1.u32 + 440, r22.u32);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826f6808
	if (cr6.eq) goto loc_826F6808;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// stw r22,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, r22.u32);
	// bne cr6,0x826f680c
	if (!cr6.eq) goto loc_826F680C;
loc_826F6808:
	// stw r21,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, r21.u32);
loc_826F680C:
	// li r22,0
	r22.s64 = 0;
	// lhz r16,98(r1)
	r16.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// lhz r15,96(r1)
	r15.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// cmpwi cr6,r5,1
	cr6.compare<int32_t>(ctx.r5.s32, 1, xer);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lhz r14,82(r1)
	r14.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// stw r3,532(r1)
	PPC_STORE_U32(ctx.r1.u32 + 532, ctx.r3.u32);
	// stw r4,484(r1)
	PPC_STORE_U32(ctx.r1.u32 + 484, ctx.r4.u32);
	// ble cr6,0x826f6980
	if (!cr6.gt) goto loc_826F6980;
	// lhz r11,106(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 106);
	// extsh r9,r16
	ctx.r9.s64 = r16.s16;
	// lhz r10,102(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// extsh r6,r15
	ctx.r6.s64 = r15.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 104);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r7,100(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 100);
	// lhz r5,90(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r28,r9,r10
	r28.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lhz r4,86(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// subf r27,r11,r10
	r27.s64 = ctx.r10.s64 - r11.s64;
	// lhz r31,88(r1)
	r31.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r30,84(r1)
	r30.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// subf r26,r9,r11
	r26.s64 = r11.s64 - ctx.r9.s64;
	// lhz r29,80(r1)
	r29.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// xor r24,r27,r28
	r24.u64 = r27.u64 ^ r28.u64;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// extsh r3,r14
	ctx.r3.s64 = r14.s16;
	// subf r27,r6,r7
	r27.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r25,r8,r7
	r25.s64 = ctx.r7.s64 - ctx.r8.s64;
	// subf r23,r6,r8
	r23.s64 = ctx.r8.s64 - ctx.r6.s64;
	// xor r22,r26,r28
	r22.u64 = r26.u64 ^ r28.u64;
	// subf r21,r5,r4
	r21.s64 = ctx.r4.s64 - ctx.r5.s64;
	// subf r28,r3,r4
	r28.s64 = ctx.r4.s64 - ctx.r3.s64;
	// xor r20,r25,r27
	r20.u64 = r25.u64 ^ r27.u64;
	// srawi r26,r24,31
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7FFFFFFF) != 0);
	r26.s64 = r24.s32 >> 31;
	// extsh r31,r31
	r31.s64 = r31.s16;
	// extsh r30,r30
	r30.s64 = r30.s16;
	// extsh r29,r29
	r29.s64 = r29.s16;
	// xor r23,r23,r27
	r23.u64 = r23.u64 ^ r27.u64;
	// srawi r27,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r27.s64 = r22.s32 >> 31;
	// subf r19,r3,r5
	r19.s64 = ctx.r5.s64 - ctx.r3.s64;
	// xor r22,r21,r28
	r22.u64 = r21.u64 ^ r28.u64;
	// subf r25,r29,r30
	r25.s64 = r30.s64 - r29.s64;
	// srawi r24,r20,31
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x7FFFFFFF) != 0);
	r24.s64 = r20.s32 >> 31;
	// subf r18,r31,r30
	r18.s64 = r30.s64 - r31.s64;
	// subf r21,r29,r31
	r21.s64 = r31.s64 - r29.s64;
	// xor r20,r19,r28
	r20.u64 = r19.u64 ^ r28.u64;
	// srawi r28,r23,31
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7FFFFFFF) != 0);
	r28.s64 = r23.s32 >> 31;
	// srawi r23,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r23.s64 = r22.s32 >> 31;
	// xor r19,r18,r25
	r19.u64 = r18.u64 ^ r25.u64;
	// xor r21,r21,r25
	r21.u64 = r21.u64 ^ r25.u64;
	// srawi r25,r20,31
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x7FFFFFFF) != 0);
	r25.s64 = r20.s32 >> 31;
	// srawi r22,r19,31
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x7FFFFFFF) != 0);
	r22.s64 = r19.s32 >> 31;
	// nor r20,r23,r25
	r20.u64 = ~(r23.u64 | r25.u64);
	// and r9,r27,r9
	ctx.r9.u64 = r27.u64 & ctx.r9.u64;
	// srawi r21,r21,31
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x7FFFFFFF) != 0);
	r21.s64 = r21.s32 >> 31;
	// and r10,r26,r10
	ctx.r10.u64 = r26.u64 & ctx.r10.u64;
	// and r6,r28,r6
	ctx.r6.u64 = r28.u64 & ctx.r6.u64;
	// and r7,r24,r7
	ctx.r7.u64 = r24.u64 & ctx.r7.u64;
	// nor r19,r26,r27
	r19.u64 = ~(r26.u64 | r27.u64);
	// and r5,r20,r5
	ctx.r5.u64 = r20.u64 & ctx.r5.u64;
	// nor r18,r24,r28
	r18.u64 = ~(r24.u64 | r28.u64);
	// and r3,r25,r3
	ctx.r3.u64 = r25.u64 & ctx.r3.u64;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// nor r17,r22,r21
	r17.u64 = ~(r22.u64 | r21.u64);
	// and r30,r22,r30
	r30.u64 = r22.u64 & r30.u64;
	// and r29,r21,r29
	r29.u64 = r21.u64 & r29.u64;
	// or r9,r6,r7
	ctx.r9.u64 = ctx.r6.u64 | ctx.r7.u64;
	// and r11,r19,r11
	r11.u64 = r19.u64 & r11.u64;
	// and r6,r23,r4
	ctx.r6.u64 = r23.u64 & ctx.r4.u64;
	// or r7,r5,r3
	ctx.r7.u64 = ctx.r5.u64 | ctx.r3.u64;
	// and r8,r18,r8
	ctx.r8.u64 = r18.u64 & ctx.r8.u64;
	// and r4,r17,r31
	ctx.r4.u64 = r17.u64 & r31.u64;
	// or r5,r29,r30
	ctx.r5.u64 = r29.u64 | r30.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 | ctx.r8.u64;
	// or r9,r7,r6
	ctx.r9.u64 = ctx.r7.u64 | ctx.r6.u64;
	// or r8,r5,r4
	ctx.r8.u64 = ctx.r5.u64 | ctx.r4.u64;
	// lwz r4,484(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	// li r22,0
	r22.s64 = 0;
	// sth r11,142(r1)
	PPC_STORE_U16(ctx.r1.u32 + 142, r11.u16);
	// sth r10,140(r1)
	PPC_STORE_U16(ctx.r1.u32 + 140, ctx.r10.u16);
	// sth r9,158(r1)
	PPC_STORE_U16(ctx.r1.u32 + 158, ctx.r9.u16);
	// sth r8,156(r1)
	PPC_STORE_U16(ctx.r1.u32 + 156, ctx.r8.u16);
	// lwz r7,140(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r6,156(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// lwz r3,532(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	// b 0x826f69a0
	goto loc_826F69A0;
loc_826F6980:
	// bne cr6,0x826f6990
	if (!cr6.eq) goto loc_826F6990;
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x826f6998
	goto loc_826F6998;
loc_826F6990:
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
loc_826F6998:
	// stw r6,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r6.u32);
	// stw r7,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r7.u32);
loc_826F69A0:
	// lwz r11,220(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f6c28
	if (cr6.eq) goto loc_826F6C28;
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r27,96(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r26,80(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r8,352(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r27,492(r1)
	PPC_STORE_U32(ctx.r1.u32 + 492, r27.u32);
	// lwzx r25,r10,r11
	r25.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwzx r24,r9,r11
	r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// stw r26,432(r1)
	PPC_STORE_U32(ctx.r1.u32 + 432, r26.u32);
	// stw r25,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r25.u32);
	// stw r24,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, r24.u32);
	// bne cr6,0x826f6b08
	if (!cr6.eq) goto loc_826F6B08;
	// lhz r11,140(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 140);
	// extsh r9,r15
	ctx.r9.s64 = r15.s16;
	// lhz r10,142(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 142);
	// extsh r8,r16
	ctx.r8.s64 = r16.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// srawi r5,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r9.s32 >> 31;
	// srawi r31,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r8.s32 >> 31;
	// xor r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r5.u64;
	// xor r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 ^ r31.u64;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x826f6a70
	if (cr6.gt) goto loc_826F6A70;
	// lhz r9,216(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 216);
	// lhz r8,218(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 218);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f6a74
	if (!cr6.gt) goto loc_826F6A74;
loc_826F6A70:
	// li r3,1
	ctx.r3.s64 = 1;
loc_826F6A74:
	// lhz r11,156(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 156);
	// extsh r8,r14
	ctx.r8.s64 = r14.s16;
	// lhz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r10,158(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 158);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// srawi r5,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r9.s32 >> 31;
	// srawi r31,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r8.s32 >> 31;
	// xor r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r5.u64;
	// xor r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 ^ r31.u64;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x826f6b00
	if (cr6.gt) goto loc_826F6B00;
	// lhz r9,256(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 256);
	// lhz r8,258(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 258);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f6c38
	if (!cr6.gt) goto loc_826F6C38;
loc_826F6B00:
	// li r4,1
	ctx.r4.s64 = 1;
	// b 0x826f6c38
	goto loc_826F6C38;
loc_826F6B08:
	// lhz r11,142(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 142);
	// extsh r9,r16
	ctx.r9.s64 = r16.s16;
	// lhz r10,140(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 140);
	// extsh r8,r15
	ctx.r8.s64 = r15.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r5,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r9.s32 >> 31;
	// srawi r31,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r8.s32 >> 31;
	// xor r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r5.u64;
	// xor r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 ^ r31.u64;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x826f6b90
	if (cr6.gt) goto loc_826F6B90;
	// lhz r9,218(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 218);
	// lhz r8,216(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 216);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f6b94
	if (!cr6.gt) goto loc_826F6B94;
loc_826F6B90:
	// li r3,1
	ctx.r3.s64 = 1;
loc_826F6B94:
	// lhz r11,158(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 158);
	// extsh r9,r14
	ctx.r9.s64 = r14.s16;
	// lhz r10,156(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 156);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r5,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r9.s32 >> 31;
	// srawi r31,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r8.s32 >> 31;
	// xor r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r5.u64;
	// xor r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 ^ r31.u64;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x826f6c20
	if (cr6.gt) goto loc_826F6C20;
	// lhz r9,258(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 258);
	// lhz r8,256(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 256);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f6c38
	if (!cr6.gt) goto loc_826F6C38;
loc_826F6C20:
	// li r4,1
	ctx.r4.s64 = 1;
	// b 0x826f6c38
	goto loc_826F6C38;
loc_826F6C28:
	// lwz r27,492(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	// lwz r25,216(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// lwz r26,432(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	// lwz r24,256(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
loc_826F6C38:
	// lwz r28,780(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	// lwz r11,1508(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1508);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f6c60
	if (cr6.eq) goto loc_826F6C60;
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r9,108(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f6c6c
	if (!cr6.gt) goto loc_826F6C6C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f6c74
	if (!cr6.eq) goto loc_826F6C74;
loc_826F6C60:
	// lwz r11,1512(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1512);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f6c74
	if (cr6.eq) goto loc_826F6C74;
loc_826F6C6C:
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x826f6c78
	goto loc_826F6C78;
loc_826F6C74:
	// lwz r10,440(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 440);
loc_826F6C78:
	// lwz r23,116(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// rlwinm r11,r23,0,15,15
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f6e80
	if (cr6.eq) goto loc_826F6E80;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826f6d88
	if (!cr6.eq) goto loc_826F6D88;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x826f6d78
	if (cr6.eq) goto loc_826F6D78;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f6d0c
	if (!cr6.lt) goto loc_826F6D0C;
loc_826F6CB4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f6d0c
	if (cr6.eq) goto loc_826F6D0C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f6cfc
	if (!cr0.lt) goto loc_826F6CFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F6CFC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f6cb4
	if (cr6.gt) goto loc_826F6CB4;
loc_826F6D0C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f6d48
	if (!cr0.lt) goto loc_826F6D48;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F6D48:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f6d68
	if (!cr6.eq) goto loc_826F6D68;
	// mr r11,r26
	r11.u64 = r26.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F6D68:
	// mr r11,r24
	r11.u64 = r24.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F6D78:
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F6D88:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826f6e70
	if (cr6.eq) goto loc_826F6E70;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f6e04
	if (!cr6.lt) goto loc_826F6E04;
loc_826F6DAC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f6e04
	if (cr6.eq) goto loc_826F6E04;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f6df4
	if (!cr0.lt) goto loc_826F6DF4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F6DF4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f6dac
	if (cr6.gt) goto loc_826F6DAC;
loc_826F6E04:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f6e40
	if (!cr0.lt) goto loc_826F6E40;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F6E40:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f6e60
	if (!cr6.eq) goto loc_826F6E60;
	// mr r11,r27
	r11.u64 = r27.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F6E60:
	// mr r11,r25
	r11.u64 = r25.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F6E70:
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F6E80:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826f6f68
	if (!cr6.eq) goto loc_826F6F68;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826f6f60
	if (cr6.eq) goto loc_826F6F60;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f6f04
	if (!cr6.lt) goto loc_826F6F04;
loc_826F6EAC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f6f04
	if (cr6.eq) goto loc_826F6F04;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f6ef4
	if (!cr0.lt) goto loc_826F6EF4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F6EF4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f6eac
	if (cr6.gt) goto loc_826F6EAC;
loc_826F6F04:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f6f40
	if (!cr0.lt) goto loc_826F6F40;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F6F40:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f6f58
	if (!cr6.eq) goto loc_826F6F58;
	// stw r27,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r27.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F6F58:
	// stw r25,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r25.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F6F60:
	// stw r7,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r7.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F6F68:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x826f7040
	if (cr6.eq) goto loc_826F7040;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f6fe4
	if (!cr6.lt) goto loc_826F6FE4;
loc_826F6F8C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f6fe4
	if (cr6.eq) goto loc_826F6FE4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f6fd4
	if (!cr0.lt) goto loc_826F6FD4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F6FD4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f6f8c
	if (cr6.gt) goto loc_826F6F8C;
loc_826F6FE4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f7020
	if (!cr0.lt) goto loc_826F7020;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F7020:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f7038
	if (!cr6.eq) goto loc_826F7038;
	// stw r26,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r26.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F7038:
	// stw r24,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r24.u32);
	// b 0x826f7044
	goto loc_826F7044;
loc_826F7040:
	// stw r6,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r6.u32);
loc_826F7044:
	// lhz r11,126(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 126);
	// lhz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 124);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lwz r7,268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lwz r29,244(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// srawi r11,r23,16
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xFFFF) != 0);
	r11.s64 = r23.s32 >> 16;
	// stw r22,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r22.u32);
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,296(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// and r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ctx.r7.u64;
	// lwz r7,276(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// lwz r9,288(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// rlwinm r9,r9,0,30,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// sth r11,4(r29)
	PPC_STORE_U16(r29.u32 + 4, r11.u16);
	// sth r10,6(r29)
	PPC_STORE_U16(r29.u32 + 6, ctx.r10.u16);
	// beq cr6,0x826f70e0
	if (cr6.eq) goto loc_826F70E0;
	// lwz r11,1508(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1508);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r4,176(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 176);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f70cc
	if (cr6.eq) goto loc_826F70CC;
	// bl 0x826eb3a0
	sub_826EB3A0(ctx, base);
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// b 0x826f70e0
	goto loc_826F70E0;
loc_826F70CC:
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// rlwinm r11,r3,1,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFE0000;
	// clrlwi r10,r3,16
	ctx.r10.u64 = ctx.r3.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
loc_826F70E0:
	// lwz r30,1516(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 1516);
	// lis r26,1
	r26.s64 = 65536;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r11,212(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// rlwinm r4,r30,17,0,14
	ctx.r4.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 17) & 0xFFFE0000;
	// stw r22,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r22.u32);
	// mr r31,r22
	r31.u64 = r22.u64;
	// stw r22,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r22.u32);
	// subf r27,r4,r26
	r27.s64 = r26.s64 - ctx.r4.s64;
	// stw r22,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r22.u32);
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// stw r22,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r22.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// stw r22,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r22.u32);
	// stw r27,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r27.u32);
	// beq cr6,0x826f72f8
	if (cr6.eq) goto loc_826F72F8;
	// lwz r10,328(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	// lwz r11,320(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826f72f8
	if (cr6.eq) goto loc_826F72F8;
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f72ac
	if (cr6.eq) goto loc_826F72AC;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// srawi r8,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 16;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r6,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// xor r11,r7,r6
	r11.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// bgt cr6,0x826f71c0
	if (cr6.gt) goto loc_826F71C0;
	// lwz r7,1560(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f71a4
	if (!cr6.lt) goto loc_826F71A4;
	// lwz r11,1576(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f71c0
	goto loc_826F71C0;
loc_826F71A4:
	// lwz r11,1580(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// lwz r7,1568(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F71C0:
	// lhz r11,62(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f71d8
	if (!cr6.gt) goto loc_826F71D8;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f71e8
	goto loc_826F71E8;
loc_826F71D8:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f71e8
	if (!cr6.lt) goto loc_826F71E8;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F71E8:
	// rlwinm r11,r30,1,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f721c
	if (!cr6.gt) goto loc_826F721C;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f7260
	goto loc_826F7260;
loc_826F721C:
	// lwz r7,1564(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f723c
	if (!cr6.lt) goto loc_826F723C;
	// lwz r9,1576(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f7260
	goto loc_826F7260;
loc_826F723C:
	// lwz r8,1572(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1572);
	// lwz r7,1580(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F7260:
	// lhz r9,64(r28)
	ctx.r9.u64 = PPC_LOAD_U16(r28.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f7288
	if (!cr6.gt) goto loc_826F7288;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// li r31,1
	r31.s64 = 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// b 0x826f72ec
	goto loc_826F72EC;
loc_826F7288:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f7298
	if (!cr6.lt) goto loc_826F7298;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F7298:
	// li r31,1
	r31.s64 = 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// b 0x826f72ec
	goto loc_826F72EC;
loc_826F72AC:
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// lwz r10,1584(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1584);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// rlwinm r8,r8,0,0,22
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFE00;
	// li r3,1
	ctx.r3.s64 = 1;
	// mullw r11,r8,r10
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// rlwinm r11,r11,0,0,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFE0000;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_826F72EC:
	// li r11,1
	r11.s64 = 1;
	// li r6,1
	ctx.r6.s64 = 1;
	// stw r11,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r11.u32);
loc_826F72F8:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f7450
	if (cr6.eq) goto loc_826F7450;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r7,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r11.s32 >> 16;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r25,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r25.s64 = ctx.r8.s32 >> 31;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// xor r8,r8,r25
	ctx.r8.u64 = ctx.r8.u64 ^ r25.u64;
	// addi r24,r1,80
	r24.s64 = ctx.r1.s64 + 80;
	// subf r8,r25,r8
	ctx.r8.s64 = ctx.r8.s64 - r25.s64;
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// stwx r11,r5,r24
	PPC_STORE_U32(ctx.r5.u32 + r24.u32, r11.u32);
	// bgt cr6,0x826f7370
	if (cr6.gt) goto loc_826F7370;
	// lwz r11,1560(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x826f7354
	if (!cr6.lt) goto loc_826F7354;
	// lwz r11,1576(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f7370
	goto loc_826F7370;
loc_826F7354:
	// lwz r11,1580(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// lwz r8,1568(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F7370:
	// lhz r11,62(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f7388
	if (!cr6.gt) goto loc_826F7388;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f7398
	goto loc_826F7398;
loc_826F7388:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f7398
	if (!cr6.lt) goto loc_826F7398;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F7398:
	// rlwinm r11,r30,1,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f73cc
	if (!cr6.gt) goto loc_826F73CC;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f7410
	goto loc_826F7410;
loc_826F73CC:
	// lwz r7,1564(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f73ec
	if (!cr6.lt) goto loc_826F73EC;
	// lwz r9,1576(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f7410
	goto loc_826F7410;
loc_826F73EC:
	// lwz r8,1572(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1572);
	// lwz r7,1580(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F7410:
	// lhz r9,64(r28)
	ctx.r9.u64 = PPC_LOAD_U16(r28.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f7428
	if (!cr6.gt) goto loc_826F7428;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x826f7438
	goto loc_826F7438;
loc_826F7428:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f7438
	if (!cr6.lt) goto loc_826F7438;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F7438:
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stwx r10,r5,r9
	PPC_STORE_U32(ctx.r5.u32 + ctx.r9.u32, ctx.r10.u32);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// b 0x826f749c
	goto loc_826F749C;
loc_826F7450:
	// srawi r7,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r7.s64 = r11.s32 >> 8;
	// lwz r10,1584(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1584);
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// rlwinm r7,r7,0,0,22
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFE00;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// mullw r7,r7,r10
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// stwx r11,r9,r5
	PPC_STORE_U32(ctx.r9.u32 + ctx.r5.u32, r11.u32);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r7,0,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFE0000;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// addi r25,r1,80
	r25.s64 = ctx.r1.s64 + 80;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// stwx r11,r9,r25
	PPC_STORE_U32(ctx.r9.u32 + r25.u32, r11.u32);
loc_826F749C:
	// lwz r11,360(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	// addi r7,r6,1
	ctx.r7.s64 = ctx.r6.s64 + 1;
	// stw r6,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r6.u32);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// ble cr6,0x826f7658
	if (!cr6.gt) goto loc_826F7658;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f7608
	if (cr6.eq) goto loc_826F7608;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r6,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r6.s64 = r11.s32 >> 16;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r4,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = ctx.r8.s32 >> 31;
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// xor r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r4.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// subf r8,r4,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r4.s64;
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// stwx r11,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, r11.u32);
	// bgt cr6,0x826f7528
	if (cr6.gt) goto loc_826F7528;
	// lwz r11,1560(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x826f750c
	if (!cr6.lt) goto loc_826F750C;
	// lwz r11,1576(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f7528
	goto loc_826F7528;
loc_826F750C:
	// lwz r11,1580(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// lwz r8,1568(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F7528:
	// lhz r11,62(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f7540
	if (!cr6.gt) goto loc_826F7540;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f7550
	goto loc_826F7550;
loc_826F7540:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f7550
	if (!cr6.lt) goto loc_826F7550;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F7550:
	// rlwinm r11,r30,1,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r6,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r6.u64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f7584
	if (!cr6.gt) goto loc_826F7584;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f75c8
	goto loc_826F75C8;
loc_826F7584:
	// lwz r6,1564(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 1564);
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// bge cr6,0x826f75a4
	if (!cr6.lt) goto loc_826F75A4;
	// lwz r9,1576(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f75c8
	goto loc_826F75C8;
loc_826F75A4:
	// lwz r8,1572(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1572);
	// lwz r6,1580(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r6,r11
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F75C8:
	// lhz r9,64(r28)
	ctx.r9.u64 = PPC_LOAD_U16(r28.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f75e0
	if (!cr6.gt) goto loc_826F75E0;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x826f75f0
	goto loc_826F75F0;
loc_826F75E0:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f75f0
	if (!cr6.lt) goto loc_826F75F0;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F75F0:
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// stwx r10,r5,r9
	PPC_STORE_U32(ctx.r5.u32 + ctx.r9.u32, ctx.r10.u32);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// b 0x826f7654
	goto loc_826F7654;
loc_826F7608:
	// srawi r6,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r6.s64 = r11.s32 >> 8;
	// lwz r10,1584(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1584);
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// rlwinm r6,r6,0,0,22
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFE00;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// mullw r6,r6,r10
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r10.s32);
	// stwx r11,r9,r5
	PPC_STORE_U32(ctx.r9.u32 + ctx.r5.u32, r11.u32);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r6,0,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFE0000;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// addi r31,r1,80
	r31.s64 = ctx.r1.s64 + 80;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// stwx r11,r9,r31
	PPC_STORE_U32(ctx.r9.u32 + r31.u32, r11.u32);
loc_826F7654:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
loc_826F7658:
	// lwz r11,1320(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1320);
	// stw r22,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r22.u32);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826f7678
	if (cr6.eq) goto loc_826F7678;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x826f7678
	if (cr6.eq) goto loc_826F7678;
	// stw r22,368(r1)
	PPC_STORE_U32(ctx.r1.u32 + 368, r22.u32);
	// b 0x826f7680
	goto loc_826F7680;
loc_826F7678:
	// li r11,1
	r11.s64 = 1;
	// stw r11,368(r1)
	PPC_STORE_U32(ctx.r1.u32 + 368, r11.u32);
loc_826F7680:
	// li r4,0
	ctx.r4.s64 = 0;
	// lhz r16,98(r1)
	r16.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// li r5,0
	ctx.r5.s64 = 0;
	// lhz r15,96(r1)
	r15.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// lhz r14,82(r1)
	r14.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// stw r4,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r4.u32);
	// stw r5,304(r1)
	PPC_STORE_U32(ctx.r1.u32 + 304, ctx.r5.u32);
	// ble cr6,0x826f77ec
	if (!cr6.gt) goto loc_826F77EC;
	// lhz r11,106(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 106);
	// extsh r9,r16
	ctx.r9.s64 = r16.s16;
	// lhz r10,102(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// extsh r6,r15
	ctx.r6.s64 = r15.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 104);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r7,100(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 100);
	// lhz r5,90(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r28,r9,r10
	r28.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lhz r4,86(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// subf r27,r11,r10
	r27.s64 = ctx.r10.s64 - r11.s64;
	// lhz r31,88(r1)
	r31.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r30,84(r1)
	r30.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// subf r26,r9,r11
	r26.s64 = r11.s64 - ctx.r9.s64;
	// lhz r29,80(r1)
	r29.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// xor r24,r27,r28
	r24.u64 = r27.u64 ^ r28.u64;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r3,r14
	ctx.r3.s64 = r14.s16;
	// subf r27,r6,r7
	r27.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r25,r8,r7
	r25.s64 = ctx.r7.s64 - ctx.r8.s64;
	// subf r23,r6,r8
	r23.s64 = ctx.r8.s64 - ctx.r6.s64;
	// xor r22,r26,r28
	r22.u64 = r26.u64 ^ r28.u64;
	// subf r21,r5,r4
	r21.s64 = ctx.r4.s64 - ctx.r5.s64;
	// subf r28,r3,r4
	r28.s64 = ctx.r4.s64 - ctx.r3.s64;
	// extsh r31,r31
	r31.s64 = r31.s16;
	// extsh r30,r30
	r30.s64 = r30.s16;
	// extsh r29,r29
	r29.s64 = r29.s16;
	// xor r20,r25,r27
	r20.u64 = r25.u64 ^ r27.u64;
	// srawi r26,r24,31
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7FFFFFFF) != 0);
	r26.s64 = r24.s32 >> 31;
	// xor r23,r23,r27
	r23.u64 = r23.u64 ^ r27.u64;
	// subf r19,r3,r5
	r19.s64 = ctx.r5.s64 - ctx.r3.s64;
	// srawi r27,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r27.s64 = r22.s32 >> 31;
	// xor r22,r21,r28
	r22.u64 = r21.u64 ^ r28.u64;
	// subf r18,r31,r30
	r18.s64 = r30.s64 - r31.s64;
	// subf r25,r29,r30
	r25.s64 = r30.s64 - r29.s64;
	// srawi r24,r20,31
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x7FFFFFFF) != 0);
	r24.s64 = r20.s32 >> 31;
	// subf r21,r29,r31
	r21.s64 = r31.s64 - r29.s64;
	// xor r20,r19,r28
	r20.u64 = r19.u64 ^ r28.u64;
	// xor r19,r18,r25
	r19.u64 = r18.u64 ^ r25.u64;
	// srawi r28,r23,31
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7FFFFFFF) != 0);
	r28.s64 = r23.s32 >> 31;
	// srawi r23,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r23.s64 = r22.s32 >> 31;
	// xor r21,r21,r25
	r21.u64 = r21.u64 ^ r25.u64;
	// srawi r25,r20,31
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x7FFFFFFF) != 0);
	r25.s64 = r20.s32 >> 31;
	// srawi r22,r19,31
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x7FFFFFFF) != 0);
	r22.s64 = r19.s32 >> 31;
	// srawi r21,r21,31
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x7FFFFFFF) != 0);
	r21.s64 = r21.s32 >> 31;
	// and r9,r27,r9
	ctx.r9.u64 = r27.u64 & ctx.r9.u64;
	// and r10,r26,r10
	ctx.r10.u64 = r26.u64 & ctx.r10.u64;
	// nor r20,r22,r21
	r20.u64 = ~(r22.u64 | r21.u64);
	// and r6,r28,r6
	ctx.r6.u64 = r28.u64 & ctx.r6.u64;
	// and r7,r24,r7
	ctx.r7.u64 = r24.u64 & ctx.r7.u64;
	// nor r19,r26,r27
	r19.u64 = ~(r26.u64 | r27.u64);
	// and r4,r23,r4
	ctx.r4.u64 = r23.u64 & ctx.r4.u64;
	// nor r18,r24,r28
	r18.u64 = ~(r24.u64 | r28.u64);
	// nor r17,r23,r25
	r17.u64 = ~(r23.u64 | r25.u64);
	// and r3,r25,r3
	ctx.r3.u64 = r25.u64 & ctx.r3.u64;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// and r30,r22,r30
	r30.u64 = r22.u64 & r30.u64;
	// and r31,r20,r31
	r31.u64 = r20.u64 & r31.u64;
	// or r9,r6,r7
	ctx.r9.u64 = ctx.r6.u64 | ctx.r7.u64;
	// and r11,r19,r11
	r11.u64 = r19.u64 & r11.u64;
	// and r6,r17,r5
	ctx.r6.u64 = r17.u64 & ctx.r5.u64;
	// or r7,r3,r4
	ctx.r7.u64 = ctx.r3.u64 | ctx.r4.u64;
	// and r8,r18,r8
	ctx.r8.u64 = r18.u64 & ctx.r8.u64;
	// or r5,r30,r31
	ctx.r5.u64 = r30.u64 | r31.u64;
	// and r4,r21,r29
	ctx.r4.u64 = r21.u64 & r29.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 | ctx.r8.u64;
	// or r8,r5,r4
	ctx.r8.u64 = ctx.r5.u64 | ctx.r4.u64;
	// lwz r5,304(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	// or r9,r7,r6
	ctx.r9.u64 = ctx.r7.u64 | ctx.r6.u64;
	// lwz r4,332(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// sth r11,134(r1)
	PPC_STORE_U16(ctx.r1.u32 + 134, r11.u16);
	// sth r10,132(r1)
	PPC_STORE_U16(ctx.r1.u32 + 132, ctx.r10.u16);
	// sth r8,136(r1)
	PPC_STORE_U16(ctx.r1.u32 + 136, ctx.r8.u16);
	// sth r9,138(r1)
	PPC_STORE_U16(ctx.r1.u32 + 138, ctx.r9.u16);
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r6,136(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// b 0x826f780c
	goto loc_826F780C;
loc_826F77EC:
	// bne cr6,0x826f77fc
	if (!cr6.eq) goto loc_826F77FC;
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x826f7804
	goto loc_826F7804;
loc_826F77FC:
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
loc_826F7804:
	// stw r6,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r6.u32);
	// stw r10,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r10.u32);
loc_826F780C:
	// lwz r11,204(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f7aa4
	if (cr6.eq) goto loc_826F7AA4;
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lwz r27,96(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r26,80(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r7,368(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r27,456(r1)
	PPC_STORE_U32(ctx.r1.u32 + 456, r27.u32);
	// lwzx r25,r9,r11
	r25.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// lwzx r24,r8,r11
	r24.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// stw r26,448(r1)
	PPC_STORE_U32(ctx.r1.u32 + 448, r26.u32);
	// stw r25,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r25.u32);
	// stw r24,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, r24.u32);
	// bne cr6,0x826f797c
	if (!cr6.eq) goto loc_826F797C;
	// lhz r11,132(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 132);
	// extsh r8,r15
	ctx.r8.s64 = r15.s16;
	// lhz r9,134(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 134);
	// extsh r7,r16
	ctx.r7.s64 = r16.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// subf r8,r8,r11
	ctx.r8.s64 = r11.s64 - ctx.r8.s64;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// srawi r3,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 31;
	// srawi r31,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r7.s32 >> 31;
	// xor r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r3.u64;
	// xor r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 ^ r31.u64;
	// subf r8,r3,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r3.s64;
	// subf r7,r31,r7
	ctx.r7.s64 = ctx.r7.s64 - r31.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmpwi cr6,r8,32
	cr6.compare<int32_t>(ctx.r8.s32, 32, xer);
	// bgt cr6,0x826f78dc
	if (cr6.gt) goto loc_826F78DC;
	// lhz r8,208(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 208);
	// lhz r7,210(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 210);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f78e4
	if (!cr6.gt) goto loc_826F78E4;
loc_826F78DC:
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r4,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r4.u32);
loc_826F78E4:
	// lhz r11,136(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 136);
	// extsh r7,r14
	ctx.r7.s64 = r14.s16;
	// lhz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,138(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 138);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// subf r8,r8,r11
	ctx.r8.s64 = r11.s64 - ctx.r8.s64;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// srawi r3,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 31;
	// srawi r31,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r7.s32 >> 31;
	// xor r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r3.u64;
	// xor r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 ^ r31.u64;
	// subf r8,r3,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r3.s64;
	// subf r7,r31,r7
	ctx.r7.s64 = ctx.r7.s64 - r31.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmpwi cr6,r8,32
	cr6.compare<int32_t>(ctx.r8.s32, 32, xer);
	// bgt cr6,0x826f7970
	if (cr6.gt) goto loc_826F7970;
	// lhz r8,248(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 248);
	// lhz r7,250(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 250);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f7ab4
	if (!cr6.gt) goto loc_826F7AB4;
loc_826F7970:
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r5,304(r1)
	PPC_STORE_U32(ctx.r1.u32 + 304, ctx.r5.u32);
	// b 0x826f7ab4
	goto loc_826F7AB4;
loc_826F797C:
	// lhz r11,134(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 134);
	// extsh r8,r16
	ctx.r8.s64 = r16.s16;
	// lhz r9,132(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 132);
	// extsh r7,r15
	ctx.r7.s64 = r15.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// subf r8,r8,r11
	ctx.r8.s64 = r11.s64 - ctx.r8.s64;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r3,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 31;
	// srawi r31,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r7.s32 >> 31;
	// xor r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r3.u64;
	// xor r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 ^ r31.u64;
	// subf r8,r3,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r3.s64;
	// subf r7,r31,r7
	ctx.r7.s64 = ctx.r7.s64 - r31.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmpwi cr6,r8,32
	cr6.compare<int32_t>(ctx.r8.s32, 32, xer);
	// bgt cr6,0x826f7a04
	if (cr6.gt) goto loc_826F7A04;
	// lhz r8,210(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 210);
	// lhz r7,208(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 208);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f7a0c
	if (!cr6.gt) goto loc_826F7A0C;
loc_826F7A04:
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r4,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r4.u32);
loc_826F7A0C:
	// lhz r11,138(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 138);
	// extsh r8,r14
	ctx.r8.s64 = r14.s16;
	// lhz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 136);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// subf r8,r8,r11
	ctx.r8.s64 = r11.s64 - ctx.r8.s64;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// srawi r3,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 31;
	// srawi r31,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r7.s32 >> 31;
	// xor r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r3.u64;
	// xor r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 ^ r31.u64;
	// subf r8,r3,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r3.s64;
	// subf r7,r31,r7
	ctx.r7.s64 = ctx.r7.s64 - r31.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmpwi cr6,r8,32
	cr6.compare<int32_t>(ctx.r8.s32, 32, xer);
	// bgt cr6,0x826f7a98
	if (cr6.gt) goto loc_826F7A98;
	// lhz r8,250(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 250);
	// lhz r7,248(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 248);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f7ab4
	if (!cr6.gt) goto loc_826F7AB4;
loc_826F7A98:
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r5,304(r1)
	PPC_STORE_U32(ctx.r1.u32 + 304, ctx.r5.u32);
	// b 0x826f7ab4
	goto loc_826F7AB4;
loc_826F7AA4:
	// lwz r24,248(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// lwz r26,448(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	// lwz r25,208(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// lwz r27,456(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 456);
loc_826F7AB4:
	// lwz r28,780(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	// lwz r11,1508(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1508);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f7adc
	if (cr6.eq) goto loc_826F7ADC;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r8,108(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f7ae8
	if (!cr6.gt) goto loc_826F7AE8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f7af0
	if (!cr6.eq) goto loc_826F7AF0;
loc_826F7ADC:
	// lwz r11,1512(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1512);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f7af0
	if (cr6.eq) goto loc_826F7AF0;
loc_826F7AE8:
	// li r9,1
	ctx.r9.s64 = 1;
	// b 0x826f7af4
	goto loc_826F7AF4;
loc_826F7AF0:
	// lwz r9,312(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
loc_826F7AF4:
	// lwz r23,116(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// rlwinm r11,r23,0,15,15
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f7ce4
	if (cr6.eq) goto loc_826F7CE4;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826f7bf8
	if (!cr6.eq) goto loc_826F7BF8;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x826f7bec
	if (cr6.eq) goto loc_826F7BEC;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f7b88
	if (!cr6.lt) goto loc_826F7B88;
loc_826F7B30:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f7b88
	if (cr6.eq) goto loc_826F7B88;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f7b78
	if (!cr0.lt) goto loc_826F7B78;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F7B78:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f7b30
	if (cr6.gt) goto loc_826F7B30;
loc_826F7B88:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f7bc4
	if (!cr0.lt) goto loc_826F7BC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F7BC4:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f7be0
	if (!cr6.eq) goto loc_826F7BE0;
	// mr r11,r26
	r11.u64 = r26.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f7ea0
	goto loc_826F7EA0;
loc_826F7BE0:
	// mr r11,r24
	r11.u64 = r24.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f7ea0
	goto loc_826F7EA0;
loc_826F7BEC:
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f7ea0
	goto loc_826F7EA0;
loc_826F7BF8:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x826f7cd8
	if (cr6.eq) goto loc_826F7CD8;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f7c74
	if (!cr6.lt) goto loc_826F7C74;
loc_826F7C1C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f7c74
	if (cr6.eq) goto loc_826F7C74;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f7c64
	if (!cr0.lt) goto loc_826F7C64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F7C64:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f7c1c
	if (cr6.gt) goto loc_826F7C1C;
loc_826F7C74:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f7cb0
	if (!cr0.lt) goto loc_826F7CB0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F7CB0:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f7ccc
	if (!cr6.eq) goto loc_826F7CCC;
	// mr r11,r27
	r11.u64 = r27.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f7ea0
	goto loc_826F7EA0;
loc_826F7CCC:
	// mr r11,r25
	r11.u64 = r25.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f7ea0
	goto loc_826F7EA0;
loc_826F7CD8:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// addis r10,r11,1
	ctx.r10.s64 = r11.s64 + 65536;
	// b 0x826f7ea0
	goto loc_826F7EA0;
loc_826F7CE4:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826f7dc4
	if (!cr6.eq) goto loc_826F7DC4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x826f7ea0
	if (cr6.eq) goto loc_826F7EA0;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f7d68
	if (!cr6.lt) goto loc_826F7D68;
loc_826F7D10:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f7d68
	if (cr6.eq) goto loc_826F7D68;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f7d58
	if (!cr0.lt) goto loc_826F7D58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F7D58:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f7d10
	if (cr6.gt) goto loc_826F7D10;
loc_826F7D68:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f7da4
	if (!cr0.lt) goto loc_826F7DA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F7DA4:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f7dbc
	if (!cr6.eq) goto loc_826F7DBC;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// b 0x826f7ea0
	goto loc_826F7EA0;
loc_826F7DBC:
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// b 0x826f7ea0
	goto loc_826F7EA0;
loc_826F7DC4:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x826f7e9c
	if (cr6.eq) goto loc_826F7E9C;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f7e40
	if (!cr6.lt) goto loc_826F7E40;
loc_826F7DE8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f7e40
	if (cr6.eq) goto loc_826F7E40;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f7e30
	if (!cr0.lt) goto loc_826F7E30;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F7E30:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f7de8
	if (cr6.gt) goto loc_826F7DE8;
loc_826F7E40:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f7e7c
	if (!cr0.lt) goto loc_826F7E7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F7E7C:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f7e94
	if (!cr6.eq) goto loc_826F7E94;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// b 0x826f7ea0
	goto loc_826F7EA0;
loc_826F7E94:
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// b 0x826f7ea0
	goto loc_826F7EA0;
loc_826F7E9C:
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
loc_826F7EA0:
	// lwz r9,328(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	// li r18,0
	r18.s64 = 0;
	// lwz r11,320(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stw r10,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r10.u32);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,268(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// stw r18,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r18.u32);
	// add r8,r11,r10
	ctx.r8.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,120(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 120);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// lhz r11,170(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 170);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// srawi r11,r23,16
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xFFFF) != 0);
	r11.s64 = r23.s32 >> 16;
	// stw r8,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, ctx.r8.u32);
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,296(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// add r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// and r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 & ctx.r6.u64;
	// lwz r6,276(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// sth r9,122(r1)
	PPC_STORE_U16(ctx.r1.u32 + 122, ctx.r9.u16);
	// sth r11,120(r1)
	PPC_STORE_U16(ctx.r1.u32 + 120, r11.u16);
	// lwz r31,120(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r31,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r31.u32);
	// beq cr6,0x826f7f64
	if (cr6.eq) goto loc_826F7F64;
	// lwz r11,1508(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1508);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r4,176(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 176);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f7f50
	if (cr6.eq) goto loc_826F7F50;
	// bl 0x826eb3a0
	sub_826EB3A0(ctx, base);
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// b 0x826f7f64
	goto loc_826F7F64;
loc_826F7F50:
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// rlwinm r11,r3,1,0,14
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFE0000;
	// clrlwi r10,r3,16
	ctx.r10.u64 = ctx.r3.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
loc_826F7F64:
	// mr r5,r18
	ctx.r5.u64 = r18.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// rlwinm r11,r31,0,15,15
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// stw r4,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r4.u32);
	// beq cr6,0x826f80cc
	if (cr6.eq) goto loc_826F80CC;
	// extsh r11,r31
	r11.s64 = r31.s16;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// srawi r7,r31,16
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = r31.s32 >> 16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x826f7fb0
	if (!cr6.gt) goto loc_826F7FB0;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// b 0x826f7fe8
	goto loc_826F7FE8;
loc_826F7FB0:
	// lwz r8,1560(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// bge cr6,0x826f7fcc
	if (!cr6.lt) goto loc_826F7FCC;
	// lwz r10,1576(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// b 0x826f7fe8
	goto loc_826F7FE8;
loc_826F7FCC:
	// lwz r9,1580(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// lwz r8,1568(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1568);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// xor r9,r8,r10
	ctx.r9.u64 = ctx.r8.u64 ^ ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// subf r8,r10,r11
	ctx.r8.s64 = r11.s64 - ctx.r10.s64;
loc_826F7FE8:
	// lhz r11,62(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 62);
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// ble cr6,0x826f8000
	if (!cr6.gt) goto loc_826F8000;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// b 0x826f8010
	goto loc_826F8010;
loc_826F8000:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x826f8010
	if (!cr6.lt) goto loc_826F8010;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_826F8010:
	// lwz r6,1516(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 1516);
	// rlwinm r11,r6,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// cmpwi cr6,r9,63
	cr6.compare<int32_t>(ctx.r9.s32, 63, xer);
	// ble cr6,0x826f8048
	if (!cr6.gt) goto loc_826F8048;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f808c
	goto loc_826F808C;
loc_826F8048:
	// lwz r7,1564(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1564);
	// cmpw cr6,r9,r7
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f8068
	if (!cr6.lt) goto loc_826F8068;
	// lwz r10,1576(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f808c
	goto loc_826F808C;
loc_826F8068:
	// lwz r9,1572(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1572);
	// lwz r7,1580(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// xor r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r10.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// srawi r9,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r9.s64 = r11.s32 >> 7;
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r9,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_826F808C:
	// lhz r10,64(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 64);
	// addi r9,r10,-2
	ctx.r9.s64 = ctx.r10.s64 + -2;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f80a4
	if (!cr6.gt) goto loc_826F80A4;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// b 0x826f80b4
	goto loc_826F80B4;
loc_826F80A4:
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826f80b4
	if (!cr6.lt) goto loc_826F80B4;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_826F80B4:
	// li r4,1
	ctx.r4.s64 = 1;
	// rlwimi r8,r11,16,0,15
	ctx.r8.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r8.u64 & 0xFFFFFFFF0000FFFF);
	// lis r3,1
	ctx.r3.s64 = 65536;
	// stw r4,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r4.u32);
	// stw r8,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r8.u32);
	// b 0x826f8118
	goto loc_826F8118;
loc_826F80CC:
	// srawi r9,r31,8
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFF) != 0);
	ctx.r9.s64 = r31.s32 >> 8;
	// lwz r11,1584(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1584);
	// lwz r6,1516(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 1516);
	// extsh r10,r31
	ctx.r10.s64 = r31.s16;
	// rlwinm r9,r9,0,0,22
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFE00;
	// stw r31,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r31.u32);
	// rlwinm r8,r6,17,0,14
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 17) & 0xFFFE0000;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r9,r9,0,0,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFE0000;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lis r3,1
	ctx.r3.s64 = 65536;
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_826F8118:
	// lwz r7,244(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r11,4(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f8278
	if (cr6.eq) goto loc_826F8278;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// srawi r8,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 16;
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r30,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r31.s32 >> 31;
	// xor r11,r31,r30
	r11.u64 = r31.u64 ^ r30.u64;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// bgt cr6,0x826f818c
	if (cr6.gt) goto loc_826F818C;
	// lwz r31,1560(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// cmpw cr6,r11,r31
	cr6.compare<int32_t>(r11.s32, r31.s32, xer);
	// bge cr6,0x826f8170
	if (!cr6.lt) goto loc_826F8170;
	// lwz r11,1576(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f818c
	goto loc_826F818C;
loc_826F8170:
	// lwz r11,1580(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// lwz r31,1568(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r31,r9
	ctx.r10.u64 = r31.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F818C:
	// lhz r11,62(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f81a4
	if (!cr6.gt) goto loc_826F81A4;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f81b4
	goto loc_826F81B4;
loc_826F81A4:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f81b4
	if (!cr6.lt) goto loc_826F81B4;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F81B4:
	// rlwinm r11,r6,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r31,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 ^ r31.u64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f81e8
	if (!cr6.gt) goto loc_826F81E8;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f822c
	goto loc_826F822C;
loc_826F81E8:
	// lwz r31,1564(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 1564);
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// bge cr6,0x826f8208
	if (!cr6.lt) goto loc_826F8208;
	// lwz r9,1576(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f822c
	goto loc_826F822C;
loc_826F8208:
	// lwz r8,1572(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1572);
	// lwz r31,1580(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r31,r11
	r11.s64 = int64_t(r31.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F822C:
	// lhz r9,64(r28)
	ctx.r9.u64 = PPC_LOAD_U16(r28.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f8254
	if (!cr6.gt) goto loc_826F8254;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r4,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r4.u32);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// b 0x826f82bc
	goto loc_826F82BC;
loc_826F8254:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f8264
	if (!cr6.lt) goto loc_826F8264;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F8264:
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r4,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r4.u32);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// b 0x826f82bc
	goto loc_826F82BC;
loc_826F8278:
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// lwz r10,1584(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1584);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// rlwinm r8,r8,0,0,22
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFE00;
	// rlwinm r31,r6,17,0,14
	r31.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 17) & 0xFFFE0000;
	// mullw r11,r8,r10
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,0,0,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFE0000;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// subf r11,r31,r11
	r11.s64 = r11.s64 - r31.s64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_826F82BC:
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// rlwinm r10,r11,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f8418
	if (cr6.eq) goto loc_826F8418;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// srawi r8,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 16;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r5,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// xor r11,r7,r5
	r11.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// subf r11,r5,r11
	r11.s64 = r11.s64 - ctx.r5.s64;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// bgt cr6,0x826f832c
	if (cr6.gt) goto loc_826F832C;
	// lwz r7,1560(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f8310
	if (!cr6.lt) goto loc_826F8310;
	// lwz r11,1576(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r10,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r10.s64 = r11.s32 >> 8;
	// b 0x826f832c
	goto loc_826F832C;
loc_826F8310:
	// lwz r11,1580(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// lwz r7,1568(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1568);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// xor r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
loc_826F832C:
	// lhz r11,62(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 62);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x826f8344
	if (!cr6.gt) goto loc_826F8344;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// b 0x826f8354
	goto loc_826F8354;
loc_826F8344:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826f8354
	if (!cr6.lt) goto loc_826F8354;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826F8354:
	// rlwinm r11,r6,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,63
	cr6.compare<int32_t>(ctx.r8.s32, 63, xer);
	// ble cr6,0x826f8388
	if (!cr6.gt) goto loc_826F8388;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x826f83cc
	goto loc_826F83CC;
loc_826F8388:
	// lwz r7,1564(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1564);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x826f83a8
	if (!cr6.lt) goto loc_826F83A8;
	// lwz r9,1576(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 1576);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// b 0x826f83cc
	goto loc_826F83CC;
loc_826F83A8:
	// lwz r8,1572(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 1572);
	// lwz r7,1580(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 1580);
	// xor r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r8,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r8.s64 = r11.s32 >> 7;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826F83CC:
	// lhz r9,64(r28)
	ctx.r9.u64 = PPC_LOAD_U16(r28.u32 + 64);
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f83f4
	if (!cr6.gt) goto loc_826F83F4;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// addi r9,r4,1
	ctx.r9.s64 = ctx.r4.s64 + 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// b 0x826f845c
	goto loc_826F845C;
loc_826F83F4:
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x826f8404
	if (!cr6.lt) goto loc_826F8404;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_826F8404:
	// addi r9,r4,1
	ctx.r9.s64 = ctx.r4.s64 + 1;
	// rlwimi r10,r11,16,0,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r10.u64 & 0xFFFFFFFF0000FFFF);
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// b 0x826f845c
	goto loc_826F845C;
loc_826F8418:
	// srawi r8,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r8.s64 = r11.s32 >> 8;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// lwz r10,1584(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 1584);
	// addi r11,r5,1
	r11.s64 = ctx.r5.s64 + 1;
	// rlwinm r8,r8,0,0,22
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFE00;
	// rlwinm r7,r6,17,0,14
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 17) & 0xFFFE0000;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// mullw r11,r8,r10
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,0,0,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFE0000;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
loc_826F845C:
	// lwz r11,1320(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1320);
	// stw r18,472(r1)
	PPC_STORE_U32(ctx.r1.u32 + 472, r18.u32);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826f8478
	if (cr6.eq) goto loc_826F8478;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// mr r19,r18
	r19.u64 = r18.u64;
	// bne cr6,0x826f847c
	if (!cr6.eq) goto loc_826F847C;
loc_826F8478:
	// li r19,1
	r19.s64 = 1;
loc_826F847C:
	// lhz r9,86(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// mr r17,r18
	r17.u64 = r18.u64;
	// lhz r11,102(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// extsh r30,r9
	r30.s64 = ctx.r9.s16;
	// lhz r9,82(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// extsh r5,r11
	ctx.r5.s64 = r11.s16;
	// lhz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 100);
	// extsh r3,r9
	ctx.r3.s64 = ctx.r9.s16;
	// lhz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// lhz r11,98(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// extsh r4,r10
	ctx.r4.s64 = ctx.r10.s16;
	// extsh r29,r9
	r29.s64 = ctx.r9.s16;
	// lhz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r31,r9
	r31.s64 = ctx.r9.s16;
	// lhz r9,106(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 106);
	// lhz r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 104);
	// subf r28,r11,r5
	r28.s64 = ctx.r5.s64 - r11.s64;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhz r7,90(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 90);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r6,88(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// subf r27,r9,r5
	r27.s64 = ctx.r5.s64 - ctx.r9.s64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r26,r11,r9
	r26.s64 = ctx.r9.s64 - r11.s64;
	// xor r24,r27,r28
	r24.u64 = r27.u64 ^ r28.u64;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// subf r25,r8,r4
	r25.s64 = ctx.r4.s64 - ctx.r8.s64;
	// subf r27,r10,r4
	r27.s64 = ctx.r4.s64 - ctx.r10.s64;
	// xor r22,r26,r28
	r22.u64 = r26.u64 ^ r28.u64;
	// subf r23,r10,r8
	r23.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r21,r7,r30
	r21.s64 = r30.s64 - ctx.r7.s64;
	// subf r28,r3,r30
	r28.s64 = r30.s64 - ctx.r3.s64;
	// xor r25,r25,r27
	r25.u64 = r25.u64 ^ r27.u64;
	// srawi r26,r24,31
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7FFFFFFF) != 0);
	r26.s64 = r24.s32 >> 31;
	// subf r20,r3,r7
	r20.s64 = ctx.r7.s64 - ctx.r3.s64;
	// xor r24,r23,r27
	r24.u64 = r23.u64 ^ r27.u64;
	// xor r23,r21,r28
	r23.u64 = r21.u64 ^ r28.u64;
	// srawi r27,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r27.s64 = r22.s32 >> 31;
	// xor r22,r20,r28
	r22.u64 = r20.u64 ^ r28.u64;
	// srawi r25,r25,31
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x7FFFFFFF) != 0);
	r25.s64 = r25.s32 >> 31;
	// srawi r28,r24,31
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7FFFFFFF) != 0);
	r28.s64 = r24.s32 >> 31;
	// srawi r24,r23,31
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7FFFFFFF) != 0);
	r24.s64 = r23.s32 >> 31;
	// srawi r23,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r23.s64 = r22.s32 >> 31;
	// nor r22,r26,r27
	r22.u64 = ~(r26.u64 | r27.u64);
	// nor r20,r24,r23
	r20.u64 = ~(r24.u64 | r23.u64);
	// and r27,r27,r11
	r27.u64 = r27.u64 & r11.u64;
	// and r26,r26,r5
	r26.u64 = r26.u64 & ctx.r5.u64;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// nor r21,r25,r28
	r21.u64 = ~(r25.u64 | r28.u64);
	// and r23,r23,r3
	r23.u64 = r23.u64 & ctx.r3.u64;
	// and r24,r24,r30
	r24.u64 = r24.u64 & r30.u64;
	// and r15,r28,r10
	r15.u64 = r28.u64 & ctx.r10.u64;
	// and r25,r25,r4
	r25.u64 = r25.u64 & ctx.r4.u64;
	// or r27,r27,r26
	r27.u64 = r27.u64 | r26.u64;
	// subf r28,r31,r29
	r28.s64 = r29.s64 - r31.s64;
	// subf r16,r6,r29
	r16.s64 = r29.s64 - ctx.r6.s64;
	// and r26,r22,r9
	r26.u64 = r22.u64 & ctx.r9.u64;
	// or r24,r23,r24
	r24.u64 = r23.u64 | r24.u64;
	// subf r14,r31,r6
	r14.s64 = ctx.r6.s64 - r31.s64;
	// and r8,r21,r8
	ctx.r8.u64 = r21.u64 & ctx.r8.u64;
	// and r23,r20,r7
	r23.u64 = r20.u64 & ctx.r7.u64;
	// or r25,r15,r25
	r25.u64 = r15.u64 | r25.u64;
	// xor r16,r16,r28
	r16.u64 = r16.u64 ^ r28.u64;
	// or r7,r27,r26
	ctx.r7.u64 = r27.u64 | r26.u64;
	// xor r22,r14,r28
	r22.u64 = r14.u64 ^ r28.u64;
	// or r28,r25,r8
	r28.u64 = r25.u64 | ctx.r8.u64;
	// or r27,r24,r23
	r27.u64 = r24.u64 | r23.u64;
	// srawi r9,r16,31
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r16.s32 >> 31;
	// srawi r8,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r22.s32 >> 31;
	// extsh r26,r7
	r26.s64 = ctx.r7.s16;
	// extsh r28,r28
	r28.s64 = r28.s16;
	// extsh r27,r27
	r27.s64 = r27.s16;
	// nor r7,r9,r8
	ctx.r7.u64 = ~(ctx.r9.u64 | ctx.r8.u64);
	// and r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 & r31.u64;
	// and r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 & r29.u64;
	// sth r26,310(r1)
	PPC_STORE_U16(ctx.r1.u32 + 310, r26.u16);
	// sth r28,308(r1)
	PPC_STORE_U16(ctx.r1.u32 + 308, r28.u16);
	// sth r27,326(r1)
	PPC_STORE_U16(ctx.r1.u32 + 326, r27.u16);
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// and r8,r7,r6
	ctx.r8.u64 = ctx.r7.u64 & ctx.r6.u64;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// sth r9,324(r1)
	PPC_STORE_U16(ctx.r1.u32 + 324, ctx.r9.u16);
	// bne cr6,0x826f868c
	if (!cr6.eq) goto loc_826F868C;
	// extsh r8,r28
	ctx.r8.s64 = r28.s16;
	// extsh r7,r26
	ctx.r7.s64 = r26.s16;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r11,r11,r7
	r11.s64 = ctx.r7.s64 - r11.s64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r6,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 31;
	// srawi r28,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r28.s64 = r11.s32 >> 31;
	// xor r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r6.u64;
	// xor r26,r11,r28
	r26.u64 = r11.u64 ^ r28.u64;
	// subf r11,r6,r10
	r11.s64 = ctx.r10.s64 - ctx.r6.s64;
	// subf r10,r28,r26
	ctx.r10.s64 = r26.s64 - r28.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// bgt cr6,0x826f8640
	if (cr6.gt) goto loc_826F8640;
	// subf r11,r4,r8
	r11.s64 = ctx.r8.s64 - ctx.r4.s64;
	// subf r10,r5,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r5.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// srawi r7,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// xor r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r7.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f8644
	if (!cr6.gt) goto loc_826F8644;
loc_826F8640:
	// li r17,1
	r17.s64 = 1;
loc_826F8644:
	// extsh r11,r9
	r11.s64 = ctx.r9.s16;
	// extsh r10,r27
	ctx.r10.s64 = r27.s16;
	// subf r9,r31,r11
	ctx.r9.s64 = r11.s64 - r31.s64;
	// subf r8,r3,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r3.s64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// srawi r6,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 31;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// xor r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r6.u64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x826f8760
	if (cr6.gt) goto loc_826F8760;
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// subf r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// b 0x826f873c
	goto loc_826F873C;
loc_826F868C:
	// extsh r8,r26
	ctx.r8.s64 = r26.s16;
	// extsh r7,r28
	ctx.r7.s64 = r28.s16;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r6,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = r11.s32 >> 31;
	// srawi r28,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	r28.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r6
	r11.u64 = r11.u64 ^ ctx.r6.u64;
	// xor r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 ^ r28.u64;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// subf r10,r28,r10
	ctx.r10.s64 = ctx.r10.s64 - r28.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// bgt cr6,0x826f86f4
	if (cr6.gt) goto loc_826F86F4;
	// subf r11,r5,r8
	r11.s64 = ctx.r8.s64 - ctx.r5.s64;
	// subf r10,r4,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r4.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// srawi r7,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// xor r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r7.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f86f8
	if (!cr6.gt) goto loc_826F86F8;
loc_826F86F4:
	// li r17,1
	r17.s64 = 1;
loc_826F86F8:
	// extsh r11,r27
	r11.s64 = r27.s16;
	// extsh r10,r9
	ctx.r10.s64 = ctx.r9.s16;
	// subf r9,r3,r11
	ctx.r9.s64 = r11.s64 - ctx.r3.s64;
	// subf r8,r31,r10
	ctx.r8.s64 = ctx.r10.s64 - r31.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// srawi r6,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 31;
	// xor r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r7.u64;
	// xor r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r6.u64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x826f8760
	if (cr6.gt) goto loc_826F8760;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// subf r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826F873C:
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826f8764
	if (!cr6.gt) goto loc_826F8764;
loc_826F8760:
	// li r18,1
	r18.s64 = 1;
loc_826F8764:
	// lwz r10,780(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	// lwz r11,1508(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 1508);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f878c
	if (cr6.eq) goto loc_826F878C;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r8,108(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// ble cr6,0x826f8798
	if (!cr6.gt) goto loc_826F8798;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f87a0
	if (!cr6.eq) goto loc_826F87A0;
loc_826F878C:
	// lwz r11,1512(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 1512);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f87a0
	if (cr6.eq) goto loc_826F87A0;
loc_826F8798:
	// li r9,1
	ctx.r9.s64 = 1;
	// b 0x826f87a4
	goto loc_826F87A4;
loc_826F87A0:
	// lwz r9,472(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 472);
loc_826F87A4:
	// lwz r28,116(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// rlwinm r11,r28,0,15,15
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x10000;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f8994
	if (cr6.eq) goto loc_826F8994;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826f88a8
	if (!cr6.eq) goto loc_826F88A8;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// beq cr6,0x826f889c
	if (cr6.eq) goto loc_826F889C;
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f8838
	if (!cr6.lt) goto loc_826F8838;
loc_826F87E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f8838
	if (cr6.eq) goto loc_826F8838;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f8828
	if (!cr0.lt) goto loc_826F8828;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8828:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f87e0
	if (cr6.gt) goto loc_826F87E0;
loc_826F8838:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f8874
	if (!cr0.lt) goto loc_826F8874;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8874:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f8890
	if (!cr6.eq) goto loc_826F8890;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F8890:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F889C:
	// lwz r11,324(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F88A8:
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// beq cr6,0x826f8988
	if (cr6.eq) goto loc_826F8988;
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f8924
	if (!cr6.lt) goto loc_826F8924;
loc_826F88CC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f8924
	if (cr6.eq) goto loc_826F8924;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f8914
	if (!cr0.lt) goto loc_826F8914;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8914:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f88cc
	if (cr6.gt) goto loc_826F88CC;
loc_826F8924:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f8960
	if (!cr0.lt) goto loc_826F8960;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8960:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f897c
	if (!cr6.eq) goto loc_826F897C;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F897C:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F8988:
	// lwz r11,308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F8994:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826f8a7c
	if (!cr6.eq) goto loc_826F8A7C;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// beq cr6,0x826f8a74
	if (cr6.eq) goto loc_826F8A74;
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f8a18
	if (!cr6.lt) goto loc_826F8A18;
loc_826F89C0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f8a18
	if (cr6.eq) goto loc_826F8A18;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f8a08
	if (!cr0.lt) goto loc_826F8A08;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8A08:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f89c0
	if (cr6.gt) goto loc_826F89C0;
loc_826F8A18:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f8a54
	if (!cr0.lt) goto loc_826F8A54;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8A54:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f8a6c
	if (!cr6.eq) goto loc_826F8A6C;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F8A6C:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F8A74:
	// lwz r11,308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F8A7C:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// beq cr6,0x826f8b54
	if (cr6.eq) goto loc_826F8B54;
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f8af8
	if (!cr6.lt) goto loc_826F8AF8;
loc_826F8AA0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f8af8
	if (cr6.eq) goto loc_826F8AF8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f8ae8
	if (!cr0.lt) goto loc_826F8AE8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8AE8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f8aa0
	if (cr6.gt) goto loc_826F8AA0;
loc_826F8AF8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f8b34
	if (!cr0.lt) goto loc_826F8B34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8B34:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f8b4c
	if (!cr6.eq) goto loc_826F8B4C;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F8B4C:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// b 0x826f8b58
	goto loc_826F8B58;
loc_826F8B54:
	// lwz r11,324(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
loc_826F8B58:
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// lhz r11,130(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 130);
	// lhz r9,128(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 128);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lwz r7,268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lwz r25,780(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	// srawi r11,r28,16
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFFFF) != 0);
	r11.s64 = r28.s32 >> 16;
	// lwz r17,264(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,296(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// and r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ctx.r7.u64;
	// lwz r7,276(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lwz r11,312(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// sth r10,6(r11)
	PPC_STORE_U16(r11.u32 + 6, ctx.r10.u16);
	// sth r9,4(r11)
	PPC_STORE_U16(r11.u32 + 4, ctx.r9.u16);
loc_826F8BBC:
	// lwz r11,480(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f8d30
	if (cr6.eq) goto loc_826F8D30;
	// lwz r11,1176(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1176);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f8ccc
	if (cr6.lt) goto loc_826F8CCC;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f8cbc
	if (!cr6.lt) goto loc_826F8CBC;
loc_826F8C1C:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f8c50
	if (cr6.lt) goto loc_826F8C50;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f8c1c
	if (cr6.eq) goto loc_826F8C1C;
	// lis r11,0
	r11.s64 = 0;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// ori r27,r11,32768
	r27.u64 = r11.u64 | 32768;
	// b 0x826f8d10
	goto loc_826F8D10;
loc_826F8C50:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F8CBC:
	// lis r11,0
	r11.s64 = 0;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// ori r27,r11,32768
	r27.u64 = r11.u64 | 32768;
	// b 0x826f8d10
	goto loc_826F8D10;
loc_826F8CCC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r27,r11,32768
	r27.u64 = r11.u64 | 32768;
loc_826F8CDC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r27
	r11.u64 = r30.u64 + r27.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f8cdc
	if (cr6.lt) goto loc_826F8CDC;
loc_826F8D10:
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826f8d3c
	if (cr6.eq) goto loc_826F8D3C;
loc_826F8D24:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,752
	ctx.r1.s64 = ctx.r1.s64 + 752;
	// b 0x8239bd10
	return;
loc_826F8D30:
	// lis r10,0
	ctx.r10.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// ori r27,r10,32768
	r27.u64 = ctx.r10.u64 | 32768;
loc_826F8D3C:
	// lwz r10,1200(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 1200);
	// lbzx r28,r10,r11
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// stb r28,5(r17)
	PPC_STORE_U8(r17.u32 + 5, r28.u8);
	// lbz r11,27(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 27);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f9010
	if (cr6.eq) goto loc_826F9010;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826f9010
	if (cr6.eq) goto loc_826F9010;
	// lbz r11,1181(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1181);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f8d9c
	if (cr6.eq) goto loc_826F8D9C;
	// lwz r10,0(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r10,r10,20,28,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xF;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f8d88
	if (cr6.eq) goto loc_826F8D88;
	// lbz r11,1182(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// b 0x826f8ff4
	goto loc_826F8FF4;
loc_826F8D88:
	// lbz r11,1180(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1180);
	// lbz r10,1185(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826f8ff4
	goto loc_826F8FF4;
loc_826F8D9C:
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r29,0
	r29.s64 = 0;
	// lbz r11,1186(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826f8e88
	if (cr6.eq) goto loc_826F8E88;
	// li r30,1
	r30.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826f8e1c
	if (!cr6.lt) goto loc_826F8E1C;
loc_826F8DC4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f8e1c
	if (cr6.eq) goto loc_826F8E1C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f8e0c
	if (!cr0.lt) goto loc_826F8E0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8E0C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f8dc4
	if (cr6.gt) goto loc_826F8DC4;
loc_826F8E1C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f8e58
	if (!cr0.lt) goto loc_826F8E58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8E58:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826f8e70
	if (cr6.eq) goto loc_826F8E70;
	// lbz r11,1182(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f8ff8
	goto loc_826F8FF8;
loc_826F8E70:
	// lbz r11,1180(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1180);
	// lbz r10,1185(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f8ff8
	goto loc_826F8FF8;
loc_826F8E88:
	// li r30,3
	r30.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826f8eec
	if (!cr6.lt) goto loc_826F8EEC;
loc_826F8E94:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f8eec
	if (cr6.eq) goto loc_826F8EEC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f8edc
	if (!cr0.lt) goto loc_826F8EDC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8EDC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f8e94
	if (cr6.gt) goto loc_826F8E94;
loc_826F8EEC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f8f28
	if (!cr0.lt) goto loc_826F8F28;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8F28:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x826f8fe8
	if (!cr6.eq) goto loc_826F8FE8;
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r30,5
	r30.s64 = 5;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826f8fa4
	if (!cr6.lt) goto loc_826F8FA4;
loc_826F8F4C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f8fa4
	if (cr6.eq) goto loc_826F8FA4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826f8f94
	if (!cr0.lt) goto loc_826F8F94;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8F94:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f8f4c
	if (cr6.gt) goto loc_826F8F4C;
loc_826F8FA4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f8fe0
	if (!cr0.lt) goto loc_826F8FE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F8FE0:
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x826f8ff0
	goto loc_826F8FF0;
loc_826F8FE8:
	// lbz r11,1180(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1180);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
loc_826F8FF0:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826F8FF4:
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
loc_826F8FF8:
	// stb r11,4(r17)
	PPC_STORE_U8(r17.u32 + 4, r11.u8);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x826f8d24
	if (cr6.lt) goto loc_826F8D24;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// bgt cr6,0x826f8d24
	if (cr6.gt) goto loc_826F8D24;
loc_826F9010:
	// lbz r11,29(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 29);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f91b0
	if (cr6.eq) goto loc_826F91B0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826f91b0
	if (cr6.eq) goto loc_826F91B0;
	// lwz r11,200(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 200);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f9118
	if (cr6.lt) goto loc_826F9118;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f9110
	if (!cr6.lt) goto loc_826F9110;
loc_826F9078:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f90a4
	if (cr6.lt) goto loc_826F90A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f9078
	if (cr6.eq) goto loc_826F9078;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f9154
	goto loc_826F9154;
loc_826F90A4:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F9110:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f9154
	goto loc_826F9154;
loc_826F9118:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F9120:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r27
	r11.u64 = r30.u64 + r27.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f9120
	if (cr6.lt) goto loc_826F9120;
loc_826F9154:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f8d24
	if (!cr6.eq) goto loc_826F8D24;
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// li r9,1
	ctx.r9.s64 = 1;
	// blt cr6,0x826f9174
	if (cr6.lt) goto loc_826F9174;
	// li r9,0
	ctx.r9.s64 = 0;
loc_826F9174:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwimi r11,r9,28,3,3
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 28) & 0x10000000) | (r11.u64 & 0xFFFFFFFFEFFFFFFF);
	// lwz r9,376(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// addi r8,r9,88
	ctx.r8.s64 = ctx.r9.s64 + 88;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// lwzx r11,r10,r9
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// rlwimi r7,r11,24,5,7
	ctx.r7.u64 = (__builtin_rotateleft32(r11.u32, 24) & 0x7000000) | (ctx.r7.u64 & 0xFFFFFFFFF8FFFFFF);
	// stw r7,0(r17)
	PPC_STORE_U32(r17.u32 + 0, ctx.r7.u32);
	// lwzx r11,r10,r8
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// rotlwi r10,r7,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// rlwimi r10,r11,20,10,11
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 20) & 0x300000) | (ctx.r10.u64 & 0xFFFFFFFFFFCFFFFF);
	// rlwinm r11,r10,0,5,3
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFF7FFFFFF;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
loc_826F91B0:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// li r23,0
	r23.s64 = 0;
	// lbz r10,29(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 29);
	// rlwinm r9,r11,0,21,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// lbz r20,5(r17)
	r20.u64 = PPC_LOAD_U8(r17.u32 + 5);
	// lbz r24,34(r25)
	r24.u64 = PPC_LOAD_U8(r25.u32 + 34);
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r22,r11,12,30,31
	r22.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// rlwinm r19,r9,27,31,31
	r19.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826f91e4
	if (cr6.eq) goto loc_826F91E4;
	// rlwinm r24,r11,8,29,31
	r24.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0x7;
loc_826F91E4:
	// lis r11,0
	r11.s64 = 0;
	// lwz r16,788(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	// addi r21,r17,6
	r21.s64 = r17.s64 + 6;
	// subfic r18,r17,-6
	xer.ca = r17.u32 <= 4294967290;
	r18.s64 = -6 - r17.s64;
	// li r14,0
	r14.s64 = 0;
	// ori r15,r11,32768
	r15.u64 = r11.u64 | 32768;
loc_826F91FC:
	// clrlwi r11,r20,31
	r11.u64 = r20.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826f961c
	if (cr6.eq) goto loc_826F961C;
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f9370
	if (cr6.eq) goto loc_826F9370;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x826f9370
	if (!cr6.eq) goto loc_826F9370;
	// lwz r11,440(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 440);
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f9314
	if (cr6.lt) goto loc_826F9314;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826f930c
	if (!cr6.lt) goto loc_826F930C;
loc_826F9274:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f92a0
	if (cr6.lt) goto loc_826F92A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f9274
	if (cr6.eq) goto loc_826F9274;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f9350
	goto loc_826F9350;
loc_826F92A0:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F930C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f9350
	goto loc_826F9350;
loc_826F9314:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F931C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r15
	r11.u64 = r30.u64 + r15.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f931c
	if (cr6.lt) goto loc_826F931C;
loc_826F9350:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f8d24
	if (!cr6.eq) goto loc_826F8D24;
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// add r10,r30,r25
	ctx.r10.u64 = r30.u64 + r25.u64;
	// lbz r24,516(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 516);
	// lbz r22,524(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + 524);
loc_826F9370:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// stb r24,0(r21)
	PPC_STORE_U8(r21.u32 + 0, r24.u8);
	// bne cr6,0x826f93c0
	if (!cr6.eq) goto loc_826F93C0;
	// lwz r31,20(r16)
	r31.u64 = PPC_LOAD_U32(r16.u32 + 20);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r4,236(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + 236);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// lbz r5,924(r25)
	ctx.r5.u64 = PPC_LOAD_U8(r25.u32 + 924);
	// bl 0x826fc940
	sub_826FC940(ctx, base);
	// rlwinm r11,r3,1,24,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFE;
	// lwz r10,24(r16)
	ctx.r10.u64 = PPC_LOAD_U32(r16.u32 + 24);
	// ori r23,r23,1
	r23.u64 = r23.u64 | 1;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// mr r28,r14
	r28.u64 = r14.u64;
	// stw r11,20(r16)
	PPC_STORE_U32(r16.u32 + 20, r11.u32);
	// stb r3,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r3.u8);
	// lwz r11,24(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,24(r16)
	PPC_STORE_U32(r16.u32 + 24, r11.u32);
	// b 0x826f9620
	goto loc_826F9620;
loc_826F93C0:
	// cmpwi cr6,r24,4
	cr6.compare<int32_t>(r24.s32, 4, xer);
	// bne cr6,0x826f9508
	if (!cr6.eq) goto loc_826F9508;
	// lwz r31,0(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r11,444(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 444);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,6,58
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 6) & 0x3F;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f94b0
	if (cr6.lt) goto loc_826F94B0;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826f94a8
	if (!cr6.lt) goto loc_826F94A8;
loc_826F9410:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f943c
	if (cr6.lt) goto loc_826F943C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f9410
	if (cr6.eq) goto loc_826F9410;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f94f0
	goto loc_826F94F0;
loc_826F943C:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F94A8:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f94f0
	goto loc_826F94F0;
loc_826F94B0:
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F94BC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r15
	r11.u64 = r30.u64 + r15.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f94bc
	if (cr6.lt) goto loc_826F94BC;
loc_826F94F0:
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826f8d24
	if (!cr6.eq) goto loc_826F8D24;
	// b 0x826f9594
	goto loc_826F9594;
loc_826F9508:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x826f9590
	if (!cr6.eq) goto loc_826F9590;
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826f9590
	if (!cr6.eq) goto loc_826F9590;
	// lwz r3,0(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826f954c
	if (!cr0.lt) goto loc_826F954C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F954C:
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// bne cr6,0x826f9588
	if (!cr6.eq) goto loc_826F9588;
	// lwz r3,0(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826f9580
	if (!cr0.lt) goto loc_826F9580;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F9580:
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// b 0x826f9594
	goto loc_826F9594;
loc_826F9588:
	// li r11,3
	r11.s64 = 3;
	// b 0x826f9594
	goto loc_826F9594;
loc_826F9590:
	// mr r11,r22
	r11.u64 = r22.u64;
loc_826F9594:
	// add r9,r11,r25
	ctx.r9.u64 = r11.u64 + r25.u64;
	// lwz r28,24(r16)
	r28.u64 = PPC_LOAD_U32(r16.u32 + 24);
	// rlwinm r10,r24,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r30,20(r16)
	r30.u64 = PPC_LOAD_U32(r16.u32 + 20);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lwz r27,236(r25)
	r27.u64 = PPC_LOAD_U32(r25.u32 + 236);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// add r8,r24,r25
	ctx.r8.u64 = r24.u64 + r25.u64;
	// lbz r29,160(r9)
	r29.u64 = PPC_LOAD_U8(ctx.r9.u32 + 160);
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// mr r31,r14
	r31.u64 = r14.u64;
	// or r23,r11,r23
	r23.u64 = r11.u64 | r23.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// lbz r26,924(r8)
	r26.u64 = PPC_LOAD_U8(ctx.r8.u32 + 924);
	// ble cr6,0x826f95fc
	if (!cr6.gt) goto loc_826F95FC;
loc_826F95D0:
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x826fc940
	sub_826FC940(ctx, base);
	// stbx r3,r31,r28
	PPC_STORE_U8(r31.u32 + r28.u32, ctx.r3.u8);
	// rlwinm r11,r3,1,24,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFE;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// cmpw cr6,r31,r29
	cr6.compare<int32_t>(r31.s32, r29.s32, xer);
	// blt cr6,0x826f95d0
	if (cr6.lt) goto loc_826F95D0;
loc_826F95FC:
	// cmpwi cr6,r30,-1
	cr6.compare<int32_t>(r30.s32, -1, xer);
	// beq cr6,0x826f8d24
	if (cr6.eq) goto loc_826F8D24;
	// lwz r11,24(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 24);
	// mr r28,r14
	r28.u64 = r14.u64;
	// stw r30,20(r16)
	PPC_STORE_U32(r16.u32 + 20, r30.u32);
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// stw r11,24(r16)
	PPC_STORE_U32(r16.u32 + 24, r11.u32);
	// b 0x826f9620
	goto loc_826F9620;
loc_826F961C:
	// stb r14,0(r21)
	PPC_STORE_U8(r21.u32 + 0, r14.u8);
loc_826F9620:
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// rlwinm r20,r20,31,1,31
	r20.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 31) & 0x7FFFFFFF;
	// add r11,r18,r21
	r11.u64 = r18.u64 + r21.u64;
	// rldicr r23,r23,8,55
	r23.u64 = __builtin_rotateleft64(r23.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// blt cr6,0x826f91fc
	if (cr6.lt) goto loc_826F91FC;
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r8,r19,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r10,1260(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 1260);
	// rldicl r4,r23,56,8
	ctx.r4.u64 = __builtin_rotateleft64(r23.u64, 56) & 0xFFFFFFFFFFFFFF;
	// rlwinm r9,r11,24,29,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0x7;
	// lbz r11,4(r17)
	r11.u64 = PPC_LOAD_U8(r17.u32 + 4);
	// lbz r7,5(r17)
	ctx.r7.u64 = PPC_LOAD_U8(r17.u32 + 5);
	// mr r29,r14
	r29.u64 = r14.u64;
	// rldicr r6,r11,8,63
	ctx.r6.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lwz r15,212(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r24,316(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// lwz r10,4(r16)
	ctx.r10.u64 = PPC_LOAD_U32(r16.u32 + 4);
	// lis r14,2
	r14.s64 = 131072;
	// neg r5,r11
	ctx.r5.s64 = -r11.s64;
	// xor r11,r5,r11
	r11.u64 = ctx.r5.u64 ^ r11.u64;
	// srawi r11,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 31;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// or r11,r8,r11
	r11.u64 = ctx.r8.u64 | r11.u64;
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// clrldi r11,r11,56
	r11.u64 = r11.u64 & 0xFF;
	// or r11,r6,r11
	r11.u64 = ctx.r6.u64 | r11.u64;
	// lwz r6,788(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	// rldicr r11,r11,48,15
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFF000000000000;
	// or r11,r11,r4
	r11.u64 = r11.u64 | ctx.r4.u64;
loc_826F96A0:
	// lwz r9,1248(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 1248);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r28,384(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	// stdx r11,r10,r9
	PPC_STORE_U64(ctx.r10.u32 + ctx.r9.u32, r11.u64);
	// lwz r11,772(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	// lwz r11,14772(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826f9748
	if (!cr6.gt) goto loc_826F9748;
	// lwz r11,1516(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1516);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f96d4
	if (!cr6.eq) goto loc_826F96D4;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x826f96e0
	goto loc_826F96E0;
loc_826F96D4:
	// lhz r11,52(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 52);
	// lhz r10,50(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 50);
	// mullw r8,r11,r10
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
loc_826F96E0:
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// lhz r9,50(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 50);
	// lwz r5,488(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r11,r8
	ctx.r7.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r7,r5
	r11.u64 = ctx.r7.u64 + ctx.r5.u64;
	// lwz r7,216(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// lwz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r5,r9,r7
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, ctx.r5.u32);
	// lwz r7,216(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// stw r10,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
	// lwz r10,216(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stwx r9,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r9.u32);
	// lwz r10,216(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 216);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
loc_826F9748:
	// lhz r9,18(r6)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r6.u32 + 18);
	// addi r15,r15,1
	r15.s64 = r15.s64 + 1;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// addi r17,r17,20
	r17.s64 = r17.s64 + 20;
	// lwz r11,4(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lwz r23,788(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r15,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r15.u32);
	// cmplw cr6,r15,r28
	cr6.compare<uint32_t>(r15.u32, r28.u32, xer);
	// stw r17,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, r17.u32);
	// li r22,16384
	r22.s64 = 16384;
	// sth r9,18(r6)
	PPC_STORE_U16(ctx.r6.u32 + 18, ctx.r9.u16);
	// li r21,119
	r21.s64 = 119;
	// stw r10,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
	// li r19,0
	r19.s64 = 0;
	// stw r11,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, r11.u32);
	// blt cr6,0x826f2e3c
	if (cr6.lt) goto loc_826F2E3C;
	// lwz r26,496(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	// lwz r27,772(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 772);
loc_826F979C:
	// lhz r9,16(r23)
	ctx.r9.u64 = PPC_LOAD_U16(r23.u32 + 16);
	// rlwinm r11,r28,1,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r24,r26
	cr6.compare<uint32_t>(r24.u32, r26.u32, xer);
	// stw r24,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r24.u32);
	// sth r9,16(r23)
	PPC_STORE_U16(r23.u32 + 16, ctx.r9.u16);
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// blt cr6,0x826f2a14
	if (cr6.lt) goto loc_826F2A14;
	// lwz r31,504(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 504);
loc_826F97CC:
	// lwz r11,32(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 32);
	// li r10,-1
	ctx.r10.s64 = -1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// subf r11,r31,r11
	r11.s64 = r11.s64 - r31.s64;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// stw r11,1304(r25)
	PPC_STORE_U32(r25.u32 + 1304, r11.u32);
	// lwz r11,32(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// ld r11,104(r25)
	r11.u64 = PPC_LOAD_U64(r25.u32 + 104);
	// lwz r10,84(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, r11.u64);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,112(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,116(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,120(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,124(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,128(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,132(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,140(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,144(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,148(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,152(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// addi r1,r1,752
	ctx.r1.s64 = ctx.r1.s64 + 752;
	// b 0x8239bd10
	return;
loc_826F9884:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,752
	ctx.r1.s64 = ctx.r1.s64 + 752;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826F9890"))) PPC_WEAK_FUNC(sub_826F9890);
PPC_FUNC_IMPL(__imp__sub_826F9890) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r9,3960(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// lwz r11,2928(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2928);
	// cmpwi cr6,r9,3
	cr6.compare<int32_t>(ctx.r9.s32, 3, xer);
	// lwz r10,2088(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2088);
	// addi r9,r11,726
	ctx.r9.s64 = r11.s64 + 726;
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r10,263
	ctx.r10.s64 = ctx.r10.s64 + 263;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, ctx.r9.u32);
	// lwzx r9,r8,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// stw r9,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, ctx.r9.u32);
	// lwz r11,2100(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 2100);
	// stw r11,2092(r31)
	PPC_STORE_U32(r31.u32 + 2092, r11.u32);
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(r31.u32 + 2096, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x826f9910
	if (!cr6.eq) goto loc_826F9910;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,456(r31)
	PPC_STORE_U32(r31.u32 + 456, ctx.r10.u32);
	// b 0x826f9914
	goto loc_826F9914;
loc_826F9910:
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
loc_826F9914:
	// lwz r10,14776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14776);
	// lwz r9,3392(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3392);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// rlwinm r10,r10,0,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFF80;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r10,3
	ctx.r10.s64 = 3;
	// beq cr6,0x826f993c
	if (cr6.eq) goto loc_826F993C;
	// stw r10,14780(r31)
	PPC_STORE_U32(r31.u32 + 14780, ctx.r10.u32);
	// stw r11,14784(r31)
	PPC_STORE_U32(r31.u32 + 14784, r11.u32);
	// b 0x826f9944
	goto loc_826F9944;
loc_826F993C:
	// stw r11,14780(r31)
	PPC_STORE_U32(r31.u32 + 14780, r11.u32);
	// stw r10,14784(r31)
	PPC_STORE_U32(r31.u32 + 14784, ctx.r10.u32);
loc_826F9944:
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826f9958
	if (!cr6.eq) goto loc_826F9958;
	// lwz r11,21472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21472);
	// b 0x826f995c
	goto loc_826F995C;
loc_826F9958:
	// lwz r11,21476(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21476);
loc_826F995C:
	// stw r11,21468(r31)
	PPC_STORE_U32(r31.u32 + 21468, r11.u32);
	// lwz r11,21000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21000);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826f9980
	if (!cr6.eq) goto loc_826F9980;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r10,21268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826f9984
	goto loc_826F9984;
loc_826F9980:
	// lwz r11,21268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21268);
loc_826F9984:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// stw r11,21264(r31)
	PPC_STORE_U32(r31.u32 + 21264, r11.u32);
	// bl 0x825ebc08
	sub_825EBC08(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// bl 0x82628f88
	sub_82628F88(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261ca78
	sub_8261CA78(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261cba0
	sub_8261CBA0(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82615470
	sub_82615470(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826F99D4"))) PPC_WEAK_FUNC(sub_826F99D4);
PPC_FUNC_IMPL(__imp__sub_826F99D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826F99D8"))) PPC_WEAK_FUNC(sub_826F99D8);
PPC_FUNC_IMPL(__imp__sub_826F99D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc4
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r16,r6
	r16.u64 = ctx.r6.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// rlwinm r8,r16,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,1240(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 1240);
	// mr r18,r5
	r18.u64 = ctx.r5.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r17,r7
	r17.u64 = ctx.r7.u64;
	// lwz r4,220(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 220);
	// clrlwi r7,r16,31
	ctx.r7.u64 = r16.u32 & 0x1;
	// std r6,216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 216, ctx.r6.u64);
	// rlwinm r6,r9,0,27,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x18;
	// lbz r10,28(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 28);
	// neg r26,r7
	r26.s64 = -ctx.r7.s64;
	// lwzx r11,r11,r8
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// cntlzw r6,r6
	ctx.r6.u64 = ctx.r6.u32 == 0 ? 32 : __builtin_clz(ctx.r6.u32);
	// lwz r8,4(r17)
	ctx.r8.u64 = PPC_LOAD_U32(r17.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// rlwinm r5,r6,27,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x1;
	// lwz r6,1248(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 1248);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lhz r10,50(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 50);
	// rlwinm r7,r8,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r27,5(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 5);
	// and r28,r11,r16
	r28.u64 = r11.u64 & r16.u64;
	// lbz r11,4(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 4);
	// lwz r3,0(r17)
	ctx.r3.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// stb r5,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r5.u8);
	// rotlwi r5,r11,2
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 2);
	// stw r31,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, r31.u32);
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// lwz r31,188(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 188);
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// add r11,r7,r6
	r11.u64 = ctx.r7.u64 + ctx.r6.u64;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r30,192(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// mullw r6,r10,r16
	ctx.r6.s64 = int64_t(ctx.r10.s32) * int64_t(r16.s32);
	// stw r16,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, r16.u32);
	// stw r3,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r3.u32);
	// stw r28,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r28.u32);
	// stw r27,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r27.u32);
	// stw r10,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r10.u32);
	// stw r11,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r11.u32);
	// stw r6,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r6.u32);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// stw r5,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r5.u32);
	// and r5,r26,r10
	ctx.r5.u64 = r26.u64 & ctx.r10.u64;
	// stw r5,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r5.u32);
	// beq cr6,0x826f9ac8
	if (cr6.eq) goto loc_826F9AC8;
	// lwz r7,228(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 228);
	// rlwinm r11,r9,12,28,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xC;
	// lwz r9,232(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 232);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// stw r7,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r7.u32);
	// b 0x826f9ad4
	goto loc_826F9AD4;
loc_826F9AC8:
	// addi r11,r29,236
	r11.s64 = r29.s64 + 236;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// addi r11,r29,248
	r11.s64 = r29.s64 + 248;
loc_826F9AD4:
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// add r11,r3,r10
	r11.u64 = ctx.r3.u64 + ctx.r10.u64;
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r15,16384
	r15.s64 = 16384;
	// add r9,r11,r31
	ctx.r9.u64 = r11.u64 + r31.u64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,0
	r11.s64 = 0;
	// lis r25,2
	r25.s64 = 131072;
	// stw r15,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r15.u32);
	// stw r15,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r15.u32);
	// stw r15,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r15.u32);
	// stw r15,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r15.u32);
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stwx r15,r8,r30
	PPC_STORE_U32(ctx.r8.u32 + r30.u32, r15.u32);
	// addi r20,r10,29840
	r20.s64 = ctx.r10.s64 + 29840;
loc_826F9B1C:
	// li r4,119
	ctx.r4.s64 = 119;
	// stw r28,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r28.u32);
	// clrlwi r9,r27,31
	ctx.r9.u64 = r27.u32 & 0x1;
	// stw r18,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r18.u32);
	// rlwinm r10,r11,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r4,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r4.u32);
	// stw r9,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r9.u32);
	// bne cr6,0x826f9bcc
	if (!cr6.eq) goto loc_826F9BCC;
	// addi r3,r11,18
	ctx.r3.s64 = r11.s64 + 18;
	// lwz r8,264(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 264);
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// lwz r7,188(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 188);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// rlwinm r9,r3,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r3,1160(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 1160);
	// add r11,r11,r18
	r11.u64 = r11.u64 + r18.u64;
	// rlwinm r31,r16,1,30,30
	r31.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0x2;
	// add r6,r6,r18
	ctx.r6.u64 = ctx.r6.u64 + r18.u64;
	// stw r3,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r3.u32);
	// add r3,r5,r18
	ctx.r3.u64 = ctx.r5.u64 + r18.u64;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// or r11,r31,r10
	r11.u64 = r31.u64 | ctx.r10.u64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// lhzx r9,r9,r29
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + r29.u32);
	// addi r11,r11,104
	r11.s64 = r11.s64 + 104;
	// rlwinm r5,r6,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r3,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r3,124(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// add r10,r5,r9
	ctx.r10.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r6,r11,r29
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// rlwinm r11,r9,5,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stw r3,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r3.u32);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// extsh r11,r6
	r11.s64 = ctx.r6.s16;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// b 0x826f9c2c
	goto loc_826F9C2C;
loc_826F9BCC:
	// addi r9,r11,63
	ctx.r9.s64 = r11.s64 + 63;
	// lwz r11,1164(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 1164);
	// srawi r10,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 1;
	// lwz r8,192(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 + r18.u64;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// clrlwi r11,r16,31
	r11.u64 = r16.u32 & 0x1;
	// addi r7,r11,102
	ctx.r7.s64 = r11.s64 + 102;
	// srawi r11,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	r11.s64 = ctx.r5.s32 >> 1;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r11,r18
	ctx.r6.u64 = r11.u64 + r18.u64;
	// lwzx r11,r9,r29
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + r29.u32);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r6,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lhzx r7,r7,r29
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + r29.u32);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// extsh r11,r7
	r11.s64 = ctx.r7.s16;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
loc_826F9C2C:
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r11,28(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 28);
	// addi r11,r11,-128
	r11.s64 = r11.s64 + -128;
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// stw r11,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r11.u32);
	// stw r11,28(r17)
	PPC_STORE_U32(r17.u32 + 28, r11.u32);
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// dcbzl r0,r11
	memset(base + ((r11.u32) & ~127), 0, 128);
	// li r27,0
	r27.s64 = 0;
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r31,0(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lbz r4,8(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// lwz r28,0(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r28.u32);
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f9d44
	if (cr6.lt) goto loc_826F9D44;
	// clrlwi r10,r30,28
	ctx.r10.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x826f9d3c
	if (!cr6.lt) goto loc_826F9D3C;
loc_826F9CA4:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826f9cd0
	if (cr6.lt) goto loc_826F9CD0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826f9ca4
	if (cr6.eq) goto loc_826F9CA4;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f9d84
	goto loc_826F9D84;
loc_826F9CD0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826F9D3C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826f9d84
	goto loc_826F9D84;
loc_826F9D44:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826F9D4C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// addis r11,r30,1
	r11.s64 = r30.s64 + 65536;
	// addi r11,r11,-32768
	r11.s64 = r11.s64 + -32768;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826f9d4c
	if (cr6.lt) goto loc_826F9D4C;
loc_826F9D84:
	// clrlwi r30,r30,16
	r30.u64 = r30.u32 & 0xFFFF;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r26,r30
	r26.u64 = r30.u64;
	// cmpw cr6,r26,r11
	cr6.compare<int32_t>(r26.s32, r11.s32, xer);
	// beq cr6,0x826f9ed4
	if (cr6.eq) goto loc_826F9ED4;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x826f9ff4
	if (cr6.eq) goto loc_826F9FF4;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x826f9dec
	if (!cr6.eq) goto loc_826F9DEC;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x826f9dd8
	if (!cr0.lt) goto loc_826F9DD8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F9DD8:
	// rlwinm r11,r26,1,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826f9eb0
	goto loc_826F9EB0;
loc_826F9DEC:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x826f9eb4
	if (!cr6.eq) goto loc_826F9EB4;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,2
	r30.s64 = 2;
	// li r28,0
	r28.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x826f9e64
	if (!cr6.lt) goto loc_826F9E64;
loc_826F9E0C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f9e64
	if (cr6.eq) goto loc_826F9E64;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826f9e54
	if (!cr0.lt) goto loc_826F9E54;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F9E54:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f9e0c
	if (cr6.gt) goto loc_826F9E0C;
loc_826F9E64:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r28
	r30.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f9ea0
	if (!cr0.lt) goto loc_826F9EA0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F9EA0:
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-3
	r11.s64 = r11.s64 + -3;
loc_826F9EB0:
	// clrlwi r30,r11,16
	r30.u64 = r11.u32 & 0xFFFF;
loc_826F9EB4:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r28,r8,0
	r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x826f9fd0
	goto loc_826F9FD0;
loc_826F9ED4:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bgt cr6,0x826f9eec
	if (cr6.gt) goto loc_826F9EEC;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// subfic r11,r11,3
	xer.ca = r11.u32 <= 3;
	r11.s64 = 3 - r11.s64;
	// b 0x826f9ef0
	goto loc_826F9EF0;
loc_826F9EEC:
	// li r11,0
	r11.s64 = 0;
loc_826F9EF0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r30,r11,8
	r30.s64 = r11.s64 + 8;
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x826f9f10
	if (!cr6.eq) goto loc_826F9F10;
	// li r11,0
	r11.s64 = 0;
	// b 0x826f9fb0
	goto loc_826F9FB0;
loc_826F9F10:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826f9f70
	if (!cr6.gt) goto loc_826F9F70;
loc_826F9F18:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826f9f70
	if (cr6.eq) goto loc_826F9F70;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826f9f60
	if (!cr0.lt) goto loc_826F9F60;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F9F60:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826f9f18
	if (cr6.gt) goto loc_826F9F18;
loc_826F9F70:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r28
	r30.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826f9fac
	if (!cr0.lt) goto loc_826F9FAC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F9FAC:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_826F9FB0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrlwi r30,r11,16
	r30.u64 = r11.u32 & 0xFFFF;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r28,r11,0
	r28.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
loc_826F9FD0:
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// bge 0x826f9fe0
	if (!cr0.lt) goto loc_826F9FE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826F9FE0:
	// rlwinm r11,r28,1,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r10,r30
	ctx.r10.s64 = r30.s16;
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// extsh r27,r11
	r27.s64 = r11.s16;
loc_826F9FF4:
	// lwz r5,208(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// sth r27,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, r27.u16);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826fa88c
	if (!cr6.eq) goto loc_826FA88C;
	// lwz r19,212(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// beq cr6,0x826fa034
	if (cr6.eq) goto loc_826FA034;
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r6,276(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 276);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x826fcd00
	sub_826FCD00(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x826fa88c
	if (cr6.lt) goto loc_826FA88C;
loc_826FA034:
	// lwz r21,84(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r22,1
	r22.s64 = 1;
	// lhz r26,50(r29)
	r26.u64 = PPC_LOAD_U16(r29.u32 + 50);
	// li r24,0
	r24.s64 = 0;
	// srawi r11,r21,2
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x3) != 0);
	r11.s64 = r21.s32 >> 2;
	// lwz r9,128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r7,92(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// li r23,0
	r23.s64 = 0;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// li r10,0
	ctx.r10.s64 = 0;
	// srw r11,r26,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r26.u32 >> (r11.u8 & 0x3F));
	// beq cr6,0x826fa094
	if (cr6.eq) goto loc_826FA094;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r8,16384
	cr6.compare<int32_t>(ctx.r8.s32, 16384, xer);
	// bne cr6,0x826fa094
	if (!cr6.eq) goto loc_826FA094;
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// li r22,8
	r22.s64 = 8;
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r24,r10,r9
	r24.s64 = ctx.r9.s64 - ctx.r10.s64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_826FA094:
	// lwz r8,112(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r31,204(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lwz r8,188(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// beq cr6,0x826fa2e4
	if (cr6.eq) goto loc_826FA2E4;
	// lwz r6,-4(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + -4);
	// cmpwi cr6,r6,16384
	cr6.compare<int32_t>(ctx.r6.s32, 16384, xer);
	// bne cr6,0x826fa2e4
	if (!cr6.eq) goto loc_826FA2E4;
	// addi r23,r9,-32
	r23.s64 = ctx.r9.s64 + -32;
	// li r22,1
	r22.s64 = 1;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x826fa618
	if (cr6.eq) goto loc_826FA618;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x826fa2e4
	if (cr6.eq) goto loc_826FA2E4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r7
	r11.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// bne cr6,0x826fa0f4
	if (!cr6.eq) goto loc_826FA0F4;
	// lhz r11,-16(r24)
	r11.u64 = PPC_LOAD_U16(r24.u32 + -16);
	// extsh r3,r11
	ctx.r3.s64 = r11.s16;
loc_826FA0F4:
	// lhz r11,16(r24)
	r11.u64 = PPC_LOAD_U16(r24.u32 + 16);
	// lhz r7,0(r23)
	ctx.r7.u64 = PPC_LOAD_U16(r23.u32 + 0);
	// lbz r6,27(r29)
	ctx.r6.u64 = PPC_LOAD_U8(r29.u32 + 27);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// extsh r28,r7
	r28.s64 = ctx.r7.s16;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x826fa2b4
	if (cr6.eq) goto loc_826FA2B4;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x826fa200
	if (cr6.eq) goto loc_826FA200;
	// cmpwi cr6,r21,4
	cr6.compare<int32_t>(r21.s32, 4, xer);
	// beq cr6,0x826fa200
	if (cr6.eq) goto loc_826FA200;
	// cmpwi cr6,r21,5
	cr6.compare<int32_t>(r21.s32, 5, xer);
	// beq cr6,0x826fa200
	if (cr6.eq) goto loc_826FA200;
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// bne cr6,0x826fa198
	if (!cr6.eq) goto loc_826FA198;
	// rlwinm r7,r26,2,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFF8;
	// lwz r11,220(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 220);
	// rlwinm r6,r8,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r7,r31
	ctx.r7.s64 = r31.s64 - ctx.r7.s64;
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// clrlwi r7,r7,26
	ctx.r7.u64 = ctx.r7.u32 & 0x3F;
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,16(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// rlwinm r4,r6,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// lwzx r7,r4,r20
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + r20.u32);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mullw r6,r11,r30
	ctx.r6.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// mullw r11,r11,r3
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r3.s32);
	// mullw r7,r6,r7
	ctx.r7.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r7,r7,r25
	ctx.r7.u64 = ctx.r7.u64 + r25.u64;
	// srawi r3,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = r11.s32 >> 18;
	// srawi r30,r7,18
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFFF) != 0);
	r30.s64 = ctx.r7.s32 >> 18;
	// b 0x826fa2b4
	goto loc_826FA2B4;
loc_826FA198:
	// cmpwi cr6,r21,2
	cr6.compare<int32_t>(r21.s32, 2, xer);
	// bne cr6,0x826fa2b4
	if (!cr6.eq) goto loc_826FA2B4;
	// rlwinm r6,r8,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r7,-8(r31)
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + -8);
	// lwz r11,220(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 220);
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// clrlwi r7,r7,26
	ctx.r7.u64 = ctx.r7.u32 & 0x3F;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r7,16(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r7,r20
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + r20.u32);
	// lwz r7,16(r6)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// mullw r6,r7,r28
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(r28.s32);
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// mullw r7,r7,r3
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r3.s32);
	// mullw r11,r6,r11
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// add r7,r7,r25
	ctx.r7.u64 = ctx.r7.u64 + r25.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// srawi r3,r7,18
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 18;
	// srawi r28,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r28.s64 = r11.s32 >> 18;
	// b 0x826fa2b4
	goto loc_826FA2B4;
loc_826FA200:
	// rlwinm r7,r26,2,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFF8;
	// lbz r6,-8(r31)
	ctx.r6.u64 = PPC_LOAD_U8(r31.u32 + -8);
	// lwz r11,220(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 220);
	// rlwinm r4,r8,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r7,r31
	ctx.r7.s64 = r31.s64 - ctx.r7.s64;
	// clrlwi r6,r6,26
	ctx.r6.u64 = ctx.r6.u32 & 0x3F;
	// add r4,r8,r4
	ctx.r4.u64 = ctx.r8.u64 + ctx.r4.u64;
	// li r15,16384
	r15.s64 = 16384;
	// rlwinm r27,r4,2,0,29
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r16,0(r7)
	r16.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbz r5,-8(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + -8);
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// clrlwi r5,r5,26
	ctx.r5.u64 = ctx.r5.u32 & 0x3F;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r4,r16,26
	ctx.r4.u64 = r16.u32 & 0x3F;
	// lwz r16,412(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// lwz r7,16(r27)
	ctx.r7.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,16(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// mullw r6,r6,r28
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r28.s32);
	// lwzx r7,r7,r20
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + r20.u32);
	// rlwinm r28,r5,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r28
	ctx.r5.u64 = ctx.r5.u64 + r28.u64;
	// rlwinm r28,r4,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// add r28,r5,r11
	r28.u64 = ctx.r5.u64 + r11.u64;
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// mullw r4,r7,r11
	ctx.r4.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// lwz r11,16(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// mullw r5,r4,r3
	ctx.r5.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r3.s32);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// mullw r7,r7,r6
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// add r6,r5,r25
	ctx.r6.u64 = ctx.r5.u64 + r25.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r7,r7,r25
	ctx.r7.u64 = ctx.r7.u64 + r25.u64;
	// srawi r3,r6,18
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = ctx.r6.s32 >> 18;
	// srawi r30,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r30.s64 = r11.s32 >> 18;
	// srawi r28,r7,18
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFFF) != 0);
	r28.s64 = ctx.r7.s32 >> 18;
loc_826FA2B4:
	// subf r7,r30,r3
	ctx.r7.s64 = ctx.r3.s64 - r30.s64;
	// subf r11,r28,r3
	r11.s64 = ctx.r3.s64 - r28.s64;
	// srawi r6,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// srawi r5,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r11.s32 >> 31;
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// xor r11,r11,r5
	r11.u64 = r11.u64 ^ ctx.r5.u64;
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r11,r5,r11
	r11.s64 = r11.s64 - ctx.r5.s64;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x826fa2e4
	if (!cr6.lt) goto loc_826FA2E4;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r22,8
	r22.s64 = 8;
loc_826FA2E4:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826fa618
	if (cr6.eq) goto loc_826FA618;
	// lbz r11,80(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// lbz r7,27(r29)
	ctx.r7.u64 = PPC_LOAD_U8(r29.u32 + 27);
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// or r22,r11,r22
	r22.u64 = r11.u64 | r22.u64;
	// beq cr6,0x826fa60c
	if (cr6.eq) goto loc_826FA60C;
	// cmplw cr6,r10,r23
	cr6.compare<uint32_t>(ctx.r10.u32, r23.u32, xer);
	// bne cr6,0x826fa488
	if (!cr6.eq) goto loc_826FA488;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x826fa354
	if (cr6.eq) goto loc_826FA354;
	// cmpwi cr6,r21,2
	cr6.compare<int32_t>(r21.s32, 2, xer);
	// beq cr6,0x826fa354
	if (cr6.eq) goto loc_826FA354;
	// cmpwi cr6,r21,4
	cr6.compare<int32_t>(r21.s32, 4, xer);
	// beq cr6,0x826fa354
	if (cr6.eq) goto loc_826FA354;
	// cmpwi cr6,r21,5
	cr6.compare<int32_t>(r21.s32, 5, xer);
	// beq cr6,0x826fa354
	if (cr6.eq) goto loc_826FA354;
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// li r8,16
	ctx.r8.s64 = 16;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_826FA338:
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x826fa338
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826FA338;
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// b 0x826fa618
	goto loc_826FA618;
loc_826FA354:
	// lbz r11,-8(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + -8);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r4,220(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 220);
	// addi r30,r1,146
	r30.s64 = ctx.r1.s64 + 146;
	// clrlwi r7,r11,26
	ctx.r7.u64 = r11.u32 & 0x3F;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// subf r31,r10,r3
	r31.s64 = ctx.r3.s64 - ctx.r10.s64;
	// rlwinm r3,r8,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// extsh r27,r11
	r27.s64 = r11.s16;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// addi r28,r1,148
	r28.s64 = ctx.r1.s64 + 148;
	// rlwinm r3,r8,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lwzx r6,r11,r20
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + r20.u32);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r11,r10,6
	r11.s64 = ctx.r10.s64 + 6;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r30,r10,r30
	r30.s64 = r30.s64 - ctx.r10.s64;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// lwz r3,16(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// subf r28,r10,r28
	r28.s64 = r28.s64 - ctx.r10.s64;
	// rlwinm r4,r3,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r1,148
	ctx.r5.s64 = ctx.r1.s64 + 148;
	// li r10,3
	ctx.r10.s64 = 3;
	// lwz r8,16(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// lwzx r4,r4,r20
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + r20.u32);
	// mullw r8,r8,r27
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r27.s32);
	// mullw r8,r4,r8
	ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// add r8,r8,r25
	ctx.r8.u64 = ctx.r8.u64 + r25.u64;
	// srawi r8,r8,18
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 18;
	// sth r8,144(r1)
	PPC_STORE_U16(ctx.r1.u32 + 144, ctx.r8.u16);
loc_826FA3D8:
	// lhz r8,-4(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhz r4,-2(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r27,2(r11)
	r27.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// mullw r8,r8,r6
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// lhz r26,4(r11)
	r26.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// mullw r8,r8,r7
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// extsh r27,r27
	r27.s64 = r27.s16;
	// mullw r4,r4,r6
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r6.s32);
	// extsh r26,r26
	r26.s64 = r26.s16;
	// mullw r3,r3,r6
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r6.s32);
	// add r24,r8,r25
	r24.u64 = ctx.r8.u64 + r25.u64;
	// mullw r27,r27,r6
	r27.s64 = int64_t(r27.s32) * int64_t(ctx.r6.s32);
	// mullw r4,r4,r7
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r7.s32);
	// mullw r8,r26,r7
	ctx.r8.s64 = int64_t(r26.s32) * int64_t(ctx.r7.s32);
	// mullw r3,r3,r7
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r7.s32);
	// mullw r27,r27,r7
	r27.s64 = int64_t(r27.s32) * int64_t(ctx.r7.s32);
	// mullw r8,r6,r8
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r8.s32);
	// add r4,r4,r25
	ctx.r4.u64 = ctx.r4.u64 + r25.u64;
	// add r3,r3,r25
	ctx.r3.u64 = ctx.r3.u64 + r25.u64;
	// srawi r26,r24,18
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3FFFF) != 0);
	r26.s64 = r24.s32 >> 18;
	// add r27,r27,r25
	r27.u64 = r27.u64 + r25.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// add r8,r8,r25
	ctx.r8.u64 = ctx.r8.u64 + r25.u64;
	// srawi r3,r3,18
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 18;
	// srawi r27,r27,18
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x3FFFF) != 0);
	r27.s64 = r27.s32 >> 18;
	// srawi r8,r8,18
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 18;
	// sth r26,-2(r5)
	PPC_STORE_U16(ctx.r5.u32 + -2, r26.u16);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r4,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, ctx.r4.u16);
	// addi r5,r5,10
	ctx.r5.s64 = ctx.r5.s64 + 10;
	// sthx r3,r31,r11
	PPC_STORE_U16(r31.u32 + r11.u32, ctx.r3.u16);
	// sthx r27,r30,r11
	PPC_STORE_U16(r30.u32 + r11.u32, r27.u16);
	// sthx r8,r28,r11
	PPC_STORE_U16(r28.u32 + r11.u32, ctx.r8.u16);
	// addi r11,r11,10
	r11.s64 = r11.s64 + 10;
	// bne cr6,0x826fa3d8
	if (!cr6.eq) goto loc_826FA3D8;
	// lhz r11,144(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 144);
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// sth r11,160(r1)
	PPC_STORE_U16(ctx.r1.u32 + 160, r11.u16);
	// b 0x826fa618
	goto loc_826FA618;
loc_826FA488:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x826fa4d0
	if (cr6.eq) goto loc_826FA4D0;
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// beq cr6,0x826fa4d0
	if (cr6.eq) goto loc_826FA4D0;
	// cmpwi cr6,r21,4
	cr6.compare<int32_t>(r21.s32, 4, xer);
	// beq cr6,0x826fa4d0
	if (cr6.eq) goto loc_826FA4D0;
	// cmpwi cr6,r21,5
	cr6.compare<int32_t>(r21.s32, 5, xer);
	// beq cr6,0x826fa4d0
	if (cr6.eq) goto loc_826FA4D0;
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// li r8,16
	ctx.r8.s64 = 16;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_826FA4B4:
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x826fa4b4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826FA4B4;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// b 0x826fa618
	goto loc_826FA618;
loc_826FA4D0:
	// rlwinm r11,r26,2,0,28
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFF8;
	// lhz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r4,220(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 220);
	// subf r11,r11,r31
	r11.s64 = r31.s64 - r11.s64;
	// subf r31,r10,r3
	r31.s64 = ctx.r3.s64 - ctx.r10.s64;
	// rlwinm r3,r8,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r8,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsh r27,r7
	r27.s64 = ctx.r7.s16;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r7,r11,26
	ctx.r7.u64 = r11.u32 & 0x3F;
	// add r3,r8,r4
	ctx.r3.u64 = ctx.r8.u64 + ctx.r4.u64;
	// lwzx r6,r6,r20
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r20.u32);
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r30,r1,146
	r30.s64 = ctx.r1.s64 + 146;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r28,r1,148
	r28.s64 = ctx.r1.s64 + 148;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,16(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// addi r11,r10,6
	r11.s64 = ctx.r10.s64 + 6;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r30,r10,r30
	r30.s64 = r30.s64 - ctx.r10.s64;
	// subf r28,r10,r28
	r28.s64 = r28.s64 - ctx.r10.s64;
	// addi r5,r1,148
	ctx.r5.s64 = ctx.r1.s64 + 148;
	// lwz r8,16(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// li r10,3
	ctx.r10.s64 = 3;
	// lwzx r4,r3,r20
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + r20.u32);
	// mullw r8,r8,r27
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r27.s32);
	// mullw r8,r4,r8
	ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// add r8,r8,r25
	ctx.r8.u64 = ctx.r8.u64 + r25.u64;
	// srawi r8,r8,18
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 18;
	// sth r8,144(r1)
	PPC_STORE_U16(ctx.r1.u32 + 144, ctx.r8.u16);
loc_826FA55C:
	// lhz r8,-4(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhz r4,-2(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r27,2(r11)
	r27.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// mullw r8,r8,r6
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// lhz r26,4(r11)
	r26.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// mullw r8,r8,r7
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// extsh r27,r27
	r27.s64 = r27.s16;
	// mullw r4,r4,r6
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r6.s32);
	// extsh r26,r26
	r26.s64 = r26.s16;
	// mullw r3,r3,r6
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r6.s32);
	// add r24,r8,r25
	r24.u64 = ctx.r8.u64 + r25.u64;
	// mullw r27,r27,r6
	r27.s64 = int64_t(r27.s32) * int64_t(ctx.r6.s32);
	// mullw r4,r4,r7
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r7.s32);
	// mullw r8,r26,r7
	ctx.r8.s64 = int64_t(r26.s32) * int64_t(ctx.r7.s32);
	// mullw r3,r3,r7
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r7.s32);
	// mullw r27,r27,r7
	r27.s64 = int64_t(r27.s32) * int64_t(ctx.r7.s32);
	// mullw r8,r6,r8
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r8.s32);
	// add r4,r4,r25
	ctx.r4.u64 = ctx.r4.u64 + r25.u64;
	// add r3,r3,r25
	ctx.r3.u64 = ctx.r3.u64 + r25.u64;
	// srawi r26,r24,18
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3FFFF) != 0);
	r26.s64 = r24.s32 >> 18;
	// add r27,r27,r25
	r27.u64 = r27.u64 + r25.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// add r8,r8,r25
	ctx.r8.u64 = ctx.r8.u64 + r25.u64;
	// srawi r3,r3,18
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3FFFF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 18;
	// srawi r27,r27,18
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x3FFFF) != 0);
	r27.s64 = r27.s32 >> 18;
	// srawi r8,r8,18
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 18;
	// sth r26,-2(r5)
	PPC_STORE_U16(ctx.r5.u32 + -2, r26.u16);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r4,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, ctx.r4.u16);
	// addi r5,r5,10
	ctx.r5.s64 = ctx.r5.s64 + 10;
	// sthx r3,r11,r31
	PPC_STORE_U16(r11.u32 + r31.u32, ctx.r3.u16);
	// sthx r27,r11,r30
	PPC_STORE_U16(r11.u32 + r30.u32, r27.u16);
	// sthx r8,r11,r28
	PPC_STORE_U16(r11.u32 + r28.u32, ctx.r8.u16);
	// addi r11,r11,10
	r11.s64 = r11.s64 + 10;
	// bne cr6,0x826fa55c
	if (!cr6.eq) goto loc_826FA55C;
	// lhz r11,144(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 144);
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// sth r11,160(r1)
	PPC_STORE_U16(ctx.r1.u32 + 160, r11.u16);
	// b 0x826fa618
	goto loc_826FA618;
loc_826FA60C:
	// cmplw cr6,r10,r24
	cr6.compare<uint32_t>(ctx.r10.u32, r24.u32, xer);
	// bne cr6,0x826fa618
	if (!cr6.eq) goto loc_826FA618;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
loc_826FA618:
	// lwz r11,28(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 28);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826fa7c0
	if (cr6.eq) goto loc_826FA7C0;
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmpwi cr6,r22,1
	cr6.compare<int32_t>(r22.s32, 1, xer);
	// lhz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// sth r8,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
	// sth r8,16(r9)
	PPC_STORE_U16(ctx.r9.u32 + 16, ctx.r8.u16);
	// bne cr6,0x826fa6f4
	if (!cr6.eq) goto loc_826FA6F4;
	// lhz r8,2(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r8.u16);
	// sth r8,2(r9)
	PPC_STORE_U16(ctx.r9.u32 + 2, ctx.r8.u16);
	// lhz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lhz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,4(r11)
	PPC_STORE_U16(r11.u32 + 4, ctx.r8.u16);
	// sth r8,4(r9)
	PPC_STORE_U16(ctx.r9.u32 + 4, ctx.r8.u16);
	// lhz r8,6(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// lhz r7,6(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,6(r11)
	PPC_STORE_U16(r11.u32 + 6, ctx.r8.u16);
	// sth r8,6(r9)
	PPC_STORE_U16(ctx.r9.u32 + 6, ctx.r8.u16);
	// lhz r8,8(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 8);
	// lhz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,8(r11)
	PPC_STORE_U16(r11.u32 + 8, ctx.r8.u16);
	// sth r8,8(r9)
	PPC_STORE_U16(ctx.r9.u32 + 8, ctx.r8.u16);
	// lhz r8,10(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 10);
	// lhz r7,10(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,10(r11)
	PPC_STORE_U16(r11.u32 + 10, ctx.r8.u16);
	// sth r8,10(r9)
	PPC_STORE_U16(ctx.r9.u32 + 10, ctx.r8.u16);
	// lhz r8,12(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 12);
	// lhz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 12);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,12(r11)
	PPC_STORE_U16(r11.u32 + 12, ctx.r8.u16);
	// sth r8,12(r9)
	PPC_STORE_U16(ctx.r9.u32 + 12, ctx.r8.u16);
	// lhz r10,14(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 14);
	// lhz r8,14(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// sth r10,14(r11)
	PPC_STORE_U16(r11.u32 + 14, ctx.r10.u16);
	// sth r10,14(r9)
	PPC_STORE_U16(ctx.r9.u32 + 14, ctx.r10.u16);
	// b 0x826fa7e4
	goto loc_826FA7E4;
loc_826FA6F4:
	// cmpwi cr6,r22,8
	cr6.compare<int32_t>(r22.s32, 8, xer);
	// bne cr6,0x826fa7cc
	if (!cr6.eq) goto loc_826FA7CC;
	// lhz r8,2(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// sth r8,2(r9)
	PPC_STORE_U16(ctx.r9.u32 + 2, ctx.r8.u16);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r8,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r8.u32);
	// ld r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// std r8,8(r9)
	PPC_STORE_U64(ctx.r9.u32 + 8, ctx.r8.u64);
	// lhz r8,2(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r7,16(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 16);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,16(r11)
	PPC_STORE_U16(r11.u32 + 16, ctx.r8.u16);
	// sth r8,18(r9)
	PPC_STORE_U16(ctx.r9.u32 + 18, ctx.r8.u16);
	// lhz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lhz r7,32(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,32(r11)
	PPC_STORE_U16(r11.u32 + 32, ctx.r8.u16);
	// sth r8,20(r9)
	PPC_STORE_U16(ctx.r9.u32 + 20, ctx.r8.u16);
	// lhz r8,6(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// lhz r7,48(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 48);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,48(r11)
	PPC_STORE_U16(r11.u32 + 48, ctx.r8.u16);
	// sth r8,22(r9)
	PPC_STORE_U16(ctx.r9.u32 + 22, ctx.r8.u16);
	// lhz r8,8(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 8);
	// lhz r7,64(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 64);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,64(r11)
	PPC_STORE_U16(r11.u32 + 64, ctx.r8.u16);
	// sth r8,24(r9)
	PPC_STORE_U16(ctx.r9.u32 + 24, ctx.r8.u16);
	// lhz r8,10(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 10);
	// lhz r7,80(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 80);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,80(r11)
	PPC_STORE_U16(r11.u32 + 80, ctx.r8.u16);
	// sth r8,26(r9)
	PPC_STORE_U16(ctx.r9.u32 + 26, ctx.r8.u16);
	// lhz r8,12(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 12);
	// lhz r7,96(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 96);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,96(r11)
	PPC_STORE_U16(r11.u32 + 96, ctx.r8.u16);
	// sth r8,28(r9)
	PPC_STORE_U16(ctx.r9.u32 + 28, ctx.r8.u16);
	// lhz r10,14(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 14);
	// lhz r8,112(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 112);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// sth r10,112(r11)
	PPC_STORE_U16(r11.u32 + 112, ctx.r10.u16);
	// sth r10,30(r9)
	PPC_STORE_U16(ctx.r9.u32 + 30, ctx.r10.u16);
	// b 0x826fa81c
	goto loc_826FA81C;
loc_826FA7C0:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// sth r10,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r10.u16);
	// sth r10,16(r9)
	PPC_STORE_U16(ctx.r9.u32 + 16, ctx.r10.u16);
loc_826FA7CC:
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// sth r10,2(r9)
	PPC_STORE_U16(ctx.r9.u32 + 2, ctx.r10.u16);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
	// ld r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// std r10,8(r9)
	PPC_STORE_U64(ctx.r9.u32 + 8, ctx.r10.u64);
loc_826FA7E4:
	// lhz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 16);
	// sth r10,18(r9)
	PPC_STORE_U16(ctx.r9.u32 + 18, ctx.r10.u16);
	// lhz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 32);
	// sth r10,20(r9)
	PPC_STORE_U16(ctx.r9.u32 + 20, ctx.r10.u16);
	// lhz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 48);
	// sth r10,22(r9)
	PPC_STORE_U16(ctx.r9.u32 + 22, ctx.r10.u16);
	// lhz r10,64(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 64);
	// sth r10,24(r9)
	PPC_STORE_U16(ctx.r9.u32 + 24, ctx.r10.u16);
	// lhz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 80);
	// sth r10,26(r9)
	PPC_STORE_U16(ctx.r9.u32 + 26, ctx.r10.u16);
	// lhz r10,96(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 96);
	// sth r10,28(r9)
	PPC_STORE_U16(ctx.r9.u32 + 28, ctx.r10.u16);
	// lhz r11,112(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 112);
	// sth r11,30(r9)
	PPC_STORE_U16(ctx.r9.u32 + 30, r11.u16);
loc_826FA81C:
	// rlwinm r11,r19,3,0,28
	r11.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 3) & 0xFFFFFFF8;
	// ld r9,216(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 216);
	// extsw r10,r19
	ctx.r10.s64 = r19.s32;
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// or r11,r11,r21
	r11.u64 = r11.u64 | r21.u64;
	// ori r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 128;
	// rlwinm r11,r11,12,0,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFFFF000;
	// or r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 | ctx.r9.u64;
	// or r11,r11,r16
	r11.u64 = r11.u64 | r16.u64;
	// rldicr r7,r9,8,55
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r9,32(r17)
	ctx.r9.u64 = PPC_LOAD_U32(r17.u32 + 32);
	// rlwinm r10,r11,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// addi r11,r21,1
	r11.s64 = r21.s64 + 1;
	// or r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 | r18.u64;
	// srawi r27,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r27.s64 = ctx.r8.s32 >> 1;
	// std r7,216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 216, ctx.r7.u64);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// lwz r10,32(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + 32);
	// stw r27,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r27.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r10,32(r17)
	PPC_STORE_U32(r17.u32 + 32, ctx.r10.u32);
	// bge cr6,0x826fa898
	if (!cr6.lt) goto loc_826FA898;
	// lwz r5,196(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r6,180(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r28,184(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// b 0x826f9b1c
	goto loc_826F9B1C;
loc_826FA88C:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// b 0x8239bd14
	return;
loc_826FA898:
	// lwz r6,396(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// rldicl r3,r7,56,8
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u64, 56) & 0xFFFFFFFFFFFFFF;
	// lbz r4,1260(r29)
	ctx.r4.u64 = PPC_LOAD_U8(r29.u32 + 1260);
	// lwz r10,192(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r9,1500(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 1500);
	// lwz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lbz r7,4(r6)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r5,24,29,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0x7;
	// lbz r31,5(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// rldicr r30,r7,8,63
	r30.u64 = __builtin_rotateleft64(ctx.r7.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lwz r6,4(r17)
	ctx.r6.u64 = PPC_LOAD_U32(r17.u32 + 4);
	// subf r7,r4,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r4.s64;
	// lwz r5,1248(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 1248);
	// rlwinm r6,r6,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,1492(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 1492);
	// neg r4,r7
	ctx.r4.s64 = -ctx.r7.s64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// xor r7,r4,r7
	ctx.r7.u64 = ctx.r4.u64 ^ ctx.r7.u64;
	// srawi r7,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 31;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// rlwinm r7,r7,6,0,25
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 6) & 0xFFFFFFC0;
	// or r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 | r31.u64;
	// clrldi r7,r7,56
	ctx.r7.u64 = ctx.r7.u64 & 0xFF;
	// or r7,r30,r7
	ctx.r7.u64 = r30.u64 | ctx.r7.u64;
	// rldicr r7,r7,48,15
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u64, 48) & 0xFFFF000000000000;
	// or r7,r7,r3
	ctx.r7.u64 = ctx.r7.u64 | ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// stdx r7,r6,r5
	PPC_STORE_U64(ctx.r6.u32 + ctx.r5.u32, ctx.r7.u64);
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + r11.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// sth r15,2(r11)
	PPC_STORE_U16(r11.u32 + 2, r15.u16);
	// sth r15,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r15.u16);
	// sth r15,2(r7)
	PPC_STORE_U16(ctx.r7.u32 + 2, r15.u16);
	// sth r15,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, r15.u16);
	// sth r15,2(r8)
	PPC_STORE_U16(ctx.r8.u32 + 2, r15.u16);
	// sth r15,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, r15.u16);
	// sth r15,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, r15.u16);
	// sth r15,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r15.u16);
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// b 0x8239bd14
	return;
}

__attribute__((alias("__imp__sub_826FA94C"))) PPC_WEAK_FUNC(sub_826FA94C);
PPC_FUNC_IMPL(__imp__sub_826FA94C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826FA950"))) PPC_WEAK_FUNC(sub_826FA950);
PPC_FUNC_IMPL(__imp__sub_826FA950) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r14,r3
	r14.u64 = ctx.r3.u64;
	// mr r18,r5
	r18.u64 = ctx.r5.u64;
	// li r15,0
	r15.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lhz r11,50(r24)
	r11.u64 = PPC_LOAD_U16(r24.u32 + 50);
	// mr r29,r15
	r29.u64 = r15.u64;
	// lhz r10,52(r24)
	ctx.r10.u64 = PPC_LOAD_U16(r24.u32 + 52);
	// rlwinm r25,r11,31,1,31
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r9,21556(r14)
	ctx.r9.u64 = PPC_LOAD_U32(r14.u32 + 21556);
	// rlwinm r22,r10,31,1,31
	r22.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r10,1516(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 1516);
	// lwz r30,188(r24)
	r30.u64 = PPC_LOAD_U32(r24.u32 + 188);
	// mullw r31,r22,r25
	r31.s64 = int64_t(r22.s32) * int64_t(r25.s32);
	// stw r25,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r25.u32);
	// stw r22,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r22.u32);
	// rlwinm r11,r31,1,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r10,r10,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// rlwinm r8,r11,7,0,24
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,20(r18)
	PPC_STORE_U32(r18.u32 + 20, ctx.r10.u32);
	// lwz r10,1516(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 1516);
	// lwz r9,21568(r14)
	ctx.r9.u64 = PPC_LOAD_U32(r14.u32 + 21568);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r7,20(r18)
	ctx.r7.u64 = PPC_LOAD_U32(r18.u32 + 20);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r8,28(r18)
	PPC_STORE_U32(r18.u32 + 28, ctx.r8.u32);
	// stw r10,24(r18)
	PPC_STORE_U32(r18.u32 + 24, ctx.r10.u32);
	// lwz r9,1516(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 1516);
	// lwz r10,21572(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 21572);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
	// stw r28,32(r18)
	PPC_STORE_U32(r18.u32 + 32, r28.u32);
	// lwz r3,192(r24)
	ctx.r3.u64 = PPC_LOAD_U32(r24.u32 + 192);
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// bl 0x826a8e48
	sub_826A8E48(ctx, base);
	// rlwinm r5,r31,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 4) & 0xFFFFFFF0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x826a8e48
	sub_826A8E48(ctx, base);
	// stw r15,0(r18)
	PPC_STORE_U32(r18.u32 + 0, r15.u32);
	// stw r15,4(r18)
	PPC_STORE_U32(r18.u32 + 4, r15.u32);
	// mr r26,r15
	r26.u64 = r15.u64;
	// sth r15,16(r18)
	PPC_STORE_U16(r18.u32 + 16, r15.u16);
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// lwz r11,1516(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1516);
	// lwz r10,268(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 268);
	// mullw r11,r11,r22
	r11.s64 = int64_t(r11.s32) * int64_t(r22.s32);
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// mullw r11,r11,r25
	r11.s64 = int64_t(r11.s32) * int64_t(r25.s32);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r17,r11,r10
	r17.u64 = r11.u64 + ctx.r10.u64;
	// beq cr6,0x826fc5fc
	if (cr6.eq) goto loc_826FC5FC;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// li r21,1
	r21.s64 = 1;
	// addi r20,r11,14080
	r20.s64 = r11.s64 + 14080;
	// lis r11,0
	r11.s64 = 0;
	// ori r16,r11,32768
	r16.u64 = r11.u64 | 32768;
	// stw r20,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r20.u32);
loc_826FAA6C:
	// addi r11,r24,1376
	r11.s64 = r24.s64 + 1376;
	// sth r15,18(r18)
	PPC_STORE_U16(r18.u32 + 18, r15.u16);
	// stw r11,1416(r24)
	PPC_STORE_U32(r24.u32 + 1416, r11.u32);
	// lwz r11,21236(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fad3c
	if (cr6.eq) goto loc_826FAD3C;
	// lwz r11,1240(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1240);
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fad3c
	if (cr6.eq) goto loc_826FAD3C;
	// lwz r11,16(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fab64
	if (cr6.eq) goto loc_826FAB64;
	// lis r11,1
	r11.s64 = 65536;
	// ori r11,r11,33712
	r11.u64 = r11.u64 | 33712;
	// lwzx r11,r14,r11
	r11.u64 = PPC_LOAD_U32(r14.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826fab64
	if (!cr6.eq) goto loc_826FAB64;
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fab44
	if (cr6.eq) goto loc_826FAB44;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r30,r21
	r30.u64 = r21.u64;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826fab1c
	if (!cr6.lt) goto loc_826FAB1C;
loc_826FAADC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fab1c
	if (cr6.eq) goto loc_826FAB1C;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x826fab0c
	if (!cr0.lt) goto loc_826FAB0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FAB0C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826faadc
	if (cr6.gt) goto loc_826FAADC;
loc_826FAB1C:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x826fab44
	if (!cr0.lt) goto loc_826FAB44;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FAB44:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x826f25e0
	sub_826F25E0(ctx, base);
	// b 0x826fad30
	goto loc_826FAD30;
loc_826FAB64:
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// ld r10,104(r24)
	ctx.r10.u64 = PPC_LOAD_U64(r24.u32 + 104);
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,112(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,116(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,120(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,124(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,128(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,132(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,136(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,140(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,144(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,148(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,152(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r31,84(r14)
	r31.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fac80
	if (cr6.eq) goto loc_826FAC80;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r30,r21
	r30.u64 = r21.u64;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826fac58
	if (!cr6.lt) goto loc_826FAC58;
loc_826FAC18:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fac58
	if (cr6.eq) goto loc_826FAC58;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x826fac48
	if (!cr0.lt) goto loc_826FAC48;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FAC48:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fac18
	if (cr6.gt) goto loc_826FAC18;
loc_826FAC58:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x826fac80
	if (!cr0.lt) goto loc_826FAC80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FAC80:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r14
	ctx.r3.u64 = r14.u64;
	// bl 0x82639b10
	sub_82639B10(ctx, base);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,104(r24)
	PPC_STORE_U64(r24.u32 + 104, r11.u64);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,112(r24)
	PPC_STORE_U32(r24.u32 + 112, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,116(r24)
	PPC_STORE_U32(r24.u32 + 116, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,120(r24)
	PPC_STORE_U32(r24.u32 + 120, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,124(r24)
	PPC_STORE_U32(r24.u32 + 124, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,128(r24)
	PPC_STORE_U32(r24.u32 + 128, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,132(r24)
	PPC_STORE_U32(r24.u32 + 132, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,136(r24)
	PPC_STORE_U32(r24.u32 + 136, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,140(r24)
	PPC_STORE_U32(r24.u32 + 140, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,144(r24)
	PPC_STORE_U32(r24.u32 + 144, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,148(r24)
	PPC_STORE_U32(r24.u32 + 148, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stw r11,152(r24)
	PPC_STORE_U32(r24.u32 + 152, r11.u32);
loc_826FAD30:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// stb r21,1187(r24)
	PPC_STORE_U8(r24.u32 + 1187, r21.u8);
	// bne cr6,0x826fb530
	if (!cr6.eq) goto loc_826FB530;
loc_826FAD3C:
	// lwz r11,3932(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 3932);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fae80
	if (cr6.eq) goto loc_826FAE80;
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// ld r10,104(r24)
	ctx.r10.u64 = PPC_LOAD_U64(r24.u32 + 104);
	// mr r3,r14
	ctx.r3.u64 = r14.u64;
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,112(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,116(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,120(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,124(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,128(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,132(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,136(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,140(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,144(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,148(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,152(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// bl 0x82639ef0
	sub_82639EF0(ctx, base);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,104(r24)
	PPC_STORE_U64(r24.u32 + 104, r11.u64);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,112(r24)
	PPC_STORE_U32(r24.u32 + 112, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,116(r24)
	PPC_STORE_U32(r24.u32 + 116, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r11,120(r24)
	PPC_STORE_U32(r24.u32 + 120, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,124(r24)
	PPC_STORE_U32(r24.u32 + 124, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,128(r24)
	PPC_STORE_U32(r24.u32 + 128, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,132(r24)
	PPC_STORE_U32(r24.u32 + 132, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,136(r24)
	PPC_STORE_U32(r24.u32 + 136, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,140(r24)
	PPC_STORE_U32(r24.u32 + 140, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,144(r24)
	PPC_STORE_U32(r24.u32 + 144, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,148(r24)
	PPC_STORE_U32(r24.u32 + 148, r11.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stw r11,152(r24)
	PPC_STORE_U32(r24.u32 + 152, r11.u32);
	// bne cr6,0x826fb530
	if (!cr6.eq) goto loc_826FB530;
loc_826FAE80:
	// mr r28,r15
	r28.u64 = r15.u64;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// beq cr6,0x826fc5cc
	if (cr6.eq) goto loc_826FC5CC;
loc_826FAE90:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// li r10,128
	ctx.r10.s64 = 128;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// dcbt r10,r11
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// lbz r10,24(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 24);
	// rlwinm r11,r11,0,29,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFE7;
	// rlwinm r11,r11,0,4,2
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFEFFFFFFF;
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// stb r10,4(r17)
	PPC_STORE_U8(r17.u32 + 4, ctx.r10.u8);
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// lwz r11,1324(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1324);
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fb024
	if (cr6.lt) goto loc_826FB024;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826fafa4
	if (!cr6.lt) goto loc_826FAFA4;
loc_826FAF10:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826faf38
	if (cr6.lt) goto loc_826FAF38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826faf10
	if (cr6.eq) goto loc_826FAF10;
	// b 0x826fafa4
	goto loc_826FAFA4;
loc_826FAF38:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826FAFA4:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fbc3c
	if (cr6.lt) goto loc_826FBC3C;
loc_826FAFB0:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bgt cr6,0x826fbc3c
	if (cr6.gt) goto loc_826FBC3C;
	// addi r10,r20,80
	ctx.r10.s64 = r20.s64 + 80;
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// lbzx r10,r30,r10
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + ctx.r10.u32);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// rlwimi r11,r10,8,21,23
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 8) & 0x700) | (r11.u64 & 0xFFFFFFFFFFFFF8FF);
	// srawi r9,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 3;
	// srawi r10,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 4;
	// clrlwi r27,r9,31
	r27.u64 = ctx.r9.u32 & 0x1;
	// clrlwi r23,r10,31
	r23.u64 = ctx.r10.u32 & 0x1;
	// rlwinm r10,r11,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// cmplwi cr6,r10,1024
	cr6.compare<uint32_t>(ctx.r10.u32, 1024, xer);
	// bne cr6,0x826fb53c
	if (!cr6.eq) goto loc_826FB53C;
	// lbz r10,27(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 27);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826fb2ec
	if (cr6.eq) goto loc_826FB2EC;
	// lbz r10,1181(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 1181);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826fb078
	if (cr6.eq) goto loc_826FB078;
	// rlwinm r11,r11,20,12,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 20) & 0xFFFFF;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fb064
	if (cr6.eq) goto loc_826FB064;
	// lbz r11,1182(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// b 0x826fb2d0
	goto loc_826FB2D0;
loc_826FB024:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826FB02C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fb02c
	if (cr6.lt) goto loc_826FB02C;
	// b 0x826fafb0
	goto loc_826FAFB0;
loc_826FB064:
	// lbz r11,1180(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1180);
	// lbz r10,1185(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826fb2d0
	goto loc_826FB2D0;
loc_826FB078:
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// mr r29,r15
	r29.u64 = r15.u64;
	// lbz r11,1186(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826fb164
	if (cr6.eq) goto loc_826FB164;
	// mr r30,r21
	r30.u64 = r21.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826fb0f8
	if (!cr6.lt) goto loc_826FB0F8;
loc_826FB0A0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fb0f8
	if (cr6.eq) goto loc_826FB0F8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fb0e8
	if (!cr0.lt) goto loc_826FB0E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB0E8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fb0a0
	if (cr6.gt) goto loc_826FB0A0;
loc_826FB0F8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fb134
	if (!cr0.lt) goto loc_826FB134;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB134:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826fb14c
	if (cr6.eq) goto loc_826FB14C;
	// lbz r11,1182(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826fb2d4
	goto loc_826FB2D4;
loc_826FB14C:
	// lbz r11,1180(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1180);
	// lbz r10,1185(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826fb2d4
	goto loc_826FB2D4;
loc_826FB164:
	// li r30,3
	r30.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826fb1c8
	if (!cr6.lt) goto loc_826FB1C8;
loc_826FB170:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fb1c8
	if (cr6.eq) goto loc_826FB1C8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fb1b8
	if (!cr0.lt) goto loc_826FB1B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB1B8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fb170
	if (cr6.gt) goto loc_826FB170;
loc_826FB1C8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fb204
	if (!cr0.lt) goto loc_826FB204;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB204:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x826fb2c4
	if (!cr6.eq) goto loc_826FB2C4;
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// li r30,5
	r30.s64 = 5;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826fb280
	if (!cr6.lt) goto loc_826FB280;
loc_826FB228:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fb280
	if (cr6.eq) goto loc_826FB280;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fb270
	if (!cr0.lt) goto loc_826FB270;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB270:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fb228
	if (cr6.gt) goto loc_826FB228;
loc_826FB280:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fb2bc
	if (!cr0.lt) goto loc_826FB2BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB2BC:
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x826fb2cc
	goto loc_826FB2CC;
loc_826FB2C4:
	// lbz r11,1180(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1180);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
loc_826FB2CC:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826FB2D0:
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
loc_826FB2D4:
	// stb r11,4(r17)
	PPC_STORE_U8(r17.u32 + 4, r11.u8);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x826fbc3c
	if (cr6.lt) goto loc_826FBC3C;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// bgt cr6,0x826fbc3c
	if (cr6.gt) goto loc_826FBC3C;
loc_826FB2EC:
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826fb360
	if (!cr6.lt) goto loc_826FB360;
loc_826FB308:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fb360
	if (cr6.eq) goto loc_826FB360;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fb350
	if (!cr0.lt) goto loc_826FB350;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB350:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fb308
	if (cr6.gt) goto loc_826FB308;
loc_826FB360:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fb39c
	if (!cr0.lt) goto loc_826FB39C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB39C:
	// lwz r10,0(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r17)
	PPC_STORE_U32(r17.u32 + 0, ctx.r10.u32);
	// beq cr6,0x826fb508
	if (cr6.eq) goto loc_826FB508;
	// lwz r11,1172(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1172);
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fb4a8
	if (cr6.lt) goto loc_826FB4A8;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826fb4a0
	if (!cr6.lt) goto loc_826FB4A0;
loc_826FB408:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826fb434
	if (cr6.lt) goto loc_826FB434;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826fb408
	if (cr6.eq) goto loc_826FB408;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fb4e4
	goto loc_826FB4E4;
loc_826FB434:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826FB4A0:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fb4e4
	goto loc_826FB4E4;
loc_826FB4A8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826FB4B0:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fb4b0
	if (cr6.lt) goto loc_826FB4B0;
loc_826FB4E4:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826fbc3c
	if (!cr6.eq) goto loc_826FBC3C;
	// lwz r11,1200(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1200);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lbz r11,1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// stb r11,5(r17)
	PPC_STORE_U8(r17.u32 + 5, r11.u8);
	// b 0x826fb50c
	goto loc_826FB50C;
loc_826FB508:
	// stb r15,5(r17)
	PPC_STORE_U8(r17.u32 + 5, r15.u8);
loc_826FB50C:
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x826f99d8
	sub_826F99D8(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x826fc594
	if (cr6.eq) goto loc_826FC594;
loc_826FB530:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_826FB53C:
	// lwz r11,1460(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1460);
	// mr r28,r15
	r28.u64 = r15.u64;
	// lwz r26,188(r24)
	r26.u64 = PPC_LOAD_U32(r24.u32 + 188);
	// lwz r25,0(r18)
	r25.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826fb614
	if (!cr6.eq) goto loc_826FB614;
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826fb5c8
	if (!cr6.lt) goto loc_826FB5C8;
loc_826FB570:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fb5c8
	if (cr6.eq) goto loc_826FB5C8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fb5b8
	if (!cr0.lt) goto loc_826FB5B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB5B8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fb570
	if (cr6.gt) goto loc_826FB570;
loc_826FB5C8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fb604
	if (!cr0.lt) goto loc_826FB604;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB604:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwimi r11,r30,7,24,24
	r11.u64 = (__builtin_rotateleft32(r30.u32, 7) & 0x80) | (r11.u64 & 0xFFFFFFFFFFFFFF7F);
	// rlwinm r11,r11,0,27,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFF9F;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
loc_826FB614:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r10,r11,0,24,26
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826fb87c
	if (!cr6.eq) goto loc_826FB87C;
	// rlwinm r11,r11,0,21,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r11,256
	cr6.compare<uint32_t>(r11.u32, 256, xer);
	// beq cr6,0x826fb86c
	if (cr6.eq) goto loc_826FB86C;
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826fb6a4
	if (!cr6.lt) goto loc_826FB6A4;
loc_826FB64C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fb6a4
	if (cr6.eq) goto loc_826FB6A4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fb694
	if (!cr0.lt) goto loc_826FB694;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB694:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fb64c
	if (cr6.gt) goto loc_826FB64C;
loc_826FB6A4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fb6e0
	if (!cr0.lt) goto loc_826FB6E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB6E0:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826fb86c
	if (cr6.eq) goto loc_826FB86C;
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826fb75c
	if (!cr6.lt) goto loc_826FB75C;
loc_826FB704:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fb75c
	if (cr6.eq) goto loc_826FB75C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fb74c
	if (!cr0.lt) goto loc_826FB74C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB74C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fb704
	if (cr6.gt) goto loc_826FB704;
loc_826FB75C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fb798
	if (!cr0.lt) goto loc_826FB798;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB798:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826fb7ac
	if (!cr6.eq) goto loc_826FB7AC;
	// rlwimi r11,r21,5,24,26
	r11.u64 = (__builtin_rotateleft32(r21.u32, 5) & 0xE0) | (r11.u64 & 0xFFFFFFFFFFFFFF1F);
	// b 0x826fb878
	goto loc_826FB878;
loc_826FB7AC:
	// rlwimi r11,r21,6,24,26
	r11.u64 = (__builtin_rotateleft32(r21.u32, 6) & 0xE0) | (r11.u64 & 0xFFFFFFFFFFFFFF1F);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r29,r15
	r29.u64 = r15.u64;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826fb828
	if (!cr6.lt) goto loc_826FB828;
loc_826FB7D0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fb828
	if (cr6.eq) goto loc_826FB828;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fb818
	if (!cr0.lt) goto loc_826FB818;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB818:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fb7d0
	if (cr6.gt) goto loc_826FB7D0;
loc_826FB828:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fb864
	if (!cr0.lt) goto loc_826FB864;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FB864:
	// mr r28,r30
	r28.u64 = r30.u64;
	// b 0x826fb87c
	goto loc_826FB87C;
loc_826FB86C:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// li r10,3
	ctx.r10.s64 = 3;
	// rlwimi r11,r10,5,24,26
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 5) & 0xE0) | (r11.u64 & 0xFFFFFFFFFFFFFF1F);
loc_826FB878:
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
loc_826FB87C:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r10,r11,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826fb93c
	if (!cr6.eq) goto loc_826FB93C;
	// rlwinm r11,r11,0,24,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE0;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// beq cr6,0x826fb8dc
	if (cr6.eq) goto loc_826FB8DC;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x826fb8dc
	if (cr6.eq) goto loc_826FB8DC;
	// lwz r11,1508(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1508);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lwz r4,176(r24)
	ctx.r4.u64 = PPC_LOAD_U32(r24.u32 + 176);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fb8c4
	if (cr6.eq) goto loc_826FB8C4;
	// bl 0x826eb3a0
	sub_826EB3A0(ctx, base);
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r3.u32);
	// b 0x826fb8dc
	goto loc_826FB8DC;
loc_826FB8C4:
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// rlwinm r10,r3,1,0,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFE0000;
	// clrlwi r9,r3,16
	ctx.r9.u64 = ctx.r3.u32 & 0xFFFF;
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stwx r10,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r10.u32);
loc_826FB8DC:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r11,r11,0,24,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE0;
	// cmplwi cr6,r11,64
	cr6.compare<uint32_t>(r11.u32, 64, xer);
	// bne cr6,0x826fbaf0
	if (!cr6.eq) goto loc_826FBAF0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826fbaf0
	if (cr6.eq) goto loc_826FBAF0;
	// lwz r11,1508(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1508);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lwz r4,176(r24)
	ctx.r4.u64 = PPC_LOAD_U32(r24.u32 + 176);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fb91c
	if (cr6.eq) goto loc_826FB91C;
	// bl 0x826eb3a0
	sub_826EB3A0(ctx, base);
	// addi r11,r25,1
	r11.s64 = r25.s64 + 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r3.u32);
	// b 0x826fbaf0
	goto loc_826FBAF0;
loc_826FB91C:
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// addi r11,r25,1
	r11.s64 = r25.s64 + 1;
	// rlwinm r10,r3,1,0,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFE0000;
	// clrlwi r9,r3,16
	ctx.r9.u64 = ctx.r3.u32 & 0xFFFF;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stwx r10,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r10.u32);
	// b 0x826fbaf0
	goto loc_826FBAF0;
loc_826FB93C:
	// lwz r11,180(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 180);
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fba30
	if (cr6.lt) goto loc_826FBA30;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826fba28
	if (!cr6.lt) goto loc_826FBA28;
loc_826FB990:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826fb9bc
	if (cr6.lt) goto loc_826FB9BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826fb990
	if (cr6.eq) goto loc_826FB990;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fba6c
	goto loc_826FBA6C;
loc_826FB9BC:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826FBA28:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fba6c
	goto loc_826FBA6C;
loc_826FBA30:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826FBA38:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fba38
	if (cr6.lt) goto loc_826FBA38;
loc_826FBA6C:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826fbc3c
	if (!cr6.eq) goto loc_826FBC3C;
	// li r29,3
	r29.s64 = 3;
	// addi r31,r24,36
	r31.s64 = r24.s64 + 36;
loc_826FBA84:
	// slw r11,r21,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (r21.u32 << (r29.u8 & 0x3F));
	// and r11,r11,r30
	r11.u64 = r11.u64 & r30.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fbae0
	if (cr6.eq) goto loc_826FBAE0;
	// lwz r11,1508(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1508);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lwz r4,176(r24)
	ctx.r4.u64 = PPC_LOAD_U32(r24.u32 + 176);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fbac0
	if (cr6.eq) goto loc_826FBAC0;
	// bl 0x826eb3a0
	sub_826EB3A0(ctx, base);
	// lhz r11,0(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r3.u32);
	// b 0x826fbae0
	goto loc_826FBAE0;
loc_826FBAC0:
	// bl 0x826eae90
	sub_826EAE90(ctx, base);
	// lhz r11,0(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// rlwinm r10,r3,1,0,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFE0000;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// clrlwi r9,r3,16
	ctx.r9.u64 = ctx.r3.u32 & 0xFFFF;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stwx r10,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r10.u32);
loc_826FBAE0:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// cmpwi cr6,r29,-1
	cr6.compare<int32_t>(r29.s32, -1, xer);
	// bgt cr6,0x826fba84
	if (cr6.gt) goto loc_826FBA84;
loc_826FBAF0:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x826fbc48
	if (cr6.eq) goto loc_826FBC48;
	// lwz r11,1176(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1176);
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fbbec
	if (cr6.lt) goto loc_826FBBEC;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826fbbe4
	if (!cr6.lt) goto loc_826FBBE4;
loc_826FBB4C:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826fbb78
	if (cr6.lt) goto loc_826FBB78;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826fbb4c
	if (cr6.eq) goto loc_826FBB4C;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fbc28
	goto loc_826FBC28;
loc_826FBB78:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826FBBE4:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fbc28
	goto loc_826FBC28;
loc_826FBBEC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826FBBF4:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fbbf4
	if (cr6.lt) goto loc_826FBBF4;
loc_826FBC28:
	// lwz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826fbc4c
	if (cr6.eq) goto loc_826FBC4C;
loc_826FBC3C:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_826FBC48:
	// mr r11,r15
	r11.u64 = r15.u64;
loc_826FBC4C:
	// lwz r10,1200(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 1200);
	// lbzx r28,r10,r11
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// stb r28,5(r17)
	PPC_STORE_U8(r17.u32 + 5, r28.u8);
	// lbz r11,27(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 27);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fbf24
	if (cr6.eq) goto loc_826FBF24;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826fbf24
	if (cr6.eq) goto loc_826FBF24;
	// lbz r11,1181(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1181);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fbcb0
	if (cr6.eq) goto loc_826FBCB0;
	// lwz r10,0(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r10,r10,20,12,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFFFFF;
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fbc9c
	if (cr6.eq) goto loc_826FBC9C;
	// lbz r11,1182(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// b 0x826fbf08
	goto loc_826FBF08;
loc_826FBC9C:
	// lbz r11,1180(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1180);
	// lbz r10,1185(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826fbf08
	goto loc_826FBF08;
loc_826FBCB0:
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// mr r29,r15
	r29.u64 = r15.u64;
	// lbz r11,1186(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826fbd9c
	if (cr6.eq) goto loc_826FBD9C;
	// mr r30,r21
	r30.u64 = r21.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826fbd30
	if (!cr6.lt) goto loc_826FBD30;
loc_826FBCD8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fbd30
	if (cr6.eq) goto loc_826FBD30;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fbd20
	if (!cr0.lt) goto loc_826FBD20;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FBD20:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fbcd8
	if (cr6.gt) goto loc_826FBCD8;
loc_826FBD30:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fbd6c
	if (!cr0.lt) goto loc_826FBD6C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FBD6C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826fbd84
	if (cr6.eq) goto loc_826FBD84;
	// lbz r11,1182(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1182);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826fbf0c
	goto loc_826FBF0C;
loc_826FBD84:
	// lbz r11,1180(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1180);
	// lbz r10,1185(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 1185);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826fbf0c
	goto loc_826FBF0C;
loc_826FBD9C:
	// li r30,3
	r30.s64 = 3;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x826fbe00
	if (!cr6.lt) goto loc_826FBE00;
loc_826FBDA8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fbe00
	if (cr6.eq) goto loc_826FBE00;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fbdf0
	if (!cr0.lt) goto loc_826FBDF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FBDF0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fbda8
	if (cr6.gt) goto loc_826FBDA8;
loc_826FBE00:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fbe3c
	if (!cr0.lt) goto loc_826FBE3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FBE3C:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x826fbefc
	if (!cr6.eq) goto loc_826FBEFC;
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// li r30,5
	r30.s64 = 5;
	// mr r29,r15
	r29.u64 = r15.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826fbeb8
	if (!cr6.lt) goto loc_826FBEB8;
loc_826FBE60:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fbeb8
	if (cr6.eq) goto loc_826FBEB8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826fbea8
	if (!cr0.lt) goto loc_826FBEA8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FBEA8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826fbe60
	if (cr6.gt) goto loc_826FBE60;
loc_826FBEB8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826fbef4
	if (!cr0.lt) goto loc_826FBEF4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FBEF4:
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x826fbf04
	goto loc_826FBF04;
loc_826FBEFC:
	// lbz r11,1180(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 1180);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
loc_826FBF04:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_826FBF08:
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
loc_826FBF0C:
	// stb r11,4(r17)
	PPC_STORE_U8(r17.u32 + 4, r11.u8);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x826fbc3c
	if (cr6.lt) goto loc_826FBC3C;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// bgt cr6,0x826fbc3c
	if (cr6.gt) goto loc_826FBC3C;
loc_826FBF24:
	// lbz r11,29(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 29);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fc0c0
	if (cr6.eq) goto loc_826FC0C0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826fc0c0
	if (cr6.eq) goto loc_826FC0C0;
	// lwz r11,200(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 200);
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fc02c
	if (cr6.lt) goto loc_826FC02C;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826fc024
	if (!cr6.lt) goto loc_826FC024;
loc_826FBF8C:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826fbfb8
	if (cr6.lt) goto loc_826FBFB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826fbf8c
	if (cr6.eq) goto loc_826FBF8C;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fc068
	goto loc_826FC068;
loc_826FBFB8:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826FC024:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fc068
	goto loc_826FC068;
loc_826FC02C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826FC034:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fc034
	if (cr6.lt) goto loc_826FC034;
loc_826FC068:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826fbc3c
	if (!cr6.eq) goto loc_826FBC3C;
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// blt cr6,0x826fc088
	if (cr6.lt) goto loc_826FC088;
	// mr r9,r15
	ctx.r9.u64 = r15.u64;
loc_826FC088:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwimi r11,r9,28,3,3
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 28) & 0x10000000) | (r11.u64 & 0xFFFFFFFFEFFFFFFF);
	// addi r9,r20,88
	ctx.r9.s64 = r20.s64 + 88;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// lwzx r11,r10,r20
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r20.u32);
	// rlwimi r8,r11,24,5,7
	ctx.r8.u64 = (__builtin_rotateleft32(r11.u32, 24) & 0x7000000) | (ctx.r8.u64 & 0xFFFFFFFFF8FFFFFF);
	// stw r8,0(r17)
	PPC_STORE_U32(r17.u32 + 0, ctx.r8.u32);
	// lwzx r11,r10,r9
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// rlwimi r10,r11,20,10,11
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 20) & 0x300000) | (ctx.r10.u64 & 0xFFFFFFFFFFCFFFFF);
	// rlwinm r11,r10,0,5,3
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFF7FFFFFF;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
loc_826FC0C0:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// mr r23,r15
	r23.u64 = r15.u64;
	// lbz r10,29(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 29);
	// rlwinm r9,r11,0,21,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// lbz r20,5(r17)
	r20.u64 = PPC_LOAD_U8(r17.u32 + 5);
	// lbz r25,34(r24)
	r25.u64 = PPC_LOAD_U8(r24.u32 + 34);
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r22,r11,12,30,31
	r22.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// rlwinm r19,r9,27,31,31
	r19.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826fc0f4
	if (cr6.eq) goto loc_826FC0F4;
	// rlwinm r25,r11,8,29,31
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0x7;
loc_826FC0F4:
	// mr r21,r15
	r21.u64 = r15.u64;
loc_826FC0F8:
	// clrlwi r11,r20,31
	r11.u64 = r20.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fc51c
	if (cr6.eq) goto loc_826FC51C;
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fc26c
	if (cr6.eq) goto loc_826FC26C;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x826fc26c
	if (!cr6.eq) goto loc_826FC26C;
	// lwz r11,440(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 440);
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fc210
	if (cr6.lt) goto loc_826FC210;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826fc208
	if (!cr6.lt) goto loc_826FC208;
loc_826FC170:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826fc19c
	if (cr6.lt) goto loc_826FC19C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826fc170
	if (cr6.eq) goto loc_826FC170;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fc24c
	goto loc_826FC24C;
loc_826FC19C:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826FC208:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fc24c
	goto loc_826FC24C;
loc_826FC210:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826FC218:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fc218
	if (cr6.lt) goto loc_826FC218;
loc_826FC24C:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826fbc3c
	if (!cr6.eq) goto loc_826FBC3C;
	// add r11,r30,r24
	r11.u64 = r30.u64 + r24.u64;
	// add r10,r30,r24
	ctx.r10.u64 = r30.u64 + r24.u64;
	// lbz r25,516(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 516);
	// lbz r22,524(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + 524);
loc_826FC26C:
	// add r10,r21,r17
	ctx.r10.u64 = r21.u64 + r17.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// stb r25,6(r10)
	PPC_STORE_U8(ctx.r10.u32 + 6, r25.u8);
	// bne cr6,0x826fc2c0
	if (!cr6.eq) goto loc_826FC2C0;
	// lwz r31,20(r18)
	r31.u64 = PPC_LOAD_U32(r18.u32 + 20);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lwz r4,236(r24)
	ctx.r4.u64 = PPC_LOAD_U32(r24.u32 + 236);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// lbz r5,924(r24)
	ctx.r5.u64 = PPC_LOAD_U8(r24.u32 + 924);
	// bl 0x826fc940
	sub_826FC940(ctx, base);
	// rlwinm r11,r3,1,24,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFE;
	// lwz r10,24(r18)
	ctx.r10.u64 = PPC_LOAD_U32(r18.u32 + 24);
	// ori r23,r23,1
	r23.u64 = r23.u64 | 1;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// mr r28,r15
	r28.u64 = r15.u64;
	// stw r11,20(r18)
	PPC_STORE_U32(r18.u32 + 20, r11.u32);
	// stb r3,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r3.u8);
	// lwz r11,24(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,24(r18)
	PPC_STORE_U32(r18.u32 + 24, r11.u32);
	// b 0x826fc524
	goto loc_826FC524;
loc_826FC2C0:
	// cmpwi cr6,r25,4
	cr6.compare<int32_t>(r25.s32, 4, xer);
	// bne cr6,0x826fc408
	if (!cr6.eq) goto loc_826FC408;
	// lwz r31,0(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r11,444(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 444);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rldicl r11,r9,6,58
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 6) & 0x3F;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fc3b0
	if (cr6.lt) goto loc_826FC3B0;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// sld r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// bge cr6,0x826fc3a8
	if (!cr6.lt) goto loc_826FC3A8;
loc_826FC310:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826fc33c
	if (cr6.lt) goto loc_826FC33C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826fc310
	if (cr6.eq) goto loc_826FC310;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fc3f0
	goto loc_826FC3F0;
loc_826FC33C:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826FC3A8:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826fc3f0
	goto loc_826FC3F0;
loc_826FC3B0:
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826FC3BC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r16
	r11.u64 = r30.u64 + r16.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fc3bc
	if (cr6.lt) goto loc_826FC3BC;
loc_826FC3F0:
	// lwz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826fbc3c
	if (!cr6.eq) goto loc_826FBC3C;
	// b 0x826fc494
	goto loc_826FC494;
loc_826FC408:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x826fc490
	if (!cr6.eq) goto loc_826FC490;
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826fc490
	if (!cr6.eq) goto loc_826FC490;
	// lwz r3,0(r24)
	ctx.r3.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826fc44c
	if (!cr0.lt) goto loc_826FC44C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FC44C:
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// bne cr6,0x826fc488
	if (!cr6.eq) goto loc_826FC488;
	// lwz r3,0(r24)
	ctx.r3.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826fc480
	if (!cr0.lt) goto loc_826FC480;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826FC480:
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// b 0x826fc494
	goto loc_826FC494;
loc_826FC488:
	// li r11,3
	r11.s64 = 3;
	// b 0x826fc494
	goto loc_826FC494;
loc_826FC490:
	// mr r11,r22
	r11.u64 = r22.u64;
loc_826FC494:
	// add r10,r11,r24
	ctx.r10.u64 = r11.u64 + r24.u64;
	// lwz r28,24(r18)
	r28.u64 = PPC_LOAD_U32(r18.u32 + 24);
	// rlwinm r9,r25,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r30,20(r18)
	r30.u64 = PPC_LOAD_U32(r18.u32 + 20);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lwz r27,236(r24)
	r27.u64 = PPC_LOAD_U32(r24.u32 + 236);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// add r8,r25,r24
	ctx.r8.u64 = r25.u64 + r24.u64;
	// lbz r29,160(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 160);
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// mr r31,r15
	r31.u64 = r15.u64;
	// or r23,r11,r23
	r23.u64 = r11.u64 | r23.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// lbz r26,924(r8)
	r26.u64 = PPC_LOAD_U8(ctx.r8.u32 + 924);
	// ble cr6,0x826fc4fc
	if (!cr6.gt) goto loc_826FC4FC;
loc_826FC4D0:
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x826fc940
	sub_826FC940(ctx, base);
	// stbx r3,r31,r28
	PPC_STORE_U8(r31.u32 + r28.u32, ctx.r3.u8);
	// rlwinm r11,r3,1,24,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFE;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// cmpw cr6,r31,r29
	cr6.compare<int32_t>(r31.s32, r29.s32, xer);
	// blt cr6,0x826fc4d0
	if (cr6.lt) goto loc_826FC4D0;
loc_826FC4FC:
	// cmpwi cr6,r30,-1
	cr6.compare<int32_t>(r30.s32, -1, xer);
	// beq cr6,0x826fbc3c
	if (cr6.eq) goto loc_826FBC3C;
	// lwz r11,24(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 24);
	// mr r28,r15
	r28.u64 = r15.u64;
	// stw r30,20(r18)
	PPC_STORE_U32(r18.u32 + 20, r30.u32);
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// stw r11,24(r18)
	PPC_STORE_U32(r18.u32 + 24, r11.u32);
	// b 0x826fc524
	goto loc_826FC524;
loc_826FC51C:
	// add r11,r21,r17
	r11.u64 = r21.u64 + r17.u64;
	// stb r15,6(r11)
	PPC_STORE_U8(r11.u32 + 6, r15.u8);
loc_826FC524:
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// rlwinm r20,r20,31,1,31
	r20.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 31) & 0x7FFFFFFF;
	// rldicr r23,r23,8,55
	r23.u64 = __builtin_rotateleft64(r23.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// cmplwi cr6,r21,6
	cr6.compare<uint32_t>(r21.u32, 6, xer);
	// blt cr6,0x826fc0f8
	if (cr6.lt) goto loc_826FC0F8;
	// lbz r10,4(r17)
	ctx.r10.u64 = PPC_LOAD_U8(r17.u32 + 4);
	// rlwinm r11,r19,7,24,24
	r11.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 7) & 0x80;
	// lbz r8,5(r17)
	ctx.r8.u64 = PPC_LOAD_U8(r17.u32 + 5);
	// li r12,1
	r12.s64 = 1;
	// rldicr r10,r10,8,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lwz r7,4(r18)
	ctx.r7.u64 = PPC_LOAD_U32(r18.u32 + 4);
	// rldicl r9,r23,56,8
	ctx.r9.u64 = __builtin_rotateleft64(r23.u64, 56) & 0xFFFFFFFFFFFFFF;
	// lwz r6,1248(r24)
	ctx.r6.u64 = PPC_LOAD_U32(r24.u32 + 1248);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// lwz r20,88(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rldicr r12,r12,54,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 54) & 0xFFFFFFFFFFFFFFFF;
	// lwz r28,80(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// lwz r26,84(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// or r9,r9,r12
	ctx.r9.u64 = ctx.r9.u64 | r12.u64;
	// lwz r22,92(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// rldicr r11,r11,48,15
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFF000000000000;
	// lwz r25,96(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// rlwinm r10,r7,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// mr r29,r15
	r29.u64 = r15.u64;
	// li r21,1
	r21.s64 = 1;
	// stdx r11,r10,r6
	PPC_STORE_U64(ctx.r10.u32 + ctx.r6.u32, r11.u64);
loc_826FC594:
	// lhz r9,18(r18)
	ctx.r9.u64 = PPC_LOAD_U16(r18.u32 + 18);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// lwz r10,0(r18)
	ctx.r10.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// addi r17,r17,20
	r17.s64 = r17.s64 + 20;
	// lwz r11,4(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 4);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// cmplw cr6,r28,r25
	cr6.compare<uint32_t>(r28.u32, r25.u32, xer);
	// sth r9,18(r18)
	PPC_STORE_U16(r18.u32 + 18, ctx.r9.u16);
	// stw r10,0(r18)
	PPC_STORE_U32(r18.u32 + 0, ctx.r10.u32);
	// stw r11,4(r18)
	PPC_STORE_U32(r18.u32 + 4, r11.u32);
	// blt cr6,0x826fae90
	if (cr6.lt) goto loc_826FAE90;
loc_826FC5CC:
	// lhz r9,16(r18)
	ctx.r9.u64 = PPC_LOAD_U16(r18.u32 + 16);
	// rlwinm r11,r25,1,0,30
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,0(r18)
	ctx.r10.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r26,r22
	cr6.compare<uint32_t>(r26.u32, r22.u32, xer);
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// sth r9,16(r18)
	PPC_STORE_U16(r18.u32 + 16, ctx.r9.u16);
	// stw r11,0(r18)
	PPC_STORE_U32(r18.u32 + 0, r11.u32);
	// blt cr6,0x826faa6c
	if (cr6.lt) goto loc_826FAA6C;
	// lwz r28,100(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
loc_826FC5FC:
	// lwz r11,32(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 32);
	// li r10,-1
	ctx.r10.s64 = -1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// stw r11,1304(r24)
	PPC_STORE_U32(r24.u32 + 1304, r11.u32);
	// lwz r11,32(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// ld r11,104(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 104);
	// lwz r10,84(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, r11.u64);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,112(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 112);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,116(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 116);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,120(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 120);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,124(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 124);
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,128(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 128);
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,132(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 132);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,136(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 136);
	// stw r10,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,140(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 140);
	// stw r10,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,144(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 144);
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,148(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 148);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r11,84(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 84);
	// lwz r10,152(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 152);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826FC6B4"))) PPC_WEAK_FUNC(sub_826FC6B4);
PPC_FUNC_IMPL(__imp__sub_826FC6B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826FC6B8"))) PPC_WEAK_FUNC(sub_826FC6B8);
PPC_FUNC_IMPL(__imp__sub_826FC6B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// rldicr r11,r11,10,53
	r11.u64 = __builtin_rotateleft64(r11.u64, 10) & 0xFFFFFFFFFFFFFC00;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r10,r11,-10
	ctx.r10.s64 = r11.s64 + -10;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826fc784
	if (!cr6.lt) goto loc_826FC784;
loc_826FC6F0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826fc718
	if (cr6.lt) goto loc_826FC718;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826fc6f0
	if (cr6.eq) goto loc_826FC6F0;
	// b 0x826fc784
	goto loc_826FC784;
loc_826FC718:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826FC784:
	// lis r11,0
	r11.s64 = 0;
	// ori r29,r11,32768
	r29.u64 = r11.u64 | 32768;
loc_826FC78C:
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r7,r10,1,62
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r9,r10,1,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u64, 1) & 0x1;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// rotlwi r10,r9,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// add r30,r10,r30
	r30.u64 = ctx.r10.u64 + r30.u64;
	// std r7,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r7.u64);
	// stw r8,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r8.u32);
	// bge cr6,0x826fc84c
	if (!cr6.lt) goto loc_826FC84C;
loc_826FC7B8:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826fc7e0
	if (cr6.lt) goto loc_826FC7E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826fc7b8
	if (cr6.eq) goto loc_826FC7B8;
	// b 0x826fc84c
	goto loc_826FC84C;
loc_826FC7E0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826FC84C:
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826fc78c
	if (cr6.lt) goto loc_826FC78C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826FC870"))) PPC_WEAK_FUNC(sub_826FC870);
PPC_FUNC_IMPL(__imp__sub_826FC870) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
loc_826FC884:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826fc8bc
	if (cr6.lt) goto loc_826FC8BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826fc884
	if (cr6.eq) goto loc_826FC884;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_826FC8BC:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r5,r11,6
	ctx.r5.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r10,r10,8,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lbz r9,5(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,3(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// rldicr r11,r10,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r10.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// stw r5,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r5.u32);
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// neg r8,r11
	ctx.r8.s64 = -r11.s64;
	// addi r11,r11,48
	r11.s64 = r11.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826FC93C"))) PPC_WEAK_FUNC(sub_826FC93C);
PPC_FUNC_IMPL(__imp__sub_826FC93C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826FC940"))) PPC_WEAK_FUNC(sub_826FC940);
PPC_FUNC_IMPL(__imp__sub_826FC940) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr5{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r2{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// addic r1,r1,-56
	xer.ca = ctx.r1.u32 > 55;
	ctx.r1.s64 = ctx.r1.s64 + -56;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// stw r12,56(r1)
	PPC_STORE_U32(ctx.r1.u32 + 56, r12.u32);
	// std r31,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, r31.u64);
	// std r30,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, r30.u64);
	// li r30,0
	r30.s64 = 0;
	// stw r3,0(r1)
	PPC_STORE_U32(ctx.r1.u32 + 0, ctx.r3.u32);
	// stw r4,8(r1)
	PPC_STORE_U32(ctx.r1.u32 + 8, ctx.r4.u32);
	// stw r5,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r5.u32);
	// stw r6,24(r1)
	PPC_STORE_U32(ctx.r1.u32 + 24, ctx.r6.u32);
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r12,8(r4)
	r12.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r2,4(r4)
	r2.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r31,36(r4)
	r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	// addi r5,r12,1
	ctx.r5.s64 = r12.s64 + 1;
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ld r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
loc_826FC998:
	// rldicl r11,r7,10,54
	r11.u64 = __builtin_rotateleft64(ctx.r7.u64, 10) & 0x3FF;
	// rldicr r11,r11,1,62
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// lhzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// clrldi r11,r8,60
	r11.u64 = ctx.r8.u64 & 0xF;
	// blt cr6,0x826fcb2c
	if (cr6.lt) goto loc_826FCB2C;
	// sld r7,r7,r11
	ctx.r7.u64 = r11.u8 & 0x40 ? 0 : (ctx.r7.u64 << (r11.u8 & 0x7F));
	// rlwinm r8,r8,28,4,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0xFFFFFFF;
	// subf. r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt 0x826fca4c
	if (cr0.lt) goto loc_826FCA4C;
loc_826FC9C4:
	// rldicl r11,r7,1,63
	r11.u64 = __builtin_rotateleft64(ctx.r7.u64, 1) & 0x1;
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// rldicr r7,r7,1,62
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// blt 0x826fcb98
	if (cr0.lt) goto loc_826FCB98;
loc_826FC9D4:
	// rldicr r12,r8,1,62
	r12.u64 = __builtin_rotateleft64(ctx.r8.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicr r11,r11,7,56
	r11.u64 = __builtin_rotateleft64(r11.u64, 7) & 0xFFFFFFFFFFFFFF80;
	// cmpw cr6,r8,r2
	cr6.compare<int32_t>(ctx.r8.s32, r2.s32, xer);
	// cmpw cr5,r8,r5
	cr5.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, xer);
	// lhzx r12,r31,r12
	r12.u64 = PPC_LOAD_U16(r31.u32 + r12.u32);
	// or r12,r12,r11
	r12.u64 = r12.u64 | r11.u64;
	// sth r12,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, r12.u16);
	// clrldi r12,r12,57
	r12.u64 = r12.u64 & 0x7F;
	// addi r3,r3,2
	ctx.r3.s64 = ctx.r3.s64 + 2;
	// addi r12,r12,1
	r12.s64 = r12.s64 + 1;
	// subf. r10,r12,r10
	ctx.r10.s64 = ctx.r10.s64 - r12.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// cror 4*cr1+eq,lt,4*cr6+eq
	// crorc eq,4*cr1+eq,4*cr5+lt
	// bne 0x826fc998
	if (!cr0.eq) goto loc_826FC998;
	// std r7,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r7.u64);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r6,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r6.u32);
	// blt cr6,0x826fcce4
	if (cr6.lt) goto loc_826FCCE4;
	// cmpw cr5,r8,r2
	cr5.compare<int32_t>(ctx.r8.s32, r2.s32, xer);
	// beq cr5,0x826fcc08
	if (cr5.eq) goto loc_826FCC08;
loc_826FCA24:
	// lwz r4,24(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 24);
	// lwz r12,56(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + 56);
	// subf r3,r4,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r4.s64;
	// mtlr r12
	// rlwinm r3,r3,31,1,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 31) & 0x7FFFFFFF;
	// ld r31,48(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// or r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 | r30.u64;
	// ld r30,40(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + 40);
	// addic r1,r1,56
	xer.ca = ctx.r1.u32 > 4294967239;
	ctx.r1.s64 = ctx.r1.s64 + 56;
	// blr 
	return;
loc_826FCA4C:
	// lwz r12,12(r4)
	r12.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// lwz r11,16(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// subf r11,r12,r11
	r11.s64 = r11.s64 - r12.s64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bgt cr6,0x826fcaf4
	if (cr6.gt) goto loc_826FCAF4;
	// addi r1,r1,-96
	ctx.r1.s64 = ctx.r1.s64 + -96;
	// std r7,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r7.u64);
	// stw r6,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r6.u32);
	// std r2,8(r1)
	PPC_STORE_U64(ctx.r1.u32 + 8, r2.u64);
	// std r3,16(r1)
	PPC_STORE_U64(ctx.r1.u32 + 16, ctx.r3.u64);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// std r4,24(r1)
	PPC_STORE_U64(ctx.r1.u32 + 24, ctx.r4.u64);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// std r12,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r12.u64);
	// addi r1,r1,-48
	ctx.r1.s64 = ctx.r1.s64 + -48;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// addi r1,r1,48
	ctx.r1.s64 = ctx.r1.s64 + 48;
	// ld r4,24(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 24);
	// ld r6,40(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 40);
	// ld r12,88(r1)
	r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// mr r12,r3
	r12.u64 = ctx.r3.u64;
	// ld r7,48(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// ld r2,8(r1)
	r2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 8);
	// cmpwi cr6,r12,1
	cr6.compare<int32_t>(r12.s32, 1, xer);
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// ld r8,56(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 56);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// ld r10,72(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 72);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r3,16(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 16);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// ld r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// bne cr6,0x826fc9c4
	if (!cr6.eq) goto loc_826FC9C4;
	// b 0x826fca4c
	goto loc_826FCA4C;
loc_826FCAF4:
	// lhz r11,0(r12)
	r11.u64 = PPC_LOAD_U16(r12.u32 + 0);
	// lhz r0,2(r12)
	r0.u64 = PPC_LOAD_U16(r12.u32 + 2);
	// addi r12,r12,6
	r12.s64 = r12.s64 + 6;
	// rldicr r11,r11,32,31
	r11.u64 = __builtin_rotateleft64(r11.u64, 32) & 0xFFFFFFFF00000000;
	// rldicr r0,r0,16,47
	r0.u64 = __builtin_rotateleft64(r0.u64, 16) & 0xFFFFFFFFFFFF0000;
	// add r11,r11,r0
	r11.u64 = r11.u64 + r0.u64;
	// lhz r0,-2(r12)
	r0.u64 = PPC_LOAD_U16(r12.u32 + -2);
	// stw r12,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, r12.u32);
	// neg r12,r6
	r12.s64 = -ctx.r6.s64;
	// add r11,r11,r0
	r11.u64 = r11.u64 + r0.u64;
	// addi r6,r6,48
	ctx.r6.s64 = ctx.r6.s64 + 48;
	// sld r11,r11,r12
	r11.u64 = r12.u8 & 0x40 ? 0 : (r11.u64 << (r12.u8 & 0x7F));
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// b 0x826fc9c4
	goto loc_826FC9C4;
loc_826FCB2C:
	// addi r1,r1,-96
	ctx.r1.s64 = ctx.r1.s64 + -96;
	// std r7,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r7.u64);
	// stw r6,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r6.u32);
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// std r2,8(r1)
	PPC_STORE_U64(ctx.r1.u32 + 8, r2.u64);
	// std r3,16(r1)
	PPC_STORE_U64(ctx.r1.u32 + 16, ctx.r3.u64);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// std r4,24(r1)
	PPC_STORE_U64(ctx.r1.u32 + 24, ctx.r4.u64);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// addi r1,r1,-72
	ctx.r1.s64 = ctx.r1.s64 + -72;
	// bl 0x826fc6b8
	sub_826FC6B8(ctx, base);
	// addi r1,r1,72
	ctx.r1.s64 = ctx.r1.s64 + 72;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// ld r4,24(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 24);
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// ld r2,8(r1)
	r2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 8);
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// ld r10,72(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 72);
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ld r3,16(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 16);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// ld r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// b 0x826fc9c4
	goto loc_826FC9C4;
loc_826FCB98:
	// addi r1,r1,-96
	ctx.r1.s64 = ctx.r1.s64 + -96;
	// std r7,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r7.u64);
	// stw r6,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r6.u32);
	// std r2,8(r1)
	PPC_STORE_U64(ctx.r1.u32 + 8, r2.u64);
	// std r3,16(r1)
	PPC_STORE_U64(ctx.r1.u32 + 16, ctx.r3.u64);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// std r4,24(r1)
	PPC_STORE_U64(ctx.r1.u32 + 24, ctx.r4.u64);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// addi r1,r1,-48
	ctx.r1.s64 = ctx.r1.s64 + -48;
	// bl 0x826fc870
	sub_826FC870(ctx, base);
	// addi r1,r1,48
	ctx.r1.s64 = ctx.r1.s64 + 48;
	// ld r4,24(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 24);
	// ld r2,8(r1)
	r2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 8);
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// ld r8,56(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 56);
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ld r10,72(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 72);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// ld r3,16(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 16);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// ld r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// b 0x826fc9d4
	goto loc_826FC9D4;
loc_826FCC08:
	// lwz r6,0(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 0);
	// lwz r7,8(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 8);
	// addi r1,r1,-96
	ctx.r1.s64 = ctx.r1.s64 + -96;
	// std r2,8(r1)
	PPC_STORE_U64(ctx.r1.u32 + 8, r2.u64);
	// std r3,16(r1)
	PPC_STORE_U64(ctx.r1.u32 + 16, ctx.r3.u64);
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// std r4,24(r1)
	PPC_STORE_U64(ctx.r1.u32 + 24, ctx.r4.u64);
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// rlwinm r5,r11,25,31,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1;
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// std r12,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r12.u64);
	// addi r1,r1,-64
	ctx.r1.s64 = ctx.r1.s64 + -64;
	// bl 0x826e5608
	sub_826E5608(ctx, base);
	// addi r1,r1,64
	ctx.r1.s64 = ctx.r1.s64 + 64;
	// ld r8,56(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 56);
	// addic. r8,r3,0
	xer.ca = ctx.r3.u32 > 4294967295;
	ctx.r8.s64 = ctx.r3.s64 + 0;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ld r2,8(r1)
	r2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 8);
	// ld r4,24(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 24);
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// ld r6,40(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 40);
	// ld r7,48(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// ld r10,72(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 72);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r12,88(r1)
	r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// ld r3,16(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 16);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// blt 0x826fcce4
	if (cr0.lt) goto loc_826FCCE4;
	// clrldi r11,r8,57
	r11.u64 = ctx.r8.u64 & 0x7F;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// subf r12,r11,r12
	r12.s64 = r12.s64 - r11.s64;
	// rlwinm r11,r8,4,28,31
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xF;
	// add r10,r10,r12
	ctx.r10.u64 = ctx.r10.u64 + r12.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826fccc4
	if (cr6.eq) goto loc_826FCCC4;
	// ori r12,r8,64
	r12.u64 = ctx.r8.u64 | 64;
	// li r30,128
	r30.s64 = 128;
	// sth r12,-2(r3)
	PPC_STORE_U16(ctx.r3.u32 + -2, r12.u16);
	// sth r11,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, r11.u16);
	// addi r3,r3,2
	ctx.r3.s64 = ctx.r3.s64 + 2;
	// b 0x826fccc8
	goto loc_826FCCC8;
loc_826FCCC4:
	// sth r8,-2(r3)
	PPC_STORE_U16(ctx.r3.u32 + -2, ctx.r8.u16);
loc_826FCCC8:
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm r8,r8,16,20,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 16) & 0xFFF;
	// ld r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// cmpw cr6,r8,r5
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, xer);
	// blt cr6,0x826fc998
	if (cr6.lt) goto loc_826FC998;
	// b 0x826fca24
	goto loc_826FCA24;
loc_826FCCE4:
	// lwz r12,56(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + 56);
	// li r3,-1
	ctx.r3.s64 = -1;
	// mtlr r12
	// ld r31,48(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// ld r30,40(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + 40);
	// addic r1,r1,56
	xer.ca = ctx.r1.u32 > 4294967239;
	ctx.r1.s64 = ctx.r1.s64 + 56;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826FCD00"))) PPC_WEAK_FUNC(sub_826FCD00);
PPC_FUNC_IMPL(__imp__sub_826FCD00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr5{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r2{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// addic r1,r1,-64
	xer.ca = ctx.r1.u32 > 63;
	ctx.r1.s64 = ctx.r1.s64 + -64;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r12,56(r1)
	PPC_STORE_U32(ctx.r1.u32 + 56, r12.u32);
	// std r31,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, r31.u64);
	// std r30,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, r30.u64);
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// stw r3,0(r1)
	PPC_STORE_U32(ctx.r1.u32 + 0, ctx.r3.u32);
	// stw r4,8(r1)
	PPC_STORE_U32(ctx.r1.u32 + 8, ctx.r4.u32);
	// stw r5,16(r1)
	PPC_STORE_U32(ctx.r1.u32 + 16, ctx.r5.u32);
	// stw r6,24(r1)
	PPC_STORE_U32(ctx.r1.u32 + 24, ctx.r6.u32);
	// std r29,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, r29.u64);
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r12,8(r4)
	r12.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r2,4(r4)
	r2.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r31,36(r4)
	r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r5,r12,1
	ctx.r5.s64 = r12.s64 + 1;
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ld r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
loc_826FCD5C:
	// rldicl r11,r7,10,54
	r11.u64 = __builtin_rotateleft64(ctx.r7.u64, 10) & 0x3FF;
	// rldicr r11,r11,1,62
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// lhzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// clrldi r11,r8,60
	r11.u64 = ctx.r8.u64 & 0xF;
	// blt cr6,0x826fcef8
	if (cr6.lt) goto loc_826FCEF8;
	// sld r7,r7,r11
	ctx.r7.u64 = r11.u8 & 0x40 ? 0 : (ctx.r7.u64 << (r11.u8 & 0x7F));
	// rlwinm r8,r8,28,4,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0xFFFFFFF;
	// subf. r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt 0x826fce18
	if (cr0.lt) goto loc_826FCE18;
loc_826FCD88:
	// rldicl r11,r7,1,63
	r11.u64 = __builtin_rotateleft64(ctx.r7.u64, 1) & 0x1;
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// rldicr r7,r7,1,62
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// blt 0x826fcf64
	if (cr0.lt) goto loc_826FCF64;
loc_826FCD98:
	// rldicr r12,r8,1,62
	r12.u64 = __builtin_rotateleft64(ctx.r8.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// cmpw cr6,r8,r2
	cr6.compare<int32_t>(ctx.r8.s32, r2.s32, xer);
	// cmpw cr5,r8,r5
	cr5.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, xer);
	// lhzx r12,r31,r12
	r12.u64 = PPC_LOAD_U16(r31.u32 + r12.u32);
	// rldicl r29,r12,56,8
	r29.u64 = __builtin_rotateleft64(r12.u64, 56) & 0xFFFFFFFFFFFFFF;
	// clrldi r12,r12,57
	r12.u64 = r12.u64 & 0x7F;
	// xor r0,r29,r11
	r0.u64 = r29.u64 ^ r11.u64;
	// add r10,r10,r12
	ctx.r10.u64 = ctx.r10.u64 + r12.u64;
	// subf r29,r11,r0
	r29.s64 = r0.s64 - r11.s64;
	// lbzx r0,r10,r30
	r0.u64 = PPC_LOAD_U8(ctx.r10.u32 + r30.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rldicr r0,r0,1,62
	r0.u64 = __builtin_rotateleft64(r0.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// cmpwi r10,64
	cr0.compare<int32_t>(ctx.r10.s32, 64, xer);
	// sthx r29,r3,r0
	PPC_STORE_U16(ctx.r3.u32 + r0.u32, r29.u16);
	// cror 4*cr1+eq,gt,4*cr6+eq
	// crorc eq,4*cr1+eq,4*cr5+lt
	// bne 0x826fcd5c
	if (!cr0.eq) goto loc_826FCD5C;
	// std r7,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r7.u64);
	// cmpwi cr6,r10,64
	cr6.compare<int32_t>(ctx.r10.s32, 64, xer);
	// stw r6,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r6.u32);
	// bgt cr6,0x826fd0bc
	if (cr6.gt) goto loc_826FD0BC;
	// cmpw cr5,r8,r2
	cr5.compare<int32_t>(ctx.r8.s32, r2.s32, xer);
	// beq cr5,0x826fcfd4
	if (cr5.eq) goto loc_826FCFD4;
loc_826FCDF8:
	// lwz r12,56(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + 56);
	// li r3,0
	ctx.r3.s64 = 0;
	// mtlr r12
	// ld r31,48(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// ld r30,40(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + 40);
	// ld r29,32(r1)
	r29.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// addic r1,r1,64
	xer.ca = ctx.r1.u32 > 4294967231;
	ctx.r1.s64 = ctx.r1.s64 + 64;
	// blr 
	return;
loc_826FCE18:
	// lwz r12,12(r4)
	r12.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// lwz r11,16(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// subf r11,r12,r11
	r11.s64 = r11.s64 - r12.s64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bgt cr6,0x826fcec0
	if (cr6.gt) goto loc_826FCEC0;
	// addi r1,r1,-96
	ctx.r1.s64 = ctx.r1.s64 + -96;
	// std r7,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r7.u64);
	// stw r6,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r6.u32);
	// std r2,8(r1)
	PPC_STORE_U64(ctx.r1.u32 + 8, r2.u64);
	// std r3,16(r1)
	PPC_STORE_U64(ctx.r1.u32 + 16, ctx.r3.u64);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// std r4,24(r1)
	PPC_STORE_U64(ctx.r1.u32 + 24, ctx.r4.u64);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// std r12,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r12.u64);
	// addi r1,r1,-48
	ctx.r1.s64 = ctx.r1.s64 + -48;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// addi r1,r1,48
	ctx.r1.s64 = ctx.r1.s64 + 48;
	// ld r4,24(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 24);
	// ld r6,40(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 40);
	// ld r12,88(r1)
	r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// mr r12,r3
	r12.u64 = ctx.r3.u64;
	// ld r7,48(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// ld r2,8(r1)
	r2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 8);
	// cmpwi cr6,r12,1
	cr6.compare<int32_t>(r12.s32, 1, xer);
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// ld r8,56(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 56);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// ld r10,72(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 72);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r3,16(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 16);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// ld r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// bne cr6,0x826fcd88
	if (!cr6.eq) goto loc_826FCD88;
	// b 0x826fce18
	goto loc_826FCE18;
loc_826FCEC0:
	// lhz r11,0(r12)
	r11.u64 = PPC_LOAD_U16(r12.u32 + 0);
	// lhz r0,2(r12)
	r0.u64 = PPC_LOAD_U16(r12.u32 + 2);
	// addi r12,r12,6
	r12.s64 = r12.s64 + 6;
	// rldicr r11,r11,32,31
	r11.u64 = __builtin_rotateleft64(r11.u64, 32) & 0xFFFFFFFF00000000;
	// rldicr r0,r0,16,47
	r0.u64 = __builtin_rotateleft64(r0.u64, 16) & 0xFFFFFFFFFFFF0000;
	// add r11,r11,r0
	r11.u64 = r11.u64 + r0.u64;
	// lhz r0,-2(r12)
	r0.u64 = PPC_LOAD_U16(r12.u32 + -2);
	// stw r12,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, r12.u32);
	// neg r12,r6
	r12.s64 = -ctx.r6.s64;
	// add r11,r11,r0
	r11.u64 = r11.u64 + r0.u64;
	// addi r6,r6,48
	ctx.r6.s64 = ctx.r6.s64 + 48;
	// sld r11,r11,r12
	r11.u64 = r12.u8 & 0x40 ? 0 : (r11.u64 << (r12.u8 & 0x7F));
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// b 0x826fcd88
	goto loc_826FCD88;
loc_826FCEF8:
	// addi r1,r1,-96
	ctx.r1.s64 = ctx.r1.s64 + -96;
	// std r7,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r7.u64);
	// stw r6,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r6.u32);
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// std r2,8(r1)
	PPC_STORE_U64(ctx.r1.u32 + 8, r2.u64);
	// std r3,16(r1)
	PPC_STORE_U64(ctx.r1.u32 + 16, ctx.r3.u64);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// std r4,24(r1)
	PPC_STORE_U64(ctx.r1.u32 + 24, ctx.r4.u64);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// addi r1,r1,-72
	ctx.r1.s64 = ctx.r1.s64 + -72;
	// bl 0x826fc6b8
	sub_826FC6B8(ctx, base);
	// addi r1,r1,72
	ctx.r1.s64 = ctx.r1.s64 + 72;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// ld r4,24(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 24);
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// ld r2,8(r1)
	r2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 8);
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// ld r10,72(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 72);
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ld r3,16(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 16);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// ld r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// b 0x826fcd88
	goto loc_826FCD88;
loc_826FCF64:
	// addi r1,r1,-96
	ctx.r1.s64 = ctx.r1.s64 + -96;
	// std r7,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r7.u64);
	// stw r6,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r6.u32);
	// std r2,8(r1)
	PPC_STORE_U64(ctx.r1.u32 + 8, r2.u64);
	// std r3,16(r1)
	PPC_STORE_U64(ctx.r1.u32 + 16, ctx.r3.u64);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// std r4,24(r1)
	PPC_STORE_U64(ctx.r1.u32 + 24, ctx.r4.u64);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// addi r1,r1,-48
	ctx.r1.s64 = ctx.r1.s64 + -48;
	// bl 0x826fc870
	sub_826FC870(ctx, base);
	// addi r1,r1,48
	ctx.r1.s64 = ctx.r1.s64 + 48;
	// ld r4,24(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 24);
	// ld r2,8(r1)
	r2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 8);
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// ld r8,56(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 56);
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ld r10,72(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 72);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// ld r3,16(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 16);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// ld r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// b 0x826fcd98
	goto loc_826FCD98;
loc_826FCFD4:
	// li r29,0
	r29.s64 = 0;
	// subf r10,r12,r10
	ctx.r10.s64 = ctx.r10.s64 - r12.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// sthx r29,r3,r0
	PPC_STORE_U16(ctx.r3.u32 + r0.u32, r29.u16);
	// lwz r6,0(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 0);
	// lwz r7,8(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 8);
	// addi r1,r1,-96
	ctx.r1.s64 = ctx.r1.s64 + -96;
	// std r2,8(r1)
	PPC_STORE_U64(ctx.r1.u32 + 8, r2.u64);
	// std r3,16(r1)
	PPC_STORE_U64(ctx.r1.u32 + 16, ctx.r3.u64);
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// std r4,24(r1)
	PPC_STORE_U64(ctx.r1.u32 + 24, ctx.r4.u64);
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// neg r5,r11
	ctx.r5.s64 = -r11.s64;
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// std r12,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r12.u64);
	// addi r1,r1,-64
	ctx.r1.s64 = ctx.r1.s64 + -64;
	// bl 0x826e5608
	sub_826E5608(ctx, base);
	// addi r1,r1,64
	ctx.r1.s64 = ctx.r1.s64 + 64;
	// ld r8,56(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 56);
	// addic. r8,r3,0
	xer.ca = ctx.r3.u32 > 4294967295;
	ctx.r8.s64 = ctx.r3.s64 + 0;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ld r2,8(r1)
	r2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 8);
	// ld r4,24(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 24);
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// ld r6,40(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 40);
	// ld r7,48(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// ld r10,72(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 72);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r12,88(r1)
	r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// ld r3,16(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 16);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// blt 0x826fd0bc
	if (cr0.lt) goto loc_826FD0BC;
	// rlwinm r11,r8,12,20,23
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xF00;
	// rlwinm r29,r8,24,24,31
	r29.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 24) & 0xFF;
	// rlwinm r12,r8,25,31,31
	r12.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1;
	// add r29,r29,r11
	r29.u64 = r29.u64 + r11.u64;
	// clrldi r11,r8,58
	r11.u64 = ctx.r8.u64 & 0x3F;
	// neg r12,r12
	r12.s64 = -r12.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// xor r29,r29,r12
	r29.u64 = r29.u64 ^ r12.u64;
	// rlwinm r8,r8,16,20,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 16) & 0xFFF;
	// subf r29,r12,r29
	r29.s64 = r29.s64 - r12.s64;
	// cmpw cr6,r8,r5
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, xer);
	// lbzx r0,r10,r30
	r0.u64 = PPC_LOAD_U8(ctx.r10.u32 + r30.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rldicr r0,r0,1,62
	r0.u64 = __builtin_rotateleft64(r0.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// sthx r29,r3,r0
	PPC_STORE_U16(ctx.r3.u32 + r0.u32, r29.u16);
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ld r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// blt cr6,0x826fcd5c
	if (cr6.lt) goto loc_826FCD5C;
	// b 0x826fcdf8
	goto loc_826FCDF8;
loc_826FD0BC:
	// lwz r12,56(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + 56);
	// li r3,-1
	ctx.r3.s64 = -1;
	// mtlr r12
	// ld r31,48(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// ld r30,40(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + 40);
	// ld r29,32(r1)
	r29.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// addic r1,r1,64
	xer.ca = ctx.r1.u32 > 4294967231;
	ctx.r1.s64 = ctx.r1.s64 + 64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826FD200"))) PPC_WEAK_FUNC(sub_826FD200);
PPC_FUNC_IMPL(__imp__sub_826FD200) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// srawi r10,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r4.s32 >> 16;
	// lhz r8,16(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 16);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lhz r9,18(r5)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r5.u32 + 18);
	// rlwinm r7,r10,0,29,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// extsh r3,r4
	ctx.r3.s64 = ctx.r4.s16;
	// rlwinm r4,r8,3,0,27
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lhz r7,50(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 50);
	// rlwinm r5,r9,3,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF0;
	// srawi r8,r3,2
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r3.s32 >> 2;
	// lhz r30,52(r11)
	r30.u64 = PPC_LOAD_U16(r11.u32 + 52);
	// srawi r9,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 2;
	// rlwinm r11,r7,3,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF0;
	// li r31,0
	r31.s64 = 0;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r6,r9,r4
	ctx.r6.u64 = ctx.r9.u64 + ctx.r4.u64;
	// rlwinm r7,r30,3,0,27
	ctx.r7.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF0;
	// beq cr6,0x826fd264
	if (cr6.eq) goto loc_826FD264;
	// li r9,-17
	ctx.r9.s64 = -17;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// b 0x826fd268
	goto loc_826FD268;
loc_826FD264:
	// li r9,-18
	ctx.r9.s64 = -18;
loc_826FD268:
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// bge cr6,0x826fd278
	if (!cr6.lt) goto loc_826FD278;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// b 0x826fd284
	goto loc_826FD284;
loc_826FD278:
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// ble cr6,0x826fd288
	if (!cr6.gt) goto loc_826FD288;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_826FD284:
	// li r31,1
	r31.s64 = 1;
loc_826FD288:
	// cmpw cr6,r6,r9
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r9.s32, xer);
	// bge cr6,0x826fd298
	if (!cr6.lt) goto loc_826FD298;
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// b 0x826fd2b0
	goto loc_826FD2B0;
loc_826FD298:
	// cmpw cr6,r6,r7
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r7.s32, xer);
	// ble cr6,0x826fd2a8
	if (!cr6.gt) goto loc_826FD2A8;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// b 0x826fd2b0
	goto loc_826FD2B0;
loc_826FD2A8:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x826fd2d0
	if (cr6.eq) goto loc_826FD2D0;
loc_826FD2B0:
	// subf r11,r5,r8
	r11.s64 = ctx.r8.s64 - ctx.r5.s64;
	// subf r7,r4,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r4.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r8,r3,30
	ctx.r8.u64 = ctx.r3.u32 & 0x3;
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r10,r10,30
	ctx.r10.u64 = ctx.r10.u32 & 0x3;
	// add r3,r9,r8
	ctx.r3.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_826FD2D0:
	// rlwimi r3,r10,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826FD2E0"))) PPC_WEAK_FUNC(sub_826FD2E0);
PPC_FUNC_IMPL(__imp__sub_826FD2E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lhz r9,18(r5)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r5.u32 + 18);
	// srawi r10,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r4.s32 >> 16;
	// lhz r8,16(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 16);
	// extsh r3,r4
	ctx.r3.s64 = ctx.r4.s16;
	// rlwinm r9,r9,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// lhz r6,50(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 50);
	// lhz r11,52(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 52);
	// rlwinm r7,r6,2,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFF8;
	// rlwinm r6,r10,0,29,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x826fd324
	if (cr6.eq) goto loc_826FD324;
	// rlwinm r11,r11,2,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFF8;
	// li r6,-9
	ctx.r6.s64 = -9;
	// addi r5,r11,1
	ctx.r5.s64 = r11.s64 + 1;
	// b 0x826fd32c
	goto loc_826FD32C;
loc_826FD324:
	// li r6,-8
	ctx.r6.s64 = -8;
	// rlwinm r5,r11,2,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFF8;
loc_826FD32C:
	// cmpwi cr6,r3,16384
	cr6.compare<int32_t>(ctx.r3.s32, 16384, xer);
	// beq cr6,0x826fd3a8
	if (cr6.eq) goto loc_826FD3A8;
	// rlwinm r11,r9,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r9,r3,2
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r3.s32 >> 2;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r8,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r8,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 2;
	// cmpwi cr6,r11,-8
	cr6.compare<int32_t>(r11.s32, -8, xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// bge cr6,0x826fd364
	if (!cr6.lt) goto loc_826FD364;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r3,r11,r3
	ctx.r3.s64 = ctx.r3.s64 - r11.s64;
	// b 0x826fd378
	goto loc_826FD378;
loc_826FD364:
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// ble cr6,0x826fd378
	if (!cr6.gt) goto loc_826FD378;
	// subf r11,r11,r7
	r11.s64 = ctx.r7.s64 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
loc_826FD378:
	// cmpw cr6,r9,r6
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, xer);
	// bge cr6,0x826fd394
	if (!cr6.lt) goto loc_826FD394;
	// subf r11,r9,r6
	r11.s64 = ctx.r6.s64 - ctx.r9.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// rlwimi r3,r10,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// blr 
	return;
loc_826FD394:
	// cmpw cr6,r9,r5
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r5.s32, xer);
	// ble cr6,0x826fd3a8
	if (!cr6.gt) goto loc_826FD3A8;
	// subf r11,r9,r5
	r11.s64 = ctx.r5.s64 - ctx.r9.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_826FD3A8:
	// rlwimi r3,r10,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826FD3B0"))) PPC_WEAK_FUNC(sub_826FD3B0);
PPC_FUNC_IMPL(__imp__sub_826FD3B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// li r11,0
	r11.s64 = 0;
	// lwz r29,4(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826fd3e0
	if (cr6.eq) goto loc_826FD3E0;
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// li r11,1
	r11.s64 = 1;
loc_826FD3E0:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826fd3fc
	if (cr6.eq) goto loc_826FD3FC;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r10,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, ctx.r10.u32);
loc_826FD3FC:
	// lwz r10,16(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826fd418
	if (cr6.eq) goto loc_826FD418;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r10,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, ctx.r10.u32);
loc_826FD418:
	// lwz r10,24(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x826fd434
	if (cr6.eq) goto loc_826FD434;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r10,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, ctx.r10.u32);
loc_826FD434:
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x826fd4e0
	if (!cr6.eq) goto loc_826FD4E0;
	// lhz r11,102(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// lhz r10,98(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// lhz r7,106(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 106);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 100);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 104);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// subf r3,r7,r11
	ctx.r3.s64 = r11.s64 - ctx.r7.s64;
	// subf r31,r10,r7
	r31.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r27,r6,r9
	r27.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r26,r8,r6
	r26.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 ^ ctx.r5.u64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// xor r27,r27,r4
	r27.u64 = r27.u64 ^ ctx.r4.u64;
	// srawi r5,r3,31
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r3.s32 >> 31;
	// xor r26,r26,r4
	r26.u64 = r26.u64 ^ ctx.r4.u64;
	// srawi r4,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r31.s32 >> 31;
	// srawi r3,r27,31
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r27.s32 >> 31;
	// srawi r31,r26,31
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r26.s32 >> 31;
	// or r27,r5,r4
	r27.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r26,r3,r31
	r26.u64 = ctx.r3.u64 | r31.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r7,r7,r27
	ctx.r7.u64 = ctx.r7.u64 & ~r27.u64;
	// andc r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 & ~r26.u64;
	// and r8,r31,r8
	ctx.r8.u64 = r31.u64 & ctx.r8.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// and r11,r5,r11
	r11.u64 = ctx.r5.u64 & r11.u64;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r9,r3,r9
	ctx.r9.u64 = ctx.r3.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 | ctx.r9.u64;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// b 0x826fd534
	goto loc_826FD534;
loc_826FD4E0:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x826fd52c
	if (!cr6.eq) goto loc_826FD52C;
	// lhz r11,98(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 98);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lhz r11,102(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// lhz r11,96(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 96);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lhz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 100);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addze r10,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// sth r10,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r10.u16);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
	// b 0x826fd534
	goto loc_826FD534;
loc_826FD52C:
	// li r11,16384
	r11.s64 = 16384;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_826FD534:
	// li r31,4
	r31.s64 = 4;
loc_826FD538:
	// lbz r11,5(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 5);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826fd634
	if (!cr6.eq) goto loc_826FD634;
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// rlwinm r8,r4,1,15,15
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0x10000;
	// lwz r10,1432(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 1432);
	// rlwinm r7,r11,5,0,26
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r9,1436(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 1436);
	// rlwinm r6,r11,5,0,26
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r11,r8,r7
	r11.s64 = ctx.r7.s64 - ctx.r8.s64;
	// subf r9,r6,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r6.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r4,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r4.s64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// rlwinm r11,r11,0,0,16
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r11,r11,0,16,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fd630
	if (cr6.eq) goto loc_826FD630;
	// lwz r11,1104(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1104);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x826fd5a8
	if (!cr6.eq) goto loc_826FD5A8;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x826fd200
	sub_826FD200(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x826fd630
	goto loc_826FD630;
loc_826FD5A8:
	// lhz r10,18(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 18);
	// extsh r11,r4
	r11.s64 = ctx.r4.s16;
	// lhz r8,16(r28)
	ctx.r8.u64 = PPC_LOAD_U16(r28.u32 + 16);
	// srawi r9,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = ctx.r4.s32 >> 16;
	// rotlwi r10,r10,5
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 5);
	// lhz r7,50(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 50);
	// rotlwi r8,r8,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 5);
	// lhz r6,52(r30)
	ctx.r6.u64 = PPC_LOAD_U16(r30.u32 + 52);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// rlwinm r10,r10,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// rotlwi r7,r7,5
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 5);
	// rotlwi r6,r6,5
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r6.u32, 5);
	// rlwinm r8,r8,0,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFC;
	// cmpwi cr6,r10,-64
	cr6.compare<int32_t>(ctx.r10.s32, -64, xer);
	// bge cr6,0x826fd5f4
	if (!cr6.lt) goto loc_826FD5F4;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// b 0x826fd604
	goto loc_826FD604;
loc_826FD5F4:
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// ble cr6,0x826fd604
	if (!cr6.gt) goto loc_826FD604;
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_826FD604:
	// cmpwi cr6,r8,-64
	cr6.compare<int32_t>(ctx.r8.s32, -64, xer);
	// bge cr6,0x826fd618
	if (!cr6.lt) goto loc_826FD618;
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r9,r10,-64
	ctx.r9.s64 = ctx.r10.s64 + -64;
	// b 0x826fd628
	goto loc_826FD628;
loc_826FD618:
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// ble cr6,0x826fd628
	if (!cr6.gt) goto loc_826FD628;
	// subf r10,r8,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r8.s64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_826FD628:
	// rlwimi r11,r9,16,0,15
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (r11.u64 & 0xFFFFFFFF0000FFFF);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
loc_826FD630:
	// stw r4,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r4.u32);
loc_826FD634:
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x826fd538
	if (!cr6.eq) goto loc_826FD538;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_826FD650"))) PPC_WEAK_FUNC(sub_826FD650);
PPC_FUNC_IMPL(__imp__sub_826FD650) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-1872(r1)
	ea = -1872 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lwz r11,21556(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21556);
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// addi r4,r3,21712
	ctx.r4.s64 = ctx.r3.s64 + 21712;
	// li r18,0
	r18.s64 = 0;
	// lwz r16,1248(r31)
	r16.u64 = PPC_LOAD_U32(r31.u32 + 1248);
	// stw r11,20(r29)
	PPC_STORE_U32(r29.u32 + 20, r11.u32);
	// lwz r11,21568(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21568);
	// stw r4,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r4.u32);
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// stw r11,24(r29)
	PPC_STORE_U32(r29.u32 + 24, r11.u32);
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lwz r3,3360(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 3360);
	// bl 0x8271d5f8
	sub_8271D5F8(ctx, base);
	// stw r18,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r18.u32);
	// mr r11,r18
	r11.u64 = r18.u64;
	// stw r18,4(r29)
	PPC_STORE_U32(r29.u32 + 4, r18.u32);
	// mr r10,r18
	ctx.r10.u64 = r18.u64;
	// sth r18,16(r29)
	PPC_STORE_U16(r29.u32 + 16, r18.u16);
	// mr r15,r18
	r15.u64 = r18.u64;
	// sth r18,18(r29)
	PPC_STORE_U16(r29.u32 + 18, r18.u16);
	// lhz r9,52(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 52);
	// lhz r20,74(r31)
	r20.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// lhz r19,76(r31)
	r19.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// rlwinm r9,r9,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// lhz r8,50(r31)
	ctx.r8.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// rlwinm r14,r8,31,1,31
	r14.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r20,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r20.u32);
	// stw r9,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r9.u32);
	// stw r19,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r19.u32);
	// beq cr6,0x826fe740
	if (cr6.eq) goto loc_826FE740;
loc_826FD6E4:
	// mr r17,r18
	r17.u64 = r18.u64;
	// stw r11,8(r29)
	PPC_STORE_U32(r29.u32 + 8, r11.u32);
	// cmplwi cr6,r14,0
	cr6.compare<uint32_t>(r14.u32, 0, xer);
	// stw r10,12(r29)
	PPC_STORE_U32(r29.u32 + 12, ctx.r10.u32);
	// sth r18,18(r29)
	PPC_STORE_U16(r29.u32 + 18, r18.u16);
	// beq cr6,0x826fe6fc
	if (cr6.eq) goto loc_826FE6FC;
loc_826FD6FC:
	// clrlwi r10,r17,29
	ctx.r10.u64 = r17.u32 & 0x7;
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// clrlwi r9,r17,28
	ctx.r9.u64 = r17.u32 & 0xF;
	// ld r25,0(r16)
	r25.u64 = PPC_LOAD_U64(r16.u32 + 0);
	// addi r8,r10,556
	ctx.r8.s64 = ctx.r10.s64 + 556;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// addi r9,r9,564
	ctx.r9.s64 = ctx.r9.s64 + 564;
	// lwz r26,4(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rldicl r6,r25,16,48
	ctx.r6.u64 = __builtin_rotateleft64(r25.u64, 16) & 0xFFFF;
	// addi r16,r16,8
	r16.s64 = r16.s64 + 8;
	// lhzx r9,r7,r31
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r7.u32 + r31.u32);
	// rlwinm r8,r20,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 1) & 0xFFFFFFFE;
	// clrlwi r24,r6,26
	r24.u64 = ctx.r6.u32 & 0x3F;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhzx r7,r10,r31
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + r31.u32);
	// rlwinm r10,r9,6,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 6) & 0xFFFFFFC0;
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// rldicl r7,r25,9,55
	ctx.r7.u64 = __builtin_rotateleft64(r25.u64, 9) & 0x1FF;
	// rlwinm r9,r9,6,0,25
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 6) & 0xFFFFFFC0;
	// clrlwi r23,r7,31
	r23.u64 = ctx.r7.u32 & 0x1;
	// dcbt r10,r11
	// add r7,r10,r20
	ctx.r7.u64 = ctx.r10.u64 + r20.u64;
	// dcbt r7,r11
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// dcbt r10,r11
	// add r10,r10,r20
	ctx.r10.u64 = ctx.r10.u64 + r20.u64;
	// dcbt r10,r11
	// lwz r11,12(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// lwz r10,312(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 312);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// dcbt r9,r10
	// lwz r10,316(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 316);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// dcbt r9,r11
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// bne cr6,0x826fdaf4
	if (!cr6.eq) goto loc_826FDAF4;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
	// srawi r11,r10,14
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFF) != 0);
	r11.s64 = ctx.r10.s32 >> 14;
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
	// xor r11,r9,r11
	r11.u64 = ctx.r9.u64 ^ r11.u64;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// stb r11,5(r26)
	PPC_STORE_U8(r26.u32 + 5, r11.u8);
	// addi r11,r26,8
	r11.s64 = r26.s64 + 8;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// srawi r7,r9,14
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 14;
	// srawi r5,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 1;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// xor r7,r5,r7
	ctx.r7.u64 = ctx.r5.u64 ^ ctx.r7.u64;
	// clrlwi r7,r7,31
	ctx.r7.u64 = ctx.r7.u32 & 0x1;
	// add r5,r7,r6
	ctx.r5.u64 = ctx.r7.u64 + ctx.r6.u64;
	// stb r7,5(r11)
	PPC_STORE_U8(r11.u32 + 5, ctx.r7.u8);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lhz r7,50(r31)
	ctx.r7.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// rotlwi r7,r7,2
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 2);
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lwz r8,-4(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + -4);
	// srawi r6,r8,14
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFF) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 14;
	// srawi r4,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r6.s32 >> 1;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// xor r6,r4,r6
	ctx.r6.u64 = ctx.r4.u64 ^ ctx.r6.u64;
	// clrlwi r6,r6,31
	ctx.r6.u64 = ctx.r6.u32 & 0x1;
	// add r5,r6,r5
	ctx.r5.u64 = ctx.r6.u64 + ctx.r5.u64;
	// stb r6,5(r11)
	PPC_STORE_U8(r11.u32 + 5, ctx.r6.u8);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// addi r26,r11,-24
	r26.s64 = r11.s64 + -24;
	// srawi r6,r7,14
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 14;
	// srawi r4,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r6.s32 >> 1;
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// xor r6,r4,r6
	ctx.r6.u64 = ctx.r4.u64 ^ ctx.r6.u64;
	// clrlwi r6,r6,31
	ctx.r6.u64 = ctx.r6.u32 & 0x1;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// add. r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// stb r4,5(r11)
	PPC_STORE_U8(r11.u32 + 5, ctx.r4.u8);
	// bne 0x826fdab0
	if (!cr0.eq) goto loc_826FDAB0;
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// extsh r3,r9
	ctx.r3.s64 = ctx.r9.s16;
	// extsh r30,r8
	r30.s64 = ctx.r8.s16;
	// extsh r4,r7
	ctx.r4.s64 = ctx.r7.s16;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// ble cr6,0x826fd878
	if (!cr6.gt) goto loc_826FD878;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// b 0x826fd880
	goto loc_826FD880;
loc_826FD878:
	// bge cr6,0x826fd880
	if (!cr6.lt) goto loc_826FD880;
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
loc_826FD880:
	// cmpw cr6,r30,r5
	cr6.compare<int32_t>(r30.s32, ctx.r5.s32, xer);
	// ble cr6,0x826fd890
	if (!cr6.gt) goto loc_826FD890;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// b 0x826fd89c
	goto loc_826FD89C;
loc_826FD890:
	// cmpw cr6,r30,r6
	cr6.compare<int32_t>(r30.s32, ctx.r6.s32, xer);
	// bge cr6,0x826fd89c
	if (!cr6.lt) goto loc_826FD89C;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
loc_826FD89C:
	// cmpw cr6,r4,r5
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r5.s32, xer);
	// ble cr6,0x826fd8ac
	if (!cr6.gt) goto loc_826FD8AC;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// b 0x826fd8b8
	goto loc_826FD8B8;
loc_826FD8AC:
	// cmpw cr6,r4,r6
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r6.s32, xer);
	// bge cr6,0x826fd8b8
	if (!cr6.lt) goto loc_826FD8B8;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
loc_826FD8B8:
	// subf r4,r6,r4
	ctx.r4.s64 = ctx.r4.s64 - ctx.r6.s64;
	// subf r4,r5,r4
	ctx.r4.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r4,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r4.s64 = temp.s64;
	// srawi r11,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	r11.s64 = ctx.r10.s32 >> 16;
	// srawi r3,r9,16
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFF) != 0);
	ctx.r3.s64 = ctx.r9.s32 >> 16;
	// srawi r30,r8,16
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFF) != 0);
	r30.s64 = ctx.r8.s32 >> 16;
	// srawi r7,r7,16
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 16;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// ble cr6,0x826fd8fc
	if (!cr6.gt) goto loc_826FD8FC;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// b 0x826fd904
	goto loc_826FD904;
loc_826FD8FC:
	// bge cr6,0x826fd904
	if (!cr6.lt) goto loc_826FD904;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
loc_826FD904:
	// cmpw cr6,r30,r8
	cr6.compare<int32_t>(r30.s32, ctx.r8.s32, xer);
	// ble cr6,0x826fd914
	if (!cr6.gt) goto loc_826FD914;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// b 0x826fd920
	goto loc_826FD920;
loc_826FD914:
	// cmpw cr6,r30,r9
	cr6.compare<int32_t>(r30.s32, ctx.r9.s32, xer);
	// bge cr6,0x826fd920
	if (!cr6.lt) goto loc_826FD920;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
loc_826FD920:
	// cmpw cr6,r7,r8
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r8.s32, xer);
	// ble cr6,0x826fd930
	if (!cr6.gt) goto loc_826FD930;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// b 0x826fd93c
	goto loc_826FD93C;
loc_826FD930:
	// cmpw cr6,r7,r9
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r9.s32, xer);
	// bge cr6,0x826fd93c
	if (!cr6.lt) goto loc_826FD93C;
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
loc_826FD93C:
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// lwz r10,16(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// rlwimi r6,r9,16,0,15
	ctx.r6.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r6.u64 & 0xFFFFFFFF0000FFFF);
	// subf r7,r8,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r8.s64;
	// rlwimi r5,r8,16,0,15
	ctx.r5.u64 = (__builtin_rotateleft32(ctx.r8.u32, 16) & 0xFFFF0000) | (ctx.r5.u64 & 0xFFFFFFFF0000FFFF);
	// lwz r8,1436(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1436);
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// add r9,r7,r3
	ctx.r9.u64 = ctx.r7.u64 + ctx.r3.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,1432(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1432);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// rlwimi r4,r11,16,0,15
	ctx.r4.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r4.u64 & 0xFFFFFFFF0000FFFF);
	// addi r11,r10,-2048
	r11.s64 = ctx.r10.s64 + -2048;
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// rlwinm r11,r11,0,0,16
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r11,r11,0,16,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fdc1c
	if (cr6.eq) goto loc_826FDC1C;
	// li r27,3
	r27.s64 = 3;
	// addi r28,r26,24
	r28.s64 = r26.s64 + 24;
loc_826FD9AC:
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// lwz r11,16(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// rlwinm r8,r4,1,15,15
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0x10000;
	// lwz r9,1436(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1436);
	// rlwinm r7,r11,5,0,26
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r10,1432(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1432);
	// rlwinm r6,r11,5,0,26
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r11,r8,r7
	r11.s64 = ctx.r7.s64 - ctx.r8.s64;
	// subf r9,r6,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r6.s64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// subf r9,r4,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r4.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// rlwinm r11,r11,0,0,16
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r11,r11,0,16,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fda98
	if (cr6.eq) goto loc_826FDA98;
	// lwz r11,1104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1104);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x826fda10
	if (!cr6.eq) goto loc_826FDA10;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826fd200
	sub_826FD200(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x826fda98
	goto loc_826FDA98;
loc_826FDA10:
	// lhz r10,18(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 18);
	// extsh r11,r4
	r11.s64 = ctx.r4.s16;
	// lhz r8,16(r29)
	ctx.r8.u64 = PPC_LOAD_U16(r29.u32 + 16);
	// srawi r9,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = ctx.r4.s32 >> 16;
	// rotlwi r10,r10,5
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 5);
	// lhz r7,50(r31)
	ctx.r7.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// rotlwi r8,r8,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 5);
	// lhz r6,52(r31)
	ctx.r6.u64 = PPC_LOAD_U16(r31.u32 + 52);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// rlwinm r10,r10,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// rotlwi r7,r7,5
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 5);
	// rotlwi r6,r6,5
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r6.u32, 5);
	// rlwinm r8,r8,0,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFC;
	// cmpwi cr6,r10,-64
	cr6.compare<int32_t>(ctx.r10.s32, -64, xer);
	// bge cr6,0x826fda5c
	if (!cr6.lt) goto loc_826FDA5C;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// b 0x826fda6c
	goto loc_826FDA6C;
loc_826FDA5C:
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// ble cr6,0x826fda6c
	if (!cr6.gt) goto loc_826FDA6C;
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_826FDA6C:
	// cmpwi cr6,r8,-64
	cr6.compare<int32_t>(ctx.r8.s32, -64, xer);
	// bge cr6,0x826fda80
	if (!cr6.lt) goto loc_826FDA80;
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r9,r10,-64
	ctx.r9.s64 = ctx.r10.s64 + -64;
	// b 0x826fda90
	goto loc_826FDA90;
loc_826FDA80:
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// ble cr6,0x826fda90
	if (!cr6.gt) goto loc_826FDA90;
	// subf r10,r8,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r8.s64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_826FDA90:
	// rlwimi r11,r9,16,0,15
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (r11.u64 & 0xFFFFFFFF0000FFFF);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
loc_826FDA98:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// stw r4,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r4.u32);
	// addi r28,r28,-8
	r28.s64 = r28.s64 + -8;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bge cr6,0x826fd9ac
	if (!cr6.lt) goto loc_826FD9AC;
	// b 0x826fdc1c
	goto loc_826FDC1C;
loc_826FDAB0:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826fd3b0
	sub_826FD3B0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,16384
	cr6.compare<int32_t>(r30.s32, 16384, xer);
	// bne cr6,0x826fdc1c
	if (!cr6.eq) goto loc_826FDC1C;
	// li r11,1
	r11.s64 = 1;
loc_826FDACC:
	// stb r11,37(r26)
	PPC_STORE_U8(r26.u32 + 37, r11.u8);
	// stb r11,45(r26)
	PPC_STORE_U8(r26.u32 + 45, r11.u8);
	// lbz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826fdf08
	if (cr6.eq) goto loc_826FDF08;
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r9,216(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r18,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, r18.u32);
	// b 0x826fdf08
	goto loc_826FDF08;
loc_826FDAF4:
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r30,16384
	cr6.compare<int32_t>(r30.s32, 16384, xer);
	// stw r30,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r30.u32);
	// bne cr6,0x826fdb1c
	if (!cr6.eq) goto loc_826FDB1C;
	// li r11,1
	r11.s64 = 1;
	// stb r11,29(r26)
	PPC_STORE_U8(r26.u32 + 29, r11.u8);
	// stb r11,21(r26)
	PPC_STORE_U8(r26.u32 + 21, r11.u8);
	// stb r11,13(r26)
	PPC_STORE_U8(r26.u32 + 13, r11.u8);
	// stb r11,5(r26)
	PPC_STORE_U8(r26.u32 + 5, r11.u8);
	// b 0x826fdacc
	goto loc_826FDACC;
loc_826FDB1C:
	// lwz r10,16(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// rlwinm r8,r30,1,15,15
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0x10000;
	// lwz r9,1432(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1432);
	// mr r11,r30
	r11.u64 = r30.u64;
	// rlwinm r6,r10,5,0,26
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r7,1436(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1436);
	// rlwinm r5,r10,5,0,26
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r10,r8,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r8.s64;
	// subf r8,r5,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r5.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r9,r30,r8
	ctx.r9.s64 = ctx.r8.s64 - r30.s64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// rlwinm r10,r10,0,0,16
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r10,r10,0,16,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826fdc08
	if (cr6.eq) goto loc_826FDC08;
	// lwz r11,1104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1104);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x826fdb84
	if (!cr6.eq) goto loc_826FDB84;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826fd200
	sub_826FD200(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// b 0x826fdc08
	goto loc_826FDC08;
loc_826FDB84:
	// lhz r10,18(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 18);
	// extsh r11,r30
	r11.s64 = r30.s16;
	// lhz r8,16(r29)
	ctx.r8.u64 = PPC_LOAD_U16(r29.u32 + 16);
	// srawi r9,r30,16
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = r30.s32 >> 16;
	// rotlwi r10,r10,5
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 5);
	// lhz r7,50(r31)
	ctx.r7.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// rotlwi r8,r8,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 5);
	// lhz r6,52(r31)
	ctx.r6.u64 = PPC_LOAD_U16(r31.u32 + 52);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// rlwinm r10,r10,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// rotlwi r7,r7,5
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 5);
	// rotlwi r6,r6,5
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r6.u32, 5);
	// rlwinm r8,r8,0,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFC;
	// cmpwi cr6,r10,-64
	cr6.compare<int32_t>(ctx.r10.s32, -64, xer);
	// bge cr6,0x826fdbd0
	if (!cr6.lt) goto loc_826FDBD0;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// b 0x826fdbe0
	goto loc_826FDBE0;
loc_826FDBD0:
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// ble cr6,0x826fdbe0
	if (!cr6.gt) goto loc_826FDBE0;
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_826FDBE0:
	// cmpwi cr6,r8,-64
	cr6.compare<int32_t>(ctx.r8.s32, -64, xer);
	// bge cr6,0x826fdbf4
	if (!cr6.lt) goto loc_826FDBF4;
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r9,r10,-64
	ctx.r9.s64 = ctx.r10.s64 + -64;
	// b 0x826fdc04
	goto loc_826FDC04;
loc_826FDBF4:
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// ble cr6,0x826fdc04
	if (!cr6.gt) goto loc_826FDC04;
	// subf r10,r8,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r8.s64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_826FDC04:
	// rlwimi r11,r9,16,0,15
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFFFF0000) | (r11.u64 & 0xFFFFFFFF0000FFFF);
loc_826FDC08:
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// stb r18,29(r26)
	PPC_STORE_U8(r26.u32 + 29, r18.u8);
	// stb r18,21(r26)
	PPC_STORE_U8(r26.u32 + 21, r18.u8);
	// stb r18,13(r26)
	PPC_STORE_U8(r26.u32 + 13, r18.u8);
	// stb r18,5(r26)
	PPC_STORE_U8(r26.u32 + 5, r18.u8);
loc_826FDC1C:
	// lbz r9,31(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 31);
	// srawi r11,r30,16
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFFF) != 0);
	r11.s64 = r30.s32 >> 16;
	// addi r10,r31,1420
	ctx.r10.s64 = r31.s64 + 1420;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// clrlwi r9,r11,30
	ctx.r9.u64 = r11.u32 & 0x3;
	// extsh r8,r30
	ctx.r8.s64 = r30.s16;
	// lbzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// clrlwi r11,r8,30
	r11.u64 = ctx.r8.u32 & 0x3;
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// srawi r6,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r6.s64 = r11.s32 >> 1;
	// srawi r11,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r11.s64 = ctx.r9.s32 >> 1;
	// beq cr6,0x826fdc84
	if (cr6.eq) goto loc_826FDC84;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// clrlwi r10,r6,31
	ctx.r10.u64 = ctx.r6.u32 & 0x1;
	// ble cr6,0x826fdc68
	if (!cr6.gt) goto loc_826FDC68;
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// b 0x826fdc6c
	goto loc_826FDC6C;
loc_826FDC68:
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
loc_826FDC6C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// ble cr6,0x826fdc80
	if (!cr6.gt) goto loc_826FDC80;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// b 0x826fdc84
	goto loc_826FDC84;
loc_826FDC80:
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_826FDC84:
	// lbz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 32);
	// rlwimi r6,r11,16,0,15
	ctx.r6.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r6.u64 & 0xFFFFFFFF0000FFFF);
	// lwz r28,4(r29)
	r28.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826fdd8c
	if (cr6.eq) goto loc_826FDD8C;
	// lwz r11,1104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1104);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x826fdcb4
	if (!cr6.eq) goto loc_826FDCB4;
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// rlwinm r10,r28,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r30,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r30.u32);
	// b 0x826fdd8c
	goto loc_826FDD8C;
loc_826FDCB4:
	// lwz r11,16(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// rlwinm r10,r30,1,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0x10000;
	// lwz r7,1424(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1424);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r5,1428(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1428);
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// rlwinm r11,r11,0,0,16
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r11,r11,0,16,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fdd80
	if (cr6.eq) goto loc_826FDD80;
	// lhz r11,18(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 18);
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r8,16(r29)
	ctx.r8.u64 = PPC_LOAD_U16(r29.u32 + 16);
	// srawi r10,r30,15
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFF) != 0);
	ctx.r10.s64 = r30.s32 >> 15;
	// rotlwi r11,r11,5
	r11.u64 = __builtin_rotateleft32(r11.u32, 5);
	// lhz r7,50(r31)
	ctx.r7.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// rotlwi r8,r8,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 5);
	// lhz r5,52(r31)
	ctx.r5.u64 = PPC_LOAD_U16(r31.u32 + 52);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r11,r11,0,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF8;
	// rotlwi r7,r7,5
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 5);
	// rotlwi r5,r5,5
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 5);
	// rlwinm r8,r8,0,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFF8;
	// cmpwi cr6,r11,-64
	cr6.compare<int32_t>(r11.s32, -64, xer);
	// bge cr6,0x826fdd44
	if (!cr6.lt) goto loc_826FDD44;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// addi r9,r11,-64
	ctx.r9.s64 = r11.s64 + -64;
	// b 0x826fdd54
	goto loc_826FDD54;
loc_826FDD44:
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// ble cr6,0x826fdd54
	if (!cr6.gt) goto loc_826FDD54;
	// subf r11,r11,r7
	r11.s64 = ctx.r7.s64 - r11.s64;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
loc_826FDD54:
	// cmpwi cr6,r8,-64
	cr6.compare<int32_t>(ctx.r8.s32, -64, xer);
	// bge cr6,0x826fdd68
	if (!cr6.lt) goto loc_826FDD68;
	// subf r11,r8,r10
	r11.s64 = ctx.r10.s64 - ctx.r8.s64;
	// addi r10,r11,-64
	ctx.r10.s64 = r11.s64 + -64;
	// b 0x826fdd78
	goto loc_826FDD78;
loc_826FDD68:
	// cmpw cr6,r8,r5
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, xer);
	// ble cr6,0x826fdd78
	if (!cr6.gt) goto loc_826FDD78;
	// subf r11,r8,r5
	r11.s64 = ctx.r5.s64 - ctx.r8.s64;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_826FDD78:
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// rlwimi r9,r10,15,0,15
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 15) & 0xFFFF0000) | (ctx.r9.u64 & 0xFFFFFFFF0000FFFF);
loc_826FDD80:
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// rlwinm r10,r28,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, ctx.r9.u32);
loc_826FDD8C:
	// lwz r11,1104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1104);
	// rlwinm r10,r6,1,15,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x10000;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x826fde58
	if (!cr6.eq) goto loc_826FDE58;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// rlwinm r9,r28,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// stwx r6,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r6.u32);
	// lwz r11,16(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// lwz r8,1440(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1440);
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r7,1444(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1444);
	// rlwinm r5,r11,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r8,r5,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r5.s64;
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// rlwinm r9,r9,0,0,16
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r9,r9,0,16,0
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826fdef4
	if (cr6.eq) goto loc_826FDEF4;
	// lwz r9,1104(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1104);
	// cmpwi cr6,r9,7
	cr6.compare<int32_t>(ctx.r9.s32, 7, xer);
	// bne cr6,0x826fde08
	if (!cr6.eq) goto loc_826FDE08;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826fd2e0
	sub_826FD2E0(ctx, base);
	// b 0x826fdef4
	goto loc_826FDEF4;
loc_826FDE08:
	// lwz r9,1424(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1424);
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r8,1428(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1428);
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// rlwinm r11,r11,0,0,16
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r11,r11,0,16,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fdef4
	if (cr6.eq) goto loc_826FDEF4;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82612030
	sub_82612030(ctx, base);
	// b 0x826fdef4
	goto loc_826FDEF4;
loc_826FDE58:
	// lwz r11,16(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// lwz r7,1440(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1440);
	// rlwinm r8,r11,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r5,1444(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1444);
	// rlwinm r4,r11,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r4,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r4.s64;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// rlwinm r8,r8,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r8,r8,0,16,0
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x826fdee4
	if (cr6.eq) goto loc_826FDEE4;
	// lwz r8,1424(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1424);
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r7,1428(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1428);
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// subf r11,r11,r7
	r11.s64 = ctx.r7.s64 - r11.s64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// rlwinm r11,r11,0,0,16
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// rlwinm r11,r11,0,16,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFF8000FFFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826fdee4
	if (cr6.eq) goto loc_826FDEE4;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82612030
	sub_82612030(ctx, base);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
loc_826FDEE4:
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// rlwinm r10,r28,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// stwx r9,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, ctx.r9.u32);
loc_826FDEF4:
	// mr r11,r18
	r11.u64 = r18.u64;
	// stw r3,32(r26)
	PPC_STORE_U32(r26.u32 + 32, ctx.r3.u32);
	// stw r3,40(r26)
	PPC_STORE_U32(r26.u32 + 40, ctx.r3.u32);
	// stb r18,37(r26)
	PPC_STORE_U8(r26.u32 + 37, r18.u8);
	// stb r18,45(r26)
	PPC_STORE_U8(r26.u32 + 45, r18.u8);
loc_826FDF08:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x826fe178
	if (cr6.eq) goto loc_826FE178;
	// clrlwi r10,r24,24
	ctx.r10.u64 = r24.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826fe178
	if (!cr6.eq) goto loc_826FE178;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826fe178
	if (!cr6.eq) goto loc_826FE178;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826fdf80
	if (!cr6.eq) goto loc_826FDF80;
	// lwz r3,8(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
	// lwz r6,296(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// lwz r11,12(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// lwz r8,316(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 316);
	// add r6,r3,r6
	ctx.r6.u64 = ctx.r3.u64 + ctx.r6.u64;
	// lwz r7,312(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 312);
	// lwz r5,412(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 412);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lwz r4,408(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 408);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// lwz r30,392(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// add r4,r4,r11
	ctx.r4.u64 = ctx.r4.u64 + r11.u64;
	// add r3,r30,r3
	ctx.r3.u64 = r30.u64 + ctx.r3.u64;
	// bl 0x8266d398
	sub_8266D398(ctx, base);
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// b 0x826fe6ac
	goto loc_826FE6AC;
loc_826FDF80:
	// addi r21,r31,48
	r21.s64 = r31.s64 + 48;
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r8,2(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// lhz r4,90(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 90);
	// extsh r10,r8
	ctx.r10.s64 = ctx.r8.s16;
	// lwz r7,296(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// clrlwi r28,r11,30
	r28.u64 = r11.u32 & 0x3;
	// lwz r9,8(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lbz r8,0(r21)
	ctx.r8.u64 = PPC_LOAD_U8(r21.u32 + 0);
	// clrlwi r30,r10,30
	r30.u64 = ctx.r10.u32 & 0x3;
	// lwz r5,1100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// srawi r8,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r8.s64 = r11.s32 >> 2;
	// srawi r6,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 2;
	// mullw r8,r8,r4
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// add r11,r8,r7
	r11.u64 = ctx.r8.u64 + ctx.r7.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// add r27,r11,r9
	r27.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// bne cr6,0x826fe024
	if (!cr6.eq) goto loc_826FE024;
	// addi r11,r11,134
	r11.s64 = r11.s64 + 134;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826fe038
	if (cr6.eq) goto loc_826FE038;
	// li r9,1
	ctx.r9.s64 = 1;
	// lbz r8,35(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 35);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r5,1100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lhz r4,90(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 90);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8266c7a8
	sub_8266C7A8(ctx, base);
	// b 0x826fe038
	goto loc_826FE038;
loc_826FE024:
	// addi r11,r11,150
	r11.s64 = r11.s64 + 150;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FE038:
	// addi r11,r31,1448
	r11.s64 = r31.s64 + 1448;
	// mr r22,r18
	r22.u64 = r18.u64;
	// mr r24,r18
	r24.u64 = r18.u64;
	// addi r23,r31,392
	r23.s64 = r31.s64 + 392;
	// stw r11,532(r31)
	PPC_STORE_U32(r31.u32 + 532, r11.u32);
loc_826FE04C:
	// srawi r30,r22,2
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3) != 0);
	r30.s64 = r22.s32 >> 2;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x826fe124
	if (cr6.eq) goto loc_826FE124;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r10,r30,45
	ctx.r10.s64 = r30.s64 + 45;
	// addi r7,r30,2
	ctx.r7.s64 = r30.s64 + 2;
	// lbzx r8,r21,r30
	ctx.r8.u64 = PPC_LOAD_U8(r21.u32 + r30.u32);
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// lwz r9,-96(r23)
	ctx.r9.u64 = PPC_LOAD_U32(r23.u32 + -96);
	// rlwinm r25,r10,1,0,30
	r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r5,1100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// lhzx r4,r25,r31
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + r31.u32);
	// extsh r10,r6
	ctx.r10.s64 = ctx.r6.s16;
	// lwzx r7,r7,r29
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + r29.u32);
	// srawi r6,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r6.s64 = r11.s32 >> 2;
	// clrlwi r27,r11,30
	r27.u64 = r11.u32 & 0x3;
	// mullw r11,r6,r4
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r4.s32);
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// srawi r8,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 2;
	// clrlwi r28,r10,30
	r28.u64 = ctx.r10.u32 & 0x3;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// add r26,r11,r9
	r26.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// bne cr6,0x826fe110
	if (!cr6.eq) goto loc_826FE110;
	// addi r11,r11,134
	r11.s64 = r11.s64 + 134;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826fe124
	if (cr6.eq) goto loc_826FE124;
	// li r9,0
	ctx.r9.s64 = 0;
	// lbz r8,35(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 35);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// lwz r5,1100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// lhzx r4,r25,r31
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + r31.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8266c7a8
	sub_8266C7A8(ctx, base);
	// b 0x826fe124
	goto loc_826FE124;
loc_826FE110:
	// addi r11,r11,150
	r11.s64 = r11.s64 + 150;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FE124:
	// lwz r11,532(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 532);
	// addi r10,r30,2
	ctx.r10.s64 = r30.s64 + 2;
	// addi r9,r30,45
	ctx.r9.s64 = r30.s64 + 45;
	// lwz r8,1100(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// rlwinm r6,r9,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r9,r11,r22
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lwzx r11,r7,r29
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + r29.u32);
	// rotlwi r9,r9,2
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// lhzx r5,r6,r31
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r6.u32 + r31.u32);
	// add r4,r9,r8
	ctx.r4.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x82706640
	sub_82706640(ctx, base);
	// addi r24,r24,8
	r24.s64 = r24.s64 + 8;
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// addi r23,r23,4
	r23.s64 = r23.s64 + 4;
	// cmpwi cr6,r24,48
	cr6.compare<int32_t>(r24.s32, 48, xer);
	// blt cr6,0x826fe04c
	if (cr6.lt) goto loc_826FE04C;
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// b 0x826fe6ac
	goto loc_826FE6AC;
loc_826FE178:
	// addi r8,r31,1454
	ctx.r8.s64 = r31.s64 + 1454;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rldicl r10,r25,8,56
	ctx.r10.u64 = __builtin_rotateleft64(r25.u64, 8) & 0xFF;
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// addi r26,r11,5
	r26.s64 = r11.s64 + 5;
	// clrlwi r10,r10,26
	ctx.r10.u64 = ctx.r10.u32 & 0x3F;
	// clrlwi r19,r24,24
	r19.u64 = r24.u32 & 0xFF;
	// stw r8,532(r31)
	PPC_STORE_U32(r31.u32 + 532, ctx.r8.u32);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r20,r25
	r20.u64 = r25.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r21,r10,r9
	r21.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lbz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r21,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r21.u32);
	// bne cr6,0x826fe284
	if (!cr6.eq) goto loc_826FE284;
	// cmpwi cr6,r23,1
	cr6.compare<int32_t>(r23.s32, 1, xer);
	// bne cr6,0x826fe284
	if (!cr6.eq) goto loc_826FE284;
	// lbz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 48);
	// lhz r8,2(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r4,90(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 90);
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// lwz r7,296(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// extsh r10,r8
	ctx.r10.s64 = ctx.r8.s16;
	// lwz r9,8(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// srawi r8,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r8.s64 = r11.s32 >> 2;
	// lwz r5,1100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// srawi r6,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 2;
	// mullw r8,r8,r4
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// clrlwi r28,r11,30
	r28.u64 = r11.u32 & 0x3;
	// add r11,r8,r7
	r11.u64 = ctx.r8.u64 + ctx.r7.u64;
	// clrlwi r30,r10,30
	r30.u64 = ctx.r10.u32 & 0x3;
	// add r27,r11,r9
	r27.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// bne cr6,0x826fe264
	if (!cr6.eq) goto loc_826FE264;
	// addi r11,r11,134
	r11.s64 = r11.s64 + 134;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826fe278
	if (cr6.eq) goto loc_826FE278;
	// li r9,1
	ctx.r9.s64 = 1;
	// lbz r8,35(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 35);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r5,1100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lhz r4,90(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 90);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8266c7a8
	sub_8266C7A8(ctx, base);
	// b 0x826fe278
	goto loc_826FE278;
loc_826FE264:
	// addi r11,r11,150
	r11.s64 = r11.s64 + 150;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FE278:
	// addi r11,r31,1448
	r11.s64 = r31.s64 + 1448;
	// li r23,2
	r23.s64 = 2;
	// stw r11,532(r31)
	PPC_STORE_U32(r31.u32 + 532, r11.u32);
loc_826FE284:
	// mr r30,r18
	r30.u64 = r18.u64;
	// mr r22,r26
	r22.u64 = r26.u64;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
loc_826FE290:
	// srawi r11,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	r11.s64 = r30.s32 >> 2;
	// lbz r10,0(r22)
	ctx.r10.u64 = PPC_LOAD_U8(r22.u32 + 0);
	// addi r9,r30,98
	ctx.r9.s64 = r30.s64 + 98;
	// mr r24,r11
	r24.u64 = r11.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r24,2
	ctx.r7.s64 = r24.s64 + 2;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r23,r24,r23
	r23.s64 = r23.s64 - r24.s64;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// rldicl r8,r20,20,44
	ctx.r8.u64 = __builtin_rotateleft64(r20.u64, 20) & 0xFFFFF;
	// addi r22,r22,8
	r22.s64 = r22.s64 + 8;
	// clrlwi r8,r8,29
	ctx.r8.u64 = ctx.r8.u32 & 0x7;
	// lwzx r10,r7,r29
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + r29.u32);
	// stw r23,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r23.u32);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// bne cr6,0x826fe68c
	if (!cr6.eq) goto loc_826FE68C;
	// clrlwi r10,r19,31
	ctx.r10.u64 = r19.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826fe580
	if (cr6.eq) goto loc_826FE580;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x826fe424
	if (!cr6.eq) goto loc_826FE424;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// addi r5,r31,932
	ctx.r5.s64 = r31.s64 + 932;
	// lwz r10,24(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// lwz r4,276(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 276);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// stw r18,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r18.u32);
	// stw r18,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r18.u32);
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// lbz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// lwz r11,260(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// stw r9,24(r29)
	PPC_STORE_U32(r29.u32 + 24, ctx.r9.u32);
	// lwz r10,260(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// stw r10,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r10.u32);
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// dcbzl r0,r10
	memset(base + ((ctx.r10.u32) & ~127), 0, 128);
	// lwz r7,128(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r21,164(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// cmplwi cr6,r7,128
	cr6.compare<uint32_t>(ctx.r7.u32, 128, xer);
	// blt cr6,0x826fe370
	if (cr6.lt) goto loc_826FE370;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82611f38
	sub_82611F38(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// b 0x826fe3ec
	goto loc_826FE3EC;
loc_826FE370:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// ble cr6,0x826fe3e8
	if (!cr6.gt) goto loc_826FE3E8;
	// lwz r3,140(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r30,156(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// lwz r28,148(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r8,132(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
loc_826FE390:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// mullw r10,r10,r3
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// add r27,r10,r30
	r27.u64 = ctx.r10.u64 + r30.u64;
	// rlwinm r10,r9,25,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 25) & 0x1;
	// clrlwi r9,r9,26
	ctx.r9.u64 = ctx.r9.u32 & 0x3F;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// xor r27,r27,r10
	r27.u64 = r27.u64 ^ ctx.r10.u64;
	// subf r10,r10,r27
	ctx.r10.s64 = r27.s64 - ctx.r10.s64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// rotlwi r26,r8,1
	r26.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lbzx r27,r8,r5
	r27.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r5.u32);
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// or r6,r27,r6
	ctx.r6.u64 = r27.u64 | ctx.r6.u64;
	// sthx r10,r26,r28
	PPC_STORE_U16(r26.u32 + r28.u32, ctx.r10.u16);
	// bne cr6,0x826fe390
	if (!cr6.eq) goto loc_826FE390;
loc_826FE3E8:
	// stw r11,20(r29)
	PPC_STORE_U32(r29.u32 + 20, r11.u32);
loc_826FE3EC:
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lwz r3,104(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// bne cr6,0x826fe410
	if (!cr6.eq) goto loc_826FE410;
	// bl 0x8261c9f8
	sub_8261C9F8(ctx, base);
	// lwz r23,120(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r24,92(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// b 0x826fe468
	goto loc_826FE468;
loc_826FE410:
	// bl 0x826ff808
	sub_826FF808(ctx, base);
	// lwz r23,120(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r24,92(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// b 0x826fe468
	goto loc_826FE468;
loc_826FE424:
	// rldicl r11,r20,24,40
	r11.u64 = __builtin_rotateleft64(r20.u64, 24) & 0xFFFFFF;
	// lwz r10,448(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// clrlwi r5,r11,28
	ctx.r5.u64 = r11.u32 & 0xF;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// add r11,r5,r31
	r11.u64 = ctx.r5.u64 + r31.u64;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// rlwinm r10,r8,0,29,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lbz r11,160(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 160);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r11,r11,117
	r11.s64 = r11.s64 + 117;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FE468:
	// cmpwi cr6,r23,2
	cr6.compare<int32_t>(r23.s32, 2, xer);
	// bge cr6,0x826fe550
	if (!cr6.lt) goto loc_826FE550;
	// srawi r11,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	r11.s64 = r30.s32 >> 2;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm r10,r30,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r8,r11,45
	ctx.r8.s64 = r11.s64 + 45;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r7,r11,2
	ctx.r7.s64 = r11.s64 + 2;
	// rlwinm r25,r8,1,0,30
	r25.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r30,74
	ctx.r8.s64 = r30.s64 + 74;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r5,r8,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r6,2(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// extsh r10,r9
	ctx.r10.s64 = ctx.r9.s16;
	// lhzx r4,r25,r31
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + r31.u32);
	// extsh r9,r6
	ctx.r9.s64 = ctx.r6.s16;
	// lbz r11,48(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 48);
	// srawi r6,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 2;
	// lwzx r8,r7,r29
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + r29.u32);
	// clrlwi r28,r9,30
	r28.u64 = ctx.r9.u32 & 0x3;
	// lwzx r7,r5,r31
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + r31.u32);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// mullw r11,r6,r4
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r4.s32);
	// lwz r5,1100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// srawi r8,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 2;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// clrlwi r27,r10,30
	r27.u64 = ctx.r10.u32 & 0x3;
	// add r26,r11,r8
	r26.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// bne cr6,0x826fe53c
	if (!cr6.eq) goto loc_826FE53C;
	// addi r11,r11,134
	r11.s64 = r11.s64 + 134;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826fe550
	if (cr6.eq) goto loc_826FE550;
	// li r9,0
	ctx.r9.s64 = 0;
	// lbz r8,35(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 35);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// lwz r5,1100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// lhzx r4,r25,r31
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + r31.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8266c7a8
	sub_8266C7A8(ctx, base);
	// b 0x826fe550
	goto loc_826FE550;
loc_826FE53C:
	// addi r11,r11,150
	r11.s64 = r11.s64 + 150;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FE550:
	// lwz r11,532(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 532);
	// addi r9,r24,45
	ctx.r9.s64 = r24.s64 + 45;
	// lwz r10,1100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r3,108(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lbzx r11,r30,r11
	r11.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// rotlwi r11,r11,2
	r11.u64 = __builtin_rotateleft32(r11.u32, 2);
	// lhzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + r31.u32);
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x82706700
	sub_82706700(ctx, base);
	// b 0x826fe68c
	goto loc_826FE68C;
loc_826FE580:
	// cmpwi cr6,r23,2
	cr6.compare<int32_t>(r23.s32, 2, xer);
	// bge cr6,0x826fe664
	if (!cr6.lt) goto loc_826FE664;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm r10,r30,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r8,r11,45
	ctx.r8.s64 = r11.s64 + 45;
	// lwz r5,1100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r25,r8,1,0,30
	r25.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r11,2
	ctx.r8.s64 = r11.s64 + 2;
	// addi r7,r30,74
	ctx.r7.s64 = r30.s64 + 74;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lhz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r6,2(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// extsh r10,r9
	ctx.r10.s64 = ctx.r9.s16;
	// lhzx r4,r25,r31
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + r31.u32);
	// extsh r9,r6
	ctx.r9.s64 = ctx.r6.s16;
	// lbz r11,48(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 48);
	// srawi r6,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 2;
	// lwzx r7,r7,r31
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + r31.u32);
	// clrlwi r28,r9,30
	r28.u64 = ctx.r9.u32 & 0x3;
	// lwzx r8,r8,r29
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r29.u32);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// mullw r11,r6,r4
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r4.s32);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// srawi r8,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 2;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// clrlwi r27,r10,30
	r27.u64 = ctx.r10.u32 & 0x3;
	// add r26,r11,r8
	r26.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// bne cr6,0x826fe650
	if (!cr6.eq) goto loc_826FE650;
	// addi r11,r11,134
	r11.s64 = r11.s64 + 134;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826fe664
	if (cr6.eq) goto loc_826FE664;
	// li r9,0
	ctx.r9.s64 = 0;
	// lbz r8,35(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 35);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// lwz r5,1100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// lhzx r4,r25,r31
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + r31.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8266c7a8
	sub_8266C7A8(ctx, base);
	// b 0x826fe664
	goto loc_826FE664;
loc_826FE650:
	// addi r11,r11,150
	r11.s64 = r11.s64 + 150;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FE664:
	// lwz r11,532(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 532);
	// addi r9,r24,45
	ctx.r9.s64 = r24.s64 + 45;
	// lwz r10,1100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1100);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r3,108(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lbzx r11,r30,r11
	r11.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// rotlwi r11,r11,2
	r11.u64 = __builtin_rotateleft32(r11.u32, 2);
	// lhzx r5,r9,r31
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + r31.u32);
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x82706640
	sub_82706640(ctx, base);
loc_826FE68C:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// rlwinm r19,r19,31,1,31
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 31) & 0x7FFFFFFF;
	// rldicr r20,r20,8,55
	r20.u64 = __builtin_rotateleft64(r20.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// cmpwi cr6,r30,6
	cr6.compare<int32_t>(r30.s32, 6, xer);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// blt cr6,0x826fe290
	if (cr6.lt) goto loc_826FE290;
	// lwz r20,136(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r19,144(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
loc_826FE6AC:
	// lhz r10,18(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 18);
	// addi r17,r17,1
	r17.s64 = r17.s64 + 1;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r8,r10,2
	ctx.r8.s64 = ctx.r10.s64 + 2;
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// addi r7,r11,2
	ctx.r7.s64 = r11.s64 + 2;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r11,12(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// sth r8,18(r29)
	PPC_STORE_U16(r29.u32 + 18, ctx.r8.u16);
	// cmplw cr6,r17,r14
	cr6.compare<uint32_t>(r17.u32, r14.u32, xer);
	// stw r7,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r7.u32);
	// stw r9,4(r29)
	PPC_STORE_U32(r29.u32 + 4, ctx.r9.u32);
	// stw r10,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r10.u32);
	// stw r11,12(r29)
	PPC_STORE_U32(r29.u32 + 12, r11.u32);
	// blt cr6,0x826fd6fc
	if (cr6.lt) goto loc_826FD6FC;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_826FE6FC:
	// lhz r9,16(r29)
	ctx.r9.u64 = PPC_LOAD_U16(r29.u32 + 16);
	// rlwinm r7,r20,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r8,0(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r15,r15,1
	r15.s64 = r15.s64 + 1;
	// addi r6,r9,2
	ctx.r6.s64 = ctx.r9.s64 + 2;
	// rlwinm r9,r19,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// sth r6,16(r29)
	PPC_STORE_U16(r29.u32 + 16, ctx.r6.u16);
	// lhz r9,50(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// stw r9,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r9.u32);
	// lwz r9,152(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// cmplw cr6,r15,r9
	cr6.compare<uint32_t>(r15.u32, ctx.r9.u32, xer);
	// blt cr6,0x826fd6e4
	if (cr6.lt) goto loc_826FD6E4;
loc_826FE740:
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// lwz r3,160(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// bl 0x8271d940
	sub_8271D940(ctx, base);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r1,r1,1872
	ctx.r1.s64 = ctx.r1.s64 + 1872;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826FE758"))) PPC_WEAK_FUNC(sub_826FE758);
PPC_FUNC_IMPL(__imp__sub_826FE758) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r21,r5
	r21.u64 = ctx.r5.u64;
	// li r22,0
	r22.s64 = 0;
	// addi r4,r20,21712
	ctx.r4.s64 = r20.s64 + 21712;
	// lwz r10,3720(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 3720);
	// lwz r9,220(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 220);
	// lwz r6,21556(r20)
	ctx.r6.u64 = PPC_LOAD_U32(r20.u32 + 21556);
	// lwz r11,224(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 224);
	// add r30,r10,r9
	r30.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r7,3724(r20)
	ctx.r7.u64 = PPC_LOAD_U32(r20.u32 + 3724);
	// lwz r10,3740(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 3740);
	// lwz r8,3728(r20)
	ctx.r8.u64 = PPC_LOAD_U32(r20.u32 + 3728);
	// add r31,r7,r11
	r31.u64 = ctx.r7.u64 + r11.u64;
	// lwz r9,3736(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 3736);
	// add r25,r10,r11
	r25.u64 = ctx.r10.u64 + r11.u64;
	// lwz r23,268(r20)
	r23.u64 = PPC_LOAD_U32(r20.u32 + 268);
	// add r28,r8,r11
	r28.u64 = ctx.r8.u64 + r11.u64;
	// lwz r27,3756(r20)
	r27.u64 = PPC_LOAD_U32(r20.u32 + 3756);
	// add r26,r9,r11
	r26.u64 = ctx.r9.u64 + r11.u64;
	// lwz r19,1248(r29)
	r19.u64 = PPC_LOAD_U32(r29.u32 + 1248);
	// lis r11,-32126
	r11.s64 = -2105409536;
	// stw r6,20(r21)
	PPC_STORE_U32(r21.u32 + 20, ctx.r6.u32);
	// lwz r7,21568(r20)
	ctx.r7.u64 = PPC_LOAD_U32(r20.u32 + 21568);
	// stw r22,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r22.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// stw r23,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r23.u32);
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// stw r7,24(r21)
	PPC_STORE_U32(r21.u32 + 24, ctx.r7.u32);
	// lwz r3,3360(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 3360);
	// lhz r11,50(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 50);
	// lhz r10,52(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 52);
	// stw r31,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r31.u32);
	// rlwinm r18,r11,31,1,31
	r18.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r28,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r28.u32);
	// rlwinm r24,r10,31,1,31
	r24.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// stw r25,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r25.u32);
	// bl 0x8271d5f8
	sub_8271D5F8(ctx, base);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x826fed54
	if (cr6.eq) goto loc_826FED54;
	// lis r17,-32126
	r17.s64 = -2105409536;
loc_826FE80C:
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// stw r27,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r27.u32);
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// stw r26,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r26.u32);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// stw r25,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r25.u32);
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// stw r9,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r9.u32);
	// stw r7,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r7.u32);
	// stw r6,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r6.u32);
	// beq cr6,0x826fed04
	if (cr6.eq) goto loc_826FED04;
	// mr r25,r18
	r25.u64 = r18.u64;
loc_826FE83C:
	// lwz r5,0(r23)
	ctx.r5.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// ld r11,0(r19)
	r11.u64 = PPC_LOAD_U64(r19.u32 + 0);
	// addi r19,r19,8
	r19.s64 = r19.s64 + 8;
	// rlwinm r10,r5,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,1024
	cr6.compare<uint32_t>(ctx.r10.u32, 1024, xer);
	// beq cr6,0x826fec9c
	if (cr6.eq) goto loc_826FEC9C;
	// rldicl r10,r11,8,56
	ctx.r10.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFF;
	// lwz r8,220(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 220);
	// rldicl r4,r11,16,48
	ctx.r4.u64 = __builtin_rotateleft64(r11.u64, 16) & 0xFFFF;
	// stw r9,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r9.u32);
	// clrlwi r10,r10,26
	ctx.r10.u64 = ctx.r10.u32 & 0x3F;
	// stw r7,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r7.u32);
	// mr r27,r11
	r27.u64 = r11.u64;
	// stw r6,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r6.u32);
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r5,0,15,15
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x10000;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lhz r10,74(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 74);
	// clrlwi r26,r4,26
	r26.u64 = ctx.r4.u32 & 0x3F;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// add r28,r11,r8
	r28.u64 = r11.u64 + ctx.r8.u64;
	// lhz r11,76(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 76);
	// addi r8,r9,8
	ctx.r8.s64 = ctx.r9.s64 + 8;
	// stw r28,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r28.u32);
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// stw r8,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r8.u32);
	// beq cr6,0x826fe8b8
	if (cr6.eq) goto loc_826FE8B8;
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// b 0x826fe8c0
	goto loc_826FE8C0;
loc_826FE8B8:
	// rotlwi r8,r10,3
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
loc_826FE8C0:
	// addi r5,r8,8
	ctx.r5.s64 = ctx.r8.s64 + 8;
	// stw r8,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r8.u32);
	// stw r10,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r10.u32);
	// stw r5,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r5.u32);
	// lwz r5,-24996(r17)
	ctx.r5.u64 = PPC_LOAD_U32(r17.u32 + -24996);
	// cmplwi cr6,r5,9
	cr6.compare<uint32_t>(ctx.r5.u32, 9, xer);
	// bgt cr6,0x826fea1c
	if (cr6.gt) goto loc_826FEA1C;
	// lis r12,-32144
	r12.s64 = -2106589184;
	// addi r12,r12,-5900
	r12.s64 = r12.s64 + -5900;
	// rlwinm r0,r5,2,0,29
	r0.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r5.u64) {
	case 0:
		goto loc_826FE91C;
	case 1:
		goto loc_826FE970;
	case 2:
		goto loc_826FE9C4;
	case 3:
		goto loc_826FE9CC;
	case 4:
		goto loc_826FEA1C;
	case 5:
		goto loc_826FEA1C;
	case 6:
		goto loc_826FEA1C;
	case 7:
		goto loc_826FEA1C;
	case 8:
		goto loc_826FE91C;
	case 9:
		goto loc_826FE970;
	default:
		__builtin_unreachable();
	}
	// lwz r19,-5860(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -5860);
	// lwz r19,-5776(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -5776);
	// lwz r19,-5692(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -5692);
	// lwz r19,-5684(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -5684);
	// lwz r19,-5604(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -5604);
	// lwz r19,-5604(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -5604);
	// lwz r19,-5604(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -5604);
	// lwz r19,-5604(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -5604);
	// lwz r19,-5860(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -5860);
	// lwz r19,-5776(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -5776);
loc_826FE91C:
	// addi r11,r9,128
	r11.s64 = ctx.r9.s64 + 128;
	// dcbt r0,r11
	// dcbt r10,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// dcbt r9,r11
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r9,r11
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// dcbt r9,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r11
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// dcbt r10,r11
	// b 0x826fea1c
	goto loc_826FEA1C;
loc_826FE970:
	// addi r11,r8,128
	r11.s64 = ctx.r8.s64 + 128;
	// dcbt r0,r11
	// dcbt r10,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// dcbt r9,r11
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r9,r11
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// dcbt r9,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r11
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// dcbt r10,r11
	// b 0x826fea1c
	goto loc_826FEA1C;
loc_826FE9C4:
	// addi r10,r7,128
	ctx.r10.s64 = ctx.r7.s64 + 128;
	// b 0x826fe9d0
	goto loc_826FE9D0;
loc_826FE9CC:
	// addi r10,r6,128
	ctx.r10.s64 = ctx.r6.s64 + 128;
loc_826FE9D0:
	// dcbt r0,r10
	// dcbt r11,r10
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r10
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// dcbt r9,r10
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r9,r10
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// dcbt r9,r10
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r10
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// dcbt r11,r10
loc_826FEA1C:
	// addi r11,r5,1
	r11.s64 = ctx.r5.s64 + 1;
	// mr r31,r22
	r31.u64 = r22.u64;
	// srawi r10,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r10.s64 = r11.s32 >> 4;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r11,-24996(r17)
	PPC_STORE_U32(r17.u32 + -24996, r11.u32);
loc_826FEA3C:
	// srawi r9,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	ctx.r9.s64 = r31.s32 >> 2;
	// rldicl r11,r27,20,44
	r11.u64 = __builtin_rotateleft64(r27.u64, 20) & 0xFFFFF;
	// clrlwi r10,r26,31
	ctx.r10.u64 = r26.u32 & 0x1;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r9,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r9.u32);
	// beq cr6,0x826fec74
	if (cr6.eq) goto loc_826FEC74;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826fec04
	if (!cr6.eq) goto loc_826FEC04;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// addi r5,r29,932
	ctx.r5.s64 = r29.s64 + 932;
	// lwz r10,24(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 24);
	// lwz r4,276(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 276);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// stw r22,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r22.u32);
	// stw r22,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r22.u32);
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lbz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// stw r11,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r11.u32);
	// lwz r11,20(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// stw r10,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r10.u32);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// lwz r11,260(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 260);
	// stw r9,24(r21)
	PPC_STORE_U32(r21.u32 + 24, ctx.r9.u32);
	// lwz r10,260(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 260);
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// stw r10,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r10.u32);
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// dcbzl r0,r10
	memset(base + ((ctx.r10.u32) & ~127), 0, 128);
	// lwz r7,200(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r28,216(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// cmplwi cr6,r7,128
	cr6.compare<uint32_t>(ctx.r7.u32, 128, xer);
	// blt cr6,0x826feae0
	if (cr6.lt) goto loc_826FEAE0;
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82611f38
	sub_82611F38(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// b 0x826feb5c
	goto loc_826FEB5C;
loc_826FEAE0:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// ble cr6,0x826feb58
	if (!cr6.gt) goto loc_826FEB58;
	// lwz r3,224(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// lwz r31,220(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lwz r30,212(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// lwz r8,228(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
loc_826FEB00:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// mullw r10,r10,r3
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// add r23,r10,r31
	r23.u64 = ctx.r10.u64 + r31.u64;
	// rlwinm r10,r9,25,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 25) & 0x1;
	// clrlwi r9,r9,26
	ctx.r9.u64 = ctx.r9.u32 & 0x3F;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// xor r23,r23,r10
	r23.u64 = r23.u64 ^ ctx.r10.u64;
	// subf r10,r10,r23
	ctx.r10.s64 = r23.s64 - ctx.r10.s64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// rotlwi r16,r8,1
	r16.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lbzx r23,r8,r5
	r23.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r5.u32);
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// or r6,r23,r6
	ctx.r6.u64 = r23.u64 | ctx.r6.u64;
	// sthx r10,r16,r30
	PPC_STORE_U16(r16.u32 + r30.u32, ctx.r10.u16);
	// bne cr6,0x826feb00
	if (!cr6.eq) goto loc_826FEB00;
loc_826FEB58:
	// stw r11,20(r21)
	PPC_STORE_U32(r21.u32 + 20, r11.u32);
loc_826FEB5C:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x826febf0
	if (!cr6.eq) goto loc_826FEBF0;
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// addi r31,r11,112
	r31.s64 = r11.s64 + 112;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addi r7,r11,32
	ctx.r7.s64 = r11.s64 + 32;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r6,r11,48
	ctx.r6.s64 = r11.s64 + 48;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r11,64
	ctx.r5.s64 = r11.s64 + 64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r4,r11,80
	ctx.r4.s64 = r11.s64 + 80;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r3,r11,96
	ctx.r3.s64 = r11.s64 + 96;
	// srawi r10,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// stw r10,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r10.u32);
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// vsplth v0,v0,1
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_set1_epi16(short(0xD0C))));
	// stvx v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x826fec48
	goto loc_826FEC48;
loc_826FEBF0:
	// lwz r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r3,128(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// bl 0x826ff808
	sub_826FF808(ctx, base);
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x826fec48
	goto loc_826FEC48;
loc_826FEC04:
	// lwz r10,448(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 448);
	// rldicl r9,r27,24,40
	ctx.r9.u64 = __builtin_rotateleft64(r27.u64, 24) & 0xFFFFFF;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// clrlwi r5,r9,28
	ctx.r5.u64 = ctx.r9.u32 & 0xF;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// rlwinm r10,r11,0,29,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x6;
	// add r11,r5,r29
	r11.u64 = ctx.r5.u64 + r29.u64;
	// lbz r11,160(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 160);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r11,r11,117
	r11.s64 = r11.s64 + 117;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r29
	r11.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FEC48:
	// lwz r11,204(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// rlwinm r8,r31,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,792(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 792);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,104(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r7,r1,176
	ctx.r7.s64 = ctx.r1.s64 + 176;
	// lwzx r5,r11,r9
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// lwzx r4,r8,r7
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FEC74:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// rlwinm r26,r26,31,1,31
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 31) & 0x7FFFFFFF;
	// rldicr r27,r27,8,55
	r27.u64 = __builtin_rotateleft64(r27.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// cmpwi cr6,r31,6
	cr6.compare<int32_t>(r31.s32, 6, xer);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// blt cr6,0x826fea3c
	if (cr6.lt) goto loc_826FEA3C;
	// lwz r23,132(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r7,112(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r6,152(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
loc_826FEC9C:
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// addi r23,r23,20
	r23.s64 = r23.s64 + 20;
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// stw r9,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r9.u32);
	// stw r7,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r7.u32);
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r11.u32);
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r6,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r6.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r23,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r23.u32);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// bne cr6,0x826fe83c
	if (!cr6.eq) goto loc_826FE83C;
	// lwz r30,84(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r28,88(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r26,100(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r25,108(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
loc_826FED04:
	// lwz r11,232(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 232);
	// addi r24,r24,-1
	r24.s64 = r24.s64 + -1;
	// lwz r10,228(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 228);
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
	// add r30,r10,r30
	r30.u64 = ctx.r10.u64 + r30.u64;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// add r27,r10,r27
	r27.u64 = ctx.r10.u64 + r27.u64;
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// stw r31,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r31.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// stw r28,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r28.u32);
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// stw r25,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r25.u32);
	// bne cr6,0x826fe80c
	if (!cr6.eq) goto loc_826FE80C;
	// lwz r3,208(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// b 0x8239bd18
	return;
loc_826FED54:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_826FED60"))) PPC_WEAK_FUNC(sub_826FED60);
PPC_FUNC_IMPL(__imp__sub_826FED60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// li r25,0
	r25.s64 = 0;
	// addi r4,r22,21712
	ctx.r4.s64 = r22.s64 + 21712;
	// lwz r6,21556(r22)
	ctx.r6.u64 = PPC_LOAD_U32(r22.u32 + 21556);
	// lwz r11,224(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 224);
	// lwz r9,3724(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 3724);
	// lwz r10,3728(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 3728);
	// lwz r8,3720(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + 3720);
	// add r30,r9,r11
	r30.u64 = ctx.r9.u64 + r11.u64;
	// lwz r7,220(r22)
	ctx.r7.u64 = PPC_LOAD_U32(r22.u32 + 220);
	// add r28,r10,r11
	r28.u64 = ctx.r10.u64 + r11.u64;
	// lwz r26,268(r22)
	r26.u64 = PPC_LOAD_U32(r22.u32 + 268);
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lwz r20,1248(r29)
	r20.u64 = PPC_LOAD_U32(r29.u32 + 1248);
	// add r31,r8,r7
	r31.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r6,20(r24)
	PPC_STORE_U32(r24.u32 + 20, ctx.r6.u32);
	// lwz r9,21568(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 21568);
	// stw r25,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r25.u32);
	// stw r26,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r26.u32);
	// stw r31,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r31.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// stw r9,24(r24)
	PPC_STORE_U32(r24.u32 + 24, ctx.r9.u32);
	// lwz r3,3360(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 3360);
	// lhz r11,50(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 50);
	// lhz r10,52(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 52);
	// stw r28,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r28.u32);
	// rlwinm r18,r11,31,1,31
	r18.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r27,r10,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// bl 0x8271d5f8
	sub_8271D5F8(ctx, base);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x826ff2e8
	if (cr6.eq) goto loc_826FF2E8;
	// lis r19,-32126
	r19.s64 = -2105409536;
loc_826FEDF4:
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// stw r7,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r7.u32);
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// beq cr6,0x826ff2b0
	if (cr6.eq) goto loc_826FF2B0;
	// mr r21,r18
	r21.u64 = r18.u64;
loc_826FEE18:
	// lwz r5,0(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// ld r11,0(r20)
	r11.u64 = PPC_LOAD_U64(r20.u32 + 0);
	// addi r20,r20,8
	r20.s64 = r20.s64 + 8;
	// rlwinm r10,r5,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,1024
	cr6.compare<uint32_t>(ctx.r10.u32, 1024, xer);
	// beq cr6,0x826ff278
	if (cr6.eq) goto loc_826FF278;
	// rldicl r10,r11,8,56
	ctx.r10.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFF;
	// lwz r8,220(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 220);
	// rldicl r4,r11,16,48
	ctx.r4.u64 = __builtin_rotateleft64(r11.u64, 16) & 0xFFFF;
	// stw r9,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r9.u32);
	// clrlwi r10,r10,26
	ctx.r10.u64 = ctx.r10.u32 & 0x3F;
	// stw r7,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r7.u32);
	// mr r26,r11
	r26.u64 = r11.u64;
	// stw r6,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r6.u32);
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r5,0,15,15
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x10000;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lhz r10,74(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 74);
	// clrlwi r23,r4,26
	r23.u64 = ctx.r4.u32 & 0x3F;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// add r28,r11,r8
	r28.u64 = r11.u64 + ctx.r8.u64;
	// lhz r11,76(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 76);
	// addi r8,r9,8
	ctx.r8.s64 = ctx.r9.s64 + 8;
	// stw r28,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r28.u32);
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// stw r8,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r8.u32);
	// beq cr6,0x826fee94
	if (cr6.eq) goto loc_826FEE94;
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// b 0x826fee9c
	goto loc_826FEE9C;
loc_826FEE94:
	// rotlwi r8,r10,3
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
loc_826FEE9C:
	// addi r5,r8,8
	ctx.r5.s64 = ctx.r8.s64 + 8;
	// stw r8,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r8.u32);
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
	// stw r5,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r5.u32);
	// lwz r5,-24996(r19)
	ctx.r5.u64 = PPC_LOAD_U32(r19.u32 + -24996);
	// cmplwi cr6,r5,9
	cr6.compare<uint32_t>(ctx.r5.u32, 9, xer);
	// bgt cr6,0x826feff8
	if (cr6.gt) goto loc_826FEFF8;
	// lis r12,-32144
	r12.s64 = -2106589184;
	// addi r12,r12,-4400
	r12.s64 = r12.s64 + -4400;
	// rlwinm r0,r5,2,0,29
	r0.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r5.u64) {
	case 0:
		goto loc_826FEEF8;
	case 1:
		goto loc_826FEF4C;
	case 2:
		goto loc_826FEFA0;
	case 3:
		goto loc_826FEFA8;
	case 4:
		goto loc_826FEFF8;
	case 5:
		goto loc_826FEFF8;
	case 6:
		goto loc_826FEFF8;
	case 7:
		goto loc_826FEFF8;
	case 8:
		goto loc_826FEEF8;
	case 9:
		goto loc_826FEF4C;
	default:
		__builtin_unreachable();
	}
	// lwz r19,-4360(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -4360);
	// lwz r19,-4276(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -4276);
	// lwz r19,-4192(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -4192);
	// lwz r19,-4184(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -4184);
	// lwz r19,-4104(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -4104);
	// lwz r19,-4104(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -4104);
	// lwz r19,-4104(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -4104);
	// lwz r19,-4104(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -4104);
	// lwz r19,-4360(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -4360);
	// lwz r19,-4276(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -4276);
loc_826FEEF8:
	// addi r11,r9,128
	r11.s64 = ctx.r9.s64 + 128;
	// dcbt r0,r11
	// dcbt r10,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// dcbt r9,r11
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r9,r11
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// dcbt r9,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r11
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// dcbt r10,r11
	// b 0x826feff8
	goto loc_826FEFF8;
loc_826FEF4C:
	// addi r11,r8,128
	r11.s64 = ctx.r8.s64 + 128;
	// dcbt r0,r11
	// dcbt r10,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// dcbt r9,r11
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r9,r11
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// dcbt r9,r11
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r11
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// dcbt r10,r11
	// b 0x826feff8
	goto loc_826FEFF8;
loc_826FEFA0:
	// addi r10,r7,128
	ctx.r10.s64 = ctx.r7.s64 + 128;
	// b 0x826fefac
	goto loc_826FEFAC;
loc_826FEFA8:
	// addi r10,r6,128
	ctx.r10.s64 = ctx.r6.s64 + 128;
loc_826FEFAC:
	// dcbt r0,r10
	// dcbt r11,r10
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r10
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// dcbt r9,r10
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r9,r10
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// dcbt r9,r10
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r10
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// dcbt r11,r10
loc_826FEFF8:
	// addi r11,r5,1
	r11.s64 = ctx.r5.s64 + 1;
	// mr r31,r25
	r31.u64 = r25.u64;
	// srawi r10,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r10.s64 = r11.s32 >> 4;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r11,-24996(r19)
	PPC_STORE_U32(r19.u32 + -24996, r11.u32);
loc_826FF018:
	// srawi r9,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	ctx.r9.s64 = r31.s32 >> 2;
	// rldicl r11,r26,20,44
	r11.u64 = __builtin_rotateleft64(r26.u64, 20) & 0xFFFFF;
	// clrlwi r10,r23,31
	ctx.r10.u64 = r23.u32 & 0x1;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r9,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r9.u32);
	// beq cr6,0x826ff250
	if (cr6.eq) goto loc_826FF250;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826ff1e0
	if (!cr6.eq) goto loc_826FF1E0;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// addi r5,r29,932
	ctx.r5.s64 = r29.s64 + 932;
	// lwz r10,24(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 24);
	// lwz r4,276(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 276);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// stw r25,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r25.u32);
	// stw r25,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r25.u32);
	// stw r11,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r11.u32);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lbz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// stw r10,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r10.u32);
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// lwz r11,260(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 260);
	// stw r9,24(r24)
	PPC_STORE_U32(r24.u32 + 24, ctx.r9.u32);
	// lwz r10,260(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 260);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// stw r10,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r10.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// dcbzl r0,r10
	memset(base + ((ctx.r10.u32) & ~127), 0, 128);
	// lwz r7,168(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// lwz r28,184(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplwi cr6,r7,128
	cr6.compare<uint32_t>(ctx.r7.u32, 128, xer);
	// blt cr6,0x826ff0bc
	if (cr6.lt) goto loc_826FF0BC;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82611f38
	sub_82611F38(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// b 0x826ff138
	goto loc_826FF138;
loc_826FF0BC:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r6,128(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// ble cr6,0x826ff134
	if (!cr6.gt) goto loc_826FF134;
	// lwz r3,192(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r31,188(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r30,180(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r8,196(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
loc_826FF0DC:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// mullw r10,r10,r3
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// add r17,r10,r31
	r17.u64 = ctx.r10.u64 + r31.u64;
	// rlwinm r10,r9,25,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 25) & 0x1;
	// clrlwi r9,r9,26
	ctx.r9.u64 = ctx.r9.u32 & 0x3F;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// xor r17,r17,r10
	r17.u64 = r17.u64 ^ ctx.r10.u64;
	// subf r10,r10,r17
	ctx.r10.s64 = r17.s64 - ctx.r10.s64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// rotlwi r16,r8,1
	r16.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lbzx r17,r8,r5
	r17.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r5.u32);
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// or r6,r17,r6
	ctx.r6.u64 = r17.u64 | ctx.r6.u64;
	// sthx r10,r16,r30
	PPC_STORE_U16(r16.u32 + r30.u32, ctx.r10.u16);
	// bne cr6,0x826ff0dc
	if (!cr6.eq) goto loc_826FF0DC;
loc_826FF134:
	// stw r11,20(r24)
	PPC_STORE_U32(r24.u32 + 20, r11.u32);
loc_826FF138:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x826ff1cc
	if (!cr6.eq) goto loc_826FF1CC;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// addi r31,r11,112
	r31.s64 = r11.s64 + 112;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addi r7,r11,32
	ctx.r7.s64 = r11.s64 + 32;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r6,r11,48
	ctx.r6.s64 = r11.s64 + 48;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r11,64
	ctx.r5.s64 = r11.s64 + 64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r4,r11,80
	ctx.r4.s64 = r11.s64 + 80;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r3,r11,96
	ctx.r3.s64 = r11.s64 + 96;
	// srawi r10,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// stw r10,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r10.u32);
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// vsplth v0,v0,1
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_set1_epi16(short(0xD0C))));
	// stvx v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x826ff224
	goto loc_826FF224;
loc_826FF1CC:
	// lwz r4,124(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x826ff808
	sub_826FF808(ctx, base);
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x826ff224
	goto loc_826FF224;
loc_826FF1E0:
	// lwz r10,448(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 448);
	// rldicl r9,r26,24,40
	ctx.r9.u64 = __builtin_rotateleft64(r26.u64, 24) & 0xFFFFFF;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// clrlwi r5,r9,28
	ctx.r5.u64 = ctx.r9.u32 & 0xF;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// rlwinm r10,r11,0,29,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x6;
	// add r11,r5,r29
	r11.u64 = ctx.r5.u64 + r29.u64;
	// lbz r11,160(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 160);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r11,r11,117
	r11.s64 = r11.s64 + 117;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r29
	r11.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FF224:
	// lwz r11,172(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// addi r9,r1,136
	ctx.r9.s64 = ctx.r1.s64 + 136;
	// rlwinm r8,r31,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,792(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 792);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r7,r1,144
	ctx.r7.s64 = ctx.r1.s64 + 144;
	// lwzx r5,r11,r9
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// lwzx r4,r8,r7
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FF250:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// rlwinm r23,r23,31,1,31
	r23.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 31) & 0x7FFFFFFF;
	// rldicr r26,r26,8,55
	r26.u64 = __builtin_rotateleft64(r26.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// cmpwi cr6,r31,6
	cr6.compare<int32_t>(r31.s32, 6, xer);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// blt cr6,0x826ff018
	if (cr6.lt) goto loc_826FF018;
	// lwz r26,96(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r9,108(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r7,120(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r6,92(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_826FF278:
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// addi r26,r26,20
	r26.s64 = r26.s64 + 20;
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// stw r7,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r7.u32);
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// stw r26,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r26.u32);
	// bne cr6,0x826fee18
	if (!cr6.eq) goto loc_826FEE18;
	// lwz r31,104(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r30,84(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r28,88(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_826FF2B0:
	// lwz r11,232(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 232);
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// lwz r10,228(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 228);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// add r31,r10,r31
	r31.u64 = ctx.r10.u64 + r31.u64;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// stw r31,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r31.u32);
	// stw r28,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r28.u32);
	// bne cr6,0x826fedf4
	if (!cr6.eq) goto loc_826FEDF4;
	// lwz r3,176(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// b 0x8239bd18
	return;
loc_826FF2E8:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_826FF2F4"))) PPC_WEAK_FUNC(sub_826FF2F4);
PPC_FUNC_IMPL(__imp__sub_826FF2F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826FF2F8"))) PPC_WEAK_FUNC(sub_826FF2F8);
PPC_FUNC_IMPL(__imp__sub_826FF2F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-1872(r1)
	ea = -1872 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lwz r9,21556(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21556);
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// li r23,0
	r23.s64 = 0;
	// addi r16,r3,21712
	r16.s64 = ctx.r3.s64 + 21712;
	// lhz r11,52(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 52);
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// lwz r8,1516(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1516);
	// rlwinm r29,r11,31,1,31
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// lhz r11,50(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// lwz r18,1248(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + 1248);
	// rlwinm r17,r11,31,1,31
	r17.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r23,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r23.u32);
	// mullw r11,r29,r17
	r11.s64 = int64_t(r29.s32) * int64_t(r17.s32);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r10,r8,r11
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// rlwinm r10,r10,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,20(r30)
	PPC_STORE_U32(r30.u32 + 20, ctx.r10.u32);
	// lwz r9,1516(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1516);
	// lwz r10,21568(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21568);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r11.u32);
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lwz r3,3360(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 3360);
	// bl 0x8271d5f8
	sub_8271D5F8(ctx, base);
	// stw r23,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r23.u32);
	// mr r11,r23
	r11.u64 = r23.u64;
	// stw r23,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r23.u32);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// sth r23,16(r30)
	PPC_STORE_U16(r30.u32 + 16, r23.u16);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// lhz r9,74(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// stw r9,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r9.u32);
	// lhz r9,76(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// stw r9,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r9.u32);
	// beq cr6,0x826ff7f0
	if (cr6.eq) goto loc_826FF7F0;
	// mr r19,r29
	r19.u64 = r29.u64;
	// lis r20,-32126
	r20.s64 = -2105409536;
loc_826FF3B8:
	// cmplwi cr6,r17,0
	cr6.compare<uint32_t>(r17.u32, 0, xer);
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// stw r10,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r10.u32);
	// sth r23,18(r30)
	PPC_STORE_U16(r30.u32 + 18, r23.u16);
	// beq cr6,0x826ff7a8
	if (cr6.eq) goto loc_826FF7A8;
	// mr r21,r17
	r21.u64 = r17.u64;
loc_826FF3D0:
	// ld r10,0(r18)
	ctx.r10.u64 = PPC_LOAD_U64(r18.u32 + 0);
	// addi r18,r18,8
	r18.s64 = r18.s64 + 8;
	// rldicl r11,r10,16,48
	r11.u64 = __builtin_rotateleft64(ctx.r10.u64, 16) & 0xFFFF;
	// clrlwi r8,r11,26
	ctx.r8.u64 = r11.u32 & 0x3F;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x826ff758
	if (cr6.eq) goto loc_826FF758;
	// rldicl r11,r10,8,56
	r11.u64 = __builtin_rotateleft64(ctx.r10.u64, 8) & 0xFF;
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// mr r24,r10
	r24.u64 = ctx.r10.u64;
	// lwz r7,-25000(r20)
	ctx.r7.u64 = PPC_LOAD_U32(r20.u32 + -25000);
	// clrlwi r11,r11,26
	r11.u64 = r11.u32 & 0x3F;
	// mr r22,r8
	r22.u64 = ctx.r8.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r7,9
	cr6.compare<uint32_t>(ctx.r7.u32, 9, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r27,r11,r9
	r27.u64 = r11.u64 + ctx.r9.u64;
	// stw r27,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r27.u32);
	// bgt cr6,0x826ff4dc
	if (cr6.gt) goto loc_826FF4DC;
	// lis r12,-32144
	r12.s64 = -2106589184;
	// addi r12,r12,-3020
	r12.s64 = r12.s64 + -3020;
	// rlwinm r0,r7,2,0,29
	r0.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r7.u64) {
	case 0:
		goto loc_826FF45C;
	case 1:
		goto loc_826FF468;
	case 2:
		goto loc_826FF474;
	case 3:
		goto loc_826FF47C;
	case 4:
		goto loc_826FF4DC;
	case 5:
		goto loc_826FF4DC;
	case 6:
		goto loc_826FF4DC;
	case 7:
		goto loc_826FF4DC;
	case 8:
		goto loc_826FF45C;
	case 9:
		goto loc_826FF468;
	default:
		__builtin_unreachable();
	}
	// lwz r19,-2980(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -2980);
	// lwz r19,-2968(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -2968);
	// lwz r19,-2956(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -2956);
	// lwz r19,-2948(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -2948);
	// lwz r19,-2852(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -2852);
	// lwz r19,-2852(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -2852);
	// lwz r19,-2852(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -2852);
	// lwz r19,-2852(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -2852);
	// lwz r19,-2980(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -2980);
	// lwz r19,-2968(r15)
	r19.u64 = PPC_LOAD_U32(r15.u32 + -2968);
loc_826FF45C:
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// b 0x826ff484
	goto loc_826FF484;
loc_826FF468:
	// lwz r11,400(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 400);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// b 0x826ff484
	goto loc_826FF484;
loc_826FF474:
	// lwz r11,408(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 408);
	// b 0x826ff480
	goto loc_826FF480;
loc_826FF47C:
	// lwz r11,412(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 412);
loc_826FF480:
	// lwz r10,12(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 12);
loc_826FF484:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,128
	ctx.r10.s64 = r11.s64 + 128;
	// dcbt r0,r10
	// lhz r11,90(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 90);
	// dcbt r11,r10
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// dcbt r9,r10
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// dcbt r9,r10
	// rotlwi r9,r11,2
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 2);
	// dcbt r9,r10
	// rotlwi r9,r11,2
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 2);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// dcbt r9,r10
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// dcbt r9,r10
	// rotlwi r9,r11,3
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 3);
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// dcbt r11,r10
loc_826FF4DC:
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// mr r26,r23
	r26.u64 = r23.u64;
	// srawi r10,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r10.s64 = r11.s32 >> 4;
	// addi r25,r31,392
	r25.s64 = r31.s64 + 392;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r11,-25000(r20)
	PPC_STORE_U32(r20.u32 + -25000, r11.u32);
loc_826FF4FC:
	// srawi r11,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r11.s64 = r26.s32 >> 2;
	// lwz r9,0(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// rldicl r10,r24,20,44
	ctx.r10.u64 = __builtin_rotateleft64(r24.u64, 20) & 0xFFFFF;
	// addi r8,r11,2
	ctx.r8.s64 = r11.s64 + 2;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// clrlwi r11,r10,28
	r11.u64 = ctx.r10.u32 & 0xF;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r11,0,28,28
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// lwzx r10,r10,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// bne cr6,0x826ff740
	if (!cr6.eq) goto loc_826FF740;
	// clrlwi r10,r22,31
	ctx.r10.u64 = r22.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826ff740
	if (cr6.eq) goto loc_826FF740;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826ff6e0
	if (!cr6.eq) goto loc_826FF6E0;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// addi r5,r31,932
	ctx.r5.s64 = r31.s64 + 932;
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// lwz r4,276(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 276);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// stw r23,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r23.u32);
	// stw r23,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r23.u32);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lbz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r11.u32);
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r11,260(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// stw r9,24(r30)
	PPC_STORE_U32(r30.u32 + 24, ctx.r9.u32);
	// lwz r10,260(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// dcbzl r0,r10
	memset(base + ((ctx.r10.u32) & ~127), 0, 128);
	// lwz r7,120(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r27,136(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmplwi cr6,r7,128
	cr6.compare<uint32_t>(ctx.r7.u32, 128, xer);
	// blt cr6,0x826ff5c4
	if (cr6.lt) goto loc_826FF5C4;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82611f38
	sub_82611F38(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// b 0x826ff640
	goto loc_826FF640;
loc_826FF5C4:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// ble cr6,0x826ff63c
	if (!cr6.gt) goto loc_826FF63C;
	// lwz r3,108(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r29,148(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r28,128(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r8,144(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
loc_826FF5E4:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// mullw r10,r10,r3
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// add r15,r10,r29
	r15.u64 = ctx.r10.u64 + r29.u64;
	// rlwinm r10,r9,25,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 25) & 0x1;
	// clrlwi r9,r9,26
	ctx.r9.u64 = ctx.r9.u32 & 0x3F;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// xor r15,r15,r10
	r15.u64 = r15.u64 ^ ctx.r10.u64;
	// subf r10,r10,r15
	ctx.r10.s64 = r15.s64 - ctx.r10.s64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// rotlwi r14,r8,1
	r14.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lbzx r15,r8,r5
	r15.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r5.u32);
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// or r6,r15,r6
	ctx.r6.u64 = r15.u64 | ctx.r6.u64;
	// sthx r10,r14,r28
	PPC_STORE_U16(r14.u32 + r28.u32, ctx.r10.u16);
	// bne cr6,0x826ff5e4
	if (!cr6.eq) goto loc_826FF5E4;
loc_826FF63C:
	// stw r11,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r11.u32);
loc_826FF640:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x826ff6d0
	if (!cr6.eq) goto loc_826FF6D0;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// addi r7,r11,32
	ctx.r7.s64 = r11.s64 + 32;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addi r6,r11,48
	ctx.r6.s64 = r11.s64 + 48;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r5,r11,64
	ctx.r5.s64 = r11.s64 + 64;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r4,r11,80
	ctx.r4.s64 = r11.s64 + 80;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r3,r11,96
	ctx.r3.s64 = r11.s64 + 96;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r29,r11,112
	r29.s64 = r11.s64 + 112;
	// srawi r10,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// stw r10,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r10.u32);
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// vsplth v0,v0,1
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_set1_epi16(short(0xD0C))));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x826ff724
	goto loc_826FF724;
loc_826FF6D0:
	// lwz r4,92(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x826ff808
	sub_826FF808(ctx, base);
	// b 0x826ff724
	goto loc_826FF724;
loc_826FF6E0:
	// lwz r10,448(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// rldicl r9,r24,24,40
	ctx.r9.u64 = __builtin_rotateleft64(r24.u64, 24) & 0xFFFFFF;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// clrlwi r5,r9,28
	ctx.r5.u64 = ctx.r9.u32 & 0xF;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// rlwinm r10,r11,0,29,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x6;
	// add r11,r5,r31
	r11.u64 = ctx.r5.u64 + r31.u64;
	// lbz r11,160(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 160);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r11,r11,117
	r11.s64 = r11.s64 + 117;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826FF724:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r4,88(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r11,r11,45
	r11.s64 = r11.s64 + 45;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r5,r11,r31
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + r31.u32);
	// bl 0x8270e138
	sub_8270E138(ctx, base);
loc_826FF740:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// rlwinm r22,r22,31,1,31
	r22.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 31) & 0x7FFFFFFF;
	// rldicr r24,r24,8,55
	r24.u64 = __builtin_rotateleft64(r24.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// cmpwi cr6,r26,6
	cr6.compare<int32_t>(r26.s32, 6, xer);
	// blt cr6,0x826ff4fc
	if (cr6.lt) goto loc_826FF4FC;
loc_826FF758:
	// lhz r10,18(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 18);
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r8,r10,2
	ctx.r8.s64 = ctx.r10.s64 + 2;
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// addi r7,r11,2
	ctx.r7.s64 = r11.s64 + 2;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// sth r8,18(r30)
	PPC_STORE_U16(r30.u32 + 18, ctx.r8.u16);
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// stw r7,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r7.u32);
	// stw r9,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r9.u32);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// stw r11,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r11.u32);
	// bne cr6,0x826ff3d0
	if (!cr6.eq) goto loc_826FF3D0;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
loc_826FF7A8:
	// lhz r9,16(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 16);
	// addi r19,r19,-1
	r19.s64 = r19.s64 + -1;
	// lwz r7,124(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// addi r6,r9,2
	ctx.r6.s64 = ctx.r9.s64 + 2;
	// lwz r9,132(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm r7,r7,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// sth r6,16(r30)
	PPC_STORE_U16(r30.u32 + 16, ctx.r6.u16);
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// lhz r9,50(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// stw r9,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r9.u32);
	// bne cr6,0x826ff3b8
	if (!cr6.eq) goto loc_826FF3B8;
loc_826FF7F0:
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// bl 0x8271d940
	sub_8271D940(ctx, base);
	// lwz r3,140(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// addi r1,r1,1872
	ctx.r1.s64 = ctx.r1.s64 + 1872;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826FF808"))) PPC_WEAK_FUNC(sub_826FF808);
PPC_FUNC_IMPL(__imp__sub_826FF808) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v69{};
	PPCVRegister v72{};
	// li r12,64
	r12.s64 = 64;
	// vspltish v30,2
	// li r9,16
	ctx.r9.s64 = 16;
	// vspltish v13,3
	// li r7,112
	ctx.r7.s64 = 112;
	// vspltish v31,4
	// vspltish v29,1
	// lvx v1,r0,r3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v25,v1,v30
	// li r11,48
	r11.s64 = 48;
	// lvx v2,r12,r3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r12.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v1,v1,v13
	// lvx v5,r9,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v24,v2,v30
	// lvx v6,r7,r3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v2,v2,v13
	// vaddshs v12,v5,v6
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// li r6,80
	ctx.r6.s64 = 80;
	// vslh v26,v5,v29
	// lvx v8,r11,r3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v11,v5,v30
	// li r10,32
	ctx.r10.s64 = 32;
	// vaddshs v2,v2,v24
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v24.s16)));
	// li r8,96
	ctx.r8.s64 = 96;
	// vslh v24,v12,v31
	// vspltish v28,0
	// vslh v10,v6,v31
	// lvx v7,r6,r3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v11,v26,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v1,v1,v25
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v25.s16)));
	// lvx v4,r10,r3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubuhm v9,v24,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvx v3,r8,r3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v25,v6,v13
	// vslh v27,v5,v30
	// vslh v24,v6,v30
	// vsubuhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsubuhm v11,v9,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vslh v9,v12,v30
	// vslh v5,v5,v13
	// vslh v6,v6,v31
	// vaddshs v12,v7,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v26,v7,v30
	// vaddshs v5,v27,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubuhm v6,v9,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vslh v27,v7,v13
	// vsubuhm v10,v10,v25
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vaddshs v5,v9,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubuhm v6,v6,v24
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vslh v9,v12,v30
	// vslh v24,v8,v30
	// vaddshs v26,v26,v27
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vslh v25,v8,v31
	// vaddshs v1,v1,v31
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsubuhm v24,v9,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v26,v9,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vslh v27,v8,v31
	// vor128 v14,v69,v69
	_mm_store_si128((__m128i*)v14.u8, _mm_load_si128((__m128i*)v69.u8));
	// vsubuhm v24,v24,v25
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vsubuhm v10,v10,v26
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vslh v26,v12,v31
	// vslh v25,v7,v30
	// vaddshs v11,v11,v24
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vslh v24,v7,v29
	// vsubuhm v9,v26,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v26,v8,v13
	// vor128 v15,v72,v72
	_mm_store_si128((__m128i*)v15.u8, _mm_load_si128((__m128i*)v72.u8));
	// vaddshs v24,v24,v25
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vslh v25,v4,v30
	// vsubuhm v26,v9,v26
	_mm_store_si128((__m128i*)v26.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vsubuhm v24,v9,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v9,v1,v2
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v2,v3,v3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vaddshs v5,v5,v24
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vslh v24,v4,v29
	// vsubuhm v26,v26,v27
	_mm_store_si128((__m128i*)v26.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vslh v3,v3,v31
	// vaddshs v24,v24,v25
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vaddshs v6,v6,v26
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vsubuhm v3,v24,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vslh v26,v2,v29
	// vslh v27,v2,v30
	// vslh v4,v4,v31
	// vaddshs v26,v26,v27
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v4,v26,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v8,v9,v4
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsubuhm v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vaddshs v4,v1,v3
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsubuhm v1,v1,v3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vaddshs v24,v8,v5
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v27,v9,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vsubuhm v28,v9,v6
	_mm_store_si128((__m128i*)v28.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vaddshs v26,v1,v11
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubuhm v29,v1,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vaddshs v25,v4,v10
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vsubuhm v30,v4,v10
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsubuhm v31,v8,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v24,v24,v13
	// vsrah v25,v25,v13
	// vsrah v26,v26,v13
	// vsrah v27,v27,v13
	// vsrah v28,v28,v13
	// vsrah v29,v29,v13
	// vmrglh v20,v24,v25
	_mm_store_si128((__m128i*)v20.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v30,v30,v13
	// vmrghh v16,v24,v25
	_mm_store_si128((__m128i*)v16.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v31,v31,v13
	// vmrghh v17,v26,v27
	_mm_store_si128((__m128i*)v17.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrglh v21,v26,v27
	_mm_store_si128((__m128i*)v21.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrglh v22,v28,v29
	_mm_store_si128((__m128i*)v22.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vmrghh v18,v28,v29
	_mm_store_si128((__m128i*)v18.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vmrghh v19,v30,v31
	_mm_store_si128((__m128i*)v19.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmrglh v23,v30,v31
	_mm_store_si128((__m128i*)v23.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vmrghw v24,v16,v17
	_mm_store_si128((__m128i*)v24.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v17.u32), _mm_load_si128((__m128i*)v16.u32)));
	// vmrglw v27,v20,v21
	_mm_store_si128((__m128i*)v27.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v21.u32), _mm_load_si128((__m128i*)v20.u32)));
	// vmrghw v28,v18,v19
	_mm_store_si128((__m128i*)v28.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v19.u32), _mm_load_si128((__m128i*)v18.u32)));
	// vmrglw v31,v22,v23
	_mm_store_si128((__m128i*)v31.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v23.u32), _mm_load_si128((__m128i*)v22.u32)));
	// vmrglw v29,v18,v19
	_mm_store_si128((__m128i*)v29.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v19.u32), _mm_load_si128((__m128i*)v18.u32)));
	// vmrglw v25,v16,v17
	_mm_store_si128((__m128i*)v25.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v17.u32), _mm_load_si128((__m128i*)v16.u32)));
	// vperm v5,v24,v28,v15
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vperm v6,v27,v31,v15
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vmrghw v26,v20,v21
	_mm_store_si128((__m128i*)v26.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v21.u32), _mm_load_si128((__m128i*)v20.u32)));
	// vmrghw v30,v22,v23
	_mm_store_si128((__m128i*)v30.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v23.u32), _mm_load_si128((__m128i*)v22.u32)));
	// vperm v3,v27,v31,v14
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vaddshs v13,v5,v6
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vspltish v27,3
	// vperm v4,v25,v29,v14
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vperm v8,v25,v29,v15
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vspltish v25,1
	// vperm v1,v24,v28,v14
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vslh v9,v13,v27
	// vperm v2,v26,v30,v14
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vslh v10,v6,v27
	// vperm v7,v26,v30,v15
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vspltish v26,2
	// vslh v20,v6,v25
	// vslh v18,v1,v25
	// vspltish v17,8
	// vslh v19,v2,v25
	// vspltish v21,6
	// vsubuhm v9,v9,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vspltish v24,0
	// vslh v1,v1,v26
	// vspltish v28,4
	// vslh v2,v2,v26
	// vslh v29,v17,v26
	// vsubuhm v11,v9,v5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vaddshs v1,v1,v18
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vaddshs v2,v2,v19
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vslh v17,v6,v26
	// vslh v18,v5,v25
	// vslh v19,v5,v25
	// vsubuhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vslh v6,v6,v27
	// vslh v5,v5,v26
	// vslh v9,v13,v25
	// vaddshs v1,v1,v29
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vsubuhm v10,v10,v17
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vaddshs v5,v5,v19
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vsubuhm v6,v9,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsubuhm v11,v11,v18
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vaddshs v12,v7,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v18,v7,v26
	// vslh v17,v7,v25
	// vaddshs v5,v9,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vslh v9,v12,v25
	// vslh v19,v8,v25
	// vaddshs v17,v17,v18
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vsubuhm v6,v6,v20
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vslh v20,v8,v27
	// vsubuhm v19,v9,v19
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vaddshs v17,v9,v17
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vslh v9,v12,v27
	// vslh v18,v4,v27
	// vsubuhm v19,v19,v20
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vsubuhm v10,v10,v17
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vslh v17,v7,v25
	// vsubuhm v9,v9,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vaddshs v11,v11,v19
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vslh v19,v8,v26
	// vaddshs v17,v17,v7
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vslh v7,v3,v26
	// vslh v20,v8,v27
	// vsubuhm v19,v9,v19
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vsubuhm v17,v9,v17
	_mm_store_si128((__m128i*)v17.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vaddshs v9,v1,v2
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v2,v3,v4
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v5,v5,v17
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vaddshs v7,v7,v3
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vslh v17,v4,v25
	// vslh v2,v2,v27
	// vsubuhm v19,v19,v20
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vsrah v23,v12,v25
	// vsrah v22,v13,v25
	// vsubuhm v7,v2,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsubuhm v2,v2,v17
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vaddshs v6,v6,v19
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v5,v5,v23
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v8,v9,v7
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsubuhm v2,v2,v18
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vsubuhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vadduhm v10,v10,v22
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vaddshs v11,v11,v22
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vsubuhm v2,v2,v4
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vaddshs v6,v6,v23
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v24,v8,v5
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubuhm v31,v8,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsubuhm v7,v1,v2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v27,v9,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vsubuhm v28,v9,v6
	_mm_store_si128((__m128i*)v28.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vaddshs v25,v7,v10
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v26,v1,v11
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v24,v24,v21
	// vsubuhm v29,v1,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vsubuhm v30,v7,v10
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsrah v25,v25,v21
	// vsrah v26,v26,v21
	// stvx v24,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v27,v27,v21
	// vsrah v28,v28,v21
	// vsrah v29,v29,v21
	// stvx v25,r9,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v30,v30,v21
	// stvx v26,r10,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v31,v31,v21
	// stvx v27,r11,r4
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v28,r12,r4
	_mm_store_si128((__m128i*)(base + ((r12.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v29,r6,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v30,r8,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v31,r7,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826FFBB0"))) PPC_WEAK_FUNC(sub_826FFBB0);
PPC_FUNC_IMPL(__imp__sub_826FFBB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v69{};
	uint32_t ea{};
	// li r7,32
	ctx.r7.s64 = 32;
	// lvx v5,r0,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r6,16
	ctx.r6.s64 = 16;
	// vspltish v12,4
	// li r8,48
	ctx.r8.s64 = 48;
	// vspltish v10,1
	// vspltish v31,3
	// vor128 v16,v69,v69
	_mm_store_si128((__m128i*)v16.u8, _mm_load_si128((__m128i*)v69.u8));
	// vspltish v11,2
	// li r10,64
	ctx.r10.s64 = 64;
	// lvx v7,r7,r3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v13,5
	// vaddshs v28,v5,v7
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// lvx v6,r6,r3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx v8,r8,r3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubuhm v29,v5,v7
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vaddshs v30,v6,v8
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vspltish v25,1
	// vslh v3,v6,v31
	// vspltish v26,2
	// vslh v1,v28,v12
	// vspltish v27,3
	// vslh v2,v29,v12
	// vspltish v17,8
	// vslh v9,v30,v10
	// vspltish v21,6
	// vslh v30,v30,v31
	// vspltish v24,0
	// vaddshs v1,v1,v28
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v28.s16)));
	// li r11,80
	r11.s64 = 80;
	// vslh v28,v6,v11
	// li r12,96
	r12.s64 = 96;
	// vaddshs v2,v2,v29
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v29.s16)));
	// li r5,112
	ctx.r5.s64 = 112;
	// vaddshs v9,v9,v30
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vslh v4,v8,v13
	// vaddshs v3,v3,v28
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v1,v1,v12
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v2,v2,v12
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsubuhm v4,v9,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vaddshs v3,v9,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v11,v2,v4
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v10,v1,v3
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsubuhm v13,v1,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vsubuhm v12,v2,v4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vsrah v11,v11,v31
	// vsrah v10,v10,v31
	// vsrah v13,v13,v31
	// vsrah v12,v12,v31
	// vmrghh v28,v10,v11
	_mm_store_si128((__m128i*)v28.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrglh v29,v10,v11
	_mm_store_si128((__m128i*)v29.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghh v30,v12,v13
	_mm_store_si128((__m128i*)v30.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrglh v31,v12,v13
	_mm_store_si128((__m128i*)v31.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghw v1,v28,v30
	_mm_store_si128((__m128i*)ctx.v1.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v30.u32), _mm_load_si128((__m128i*)v28.u32)));
	// vmrglw v3,v29,v31
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v31.u32), _mm_load_si128((__m128i*)v29.u32)));
	// vmrghw v2,v29,v31
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v31.u32), _mm_load_si128((__m128i*)v29.u32)));
	// vslh v29,v17,v26
	// vmrglw v4,v28,v30
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v30.u32), _mm_load_si128((__m128i*)v28.u32)));
	// vsldoi v5,v1,v1,8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v1.u8), 8));
	// vslh v18,v1,v25
	// vsldoi v6,v3,v3,8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v3.u8), 8));
	// vslh v1,v1,v26
	// vslh v19,v2,v25
	// vsldoi v7,v2,v2,8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v2.u8), 8));
	// vslh v2,v2,v26
	// vspltish v28,4
	// vsldoi v8,v4,v4,8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v4.u8), 8));
	// vaddshs v13,v5,v6
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v1,v1,v18
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vslh v10,v6,v27
	// vaddshs v2,v2,v19
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vslh v9,v13,v27
	// vslh v17,v6,v26
	// vslh v20,v6,v25
	// vaddshs v1,v1,v29
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vsubuhm v9,v9,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v18,v5,v25
	// vslh v19,v5,v25
	// vslh v6,v6,v27
	// vsubuhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsubuhm v11,v9,v5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vslh v5,v5,v26
	// addi r3,r4,4
	ctx.r3.s64 = ctx.r4.s64 + 4;
	// vsubuhm v10,v10,v17
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vslh v9,v13,v25
	// vsubuhm v11,v11,v18
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vaddshs v5,v5,v19
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v12,v7,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v18,v7,v26
	// vslh v17,v7,v25
	// vsubuhm v6,v9,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vaddshs v5,v9,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vslh v9,v12,v25
	// vaddshs v17,v17,v18
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vslh v19,v8,v25
	// vsubuhm v6,v6,v20
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vslh v20,v8,v27
	// vaddshs v17,v9,v17
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vsubuhm v19,v9,v19
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vslh v9,v12,v27
	// vslh v18,v4,v27
	// vsubuhm v10,v10,v17
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vsubuhm v19,v19,v20
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vslh v17,v7,v25
	// vsubuhm v9,v9,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v20,v8,v27
	// vaddshs v11,v11,v19
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v17,v17,v7
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vslh v19,v8,v26
	// vslh v7,v3,v26
	// vsrah v23,v12,v25
	// vsubuhm v17,v9,v17
	_mm_store_si128((__m128i*)v17.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vsubuhm v19,v9,v19
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vaddshs v9,v1,v2
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v2,v3,v4
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v5,v5,v17
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v17.s16)));
	// vaddshs v7,v7,v3
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vslh v17,v4,v25
	// vslh v2,v2,v27
	// vsrah v22,v13,v25
	// vaddshs v5,v5,v23
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vsubuhm v19,v19,v20
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)v20.u8)));
	// vsubuhm v7,v2,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsubuhm v2,v2,v17
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vadduhm v10,v10,v22
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vaddshs v6,v6,v19
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v8,v9,v7
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsubuhm v2,v2,v18
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vsubuhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vaddshs v6,v6,v23
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v24,v8,v5
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubuhm v2,v2,v4
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vaddshs v11,v11,v22
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vsubuhm v31,v8,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v24,v24,v21
	// vsubuhm v7,v1,v2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v27,v9,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vperm v24,v24,v24,v16
	_mm_store_si128((__m128i*)v24.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vsubuhm v28,v9,v6
	_mm_store_si128((__m128i*)v28.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vaddshs v25,v7,v10
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vsubuhm v30,v7,v10
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vaddshs v26,v1,v11
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubuhm v29,v1,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vsrah v25,v25,v21
	// vsrah v31,v31,v21
	// vsrah v27,v27,v21
	// stvewx v24,r0,r4
	ea = (ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v24.u32[3 - ((ea & 0xF) >> 2)]);
	// vsrah v28,v28,v21
	// stvewx v24,r0,r3
	ea = (ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v24.u32[3 - ((ea & 0xF) >> 2)]);
	// vperm v25,v25,v25,v16
	_mm_store_si128((__m128i*)v25.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vsrah v30,v30,v21
	// vsrah v26,v26,v21
	// vperm v31,v31,v31,v16
	_mm_store_si128((__m128i*)v31.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vsrah v29,v29,v21
	// vperm v27,v27,v27,v16
	_mm_store_si128((__m128i*)v27.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vperm v28,v28,v28,v16
	_mm_store_si128((__m128i*)v28.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vperm v30,v30,v30,v16
	_mm_store_si128((__m128i*)v30.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vperm v26,v26,v26,v16
	_mm_store_si128((__m128i*)v26.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v16.u8)));
	// vperm v29,v29,v29,v16
	_mm_store_si128((__m128i*)v29.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v16.u8)));
	// stvewx v25,r6,r4
	ea = (ctx.r6.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v25.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v25,r6,r3
	ea = (ctx.r6.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v25.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v26,r7,r4
	ea = (ctx.r7.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v26,r7,r3
	ea = (ctx.r7.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v27,r8,r4
	ea = (ctx.r8.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v27,r8,r3
	ea = (ctx.r8.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v28,r10,r4
	ea = (ctx.r10.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v28,r10,r3
	ea = (ctx.r10.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v29,r11,r4
	ea = (r11.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v29,r11,r3
	ea = (r11.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v30,r12,r4
	ea = (r12.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v30,r12,r3
	ea = (r12.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v31,r5,r4
	ea = (ctx.r5.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v31.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v31,r5,r3
	ea = (ctx.r5.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v31.u32[3 - ((ea & 0xF) >> 2)]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826FFE70"))) PPC_WEAK_FUNC(sub_826FFE70);
PPC_FUNC_IMPL(__imp__sub_826FFE70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v69{};
	PPCVRegister v72{};
	// li r11,48
	r11.s64 = 48;
	// vspltish v30,2
	// li r10,32
	ctx.r10.s64 = 32;
	// lvx v1,r0,r3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v13,3
	// li r9,16
	ctx.r9.s64 = 16;
	// vsldoi v5,v1,v1,8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v1.u8), 8));
	// vor128 v14,v69,v69
	_mm_store_si128((__m128i*)v14.u8, _mm_load_si128((__m128i*)v69.u8));
	// vspltish v31,4
	// vslh v25,v1,v30
	// lvx v3,r11,r3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v29,1
	// vsldoi v6,v3,v3,8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v3.u8), 8));
	// lvx v2,r10,r3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v7,v2,v2,8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v2.u8), 8));
	// vslh v24,v2,v30
	// vslh v2,v2,v13
	// lvx v4,r9,r3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v26,v5,v29
	// vsldoi v8,v4,v4,8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v4.u8), 8));
	// vaddshs v12,v5,v6
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vslh v11,v5,v30
	// vaddshs v2,v2,v24
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vslh v1,v1,v13
	// vslh v24,v12,v31
	// vaddshs v11,v26,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vslh v10,v6,v31
	// vaddshs v1,v1,v25
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vsubuhm v9,v24,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v25,v6,v13
	// vslh v27,v5,v30
	// vslh v24,v6,v30
	// vsubuhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsubuhm v11,v9,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vslh v9,v12,v30
	// vslh v5,v5,v13
	// vslh v6,v6,v31
	// vaddshs v12,v7,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v26,v7,v30
	// vaddshs v5,v27,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubuhm v6,v9,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vslh v27,v7,v13
	// vsubuhm v10,v10,v25
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vaddshs v5,v9,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubuhm v6,v6,v24
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vslh v9,v12,v30
	// vslh v24,v8,v30
	// vaddshs v26,v26,v27
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vslh v25,v8,v31
	// vaddshs v1,v1,v31
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsubuhm v24,v9,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v26,v9,v26
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vslh v27,v8,v31
	// vor128 v15,v72,v72
	_mm_store_si128((__m128i*)v15.u8, _mm_load_si128((__m128i*)v72.u8));
	// vsubuhm v24,v24,v25
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vsubuhm v10,v10,v26
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vslh v26,v12,v31
	// vslh v25,v7,v30
	// vaddshs v11,v11,v24
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vslh v24,v7,v29
	// vsubuhm v9,v26,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vslh v26,v8,v13
	// vaddshs v24,v24,v25
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vslh v25,v4,v30
	// vsubuhm v26,v9,v26
	_mm_store_si128((__m128i*)v26.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vsubuhm v24,v9,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vaddshs v9,v1,v2
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v26,v26,v27
	_mm_store_si128((__m128i*)v26.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vsubuhm v1,v1,v2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v2,v3,v3
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vaddshs v5,v5,v24
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vaddshs v6,v6,v26
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vslh v24,v4,v29
	// vslh v26,v2,v29
	// vslh v27,v2,v30
	// vslh v4,v4,v31
	// vaddshs v24,v24,v25
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vslh v3,v3,v31
	// vaddshs v26,v26,v27
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vsubuhm v3,v24,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vaddshs v4,v26,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v8,v9,v4
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vspltish v12,3
	// vsubuhm v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vaddshs v4,v1,v3
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsubuhm v1,v1,v3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vaddshs v24,v8,v5
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubuhm v31,v8,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vaddshs v25,v4,v10
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vsubuhm v30,v4,v10
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vspltish v10,1
	// vaddshs v26,v1,v11
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubuhm v29,v1,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vspltish v11,2
	// vaddshs v27,v9,v6
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vsubuhm v28,v9,v6
	_mm_store_si128((__m128i*)v28.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vspltish v9,6
	// vsrah v30,v30,v13
	// vsrah v26,v26,v13
	// vsrah v29,v29,v13
	// vsrah v28,v28,v13
	// vsrah v31,v31,v13
	// vsrah v24,v24,v13
	// vsrah v25,v25,v13
	// vsrah v27,v27,v13
	// vmrghh v18,v28,v29
	_mm_store_si128((__m128i*)v18.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vmrghh v19,v30,v31
	_mm_store_si128((__m128i*)v19.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vspltish v30,8
	// vmrghh v16,v24,v25
	_mm_store_si128((__m128i*)v16.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vmrghh v17,v26,v27
	_mm_store_si128((__m128i*)v17.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrghw v28,v18,v19
	_mm_store_si128((__m128i*)v28.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v19.u32), _mm_load_si128((__m128i*)v18.u32)));
	// vmrglw v29,v18,v19
	_mm_store_si128((__m128i*)v29.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v19.u32), _mm_load_si128((__m128i*)v18.u32)));
	// vslh v1,v30,v11
	// vspltish v13,4
	// vmrghw v24,v16,v17
	_mm_store_si128((__m128i*)v24.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v17.u32), _mm_load_si128((__m128i*)v16.u32)));
	// vmrglw v25,v16,v17
	_mm_store_si128((__m128i*)v25.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v17.u32), _mm_load_si128((__m128i*)v16.u32)));
	// vperm v5,v24,v28,v15
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vperm v4,v24,v28,v14
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vperm v6,v25,v29,v14
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v14.u8)));
	// vperm v7,v25,v29,v15
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v15.u8)));
	// vslh v26,v5,v11
	// vslh v30,v5,v13
	// vsubuhm v3,v4,v6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vaddshs v6,v6,v4
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vslh v28,v7,v11
	// vaddshs v26,v26,v5
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vslh v31,v7,v13
	// vslh v4,v6,v12
	// vaddshs v28,v28,v7
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vslh v8,v3,v12
	// vsubuhm v27,v30,v26
	_mm_store_si128((__m128i*)v27.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vsrah v30,v6,v10
	// vaddshs v4,v4,v1
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsubuhm v29,v31,v28
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vsrah v31,v3,v10
	// vaddshs v8,v8,v1
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v2,v27,v28
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v4,v4,v30
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vsubuhm v5,v26,v29
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vaddshs v8,v8,v31
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v10,v4,v2
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v13,v4,v2
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v11,v8,v5
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubuhm v12,v8,v5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v10,v10,v9
	// vsrah v13,v13,v9
	// vsrah v11,v11,v9
	// vsrah v12,v12,v9
	// stvx v10,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r11,r4
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r9,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r10,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_827000D4"))) PPC_WEAK_FUNC(sub_827000D4);
PPC_FUNC_IMPL(__imp__sub_827000D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827000D8"))) PPC_WEAK_FUNC(sub_827000D8);
PPC_FUNC_IMPL(__imp__sub_827000D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v69{};
	uint32_t ea{};
	// li r9,16
	ctx.r9.s64 = 16;
	// lvx v5,r0,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v13,4
	// li r10,32
	ctx.r10.s64 = 32;
	// vsldoi v6,v5,v5,8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v5.u8), 8));
	// li r11,48
	r11.s64 = 48;
	// vspltish v10,1
	// addi r12,r4,4
	r12.s64 = ctx.r4.s64 + 4;
	// vspltish v12,3
	// lvx v7,r9,r3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v11,2
	// vaddshs v28,v5,v7
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsldoi v8,v7,v7,8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v7.u8), 8));
	// vsubuhm v29,v5,v7
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vspltish v14,5
	// vslh v3,v6,v12
	// vslh v1,v28,v13
	// vaddshs v30,v6,v8
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v2,v29,v13
	// vslh v4,v8,v14
	// vaddshs v1,v1,v28
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vslh v9,v30,v10
	// vslh v30,v30,v12
	// vslh v28,v6,v11
	// vaddshs v2,v2,v29
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v1,v1,v13
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v9,v9,v30
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v3,v3,v28
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v2,v2,v13
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vsubuhm v4,v9,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vaddshs v3,v9,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vspltish v9,6
	// vaddshs v21,v2,v4
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsubuhm v22,v2,v4
	_mm_store_si128((__m128i*)v22.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vaddshs v20,v1,v3
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsubuhm v23,v1,v3
	_mm_store_si128((__m128i*)v23.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vsrah v21,v21,v12
	// vsrah v22,v22,v12
	// vsrah v20,v20,v12
	// vsrah v23,v23,v12
	// vmrghh v28,v20,v21
	_mm_store_si128((__m128i*)v28.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)v20.u16)));
	// vmrghh v30,v22,v23
	_mm_store_si128((__m128i*)v30.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vmrghw v4,v28,v30
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v30.u32), _mm_load_si128((__m128i*)v28.u32)));
	// vmrglw v6,v28,v30
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v30.u32), _mm_load_si128((__m128i*)v28.u32)));
	// vspltish v30,8
	// vsldoi v5,v4,v4,8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v4.u8), 8));
	// vsldoi v7,v6,v6,8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v6.u8), 8));
	// vsubuhm v3,v4,v6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vaddshs v6,v6,v4
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vslh v1,v30,v11
	// vslh v26,v5,v11
	// vslh v28,v7,v11
	// vslh v30,v5,v13
	// vslh v31,v7,v13
	// vaddshs v26,v26,v5
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v28,v28,v7
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vslh v4,v6,v12
	// vslh v8,v3,v12
	// vsubuhm v27,v30,v26
	_mm_store_si128((__m128i*)v27.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vsubuhm v29,v31,v28
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vsrah v30,v6,v10
	// vsrah v31,v3,v10
	// vaddshs v4,v4,v1
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v8,v8,v1
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v2,v27,v28
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vsubuhm v5,v26,v29
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vaddshs v4,v4,v30
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v8,v8,v31
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vor128 v1,v69,v69
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v69.u8));
	// vaddshs v10,v4,v2
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsubuhm v13,v4,v2
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vaddshs v11,v8,v5
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubuhm v12,v8,v5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v10,v10,v9
	// vsrah v13,v13,v9
	// vsrah v11,v11,v9
	// vsrah v12,v12,v9
	// vperm v10,v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vperm v13,v13,v13,v1
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vperm v11,v11,v11,v1
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vperm v12,v12,v12,v1
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// stvewx v10,r0,r4
	ea = (ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v10,r0,r12
	ea = (r12.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v13,r11,r4
	ea = (r11.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v13,r11,r12
	ea = (r11.u32 + r12.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v11,r9,r4
	ea = (ctx.r9.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v11,r9,r12
	ea = (ctx.r9.u32 + r12.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v12,r10,r4
	ea = (ctx.r10.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v12,r10,r12
	ea = (ctx.r10.u32 + r12.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82700240"))) PPC_WEAK_FUNC(sub_82700240);
PPC_FUNC_IMPL(__imp__sub_82700240) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// vspltisb v31,0
	_mm_store_si128((__m128i*)v31.u8, _mm_set1_epi8(char(0x0)));
	// li r8,16
	ctx.r8.s64 = 16;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x827002f8
	if (!cr6.eq) goto loc_827002F8;
	// rlwinm r6,r4,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r12,r4,2,0,29
	r12.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// li r7,8
	ctx.r7.s64 = 8;
loc_8270025C:
	// add r10,r3,r6
	ctx.r10.u64 = ctx.r3.u64 + ctx.r6.u64;
	// add r9,r3,r4
	ctx.r9.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r11,r10,r4
	r11.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvlx v1,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v2,r3,r8
	temp.u32 = ctx.r3.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v10,r9,r8
	temp.u32 = ctx.r9.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v18,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v19,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)v19.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v23,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v24,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)v24.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v3,v1,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v11,v9,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vor v20,v18,v19
	_mm_store_si128((__m128i*)v20.u8, _mm_or_si128(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vor v25,v23,v24
	_mm_store_si128((__m128i*)v25.u8, _mm_or_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)v24.u8)));
	// add r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 + r12.u64;
	// vmrghb v4,v31,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrglb v5,v31,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrghb v12,v31,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrglb v13,v31,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrghb v21,v31,v20
	_mm_store_si128((__m128i*)v21.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrglb v22,v31,v20
	_mm_store_si128((__m128i*)v22.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrghb v26,v31,v25
	_mm_store_si128((__m128i*)v26.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrglb v27,v31,v25
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v31.u8)));
	// addi r9,r5,48
	ctx.r9.s64 = ctx.r5.s64 + 48;
	// addi r10,r5,96
	ctx.r10.s64 = ctx.r5.s64 + 96;
	// addi r11,r5,144
	r11.s64 = ctx.r5.s64 + 144;
	// stvx v4,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r5,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,192
	ctx.r5.s64 = ctx.r5.s64 + 192;
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r9,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v21,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v22,r10,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v26,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v27,r11,r8
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addic. r7,r7,-4
	xer.ca = ctx.r7.u32 > 3;
	ctx.r7.s64 = ctx.r7.s64 + -4;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x8270025c
	if (!cr0.eq) goto loc_8270025C;
	// blr 
	return;
loc_827002F8:
	// li r9,2
	ctx.r9.s64 = 2;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// li r9,32
	ctx.r9.s64 = 32;
	// add r11,r4,r4
	r11.u64 = ctx.r4.u64 + ctx.r4.u64;
	// lvsl v28,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
loc_82700310:
	// add r10,r3,r4
	ctx.r10.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvx128 v1,r0,r3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v2,r3,r8
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v3,r3,r9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// addi r12,r5,48
	r12.s64 = ctx.r5.s64 + 48;
	// lvx128 v21,r0,r10
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v22,r10,r8
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v23,r10,r9
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v1,v2,v28
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vperm v8,v2,v3,v28
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vperm v26,v21,v22,v28
	_mm_store_si128((__m128i*)v26.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vperm v27,v22,v23,v28
	_mm_store_si128((__m128i*)v27.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vmrghb v5,v31,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrglb v6,v31,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrghb v8,v31,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrghb v24,v31,v26
	_mm_store_si128((__m128i*)v24.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrglb v25,v31,v26
	_mm_store_si128((__m128i*)v25.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vmrghb v27,v31,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v31.u8)));
	// addic. r7,r7,-2
	xer.ca = ctx.r7.u32 > 1;
	ctx.r7.s64 = ctx.r7.s64 + -2;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stvx v5,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v6,r5,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r5,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,96
	ctx.r5.s64 = ctx.r5.s64 + 96;
	// stvx v24,r0,r12
	_mm_store_si128((__m128i*)(base + ((r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v25,r12,r8
	_mm_store_si128((__m128i*)(base + ((r12.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v27,r12,r9
	_mm_store_si128((__m128i*)(base + ((r12.u32 + ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne 0x82700310
	if (!cr0.eq) goto loc_82700310;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82700384"))) PPC_WEAK_FUNC(sub_82700384);
PPC_FUNC_IMPL(__imp__sub_82700384) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82700388"))) PPC_WEAK_FUNC(sub_82700388);
PPC_FUNC_IMPL(__imp__sub_82700388) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCRegister temp{};
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// vspltish v12,8
	// vspltish v11,-1
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// vspltish v3,1
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// vslh v23,v11,v12
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// vspltish v0,2
	// stvx v12,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v10,4
	// lvsl v6,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vspltish v31,5
	// vor v4,v13,v13
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// lvsl v5,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// stvx v11,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// stvx v7,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvx128 v30,r0,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v29,r0,r8
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x827004c4
	if (!cr6.eq) goto loc_827004C4;
	// vperm v9,v30,v29,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vmrghb v8,v13,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v12,v13,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v13,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x827005dc
	if (!cr6.gt) goto loc_827005DC;
	// li r8,0
	ctx.r8.s64 = 0;
loc_82700438:
	// vslh v6,v12,v10
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v7,v12,v31
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vslh v5,v11,v3
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vslh v9,v12,v0
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vadduhm v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vslh v6,v11,v10
	// vadduhm v9,v9,v12
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// lvx128 v29,r0,r7
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v5,v8,v0
	// vor v8,v12,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vor v12,v11,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubshs v30,v13,v5
	// lvsl v5,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v11,v11,v29,v5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vadduhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrghb v11,v13,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v9,v9,v2
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v7,v11,v0
	// vsubshs v7,v11,v7
	// vadduhm v7,v7,v30
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vadduhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsrah v9,v9,v1
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v4,v4,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x82700438
	if (cr6.lt) goto loc_82700438;
	// b 0x827005dc
	goto loc_827005DC;
loc_827004C4:
	// vperm v8,v30,v29,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vmrghb v30,v13,v12
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v29,v13,v12
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v12,v13,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v9,v13,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v11,v13,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x827005dc
	if (!cr6.gt) goto loc_827005DC;
	// li r8,0
	ctx.r8.s64 = 0;
loc_827004EC:
	// vslh v5,v12,v10
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v6,v12,v31
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vslh v28,v9,v3
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vslh v7,v12,v0
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v29,v29,v0
	// vadduhm v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvx128 v27,r0,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v5,v9,v10
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vadduhm v7,v7,v12
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// lvx128 v26,r0,r7
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v24,v13,v29
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// vor v29,v11,v11
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vadduhm v28,v28,v5
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v30,v0
	// vadduhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vsubshs v30,v13,v5
	// vslh v5,v11,v0
	// vadduhm v25,v5,v11
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvsl v5,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v6,v27,v26,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vadduhm v5,v7,v28
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v27,v8,v3
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v5,v5,v2
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v28,v7,v0
	// vsubshs v28,v7,v28
	// vadduhm v30,v28,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v28,v11,v10
	// vadduhm v5,v5,v30
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v30,v11,v31
	// vor v11,v8,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vsrah v5,v5,v1
	// vadduhm v28,v28,v30
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v30,v8,v10
	// vor v8,v6,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v27,v27,v30
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v30,v6,v0
	// vsubshs v26,v6,v30
	// vor v30,v12,v12
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vor v12,v9,v9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vadduhm v7,v25,v28
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vor v6,v4,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vadduhm v28,v26,v24
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v5,v7,v27
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v5,v5,v2
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v7,v5,v28
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vsrah v7,v7,v1
	// vor v4,v6,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x827004ec
	if (cr6.lt) goto loc_827004EC;
loc_827005DC:
	// vand v0,v4,v23
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vcmpgtuh. v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_827005F4"))) PPC_WEAK_FUNC(sub_827005F4);
PPC_FUNC_IMPL(__imp__sub_827005F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827005F8"))) PPC_WEAK_FUNC(sub_827005F8);
PPC_FUNC_IMPL(__imp__sub_827005F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// vspltish v13,8
	// vspltish v12,-1
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// vspltish v10,3
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// vslh v29,v12,v13
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// vor v11,v0,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// stvx v13,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v13,v13,v12,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v6,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// stvx v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v12,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// stvx v7,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v8,v5,v4,v3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// bne cr6,0x82700750
	if (!cr6.eq) goto loc_82700750;
	// vmrghb v6,v0,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vmrghb v9,v0,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v13,v0,v8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// ble cr6,0x82700814
	if (!cr6.gt) goto loc_82700814;
	// li r8,0
	ctx.r8.s64 = 0;
loc_8270069C:
	// vadduhm v12,v9,v13
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// vslh v4,v12,v10
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// lvx128 v7,r0,r7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vadduhm v12,v12,v4
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// lvsl v5,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v30,v12,v2
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vperm v12,v8,v7,v5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvx128 v31,r0,r7
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,48
	ctx.r7.s64 = ctx.r10.s64 + 48;
	// vperm v8,v4,v31,v3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v7,v13,v12
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v6,v6,v12
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v4,v7,v10
	// vsubshs v6,v0,v6
	// vsubshs v5,v0,v9
	// vadduhm v9,v7,v4
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v4,v30,v6
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vor v6,v13,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vor v13,v8,v8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vadduhm v7,v9,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vor v9,v12,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vsrah v12,v4,v1
	// vadduhm v8,v7,v5
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vor v11,v11,v12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// vsrah v8,v8,v1
	// stvx v8,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v11,v11,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// blt cr6,0x8270069c
	if (cr6.lt) goto loc_8270069C;
	// b 0x82700814
	goto loc_82700814;
loc_82700750:
	// vmrghb v5,v0,v13
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vmrglb v4,v0,v13
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v13,v0,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// ble cr6,0x82700814
	if (!cr6.gt) goto loc_82700814;
	// li r8,0
	ctx.r8.s64 = 0;
loc_82700774:
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v31,v5,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vor v30,v4,v4
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vadduhm v6,v12,v8
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v7,v13,v9
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vor v5,v13,v13
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vor v4,v12,v12
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vor v12,v8,v8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vor v13,v9,v9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v28,v7,v10
	// vperm v8,v9,v8,v3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vslh v27,v6,v10
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// vadduhm v7,v7,v28
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v6,v6,v27
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vmrglb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v7,v7,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v3,v31,v9
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v31,v30,v8
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v6,v6,v2
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vsubshs v3,v0,v3
	// vsubshs v31,v0,v31
	// vadduhm v7,v7,v3
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v6,v6,v31
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vsrah v7,v7,v1
	// vsrah v6,v6,v1
	// vor v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx v7,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// stvx v6,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v11,v11,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// blt cr6,0x82700774
	if (cr6.lt) goto loc_82700774;
loc_82700814:
	// vand v13,v11,v29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vcmpgtuh. v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8270082C"))) PPC_WEAK_FUNC(sub_8270082C);
PPC_FUNC_IMPL(__imp__sub_8270082C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82700830"))) PPC_WEAK_FUNC(sub_82700830);
PPC_FUNC_IMPL(__imp__sub_82700830) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// vspltish v12,8
	// vspltish v11,-1
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// vspltish v30,1
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// vslh v23,v11,v12
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// vspltish v0,2
	// stvx v12,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v10,4
	// lvsl v6,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vspltish v29,5
	// vor v31,v13,v13
	_mm_store_si128((__m128i*)v31.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// stvx v11,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// stvx v7,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x8270096c
	if (!cr6.eq) goto loc_8270096C;
	// vperm v8,v5,v4,v3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vmrghb v9,v13,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v13,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v12,v13,v8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82700a84
	if (!cr6.gt) goto loc_82700A84;
	// li r8,0
	ctx.r8.s64 = 0;
loc_827008E0:
	// vslh v6,v12,v10
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v7,v12,v29
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vslh v5,v11,v30
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vslh v4,v9,v0
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v8,v12,v0
	// vadduhm v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v6,v11,v10
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vadduhm v8,v8,v12
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vsubshs v5,v9,v4
	// lvx128 v4,r0,r7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v9,v11,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v12,v12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v12,v12,v4,v3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vadduhm v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghb v12,v13,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v8,v8,v6
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v7,v12,v0
	// vadduhm v8,v8,v2
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vsubshs v7,v13,v7
	// vadduhm v7,v5,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsrah v8,v8,v1
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v31,v31,v8
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x827008e0
	if (cr6.lt) goto loc_827008E0;
	// b 0x82700a84
	goto loc_82700A84;
loc_8270096C:
	// vperm v5,v5,v4,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vmrghb v7,v13,v12
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v12
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v12,v13,v5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v11,v13,v5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82700a84
	if (!cr6.gt) goto loc_82700A84;
	// li r8,0
	ctx.r8.s64 = 0;
loc_82700994:
	// vslh v3,v12,v10
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v4,v12,v29
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vslh v28,v9,v30
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vslh v27,v7,v0
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v5,v12,v0
	// vadduhm v4,v3,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vslh v3,v9,v10
	// lvx128 v26,r0,r7
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// vadduhm v5,v5,v12
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v3,v28,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vsubshs v28,v7,v27
	// lvx128 v27,r0,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v7,v11,v0
	// vadduhm v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v25,v7,v11
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v7,v27,v26,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vadduhm v3,v5,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v27,v8,v30
	// vmrghb v5,v13,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmrglb v4,v13,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v7,v5,v0
	// vslh v24,v4,v0
	// vsubshs v7,v13,v7
	// vsubshs v24,v13,v24
	// vadduhm v7,v28,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v28,v11,v10
	// vadduhm v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v3,v11,v29
	// vadduhm v28,v28,v3
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v8,v10
	// vadduhm v27,v27,v3
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v6,v0
	// vsubshs v26,v6,v3
	// vsrah v3,v7,v1
	// vor v7,v9,v9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v12,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vor v12,v5,v5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vadduhm v5,v25,v28
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v28.u16)));
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v6,v8,v8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vor v8,v11,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v4,v4
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vor v4,v31,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vadduhm v3,v5,v27
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v28,v26,v24
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v5,v3,v28
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vsrah v5,v5,v1
	// vor v31,v4,v5
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// stvx v5,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82700994
	if (cr6.lt) goto loc_82700994;
loc_82700A84:
	// vand v0,v31,v23
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vcmpgtuh. v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82700A9C"))) PPC_WEAK_FUNC(sub_82700A9C);
PPC_FUNC_IMPL(__imp__sub_82700A9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82700AA0"))) PPC_WEAK_FUNC(sub_82700AA0);
PPC_FUNC_IMPL(__imp__sub_82700AA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// cntlzw r11,r8
	r11.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// vspltish v13,3
	// addi r8,r1,-96
	ctx.r8.s64 = ctx.r1.s64 + -96;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r6,3
	cr6.compare<int32_t>(ctx.r6.s32, 3, xer);
	// stw r10,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r10.u32);
	// addi r9,r1,-112
	ctx.r9.s64 = ctx.r1.s64 + -112;
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// stvx v13,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,1
	ctx.r9.s64 = 1;
	// slw r8,r9,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// stw r8,-124(r1)
	PPC_STORE_U32(ctx.r1.u32 + -124, ctx.r8.u32);
	// bne cr6,0x82700c28
	if (!cr6.eq) goto loc_82700C28;
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// lvsl v6,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lvx128 v4,r0,r7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v5,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v9,v9,v4,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-96
	ctx.r9.s64 = ctx.r1.s64 + -96;
	// vperm v10,v10,v8,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-96
	ctx.r9.s64 = ctx.r1.s64 + -96;
	// vmrghb v8,v0,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v7,v0,v10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v6,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-96
	ctx.r9.s64 = ctx.r1.s64 + -96;
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v12,v11,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// addi r9,r1,-96
	ctx.r9.s64 = ctx.r1.s64 + -96;
	// vmrghb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v10,v0,v11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v11,v0,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// ble cr6,0x82700e00
	if (!cr6.gt) goto loc_82700E00;
	// li r9,0
	ctx.r9.s64 = 0;
loc_82700B7C:
	// vor v6,v12,v12
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v12,v11,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// vor v11,v8,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vor v30,v10,v10
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// vor v10,v9,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vadduhm v8,v12,v11
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v4,r0,r7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// vperm v7,v5,v4,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// vadduhm v29,v10,v9
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v31,v8,v13
	// vadduhm v5,v8,v31
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vmrghb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v6,v6,v8
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v5,v5,v2
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v3,v30,v7
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubshs v4,v0,v6
	// vor v6,v29,v29
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v29.u8));
	// vsubshs v3,v0,v3
	// vadduhm v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vslh v4,v6,v13
	// vsrah v5,v5,v1
	// vadduhm v6,v6,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v6,v6,v2
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v6,v6,v3
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vsrah v6,v6,v1
	// stvx v6,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82700b7c
	if (cr6.lt) goto loc_82700B7C;
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82700C28:
	// li r11,16
	r11.s64 = 16;
	// stw r3,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, ctx.r3.u32);
	// li r9,32
	ctx.r9.s64 = 32;
	// lvlx v9,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,-64
	r30.s64 = ctx.r1.s64 + -64;
	// li r6,16
	ctx.r6.s64 = 16;
	// li r7,32
	ctx.r7.s64 = 32;
	// lvlx v11,r3,r11
	temp.u32 = ctx.r3.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r31,16
	r31.s64 = 16;
	// lvrx v12,r3,r11
	temp.u32 = ctx.r3.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvrx v10,r3,r9
	temp.u32 = ctx.r3.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r3,r1,-80
	ctx.r3.s64 = ctx.r1.s64 + -80;
	// vor v12,v9,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// li r9,16
	ctx.r9.s64 = 16;
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// li r5,16
	ctx.r5.s64 = 16;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r11,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, r11.u32);
	// stvx v12,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v7,v0,v12
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v11,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v10,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r1,-48
	ctx.r9.s64 = ctx.r1.s64 + -48;
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r3,32
	ctx.r3.s64 = 32;
	// lvrx v12,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v12,v6,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v9,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v10,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// vmrghb v3,v0,v12
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, r11.u32);
	// vmrglb v31,v0,v12
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v12,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v6,r11,r5
	temp.u32 = r11.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v5,v10,v12
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v4,r11,r3
	temp.u32 = r11.u32 + ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v8,v8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v4,v6,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vor v8,v7,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v10,v11,v11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vmrghb v6,v0,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v11,v3,v3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vmrglb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v7,v31,v31
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v31.u8));
	// vmrghb v4,v0,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// ble cr6,0x82700e00
	if (!cr6.gt) goto loc_82700E00;
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
loc_82700D0C:
	// vor v31,v12,v12
	_mm_store_si128((__m128i*)v31.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v12,v11,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// vor v11,v6,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vor v26,v8,v8
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// addi r6,r11,32
	ctx.r6.s64 = r11.s64 + 32;
	// vor v8,v7,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// vor v7,v5,v5
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// lvx128 v30,r0,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v6,v12,v11
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v5,r0,r7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v27,v10,v10
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v9,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// lvx128 v29,r0,r6
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v9,v4,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// lvsl v4,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v28,v6,v13
	// addi r7,r10,-32
	ctx.r7.s64 = ctx.r10.s64 + -32;
	// vadduhm v25,v8,v7
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// addi r6,r10,-16
	ctx.r6.s64 = ctx.r10.s64 + -16;
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// vadduhm v6,v6,v28
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v28,v6,v2
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vperm v6,v30,v5,v3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vperm v5,v5,v29,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrglb v3,v0,v6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v30,v0,v5
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v4,v0,v6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v5,v3,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vor v3,v25,v25
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)v25.u8));
	// vor v6,v4,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vor v4,v30,v30
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)v30.u8));
	// vadduhm v29,v26,v5
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v30,v3,v13
	// vadduhm v31,v31,v6
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v26,v10,v9
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubshs v29,v0,v29
	// vadduhm v3,v3,v30
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vsubshs v31,v0,v31
	// vadduhm v30,v3,v2
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vor v3,v26,v26
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)v26.u8));
	// vadduhm v31,v28,v31
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v28,v27,v4
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v30,v30,v29
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v29,v3,v13
	// vsrah v31,v31,v1
	// vadduhm v3,v3,v29
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v29.u16)));
	// stvx v31,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v31,v0,v28
	// vadduhm v3,v3,v2
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v3,v3,v31
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vsrah v3,v3,v1
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v3,v30,v1
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// stvx v3,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82700d0c
	if (cr6.lt) goto loc_82700D0C;
loc_82700E00:
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82700E10"))) PPC_WEAK_FUNC(sub_82700E10);
PPC_FUNC_IMPL(__imp__sub_82700E10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	// vspltish v12,-1
	// addi r10,r4,1
	ctx.r10.s64 = ctx.r4.s64 + 1;
	// vspltish v0,8
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// vspltisb v3,0
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_set1_epi8(char(0x0)));
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// vspltish v31,1
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// vspltish v13,2
	// vslh v28,v12,v0
	// vspltish v30,5
	// vspltish v12,4
	// vspltish v8,0
	// bne cr6,0x82700ecc
	if (!cr6.eq) goto loc_82700ECC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82700fb4
	if (!cr6.gt) goto loc_82700FB4;
loc_82700E50:
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v7,v0,v13
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v11,v0,v10,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v9,v0,v10,4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 12));
	// vsldoi v0,v0,v10,6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 10));
	// vsubshs v10,v3,v7
	// vslh v7,v11,v13
	// vslh v6,v11,v30
	// vslh v5,v11,v12
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v7,v5,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v9,v12
	// vslh v9,v9,v31
	// vadduhm v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v0,v13
	// vadduhm v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubshs v0,v0,v6
	// vadduhm v11,v11,v2
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v0,v0,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v0,v11,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsrah v0,v0,v1
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r11,48
	r11.s64 = r11.s64 + 48;
	// bne cr6,0x82700e50
	if (!cr6.eq) goto loc_82700E50;
	// b 0x82700fb4
	goto loc_82700FB4;
loc_82700ECC:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82700fb4
	if (!cr6.gt) goto loc_82700FB4;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
loc_82700ED8:
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r11,-16
	ctx.r9.s64 = r11.s64 + -16;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v10,v0,v9,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vsldoi v7,v0,v9,4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v6,v0,v9,6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vsldoi v9,v11,v0,2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v5,v11,v0,4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vsldoi v4,v11,v0,6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vslh v11,v11,v13
	// vslh v29,v9,v13
	// vslh v27,v9,v30
	// vslh v26,v9,v12
	// vsubshs v11,v3,v11
	// vadduhm v9,v29,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v29,v26,v27
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v27,v5,v12
	// vslh v5,v5,v31
	// vadduhm v9,v9,v29
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v29,v10,v12
	// vadduhm v5,v5,v27
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v27,v4,v13
	// vadduhm v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vsubshs v4,v4,v27
	// vadduhm v5,v9,v2
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v11,v4,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vslh v9,v10,v13
	// vslh v4,v10,v30
	// vadduhm v11,v5,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v9,v29,v4
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vslh v4,v7,v12
	// vslh v7,v7,v31
	// vslh v29,v0,v13
	// vadduhm v9,v10,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v7,v7,v4
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vslh v4,v6,v13
	// vadduhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubshs v0,v6,v4
	// vsubshs v6,v3,v29
	// vadduhm v10,v0,v6
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vsrah v0,v11,v1
	// vadduhm v11,v9,v2
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vor v9,v8,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v0,v11,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsrah v0,v0,v1
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v8,v9,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r11,48
	r11.s64 = r11.s64 + 48;
	// bne cr6,0x82700ed8
	if (!cr6.eq) goto loc_82700ED8;
loc_82700FB4:
	// vand v0,v8,v28
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vcmpgtuh. v0,v0,v3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82700FCC"))) PPC_WEAK_FUNC(sub_82700FCC);
PPC_FUNC_IMPL(__imp__sub_82700FCC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82700FD0"))) PPC_WEAK_FUNC(sub_82700FD0);
PPC_FUNC_IMPL(__imp__sub_82700FD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// vspltish v0,8
	// addi r10,r4,1
	ctx.r10.s64 = ctx.r4.s64 + 1;
	// vspltish v13,-1
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// vspltisb v10,0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_set1_epi8(char(0x0)));
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// vspltish v9,3
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// vspltish v11,0
	// vslh v3,v13,v0
	// bne cr6,0x82701058
	if (!cr6.eq) goto loc_82701058;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x827010f4
	if (!cr6.gt) goto loc_827010F4;
loc_82701004:
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v12,v0,v13,2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 14));
	// vsldoi v8,v0,v13,4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 12));
	// vsldoi v7,v0,v13,6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 10));
	// vadduhm v13,v12,v8
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v12,v13,v9
	// vsubshs v0,v10,v0
	// vadduhm v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsrah v0,v0,v1
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r11,48
	r11.s64 = r11.s64 + 48;
	// bne cr6,0x82701004
	if (!cr6.eq) goto loc_82701004;
	// b 0x827010f4
	goto loc_827010F4;
loc_82701058:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x827010f4
	if (!cr6.gt) goto loc_827010F4;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
loc_82701064:
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r11,-16
	ctx.r9.s64 = r11.s64 + -16;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v8,v0,v12,2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 14));
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v7,v0,v12,4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 12));
	// vsldoi v12,v0,v12,6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 10));
	// vsldoi v6,v13,v0,2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v5,v13,v0,4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vadduhm v7,v8,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsldoi v4,v13,v0,6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vadduhm v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v0,v6,v5
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v13,v13,v4
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vsubshs v8,v10,v8
	// vslh v12,v0,v9
	// vsubshs v13,v10,v13
	// vadduhm v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v12,v0,v2
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vor v0,v7,v7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vadduhm v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vslh v12,v0,v9
	// vadduhm v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v0,v0,v2
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v12,v0,v8
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vsrah v0,v13,v1
	// vsrah v13,v12,v1
	// vor v12,v11,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,48
	r11.s64 = r11.s64 + 48;
	// vor v11,v12,v13
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// bne cr6,0x82701064
	if (!cr6.eq) goto loc_82701064;
loc_827010F4:
	// vand v0,v11,v3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vcmpgtuh. v0,v0,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8270110C"))) PPC_WEAK_FUNC(sub_8270110C);
PPC_FUNC_IMPL(__imp__sub_8270110C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82701110"))) PPC_WEAK_FUNC(sub_82701110);
PPC_FUNC_IMPL(__imp__sub_82701110) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// vspltish v0,8
	// addi r10,r4,1
	ctx.r10.s64 = ctx.r4.s64 + 1;
	// vspltish v12,-1
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// vspltisb v5,0
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_set1_epi8(char(0x0)));
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// vspltish v4,1
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// vspltish v13,2
	// vslh v28,v12,v0
	// vspltish v11,4
	// vspltish v3,5
	// vspltish v8,0
	// bne cr6,0x827011cc
	if (!cr6.eq) goto loc_827011CC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x827012b4
	if (!cr6.gt) goto loc_827012B4;
loc_82701150:
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v7,v0,v13
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v12,v0,v10,4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 12));
	// vsldoi v9,v0,v10,2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v10,v0,v10,6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 10));
	// vsubshs v0,v0,v7
	// vslh v7,v12,v13
	// vslh v6,v12,v3
	// vslh v31,v12,v11
	// vslh v10,v10,v13
	// vadduhm v12,v7,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v7,v31,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v9,v11
	// vslh v9,v9,v4
	// vsubshs v10,v5,v10
	// vadduhm v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v0,v0,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v12,v12,v9
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v12,v12,v2
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsrah v0,v0,v1
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r11,48
	r11.s64 = r11.s64 + 48;
	// bne cr6,0x82701150
	if (!cr6.eq) goto loc_82701150;
	// b 0x827012b4
	goto loc_827012B4;
loc_827011CC:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x827012b4
	if (!cr6.gt) goto loc_827012B4;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
loc_827011D8:
	// addi r9,r11,-16
	ctx.r9.s64 = r11.s64 + -16;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v29,v12,v13
	// vsldoi v10,v0,v9,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v7,v0,v9,2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vsldoi v31,v0,v9,6
	_mm_store_si128((__m128i*)v31.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vsldoi v9,v12,v0,4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vsldoi v6,v12,v0,2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v30,v12,v0,6
	_mm_store_si128((__m128i*)v30.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vsubshs v12,v12,v29
	// vslh v31,v31,v13
	// vslh v29,v9,v13
	// vslh v27,v9,v3
	// vslh v26,v9,v11
	// vslh v30,v30,v13
	// vadduhm v9,v29,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubshs v31,v5,v31
	// vadduhm v29,v26,v27
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v27,v6,v11
	// vslh v6,v6,v4
	// vsubshs v30,v5,v30
	// vadduhm v9,v9,v29
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v29,v10,v11
	// vadduhm v6,v6,v27
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v12,v12,v30
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v30,v10,v3
	// vadduhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v6,v9,v2
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v9,v10,v13
	// vadduhm v12,v6,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v9,v29,v30
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v30,v7,v11
	// vslh v7,v7,v4
	// vadduhm v9,v10,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v7,v7,v30
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v30,v0,v13
	// vadduhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubshs v0,v0,v30
	// vadduhm v10,v0,v31
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vsrah v0,v12,v1
	// vadduhm v12,v9,v2
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vor v9,v8,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v0,v12,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsrah v0,v0,v1
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v8,v9,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r11,48
	r11.s64 = r11.s64 + 48;
	// bne cr6,0x827011d8
	if (!cr6.eq) goto loc_827011D8;
loc_827012B4:
	// vand v0,v8,v28
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vcmpgtuh. v0,v0,v5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_827012CC"))) PPC_WEAK_FUNC(sub_827012CC);
PPC_FUNC_IMPL(__imp__sub_827012CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827012D0"))) PPC_WEAK_FUNC(sub_827012D0);
PPC_FUNC_IMPL(__imp__sub_827012D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v64{};
	PPCVRegister v65{};
	PPCRegister temp{};
	// vspltisb v31,0
	_mm_store_si128((__m128i*)v31.u8, _mm_set1_epi8(char(0x0)));
	// vor128 v30,v64,v64
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)v64.u8));
	// vor128 v29,v65,v65
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)v65.u8));
	// li r8,16
	ctx.r8.s64 = 16;
	// rlwinm r12,r4,2,0,29
	r12.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne cr6,0x82701390
	if (!cr6.eq) goto loc_82701390;
	// rlwinm r6,r4,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// li r7,8
	ctx.r7.s64 = 8;
loc_827012F4:
	// add r10,r3,r6
	ctx.r10.u64 = ctx.r3.u64 + ctx.r6.u64;
	// add r9,r3,r4
	ctx.r9.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r11,r10,r4
	r11.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvlx v1,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v2,r3,r8
	temp.u32 = ctx.r3.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v10,r9,r8
	temp.u32 = ctx.r9.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v18,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v19,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)v19.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v23,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v24,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)v24.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v3,v1,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v11,v9,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vor v20,v18,v19
	_mm_store_si128((__m128i*)v20.u8, _mm_or_si128(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vor v25,v23,v24
	_mm_store_si128((__m128i*)v25.u8, _mm_or_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)v24.u8)));
	// add r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 + r12.u64;
	// vperm v4,v3,v31,v30
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vperm v5,v3,v31,v29
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vperm v12,v11,v31,v30
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vperm v13,v11,v31,v29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vperm v21,v20,v31,v30
	_mm_store_si128((__m128i*)v21.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vperm v22,v20,v31,v29
	_mm_store_si128((__m128i*)v22.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vperm v26,v25,v31,v30
	_mm_store_si128((__m128i*)v26.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vperm v27,v25,v31,v29
	_mm_store_si128((__m128i*)v27.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v29.u8)));
	// addi r9,r5,48
	ctx.r9.s64 = ctx.r5.s64 + 48;
	// addi r10,r5,96
	ctx.r10.s64 = ctx.r5.s64 + 96;
	// addi r11,r5,144
	r11.s64 = ctx.r5.s64 + 144;
	// stvx v4,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r5,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,192
	ctx.r5.s64 = ctx.r5.s64 + 192;
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r9,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v21,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v22,r10,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v26,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v27,r11,r8
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addic. r7,r7,-4
	xer.ca = ctx.r7.u32 > 3;
	ctx.r7.s64 = ctx.r7.s64 + -4;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x827012f4
	if (!cr0.eq) goto loc_827012F4;
	// b 0x82701430
	goto loc_82701430;
loc_82701390:
	// rlwinm r6,r4,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// li r7,16
	ctx.r7.s64 = 16;
loc_82701398:
	// add r10,r3,r6
	ctx.r10.u64 = ctx.r3.u64 + ctx.r6.u64;
	// add r9,r3,r4
	ctx.r9.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r11,r10,r4
	r11.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvlx v1,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v2,r3,r8
	temp.u32 = ctx.r3.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v10,r9,r8
	temp.u32 = ctx.r9.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v18,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v19,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)v19.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v23,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v24,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)v24.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v3,v1,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v11,v9,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vor v20,v18,v19
	_mm_store_si128((__m128i*)v20.u8, _mm_or_si128(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)v19.u8)));
	// vor v25,v23,v24
	_mm_store_si128((__m128i*)v25.u8, _mm_or_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)v24.u8)));
	// add r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 + r12.u64;
	// vperm v4,v3,v31,v30
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vperm v5,v3,v31,v29
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vperm v12,v11,v31,v30
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vperm v13,v11,v31,v29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vperm v21,v20,v31,v30
	_mm_store_si128((__m128i*)v21.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vperm v22,v20,v31,v29
	_mm_store_si128((__m128i*)v22.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vperm v26,v25,v31,v30
	_mm_store_si128((__m128i*)v26.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vperm v27,v25,v31,v29
	_mm_store_si128((__m128i*)v27.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v29.u8)));
	// addi r9,r5,48
	ctx.r9.s64 = ctx.r5.s64 + 48;
	// addi r10,r5,96
	ctx.r10.s64 = ctx.r5.s64 + 96;
	// addi r11,r5,144
	r11.s64 = ctx.r5.s64 + 144;
	// stvx v4,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r5,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,192
	ctx.r5.s64 = ctx.r5.s64 + 192;
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r9,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v21,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v22,r10,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v26,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v27,r11,r8
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addic. r7,r7,-4
	xer.ca = ctx.r7.u32 > 3;
	ctx.r7.s64 = ctx.r7.s64 + -4;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x82701398
	if (!cr0.eq) goto loc_82701398;
loc_82701430:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82701438"))) PPC_WEAK_FUNC(sub_82701438);
PPC_FUNC_IMPL(__imp__sub_82701438) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vspltish v0,-5
	// addi r11,r6,1056
	r11.s64 = ctx.r6.s64 + 1056;
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
	// vspltish v1,6
	// li r8,0
	ctx.r8.s64 = 0;
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vsrh v0,v0,v0
	// subf r3,r4,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v0,v13
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// bl 0x82700388
	sub_82700388(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82701480"))) PPC_WEAK_FUNC(sub_82701480);
PPC_FUNC_IMPL(__imp__sub_82701480) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r6,1056
	r11.s64 = ctx.r6.s64 + 1056;
	// vspltish v0,7
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
	// vspltish v1,4
	// li r8,0
	ctx.r8.s64 = 0;
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r4,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v0,v13
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// bl 0x827005f8
	sub_827005F8(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_827014C4"))) PPC_WEAK_FUNC(sub_827014C4);
PPC_FUNC_IMPL(__imp__sub_827014C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827014C8"))) PPC_WEAK_FUNC(sub_827014C8);
PPC_FUNC_IMPL(__imp__sub_827014C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vspltish v0,-5
	// addi r11,r6,1056
	r11.s64 = ctx.r6.s64 + 1056;
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
	// vspltish v1,6
	// li r8,0
	ctx.r8.s64 = 0;
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vsrh v0,v0,v0
	// subf r3,r4,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v0,v13
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// bl 0x82700830
	sub_82700830(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82701510"))) PPC_WEAK_FUNC(sub_82701510);
PPC_FUNC_IMPL(__imp__sub_82701510) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vspltish v13,8
	// addi r11,r6,1056
	r11.s64 = ctx.r6.s64 + 1056;
	// vspltish v0,2
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// vslh v0,v13,v0
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// vsubshs v0,v0,v13
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82700240
	sub_82700240(ctx, base);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vspltish v1,6
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82700e10
	sub_82700E10(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82701590"))) PPC_WEAK_FUNC(sub_82701590);
PPC_FUNC_IMPL(__imp__sub_82701590) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r10,r7,31
	ctx.r10.u64 = ctx.r7.u32 & 0x1;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// vspltish v12,4
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// vspltish v31,5
	// addi r5,r10,3
	ctx.r5.s64 = ctx.r10.s64 + 3;
	// vspltish v10,15
	// addi r10,r1,176
	ctx.r10.s64 = ctx.r1.s64 + 176;
	// vspltish v0,2
	// addi r8,r6,1056
	ctx.r8.s64 = ctx.r6.s64 + 1056;
	// vrlh v9,v12,v12
	// li r6,1
	ctx.r6.s64 = 1;
	// vor v30,v31,v31
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)v31.u8));
	// vspltish v1,1
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// stw r3,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, ctx.r3.u32);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,256
	ctx.r8.s64 = ctx.r1.s64 + 256;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// vaddshs v29,v10,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubshs v20,v9,v11
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// stw r7,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, ctx.r7.u32);
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// stw r9,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r9.u32);
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v1,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// stvx v30,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// slw r8,r6,r5
	ctx.r8.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
	// stvx v29,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// stvx v20,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x827017c0
	if (!cr6.eq) goto loc_827017C0;
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r11,r9
	ctx.r6.u64 = r11.u64 + ctx.r9.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r6,16
	ctx.r5.s64 = ctx.r6.s64 + 16;
	// lvx128 v9,r0,r6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r11,r6,r9
	r11.u64 = ctx.r6.u64 + ctx.r9.u64;
	// lvsl v6,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// vmrghb v5,v13,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v4,v13,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v10,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82701a00
	if (!cr6.gt) goto loc_82701A00;
	// li r6,0
	ctx.r6.s64 = 0;
loc_827016D4:
	// vslh v3,v11,v12
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vslh v6,v11,v30
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vslh v2,v9,v1
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// vslh v7,v11,v0
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// vslh v5,v5,v0
	// vadduhm v6,v3,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v3,v9,v12
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvx128 v27,r0,r5
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v5,v13,v5
	// addi r5,r10,16
	ctx.r5.s64 = ctx.r10.s64 + 16;
	// vadduhm v3,v2,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v2,v10,v0
	// vadduhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v26,v2,v10
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v6,v28,v27,v2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vadduhm v3,v7,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v28,v8,v1
	// vslh v27,v4,v0
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v3,v3,v29
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v27,v13,v27
	// vslh v2,v7,v0
	// vsubshs v2,v7,v2
	// vadduhm v5,v2,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v2,v10,v12
	// vadduhm v3,v3,v5
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v10,v30
	// vsrah v3,v3,v31
	// vadduhm v2,v2,v5
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v8,v12
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v28,v28,v5
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v6,v0
	// vsubshs v4,v6,v5
	// vor v5,v11,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v9,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vadduhm v7,v26,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v27,v4,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v4,v10,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v8,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v6,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v6,v7,v28
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v6,v6,v29
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v7,v6,v27
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v7,v7,v31
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x827016d4
	if (cr6.lt) goto loc_827016D4;
	// b 0x82701a00
	goto loc_82701A00;
loc_827017C0:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// li r6,32
	ctx.r6.s64 = 32;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r31,r1,160
	r31.s64 = ctx.r1.s64 + 160;
	// addi r29,r1,192
	r29.s64 = ctx.r1.s64 + 192;
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r5,32
	ctx.r5.s64 = 32;
	// lvrx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r6,16
	ctx.r6.s64 = 16;
	// lvrx v11,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v11,v8,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// li r30,16
	r30.s64 = 16;
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// stvx v10,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v2,v13,v11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v7,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,224
	ctx.r6.s64 = ctx.r1.s64 + 224;
	// lvrx v11,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r31,32
	r31.s64 = 32;
	// vor v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v8,r11,r5
	temp.u32 = r11.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v7,v9,v8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// vmrghb v28,v13,v11
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// vmrglb v27,v13,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v8,v28,v28
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)v28.u8));
	// lvrx v11,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v7,v13,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v5,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v4,v9,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v3,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v11,v6,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v3,v5,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vor v9,v2,v2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vmrghb v5,v13,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v6,v27,v27
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v27.u8));
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v3,v13,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82701a00
	if (!cr6.gt) goto loc_82701A00;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_8270189C:
	// vor v27,v11,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v11,v8,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// vor v22,v9,v9
	_mm_store_si128((__m128i*)v22.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// addi r4,r11,32
	ctx.r4.s64 = r11.s64 + 32;
	// vor v9,v6,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v4,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// vslh v8,v11,v0
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v23,v11,v12
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v24,v11,v30
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// lvx128 v26,r0,r4
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v27,v27,v0
	// vadduhm v25,v8,v11
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r4,r10,-16
	ctx.r4.s64 = ctx.r10.s64 + -16;
	// vor v8,v5,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vadduhm v24,v23,v24
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vor v23,v10,v10
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v7,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v3,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vslh v5,v8,v12
	// vslh v3,v8,v1
	// vadduhm v25,v25,v24
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsubshs v27,v13,v27
	// vslh v24,v9,v12
	// vadduhm v21,v3,v5
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// lvx128 v5,r0,r5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v4,v28,v5,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvsl v3,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v5,v5,v26,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vslh v26,v9,v30
	// vslh v28,v9,v0
	// addi r5,r10,-32
	ctx.r5.s64 = ctx.r10.s64 + -32;
	// vadduhm v25,v25,v21
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vmrghb v3,v13,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v22,v22,v0
	// vmrghb v2,v13,v5
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v26,v24,v26
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v28,v28,v9
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v24,v6,v1
	// vor v5,v3,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vor v3,v2,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vslh v21,v4,v0
	// vadduhm v28,v28,v26
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v2,v5,v0
	// vsubshs v2,v5,v2
	// vadduhm v2,v2,v27
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v27,v6,v12
	// vor v26,v2,v2
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vadduhm v27,v24,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsubshs v24,v4,v21
	// vsubshs v21,v13,v22
	// vslh v22,v10,v0
	// vadduhm v2,v28,v27
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v27,v25,v29
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v28,v24,v21
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vslh v25,v7,v1
	// vadduhm v2,v2,v29
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v27,v27,v26
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v26,v10,v12
	// vslh v24,v23,v0
	// vadduhm v2,v2,v28
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v28,v10,v30
	// vsrah v27,v27,v31
	// vsubshs v24,v13,v24
	// vsrah v2,v2,v31
	// vadduhm v28,v26,v28
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v26,v7,v12
	// stvx v27,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v22,v22,v10
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stvx v2,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v26,v25,v26
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v25,v3,v0
	// vsubshs v25,v3,v25
	// vadduhm v25,v25,v24
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v2,v22,v28
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v28,v2,v26
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vadduhm v28,v28,v29
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v2,v28,v25
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vsrah v2,v2,v31
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x8270189c
	if (cr6.lt) goto loc_8270189C;
loc_82701A00:
	// vor v2,v20,v20
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v20.u8));
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// vspltish v1,7
	// bl 0x82700e10
	sub_82700E10(ctx, base);
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82701A1C"))) PPC_WEAK_FUNC(sub_82701A1C);
PPC_FUNC_IMPL(__imp__sub_82701A1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82701A20"))) PPC_WEAK_FUNC(sub_82701A20);
PPC_FUNC_IMPL(__imp__sub_82701A20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v24{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r9,r6,1056
	ctx.r9.s64 = ctx.r6.s64 + 1056;
	// vspltish v1,3
	// vspltish v0,4
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// rlwinm r10,r31,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// li r8,0
	ctx.r8.s64 = 0;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlh v0,v0,v0
	// vaddshs v2,v1,v13
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// addi r6,r10,3
	ctx.r6.s64 = ctx.r10.s64 + 3;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// vsubshs v24,v0,v13
	// bl 0x82700aa0
	sub_82700AA0(ctx, base);
	// vor v2,v24,v24
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v24.u8));
	// li r5,0
	ctx.r5.s64 = 0;
	// vspltish v1,7
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82700e10
	sub_82700E10(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82701AA0"))) PPC_WEAK_FUNC(sub_82701AA0);
PPC_FUNC_IMPL(__imp__sub_82701AA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r10,r7,31
	ctx.r10.u64 = ctx.r7.u32 & 0x1;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// vspltish v12,4
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// vspltish v4,5
	// addi r5,r10,3
	ctx.r5.s64 = ctx.r10.s64 + 3;
	// vspltish v10,15
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// vspltish v0,2
	// addi r8,r6,1056
	ctx.r8.s64 = ctx.r6.s64 + 1056;
	// vrlh v9,v12,v12
	// li r6,1
	ctx.r6.s64 = 1;
	// vor v3,v4,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vspltish v5,1
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// stw r3,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, ctx.r3.u32);
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,224
	ctx.r8.s64 = ctx.r1.s64 + 224;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// vaddshs v1,v10,v11
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubshs v20,v9,v11
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// stw r9,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, ctx.r9.u32);
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// stw r7,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r7.u32);
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v5,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,256
	ctx.r10.s64 = ctx.r1.s64 + 256;
	// slw r8,r6,r5
	ctx.r8.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
	// stvx v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stw r8,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r8.u32);
	// stvx v20,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82701cd8
	if (!cr6.eq) goto loc_82701CD8;
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r11,r9
	ctx.r6.u64 = r11.u64 + ctx.r9.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r6,16
	ctx.r5.s64 = ctx.r6.s64 + 16;
	// lvx128 v6,r0,r6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r11,r6,r9
	r11.u64 = ctx.r6.u64 + ctx.r9.u64;
	// lvsl v2,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v31,r0,r5
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v10,v6,v31,v2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82701f24
	if (!cr6.gt) goto loc_82701F24;
	// li r6,0
	ctx.r6.s64 = 0;
loc_82701BE4:
	// vor v31,v9,v9
	_mm_store_si128((__m128i*)v31.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v9,v11,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vor v11,v7,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// vslh v28,v31,v0
	// vslh v29,v9,v5
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vslh v30,v11,v12
	// vslh v2,v11,v3
	// lvx128 v27,r0,r5
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v7,v11,v0
	// addi r5,r10,16
	ctx.r5.s64 = ctx.r10.s64 + 16;
	// vadduhm v2,v30,v2
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v30,v9,v12
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v30,v29,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vsubshs v29,v31,v28
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v7,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vor v31,v8,v8
	_mm_store_si128((__m128i*)v31.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v10,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v6,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v2,v7,v30
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vperm v7,v28,v27,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vslh v27,v8,v5
	// vmrghb v6,v13,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v28,v2,v1
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vmrglb v30,v13,v7
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v6,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v30,v30
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v30.u8));
	// vslh v30,v7,v0
	// vslh v26,v6,v0
	// vsubshs v30,v13,v30
	// vadduhm v2,v29,v30
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v10,v12
	// vslh v30,v10,v3
	// vadduhm v28,v28,v2
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v2,v10,v0
	// vadduhm v30,v29,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v8,v12
	// vadduhm v2,v2,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v29,v27,v29
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v27,v31,v0
	// vadduhm v2,v2,v30
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vsubshs v31,v31,v27
	// vsubshs v27,v13,v26
	// vadduhm v27,v31,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v31,v28,v4
	// stvx v31,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v31,v2,v29
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v29.u16)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v31,v31,v1
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v2,v31,v27
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v2,v2,v4
	// stvx v2,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82701be4
	if (cr6.lt) goto loc_82701BE4;
	// b 0x82701f24
	goto loc_82701F24;
loc_82701CD8:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// li r6,32
	ctx.r6.s64 = 32;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r31,r1,176
	r31.s64 = ctx.r1.s64 + 176;
	// addi r29,r1,208
	r29.s64 = ctx.r1.s64 + 208;
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r5,32
	ctx.r5.s64 = 32;
	// lvrx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r6,16
	ctx.r6.s64 = 16;
	// lvrx v11,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v11,v8,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// li r30,16
	r30.s64 = 16;
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// stvx v10,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v2,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,240
	ctx.r6.s64 = ctx.r1.s64 + 240;
	// lvrx v11,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v29,v13,v10
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v11,v2,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v8,r11,r5
	temp.u32 = r11.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v10,v9,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// li r31,32
	r31.s64 = 32;
	// vor v8,v7,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v29,v29
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v29.u8));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// vmrglb v27,v13,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// vmrghb v28,v13,v11
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v30,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)v30.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvrx v11,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v31,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v2,v9,v11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vor v31,v31,v30
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vor v11,v28,v28
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v28.u8));
	// vor v9,v27,v27
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v27.u8));
	// vmrghb v30,v13,v2
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v29,v13,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v31,v13,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82701f24
	if (!cr6.gt) goto loc_82701F24;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_82701DB4:
	// vor v28,v8,v8
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v8,v11,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vor v11,v30,v30
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v30.u8));
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// addi r4,r11,32
	ctx.r4.s64 = r11.s64 + 32;
	// vslh v21,v28,v0
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// vslh v22,v8,v5
	// lvx128 v30,r0,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v27,v11,v0
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v26,v11,v3
	// vslh v23,v11,v12
	// lvx128 v25,r0,r4
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v28,v28,v21
	// addi r4,r10,-16
	ctx.r4.s64 = ctx.r10.s64 + -16;
	// vadduhm v24,v27,v11
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vor v27,v7,v7
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v10,v10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v31,v31
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)v31.u8));
	// vslh v31,v8,v12
	// vadduhm v23,v23,v26
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vor v26,v6,v6
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v9,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vadduhm v22,v22,v31
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v31.u16)));
	// lvx128 v31,r0,r5
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v9,v29,v29
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v29.u8));
	// lvsl v29,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// vperm v30,v30,v31,v2
	_mm_store_si128((__m128i*)v30.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vadduhm v24,v24,v23
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v23,v9,v12
	// stvx v29,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// vmrglb v29,v13,v30
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v24,v24,v22
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vslh v22,v6,v5
	// lvx128 v2,r0,r5
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r10,-32
	ctx.r5.s64 = ctx.r10.s64 + -32;
	// vperm v2,v31,v25,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vslh v25,v9,v3
	// vmrghb v31,v13,v2
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v25,v23,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vmrghb v2,v13,v30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v30,v2,v2
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vslh v2,v9,v0
	// vslh v21,v30,v0
	// vadduhm v2,v2,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubshs v23,v13,v21
	// vslh v21,v26,v0
	// vadduhm v2,v2,v25
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v28,v28,v23
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v23,v6,v12
	// vsubshs v26,v26,v21
	// vslh v21,v10,v0
	// vor v25,v28,v28
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)v28.u8));
	// vadduhm v23,v22,v23
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v22,v29,v0
	// vadduhm v21,v21,v10
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v2,v2,v23
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vsubshs v22,v13,v22
	// vslh v23,v31,v0
	// vadduhm v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v28,v26,v22
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vadduhm v26,v24,v1
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v24,v7,v5
	// vadduhm v2,v2,v28
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v26,v26,v25
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v25,v10,v12
	// vslh v28,v10,v3
	// vsrah v2,v2,v4
	// vadduhm v28,v25,v28
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v25,v7,v12
	// vadduhm v25,v24,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v24,v27,v0
	// vsubshs v27,v27,v24
	// vsubshs v24,v13,v23
	// vadduhm v24,v27,v24
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v27,v26,v4
	// stvx v2,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v2,v21,v28
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)v28.u16)));
	// stvx v27,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vadduhm v28,v2,v25
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v28,v28,v1
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v2,v28,v24
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v2,v2,v4
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x82701db4
	if (cr6.lt) goto loc_82701DB4;
loc_82701F24:
	// vor v2,v20,v20
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v20.u8));
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// vspltish v1,7
	// bl 0x82700e10
	sub_82700E10(ctx, base);
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82701F40"))) PPC_WEAK_FUNC(sub_82701F40);
PPC_FUNC_IMPL(__imp__sub_82701F40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r6,1056
	r11.s64 = ctx.r6.s64 + 1056;
	// vspltish v0,8
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vsubshs v0,v0,v13
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82700240
	sub_82700240(ctx, base);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vspltish v1,4
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82700fd0
	sub_82700FD0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82701FB8"))) PPC_WEAK_FUNC(sub_82701FB8);
PPC_FUNC_IMPL(__imp__sub_82701FB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// vspltish v19,7
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// addi r5,r1,240
	ctx.r5.s64 = ctx.r1.s64 + 240;
	// vspltish v0,2
	// clrlwi r10,r7,31
	ctx.r10.u64 = ctx.r7.u32 & 0x1;
	// vspltish v29,3
	// addi r8,r6,1056
	ctx.r8.s64 = ctx.r6.s64 + 1056;
	// vspltish v12,4
	// li r6,1
	ctx.r6.s64 = 1;
	// vspltish v30,5
	// vspltish v31,1
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// stvx v19,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r10,3
	ctx.r5.s64 = ctx.r10.s64 + 3;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// vrlh v10,v12,v12
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,272
	ctx.r8.s64 = ctx.r1.s64 + 272;
	// vaddshs v28,v29,v11
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stw r3,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r3.u32);
	// addi r4,r1,288
	ctx.r4.s64 = ctx.r1.s64 + 288;
	// stw r7,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, ctx.r7.u32);
	// vsubshs v18,v10,v11
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,176
	ctx.r8.s64 = ctx.r1.s64 + 176;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// stw r9,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r9.u32);
	// stvx v31,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// stvx v30,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v29,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// slw r8,r6,r5
	ctx.r8.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
	// stvx v28,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// stvx v18,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x827021f8
	if (!cr6.eq) goto loc_827021F8;
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r11,r9
	ctx.r6.u64 = r11.u64 + ctx.r9.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r6,16
	ctx.r5.s64 = ctx.r6.s64 + 16;
	// lvx128 v9,r0,r6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r11,r6,r9
	r11.u64 = ctx.r6.u64 + ctx.r9.u64;
	// lvsl v6,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// vmrghb v5,v13,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v4,v13,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v10,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82702438
	if (!cr6.gt) goto loc_82702438;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8270210C:
	// vslh v3,v11,v12
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vslh v6,v11,v30
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vslh v2,v9,v31
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// vslh v7,v11,v0
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// vslh v5,v5,v0
	// vadduhm v6,v3,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v3,v9,v12
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvx128 v27,r0,r5
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v5,v13,v5
	// addi r5,r10,16
	ctx.r5.s64 = ctx.r10.s64 + 16;
	// vadduhm v3,v2,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v2,v10,v0
	// vadduhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v26,v2,v10
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v6,v2,v27,v1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v3,v7,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v1,v8,v31
	// vslh v27,v4,v0
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v3,v3,v28
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v27,v13,v27
	// vslh v2,v7,v0
	// vsubshs v2,v7,v2
	// vadduhm v5,v2,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v2,v10,v12
	// vadduhm v3,v3,v5
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v10,v30
	// vsrah v3,v3,v29
	// vadduhm v2,v2,v5
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v8,v12
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v1,v1,v5
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v6,v0
	// vsubshs v4,v6,v5
	// vor v5,v11,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v9,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vadduhm v7,v26,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v27,v4,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v4,v10,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v8,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v6,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v6,v7,v1
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v6,v6,v28
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v7,v6,v27
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v7,v7,v29
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x8270210c
	if (cr6.lt) goto loc_8270210C;
	// b 0x82702438
	goto loc_82702438;
loc_827021F8:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// li r6,32
	ctx.r6.s64 = 32;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r31,r1,192
	r31.s64 = ctx.r1.s64 + 192;
	// addi r29,r1,224
	r29.s64 = ctx.r1.s64 + 224;
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r5,32
	ctx.r5.s64 = 32;
	// lvrx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r6,16
	ctx.r6.s64 = 16;
	// lvrx v11,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v11,v8,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// li r30,16
	r30.s64 = 16;
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// stvx v10,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v2,v13,v11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v7,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,256
	ctx.r6.s64 = ctx.r1.s64 + 256;
	// lvrx v11,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r31,32
	r31.s64 = 32;
	// vor v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v8,r11,r5
	temp.u32 = r11.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v7,v9,v8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// vmrghb v1,v13,v11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// vmrglb v27,v13,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v8,v1,v1
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// lvrx v11,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v7,v13,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v5,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v4,v9,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v3,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v11,v6,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v3,v5,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vor v9,v2,v2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vmrghb v5,v13,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v6,v27,v27
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v27.u8));
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v3,v13,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82702438
	if (!cr6.gt) goto loc_82702438;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_827022D4:
	// vor v26,v11,v11
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v11,v8,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vor v22,v10,v10
	_mm_store_si128((__m128i*)v22.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// vor v10,v7,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// addi r4,r11,32
	ctx.r4.s64 = r11.s64 + 32;
	// vor v7,v3,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// vslh v8,v11,v0
	// lvx128 v27,r0,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v21,v9,v9
	_mm_store_si128((__m128i*)v21.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vor v9,v6,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vor v6,v4,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// lvx128 v25,r0,r4
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v24,v8,v11
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r4,r10,-16
	ctx.r4.s64 = ctx.r10.s64 + -16;
	// vor v8,v5,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vslh v1,v11,v30
	// vslh v23,v11,v12
	// vslh v5,v8,v12
	// vslh v3,v8,v31
	// vadduhm v23,v23,v1
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v9,v0
	// vadduhm v20,v3,v5
	_mm_store_si128((__m128i*)v20.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// lvx128 v5,r0,r5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v4,v27,v5,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvsl v3,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v5,v5,v25,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vadduhm v27,v1,v9
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v1,v26,v0
	// addi r5,r10,-32
	ctx.r5.s64 = ctx.r10.s64 + -32;
	// vadduhm v24,v24,v23
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vmrghb v3,v13,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v23,v9,v12
	// vmrghb v2,v13,v5
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v25,v9,v30
	// vsubshs v26,v13,v1
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v1,v6,v12
	// vor v5,v3,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vor v3,v2,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vadduhm v25,v23,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v24,v24,v20
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v20.u16)));
	// vslh v2,v5,v0
	// vslh v23,v4,v0
	// vadduhm v27,v27,v25
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v25,v7,v31
	// vsubshs v2,v5,v2
	// vsubshs v23,v4,v23
	// vadduhm v2,v2,v26
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v26,v6,v31
	// vadduhm v26,v26,v1
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v21,v0
	// vsubshs v20,v13,v1
	// vslh v1,v10,v0
	// vadduhm v21,v1,v10
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vor v1,v2,v2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vadduhm v2,v27,v26
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vadduhm v27,v23,v20
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v20.u16)));
	// vadduhm v26,v24,v28
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v23,v22,v0
	// vadduhm v2,v2,v28
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v26,v26,v1
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v10,v30
	// vadduhm v2,v2,v27
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v27,v10,v12
	// vsubshs v23,v13,v23
	// vsrah v26,v26,v29
	// vsrah v2,v2,v29
	// vadduhm v27,v27,v1
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v7,v12
	// stvx v26,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v2,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v25,v25,v1
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v3,v0
	// vsubshs v24,v3,v1
	// vadduhm v1,v24,v23
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v2,v21,v27
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v27,v2,v25
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v27,v27,v28
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v2,v27,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsrah v2,v2,v29
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x827022d4
	if (cr6.lt) goto loc_827022D4;
loc_82702438:
	// vor v2,v18,v18
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v18.u8));
	// li r5,0
	ctx.r5.s64 = 0;
	// vor v1,v19,v19
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v19.u8));
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// bl 0x82700fd0
	sub_82700FD0(ctx, base);
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82702454"))) PPC_WEAK_FUNC(sub_82702454);
PPC_FUNC_IMPL(__imp__sub_82702454) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82702458"))) PPC_WEAK_FUNC(sub_82702458);
PPC_FUNC_IMPL(__imp__sub_82702458) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v24{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vspltish v0,4
	// addi r9,r6,1056
	ctx.r9.s64 = ctx.r6.s64 + 1056;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// vspltish v1,1
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// rlwinm r10,r31,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// vrlh v0,v0,v0
	// li r8,0
	ctx.r8.s64 = 0;
	// lvx128 v2,r0,r9
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r10,3
	ctx.r6.s64 = ctx.r10.s64 + 3;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// vsubshs v24,v0,v2
	// bl 0x82700aa0
	sub_82700AA0(ctx, base);
	// vor v2,v24,v24
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v24.u8));
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// vspltish v1,7
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82700fd0
	sub_82700FD0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_827024D4"))) PPC_WEAK_FUNC(sub_827024D4);
PPC_FUNC_IMPL(__imp__sub_827024D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827024D8"))) PPC_WEAK_FUNC(sub_827024D8);
PPC_FUNC_IMPL(__imp__sub_827024D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// vspltish v20,7
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// addi r5,r1,240
	ctx.r5.s64 = ctx.r1.s64 + 240;
	// vspltish v0,2
	// clrlwi r10,r7,31
	ctx.r10.u64 = ctx.r7.u32 & 0x1;
	// vspltish v3,3
	// addi r8,r6,1056
	ctx.r8.s64 = ctx.r6.s64 + 1056;
	// vspltish v12,4
	// li r6,1
	ctx.r6.s64 = 1;
	// vspltish v4,5
	// vspltish v5,1
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// stvx v20,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r10,3
	ctx.r5.s64 = ctx.r10.s64 + 3;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// vrlh v10,v12,v12
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,272
	ctx.r8.s64 = ctx.r1.s64 + 272;
	// vaddshs v31,v3,v11
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stw r3,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r3.u32);
	// addi r4,r1,288
	ctx.r4.s64 = ctx.r1.s64 + 288;
	// stw r7,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, ctx.r7.u32);
	// vsubshs v19,v10,v11
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,176
	ctx.r8.s64 = ctx.r1.s64 + 176;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// stw r9,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r9.u32);
	// stvx v5,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// stvx v4,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// slw r8,r6,r5
	ctx.r8.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
	// stvx v31,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// stvx v19,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82702720
	if (!cr6.eq) goto loc_82702720;
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r11,r9
	ctx.r6.u64 = r11.u64 + ctx.r9.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r6,16
	ctx.r5.s64 = ctx.r6.s64 + 16;
	// lvx128 v6,r0,r6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r11,r6,r9
	r11.u64 = ctx.r6.u64 + ctx.r9.u64;
	// lvsl v2,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v1,r0,r5
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v10,v6,v1,v2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82702958
	if (!cr6.gt) goto loc_82702958;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8270262C:
	// vor v30,v9,v9
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v9,v11,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vor v11,v7,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// vslh v28,v30,v0
	// vslh v29,v9,v5
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vslh v1,v11,v12
	// vslh v2,v11,v4
	// lvx128 v27,r0,r5
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v7,v11,v0
	// addi r5,r10,16
	ctx.r5.s64 = ctx.r10.s64 + 16;
	// vadduhm v2,v1,v2
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v1,v9,v12
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v1,v29,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v7,v7,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vsubshs v29,v30,v28
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v30,v8,v8
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v10,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v6,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v2,v7,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vperm v7,v28,v27,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vslh v27,v8,v5
	// vmrghb v6,v13,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v28,v2,v31
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vmrglb v1,v13,v7
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v6,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v1,v1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vslh v1,v7,v0
	// vslh v26,v6,v0
	// vsubshs v1,v13,v1
	// vadduhm v2,v29,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v29,v10,v12
	// vslh v1,v10,v4
	// vadduhm v28,v28,v2
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v2,v10,v0
	// vadduhm v1,v29,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v29,v8,v12
	// vadduhm v2,v2,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v29,v27,v29
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v27,v30,v0
	// vadduhm v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsubshs v30,v30,v27
	// vadduhm v1,v2,v29
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vsubshs v27,v13,v26
	// vadduhm v1,v1,v31
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v27,v30,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v30,v28,v3
	// vadduhm v2,v1,v27
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v27.u16)));
	// stvx v30,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vsrah v2,v2,v3
	// stvx v2,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x8270262c
	if (cr6.lt) goto loc_8270262C;
	// b 0x82702958
	goto loc_82702958;
loc_82702720:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// li r6,32
	ctx.r6.s64 = 32;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r31,r1,192
	r31.s64 = ctx.r1.s64 + 192;
	// addi r29,r1,224
	r29.s64 = ctx.r1.s64 + 224;
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r5,32
	ctx.r5.s64 = 32;
	// lvrx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r6,16
	ctx.r6.s64 = 16;
	// lvrx v11,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v11,v8,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// li r30,16
	r30.s64 = 16;
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// stvx v10,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v2,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,256
	ctx.r6.s64 = ctx.r1.s64 + 256;
	// lvrx v11,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v29,v13,v10
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v11,v2,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v8,r11,r5
	temp.u32 = r11.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v10,v9,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// li r31,32
	r31.s64 = 32;
	// vor v8,v7,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v29,v29
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v29.u8));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// vmrghb v28,v13,v11
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// vmrglb v27,v13,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v30,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)v30.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvrx v11,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v1,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v2,v9,v11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vor v1,v1,v30
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vor v11,v28,v28
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v28.u8));
	// vor v9,v27,v27
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v27.u8));
	// vmrghb v29,v13,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v28,v13,v2
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v30,v13,v1
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82702958
	if (!cr6.gt) goto loc_82702958;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_827027FC:
	// vor v27,v8,v8
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v8,v11,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vor v11,v29,v29
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v29.u8));
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// addi r4,r11,32
	ctx.r4.s64 = r11.s64 + 32;
	// vslh v18,v27,v0
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lvx128 v29,r0,r11
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v26,v11,v4
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v25,v11,v12
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vslh v1,v11,v0
	// lvx128 v24,r0,r4
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v27,v27,v18
	// addi r4,r10,-16
	ctx.r4.s64 = ctx.r10.s64 + -16;
	// vadduhm v22,v25,v26
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vor v26,v7,v7
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v10,v10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vadduhm v23,v1,v11
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vor v10,v30,v30
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)v30.u8));
	// vslh v1,v8,v12
	// vslh v30,v8,v5
	// vor v25,v6,v6
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v9,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v28,v28
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v28.u8));
	// vadduhm v21,v30,v1
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// lvx128 v30,r0,r5
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v1,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v29,v29,v30,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vperm v2,v30,v24,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v23,v23,v22
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vslh v24,v9,v12
	// addi r5,r10,-32
	ctx.r5.s64 = ctx.r10.s64 + -32;
	// vslh v1,v9,v4
	// vmrglb v28,v13,v29
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v30,v13,v2
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v2,v13,v29
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v1,v24,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vor v29,v2,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vslh v2,v9,v0
	// vslh v22,v29,v0
	// vadduhm v2,v2,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubshs v24,v13,v22
	// vslh v22,v6,v5
	// vadduhm v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v18,v27,v24
	_mm_store_si128((__m128i*)v18.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v24,v23,v21
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vslh v23,v6,v12
	// vslh v21,v25,v0
	// vadduhm v27,v24,v31
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v23,v22,v23
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v22,v28,v0
	// vsubshs v25,v25,v21
	// vslh v24,v7,v5
	// vadduhm v2,v2,v23
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vsubshs v22,v13,v22
	// vslh v21,v10,v0
	// vadduhm v27,v27,v18
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v18.u16)));
	// vadduhm v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v1,v25,v22
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vslh v25,v10,v12
	// vadduhm v21,v21,v10
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v23,v30,v0
	// vadduhm v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v10,v4
	// vsrah v27,v27,v3
	// vsrah v2,v2,v3
	// vadduhm v1,v25,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v25,v7,v12
	// stvx v27,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v2,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v2,v21,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v25,v24,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v24,v26,v0
	// vadduhm v1,v2,v25
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vsubshs v26,v26,v24
	// vsubshs v24,v13,v23
	// vadduhm v26,v26,v24
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v1,v1,v31
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v2,v1,v26
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vsrah v2,v2,v3
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x827027fc
	if (cr6.lt) goto loc_827027FC;
loc_82702958:
	// vor v2,v19,v19
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v19.u8));
	// li r5,0
	ctx.r5.s64 = 0;
	// vor v1,v20,v20
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v20.u8));
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// bl 0x82700fd0
	sub_82700FD0(ctx, base);
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82702974"))) PPC_WEAK_FUNC(sub_82702974);
PPC_FUNC_IMPL(__imp__sub_82702974) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82702978"))) PPC_WEAK_FUNC(sub_82702978);
PPC_FUNC_IMPL(__imp__sub_82702978) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vspltish v13,8
	// addi r11,r6,1056
	r11.s64 = ctx.r6.s64 + 1056;
	// vspltish v0,2
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// vslh v0,v13,v0
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// vsubshs v0,v0,v13
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82700240
	sub_82700240(ctx, base);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vspltish v1,6
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82701110
	sub_82701110(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_827029F8"))) PPC_WEAK_FUNC(sub_827029F8);
PPC_FUNC_IMPL(__imp__sub_827029F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r10,r7,31
	ctx.r10.u64 = ctx.r7.u32 & 0x1;
	// vspltish v19,7
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// vspltish v30,5
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// vspltish v31,1
	// addi r5,r10,3
	ctx.r5.s64 = ctx.r10.s64 + 3;
	// vspltish v12,4
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// vspltish v10,15
	// addi r8,r6,1056
	ctx.r8.s64 = ctx.r6.s64 + 1056;
	// vor v29,v30,v30
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)v30.u8));
	// li r6,1
	ctx.r6.s64 = 1;
	// vspltish v0,2
	// vrlh v9,v12,v12
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// stw r3,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r3.u32);
	// stvx v19,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,288
	ctx.r8.s64 = ctx.r1.s64 + 288;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// vaddshs v28,v10,v11
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// vsubshs v18,v9,v11
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// stw r7,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, ctx.r7.u32);
	// addi r10,r1,176
	ctx.r10.s64 = ctx.r1.s64 + 176;
	// stw r9,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r9.u32);
	// stvx v31,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,272
	ctx.r8.s64 = ctx.r1.s64 + 272;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// stvx v13,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v30,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// stvx v29,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// slw r8,r6,r5
	ctx.r8.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
	// stvx v28,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// stvx v18,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82702c3c
	if (!cr6.eq) goto loc_82702C3C;
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r11,r9
	ctx.r6.u64 = r11.u64 + ctx.r9.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r6,16
	ctx.r5.s64 = ctx.r6.s64 + 16;
	// lvx128 v9,r0,r6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r11,r6,r9
	r11.u64 = ctx.r6.u64 + ctx.r9.u64;
	// lvsl v6,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// vmrghb v5,v13,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v4,v13,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v10,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82702e7c
	if (!cr6.gt) goto loc_82702E7C;
	// li r6,0
	ctx.r6.s64 = 0;
loc_82702B50:
	// vslh v3,v11,v12
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vslh v6,v11,v29
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vslh v2,v9,v31
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// vslh v7,v11,v0
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// vslh v5,v5,v0
	// vadduhm v6,v3,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v3,v9,v12
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvx128 v27,r0,r5
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v5,v13,v5
	// addi r5,r10,16
	ctx.r5.s64 = ctx.r10.s64 + 16;
	// vadduhm v3,v2,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v2,v10,v0
	// vadduhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v26,v2,v10
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v6,v2,v27,v1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v3,v7,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v1,v8,v31
	// vslh v27,v4,v0
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v3,v3,v28
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v27,v13,v27
	// vslh v2,v7,v0
	// vsubshs v2,v7,v2
	// vadduhm v5,v2,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v2,v10,v12
	// vadduhm v3,v3,v5
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v10,v29
	// vsrah v3,v3,v30
	// vadduhm v2,v2,v5
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v8,v12
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v1,v1,v5
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v6,v0
	// vsubshs v4,v6,v5
	// vor v5,v11,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v9,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vadduhm v7,v26,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v27,v4,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v4,v10,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v8,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v6,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v6,v7,v1
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v6,v6,v28
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v7,v6,v27
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v7,v7,v30
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82702b50
	if (cr6.lt) goto loc_82702B50;
	// b 0x82702e7c
	goto loc_82702E7C;
loc_82702C3C:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// li r6,32
	ctx.r6.s64 = 32;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r31,r1,192
	r31.s64 = ctx.r1.s64 + 192;
	// addi r29,r1,224
	r29.s64 = ctx.r1.s64 + 224;
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r5,32
	ctx.r5.s64 = 32;
	// lvrx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r6,16
	ctx.r6.s64 = 16;
	// lvrx v11,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v11,v8,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// li r30,16
	r30.s64 = 16;
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// stvx v10,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v2,v13,v11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v7,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,256
	ctx.r6.s64 = ctx.r1.s64 + 256;
	// lvrx v11,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r31,32
	r31.s64 = 32;
	// vor v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v8,r11,r5
	temp.u32 = r11.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v7,v9,v8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// vmrghb v1,v13,v11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// vmrglb v27,v13,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v8,v1,v1
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// lvrx v11,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v7,v13,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v5,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v4,v9,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v3,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v11,v6,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v3,v5,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vor v9,v2,v2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vmrghb v5,v13,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v6,v27,v27
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v27.u8));
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v3,v13,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82702e7c
	if (!cr6.gt) goto loc_82702E7C;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_82702D18:
	// vor v26,v11,v11
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v11,v8,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vor v22,v10,v10
	_mm_store_si128((__m128i*)v22.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// vor v10,v7,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// addi r4,r11,32
	ctx.r4.s64 = r11.s64 + 32;
	// vor v7,v3,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// vslh v8,v11,v0
	// lvx128 v27,r0,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v21,v9,v9
	_mm_store_si128((__m128i*)v21.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vor v9,v6,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vor v6,v4,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// lvx128 v25,r0,r4
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v24,v8,v11
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r4,r10,-16
	ctx.r4.s64 = ctx.r10.s64 + -16;
	// vor v8,v5,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vslh v1,v11,v29
	// vslh v23,v11,v12
	// vslh v5,v8,v12
	// vslh v3,v8,v31
	// vadduhm v23,v23,v1
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v9,v0
	// vadduhm v20,v3,v5
	_mm_store_si128((__m128i*)v20.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// lvx128 v5,r0,r5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v4,v27,v5,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvsl v3,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v5,v5,v25,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vadduhm v27,v1,v9
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v1,v26,v0
	// addi r5,r10,-32
	ctx.r5.s64 = ctx.r10.s64 + -32;
	// vadduhm v24,v24,v23
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vmrghb v3,v13,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v23,v9,v12
	// vmrghb v2,v13,v5
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v25,v9,v29
	// vsubshs v26,v13,v1
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v1,v6,v12
	// vor v5,v3,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vor v3,v2,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vadduhm v25,v23,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v24,v24,v20
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v20.u16)));
	// vslh v2,v5,v0
	// vslh v23,v4,v0
	// vadduhm v27,v27,v25
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v25,v7,v31
	// vsubshs v2,v5,v2
	// vsubshs v23,v4,v23
	// vadduhm v2,v2,v26
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v26,v6,v31
	// vadduhm v26,v26,v1
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v21,v0
	// vsubshs v20,v13,v1
	// vslh v1,v10,v0
	// vadduhm v21,v1,v10
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vor v1,v2,v2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vadduhm v2,v27,v26
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vadduhm v27,v23,v20
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v20.u16)));
	// vadduhm v26,v24,v28
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v23,v22,v0
	// vadduhm v2,v2,v28
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v26,v26,v1
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v10,v29
	// vadduhm v2,v2,v27
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v27,v10,v12
	// vsubshs v23,v13,v23
	// vsrah v26,v26,v30
	// vsrah v2,v2,v30
	// vadduhm v27,v27,v1
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v7,v12
	// stvx v26,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v2,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v25,v25,v1
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v3,v0
	// vsubshs v24,v3,v1
	// vadduhm v1,v24,v23
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v2,v21,v27
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v27,v2,v25
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v27,v27,v28
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v2,v27,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsrah v2,v2,v30
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x82702d18
	if (cr6.lt) goto loc_82702D18;
loc_82702E7C:
	// vor v2,v18,v18
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v18.u8));
	// li r5,0
	ctx.r5.s64 = 0;
	// vor v1,v19,v19
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v19.u8));
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// bl 0x82701110
	sub_82701110(ctx, base);
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82702E98"))) PPC_WEAK_FUNC(sub_82702E98);
PPC_FUNC_IMPL(__imp__sub_82702E98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v24{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r9,r6,1056
	ctx.r9.s64 = ctx.r6.s64 + 1056;
	// vspltish v1,3
	// vspltish v0,4
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// rlwinm r10,r31,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// li r8,0
	ctx.r8.s64 = 0;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlh v0,v0,v0
	// vaddshs v2,v1,v13
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// addi r6,r10,3
	ctx.r6.s64 = ctx.r10.s64 + 3;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// vsubshs v24,v0,v13
	// bl 0x82700aa0
	sub_82700AA0(ctx, base);
	// vor v2,v24,v24
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v24.u8));
	// li r5,0
	ctx.r5.s64 = 0;
	// vspltish v1,7
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82701110
	sub_82701110(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82702F18"))) PPC_WEAK_FUNC(sub_82702F18);
PPC_FUNC_IMPL(__imp__sub_82702F18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r10,r7,31
	ctx.r10.u64 = ctx.r7.u32 & 0x1;
	// vspltish v20,7
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// vspltish v4,5
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// vspltish v5,1
	// addi r5,r10,3
	ctx.r5.s64 = ctx.r10.s64 + 3;
	// vspltish v12,4
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// vspltish v10,15
	// addi r8,r6,1056
	ctx.r8.s64 = ctx.r6.s64 + 1056;
	// vor v3,v4,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// li r6,1
	ctx.r6.s64 = 1;
	// vspltish v0,2
	// vrlh v9,v12,v12
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// stw r3,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r3.u32);
	// stvx v20,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,288
	ctx.r8.s64 = ctx.r1.s64 + 288;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// vaddshs v31,v10,v11
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// vsubshs v19,v9,v11
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// stw r7,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, ctx.r7.u32);
	// addi r10,r1,176
	ctx.r10.s64 = ctx.r1.s64 + 176;
	// stw r9,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r9.u32);
	// stvx v5,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,272
	ctx.r8.s64 = ctx.r1.s64 + 272;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// stvx v13,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v4,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// slw r8,r6,r5
	ctx.r8.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
	// stvx v31,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// stvx v19,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82703164
	if (!cr6.eq) goto loc_82703164;
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r11,r9
	ctx.r6.u64 = r11.u64 + ctx.r9.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r6,16
	ctx.r5.s64 = ctx.r6.s64 + 16;
	// lvx128 v6,r0,r6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r11,r6,r9
	r11.u64 = ctx.r6.u64 + ctx.r9.u64;
	// lvsl v2,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v1,r0,r5
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v10,v6,v1,v2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x8270339c
	if (!cr6.gt) goto loc_8270339C;
	// li r6,0
	ctx.r6.s64 = 0;
loc_82703070:
	// vor v30,v9,v9
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v9,v11,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vor v11,v7,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// vslh v28,v30,v0
	// vslh v29,v9,v5
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vslh v1,v11,v12
	// vslh v2,v11,v3
	// lvx128 v27,r0,r5
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v7,v11,v0
	// addi r5,r10,16
	ctx.r5.s64 = ctx.r10.s64 + 16;
	// vadduhm v2,v1,v2
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v1,v9,v12
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v1,v29,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v7,v7,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vsubshs v29,v30,v28
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v30,v8,v8
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v10,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v6,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v2,v7,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vperm v7,v28,v27,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vslh v27,v8,v5
	// vmrghb v6,v13,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v28,v2,v31
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vmrglb v1,v13,v7
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v6,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v1,v1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vslh v1,v7,v0
	// vslh v26,v6,v0
	// vsubshs v1,v13,v1
	// vadduhm v2,v29,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v29,v10,v12
	// vslh v1,v10,v3
	// vadduhm v28,v28,v2
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v2,v10,v0
	// vadduhm v1,v29,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v29,v8,v12
	// vadduhm v2,v2,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v29,v27,v29
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v27,v30,v0
	// vadduhm v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsubshs v30,v30,v27
	// vadduhm v1,v2,v29
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vsubshs v27,v13,v26
	// vadduhm v1,v1,v31
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v27,v30,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v30,v28,v4
	// vadduhm v2,v1,v27
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v27.u16)));
	// stvx v30,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vsrah v2,v2,v4
	// stvx v2,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82703070
	if (cr6.lt) goto loc_82703070;
	// b 0x8270339c
	goto loc_8270339C;
loc_82703164:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// li r6,32
	ctx.r6.s64 = 32;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r31,r1,192
	r31.s64 = ctx.r1.s64 + 192;
	// addi r29,r1,224
	r29.s64 = ctx.r1.s64 + 224;
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r5,32
	ctx.r5.s64 = 32;
	// lvrx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r6,16
	ctx.r6.s64 = 16;
	// lvrx v11,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v11,v8,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// li r30,16
	r30.s64 = 16;
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// stvx v10,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v7,v13,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v2,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,256
	ctx.r6.s64 = ctx.r1.s64 + 256;
	// lvrx v11,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v29,v13,v10
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v11,v2,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v8,r11,r5
	temp.u32 = r11.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v10,v9,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// li r31,32
	r31.s64 = 32;
	// vor v8,v7,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v29,v29
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v29.u8));
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// vmrghb v28,v13,v11
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// vmrglb v27,v13,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v30,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)v30.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvrx v11,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v1,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v2,v9,v11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vor v1,v1,v30
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vor v11,v28,v28
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v28.u8));
	// vor v9,v27,v27
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v27.u8));
	// vmrghb v29,v13,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v28,v13,v2
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v30,v13,v1
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x8270339c
	if (!cr6.gt) goto loc_8270339C;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_82703240:
	// vor v27,v8,v8
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vor v8,v11,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vor v11,v29,v29
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v29.u8));
	// addi r5,r11,16
	ctx.r5.s64 = r11.s64 + 16;
	// addi r4,r11,32
	ctx.r4.s64 = r11.s64 + 32;
	// vslh v18,v27,v0
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lvx128 v29,r0,r11
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v26,v11,v3
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v25,v11,v12
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// vslh v1,v11,v0
	// lvx128 v24,r0,r4
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v27,v27,v18
	// addi r4,r10,-16
	ctx.r4.s64 = ctx.r10.s64 + -16;
	// vadduhm v22,v25,v26
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vor v26,v7,v7
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v10,v10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vadduhm v23,v1,v11
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vor v10,v30,v30
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)v30.u8));
	// vslh v1,v8,v12
	// vslh v30,v8,v5
	// vor v25,v6,v6
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v9,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v28,v28
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v28.u8));
	// vadduhm v21,v30,v1
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// lvx128 v30,r0,r5
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v1,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v29,v29,v30,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vperm v2,v30,v24,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v23,v23,v22
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vslh v24,v9,v12
	// addi r5,r10,-32
	ctx.r5.s64 = ctx.r10.s64 + -32;
	// vslh v1,v9,v3
	// vmrglb v28,v13,v29
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v30,v13,v2
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v2,v13,v29
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v1,v24,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vor v29,v2,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vslh v2,v9,v0
	// vslh v22,v29,v0
	// vadduhm v2,v2,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubshs v24,v13,v22
	// vslh v22,v6,v5
	// vadduhm v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v18,v27,v24
	_mm_store_si128((__m128i*)v18.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v24,v23,v21
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vslh v23,v6,v12
	// vslh v21,v25,v0
	// vadduhm v27,v24,v31
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v23,v22,v23
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v22,v28,v0
	// vsubshs v25,v25,v21
	// vslh v24,v7,v5
	// vadduhm v2,v2,v23
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vsubshs v22,v13,v22
	// vslh v21,v10,v0
	// vadduhm v27,v27,v18
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v18.u16)));
	// vadduhm v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v1,v25,v22
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vslh v25,v10,v12
	// vadduhm v21,v21,v10
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v23,v30,v0
	// vadduhm v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v10,v3
	// vsrah v27,v27,v4
	// vsrah v2,v2,v4
	// vadduhm v1,v25,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v25,v7,v12
	// stvx v27,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v2,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v2,v21,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v25,v24,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v24,v26,v0
	// vadduhm v1,v2,v25
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vsubshs v26,v26,v24
	// vsubshs v24,v13,v23
	// vadduhm v26,v26,v24
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v1,v1,v31
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v2,v1,v26
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vsrah v2,v2,v4
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x82703240
	if (cr6.lt) goto loc_82703240;
loc_8270339C:
	// vor v2,v19,v19
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v19.u8));
	// li r5,0
	ctx.r5.s64 = 0;
	// vor v1,v20,v20
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v20.u8));
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// bl 0x82701110
	sub_82701110(ctx, base);
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_827033B8"))) PPC_WEAK_FUNC(sub_827033B8);
PPC_FUNC_IMPL(__imp__sub_827033B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// vspltish v13,2
	// cmpwi cr6,r6,8
	cr6.compare<int32_t>(ctx.r6.s32, 8, xer);
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// vspltish v12,1
	// vsrah v11,v1,v13
	// bne cr6,0x827034e8
	if (!cr6.eq) goto loc_827034E8;
	// rlwinm r6,r4,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r4,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// li r7,2
	ctx.r7.s64 = 2;
loc_827033E4:
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// lvx128 v10,r0,r3
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r6,r3
	r11.u64 = ctx.r6.u64 + ctx.r3.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r9,r3,r4
	ctx.r9.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r10,r31,r3
	ctx.r10.u64 = r31.u64 + ctx.r3.u64;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// addi r30,r9,16
	r30.s64 = ctx.r9.s64 + 16;
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r8,16
	r11.s64 = ctx.r8.s64 + 16;
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v5,r0,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// lvsl v3,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v6,v6,v5,v2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvx128 v4,r0,r30
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,96
	ctx.r9.s64 = ctx.r5.s64 + 96;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v8,v8,v4,v3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// lvx128 v29,r0,r11
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v4,v0,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// lvx128 v30,r0,r3
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v9,v29,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v31,r0,r10
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v0,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v1,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v9,v0,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v5,v31,v30,v1
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// addi r8,r5,144
	ctx.r8.s64 = ctx.r5.s64 + 144;
	// vmrghb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vaddshs v6,v4,v10
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// vaddshs v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// vmrghb v7,v0,v5
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v10,v10,v12
	// vaddshs v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vslh v7,v6,v12
	// vslh v9,v9,v12
	// vaddshs v10,v10,v11
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vslh v8,v8,v12
	// vaddshs v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v9,v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v10,v10,v13
	// vaddshs v8,v8,v11
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v7,v7,v13
	// vsrah v9,v9,v13
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v8,v8,v13
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,192
	ctx.r5.s64 = ctx.r5.s64 + 192;
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x827033e4
	if (!cr6.eq) goto loc_827033E4;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_827034E8:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x82703680
	if (!cr6.gt) goto loc_82703680;
	// addi r11,r6,-1
	r11.s64 = ctx.r6.s64 + -1;
	// rlwinm r6,r4,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r31,r4,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
loc_82703504:
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// lvx128 v10,r0,r3
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r6,r3
	r11.u64 = ctx.r6.u64 + ctx.r3.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r9,r3,r4
	ctx.r9.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r10,r31,r3
	ctx.r10.u64 = r31.u64 + ctx.r3.u64;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r8,16
	r11.s64 = ctx.r8.s64 + 16;
	// addi r30,r9,16
	r30.s64 = ctx.r9.s64 + 16;
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// lvsl v9,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvsl v3,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r5,96
	ctx.r9.s64 = ctx.r5.s64 + 96;
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// lvx128 v4,r0,r30
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v29,r0,r8
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r5,144
	ctx.r8.s64 = ctx.r5.s64 + 144;
	// lvx128 v31,r0,r10
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v30,r0,r3
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v9,v8,v4,v3
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vperm v8,v6,v5,v2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvsl v1,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v3,v0,v10
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v7,v31,v30,v1
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrghb v5,v0,v9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v4,v0,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v31,v0,v10
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v1,v0,v7
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v2,v3,v5
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vmrglb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vmrglb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v5,v5,v12
	// vaddshs v5,v5,v11
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v5,v5,v13
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// vperm v6,v29,v28,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrghb v3,v0,v6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v8,v0,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v6,v2,v12
	// vaddshs v2,v31,v10
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v4,v4,v3
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v1,v10,v9
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v4,v4,v12
	// vslh v3,v3,v12
	// vaddshs v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vslh v9,v9,v12
	// vaddshs v6,v6,v11
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v4,v4,v11
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v3,v3,v11
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vslh v10,v2,v12
	// vslh v7,v1,v12
	// vsrah v6,v6,v13
	// vsrah v4,v4,v13
	// vsrah v3,v3,v13
	// vaddshs v10,v10,v11
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vslh v8,v8,v12
	// stvx v6,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v4,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v9,v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v3,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v10,v10,v13
	// vaddshs v8,v8,v11
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v7,v7,v13
	// vsrah v9,v9,v13
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// vsrah v8,v8,v13
	// addi r9,r5,64
	ctx.r9.s64 = ctx.r5.s64 + 64;
	// addi r8,r5,112
	ctx.r8.s64 = ctx.r5.s64 + 112;
	// addi r30,r5,160
	r30.s64 = ctx.r5.s64 + 160;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,192
	ctx.r5.s64 = ctx.r5.s64 + 192;
	// stvx v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stvx v9,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82703504
	if (!cr6.eq) goto loc_82703504;
loc_82703680:
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8270368C"))) PPC_WEAK_FUNC(sub_8270368C);
PPC_FUNC_IMPL(__imp__sub_8270368C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82703690"))) PPC_WEAK_FUNC(sub_82703690);
PPC_FUNC_IMPL(__imp__sub_82703690) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcfc
	// add r9,r3,r4
	ctx.r9.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r31,r9,16
	r31.s64 = ctx.r9.s64 + 16;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v13,1
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vperm v9,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v8,r0,r31
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// vperm v12,v12,v8,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r8,16
	r11.s64 = ctx.r8.s64 + 16;
	// addi r31,r10,16
	r31.s64 = ctx.r10.s64 + 16;
	// vmrghb v4,v0,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r7,8
	cr6.compare<int32_t>(ctx.r7.s32, 8, xer);
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v11,v11,v8,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v10,r0,r10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,96
	ctx.r9.s64 = ctx.r5.s64 + 96;
	// lvx128 v5,r0,r31
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r5,144
	ctx.r8.s64 = ctx.r5.s64 + 144;
	// lvsl v6,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v9,v9,v8,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v10,v10,v5,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vslh v8,v12,v13
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v8,v8,v4
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vslh v7,v11,v13
	// vslh v5,v9,v13
	// vslh v6,v10,v13
	// vadduhm v7,v7,v12
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v12,v8,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v5,v5,v11
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v6,v6,v9
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v12,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v8,v6,v10
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82703848
	if (!cr6.eq) goto loc_82703848;
	// add r11,r10,r4
	r11.u64 = ctx.r10.u64 + ctx.r4.u64;
	// rlwinm r10,r4,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r30,r11,16
	r30.s64 = r11.s64 + 16;
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// addi r29,r9,16
	r29.s64 = ctx.r9.s64 + 16;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r10,r4
	r11.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v9,r0,r30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r5,240
	r31.s64 = ctx.r5.s64 + 240;
	// vperm v12,v12,v9,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// lvx128 v9,r0,r29
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r5,192
	ctx.r8.s64 = ctx.r5.s64 + 192;
	// vperm v11,v11,v9,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v8,v9,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v9,v0,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r0,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,288
	r11.s64 = ctx.r5.s64 + 288;
	// vperm v7,v11,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// addi r10,r5,336
	ctx.r10.s64 = ctx.r5.s64 + 336;
	// vmrghb v11,v0,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v8,v12,v13
	// vslh v6,v9,v13
	// vmrghb v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v7,v11,v13
	// vslh v13,v0,v13
	// vadduhm v10,v10,v12
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v12,v7,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v10,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v12,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v6,v0
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v13,v12,v9
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82703848:
	// cmpwi cr6,r6,8
	cr6.compare<int32_t>(ctx.r6.s32, 8, xer);
	// bne cr6,0x827038bc
	if (!cr6.eq) goto loc_827038BC;
	// rlwinm r31,r4,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r3,8
	ctx.r8.s64 = ctx.r3.s64 + 8;
	// addi r9,r5,16
	ctx.r9.s64 = ctx.r5.s64 + 16;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x827038bc
	if (!cr6.gt) goto loc_827038BC;
	// addi r10,r7,-1
	ctx.r10.s64 = ctx.r7.s64 + -1;
	// add r11,r31,r8
	r11.u64 = r31.u64 + ctx.r8.u64;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// subf r30,r31,r4
	r30.s64 = ctx.r4.s64 - r31.s64;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
loc_82703878:
	// lbzx r10,r30,r11
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// rotlwi r5,r10,1
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// lbz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// rotlwi r3,r6,1
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// add r5,r10,r5
	ctx.r5.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// sth r5,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r5.u16);
	// sth r10,48(r9)
	PPC_STORE_U16(ctx.r9.u32 + 48, ctx.r10.u16);
	// addi r9,r9,96
	ctx.r9.s64 = ctx.r9.s64 + 96;
	// bne cr6,0x82703878
	if (!cr6.eq) goto loc_82703878;
loc_827038BC:
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_827038C0"))) PPC_WEAK_FUNC(sub_827038C0);
PPC_FUNC_IMPL(__imp__sub_827038C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r3,r4
	ctx.r8.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v13,2
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vperm v11,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r10,r4
	ctx.r7.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// vperm v10,v12,v10,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v12,v1,v13
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r10,r9,16
	ctx.r10.s64 = ctx.r9.s64 + 16;
	// lvx128 v6,r0,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r7,16
	ctx.r8.s64 = ctx.r7.s64 + 16;
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v9,v9,v6,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v5,r0,r6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v3,v0,v11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v4,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r9,r4
	r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// vperm v7,v8,v5,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v6,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// vperm v6,v8,v4,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v5,r0,r7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v30,v0,v7
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v2,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vor v29,v7,v7
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vperm v8,v5,v8,v2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// lvx128 v1,r0,r9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v2,v0,v10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v5,v5,v1,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v4,r0,r10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v1,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vor v7,v30,v30
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v30.u8));
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v31,r0,r11
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v30,v0,v29
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v4,v4,v31,v1
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrglb v1,v0,v9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v31,v0,v8
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v29,v0,v6
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v28,v0,v5
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v27,v0,v4
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v0,v0,v4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsldoi v4,v11,v3,2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v3.u8), 14));
	// vsldoi v3,v10,v2,2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v2.u8), 14));
	// vsldoi v2,v9,v1,2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v1.u8), 14));
	// vsldoi v1,v8,v31,2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8), 14));
	// vsldoi v31,v7,v30,2
	_mm_store_si128((__m128i*)v31.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v30.u8), 14));
	// vsubshs v4,v4,v11
	// vsubshs v3,v3,v10
	// vsldoi v30,v6,v29,2
	_mm_store_si128((__m128i*)v30.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v29.u8), 14));
	// vsubshs v2,v2,v9
	// vsubshs v1,v1,v8
	// vsubshs v31,v31,v7
	// vsldoi v29,v5,v28,2
	_mm_store_si128((__m128i*)v29.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v28.u8), 14));
	// vsubshs v30,v30,v6
	// vsldoi v28,v0,v27,2
	_mm_store_si128((__m128i*)v28.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v27.u8), 14));
	// vaddshs v4,v4,v12
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v3,v3,v12
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// vaddshs v2,v2,v12
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// addi r10,r5,96
	ctx.r10.s64 = ctx.r5.s64 + 96;
	// vsubshs v29,v29,v5
	// addi r9,r5,144
	ctx.r9.s64 = ctx.r5.s64 + 144;
	// vsubshs v28,v28,v0
	// addi r8,r5,192
	ctx.r8.s64 = ctx.r5.s64 + 192;
	// vaddshs v1,v1,v12
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// addi r7,r5,240
	ctx.r7.s64 = ctx.r5.s64 + 240;
	// vaddshs v31,v31,v12
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// addi r6,r5,288
	ctx.r6.s64 = ctx.r5.s64 + 288;
	// vaddshs v30,v30,v12
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// addi r4,r5,336
	ctx.r4.s64 = ctx.r5.s64 + 336;
	// vaddshs v29,v29,v12
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v12,v28,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsrah v4,v4,v13
	// vsrah v3,v3,v13
	// vsrah v2,v2,v13
	// vsrah v1,v1,v13
	// vsrah v31,v31,v13
	// vsrah v30,v30,v13
	// vsrah v29,v29,v13
	// vsrah v13,v12,v13
	// vaddshs v12,v4,v11
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v11,v3,v10
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v10,v2,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v9,v1,v8
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v8,v31,v7
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// stvx v12,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v30,v6
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v12,v29,v5
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82703AB8"))) PPC_WEAK_FUNC(sub_82703AB8);
PPC_FUNC_IMPL(__imp__sub_82703AB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// vspltish v13,2
	// cmpwi cr6,r7,8
	cr6.compare<int32_t>(ctx.r7.s32, 8, xer);
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// vspltish v12,1
	// vsrah v11,v1,v13
	// bne cr6,0x82703be0
	if (!cr6.eq) goto loc_82703BE0;
	// rlwinm r7,r4,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// li r8,2
	ctx.r8.s64 = 2;
loc_82703ADC:
	// addi r10,r3,16
	ctx.r10.s64 = ctx.r3.s64 + 16;
	// lvx128 v10,r0,r3
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r7,r3
	r11.u64 = ctx.r7.u64 + ctx.r3.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r9,r3,r4
	ctx.r9.u64 = ctx.r3.u64 + ctx.r4.u64;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lvx128 v5,r0,r6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r5,144
	ctx.r6.s64 = ctx.r5.s64 + 144;
	// lvsl v4,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r5,96
	ctx.r9.s64 = ctx.r5.s64 + 96;
	// lvx128 v3,r0,r3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v9,v8,v5,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v31,r0,r11
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v8,v6,v3,v2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvx128 v7,r0,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v6,v0,v10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v1,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v7,v7,v31,v1
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// vmrglb v5,v0,v9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// add r3,r10,r4
	ctx.r3.u64 = ctx.r10.u64 + ctx.r4.u64;
	// vmrglb v4,v0,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v3,v0,v7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsldoi v6,v10,v6,2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8), 14));
	// vsldoi v5,v9,v5,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v5.u8), 14));
	// vsldoi v4,v8,v4,2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8), 14));
	// vsldoi v3,v7,v3,2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v3.u8), 14));
	// vaddshs v10,v10,v6
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v8,v8,v4
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v7,v7,v3
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vslh v10,v10,v12
	// vslh v9,v9,v12
	// vslh v8,v8,v12
	// vslh v7,v7,v12
	// vaddshs v10,v10,v11
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v9,v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v8,v8,v11
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v10,v10,v13
	// vsrah v9,v9,v13
	// vsrah v8,v8,v13
	// vsrah v7,v7,v13
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,192
	ctx.r5.s64 = ctx.r5.s64 + 192;
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82703adc
	if (!cr6.eq) goto loc_82703ADC;
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82703BE0:
	// rlwinm r31,r4,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// li r9,4
	ctx.r9.s64 = 4;
loc_82703BEC:
	// add r10,r3,r4
	ctx.r10.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvx128 v4,r0,r3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r31,r3
	r11.u64 = r31.u64 + ctx.r3.u64;
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v2,r0,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lbz r11,0(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v9,v4,v8,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v6,r0,r7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lbz r8,0(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lvx128 v3,r0,r6
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v8,v10,v3,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v5,r0,r10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v6,v2,v6,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// sth r11,-80(r1)
	PPC_STORE_U16(ctx.r1.u32 + -80, r11.u16);
	// vmrglb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lbz r11,0(r7)
	r11.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// addi r7,r5,144
	ctx.r7.s64 = ctx.r5.s64 + 144;
	// sth r8,-48(r1)
	PPC_STORE_U16(ctx.r1.u32 + -48, ctx.r8.u16);
	// vmrghb v3,v0,v8
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v2,v0,v6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r8,r5,96
	ctx.r8.s64 = ctx.r5.s64 + 96;
	// sth r11,-64(r1)
	PPC_STORE_U16(ctx.r1.u32 + -64, r11.u16);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// vperm v5,v5,v4,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v4,v0,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v8,v0,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// sth r11,-32(r1)
	PPC_STORE_U16(ctx.r1.u32 + -32, r11.u16);
	// vmrglb v6,v0,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsldoi v1,v4,v10,2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v31,v2,v8,2
	_mm_store_si128((__m128i*)v31.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v8.u8), 14));
	// vsldoi v30,v3,v9,2
	_mm_store_si128((__m128i*)v30.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vsldoi v29,v5,v6,2
	_mm_store_si128((__m128i*)v29.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8), 14));
	// vaddshs v4,v4,v1
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v3,v3,v30
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v5,v5,v29
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vslh v4,v4,v12
	// vslh v2,v2,v12
	// vslh v3,v3,v12
	// vslh v5,v5,v12
	// vaddshs v4,v4,v11
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v2,v2,v11
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v3,v3,v11
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v5,v5,v11
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v4,v4,v13
	// vsrah v2,v2,v13
	// vsrah v3,v3,v13
	// vsrah v5,v5,v13
	// stvx v4,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v3,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-80
	r11.s64 = ctx.r1.s64 + -80;
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-64
	r11.s64 = ctx.r1.s64 + -64;
	// vsldoi v28,v10,v28,2
	_mm_store_si128((__m128i*)v28.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v28.u8), 14));
	// vaddshs v10,v10,v28
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)v28.s16)));
	// lvx128 v27,r0,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// vsldoi v27,v8,v27,2
	_mm_store_si128((__m128i*)v27.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v27.u8), 14));
	// vslh v10,v10,v12
	// lvx128 v26,r0,r11
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// vsldoi v26,v9,v26,2
	_mm_store_si128((__m128i*)v26.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v26.u8), 14));
	// vaddshs v8,v8,v27
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v27.s16)));
	// lvx128 v25,r0,r11
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// vsldoi v25,v6,v25,2
	_mm_store_si128((__m128i*)v25.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v25.u8), 14));
	// vaddshs v9,v9,v26
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vslh v8,v8,v12
	// vaddshs v6,v6,v25
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v25.s16)));
	// stvx v2,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v9,v9,v12
	// vslh v6,v6,v12
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// vaddshs v10,v10,v11
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r8,r5,64
	ctx.r8.s64 = ctx.r5.s64 + 64;
	// vaddshs v8,v8,v11
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r7,r5,112
	ctx.r7.s64 = ctx.r5.s64 + 112;
	// vaddshs v9,v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r6,r5,160
	ctx.r6.s64 = ctx.r5.s64 + 160;
	// vaddshs v6,v6,v11
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// vsrah v10,v10,v13
	// add r3,r10,r4
	ctx.r3.u64 = ctx.r10.u64 + ctx.r4.u64;
	// vsrah v8,v8,v13
	// addi r5,r5,192
	ctx.r5.s64 = ctx.r5.s64 + 192;
	// vsrah v9,v9,v13
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vsrah v6,v6,v13
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v6,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82703bec
	if (!cr6.eq) goto loc_82703BEC;
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82703D90"))) PPC_WEAK_FUNC(sub_82703D90);
PPC_FUNC_IMPL(__imp__sub_82703D90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// add r8,r3,r4
	ctx.r8.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvx128 v11,r0,r3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// vspltish v13,2
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// vspltish v12,1
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// lvsl v6,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r10,r4
	ctx.r7.u64 = ctx.r10.u64 + ctx.r4.u64;
	// vperm v9,v9,v5,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvsl v6,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v5,r0,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r7,16
	ctx.r10.s64 = ctx.r7.s64 + 16;
	// vperm v8,v8,v5,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v6,r0,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r9,16
	ctx.r8.s64 = ctx.r9.s64 + 16;
	// vperm v7,v7,v6,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// add r11,r9,r4
	r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// lvx128 v3,r0,r10
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v11,v1,v13
	// lvx128 v6,r0,r7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v4,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrglb v2,v0,v10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v6,v6,v3,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v1,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// vperm v5,v5,v4,v1
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// vmrglb v30,v0,v6
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v29,v0,v7
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v1,r0,r9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v28,v0,v5
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v4,v4,v1,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// lvx128 v3,r0,r10
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r11
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v1,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v3,v3,v31,v1
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrglb v1,v0,v9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v31,v0,v8
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v27,v0,v4
	_mm_store_si128((__m128i*)v27.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v26,v0,v3
	_mm_store_si128((__m128i*)v26.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v4,v0,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v0,v0,v3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsldoi v3,v10,v2,2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v2.u8), 14));
	// vsldoi v2,v9,v1,2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v1.u8), 14));
	// vsldoi v1,v8,v31,2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8), 14));
	// vsldoi v31,v6,v30,2
	_mm_store_si128((__m128i*)v31.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v30.u8), 14));
	// vsldoi v30,v7,v29,2
	_mm_store_si128((__m128i*)v30.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v29.u8), 14));
	// vsldoi v29,v5,v28,2
	_mm_store_si128((__m128i*)v29.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v28.u8), 14));
	// vslh v25,v2,v12
	// vsldoi v28,v4,v27,2
	_mm_store_si128((__m128i*)v28.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v27.u8), 14));
	// vslh v24,v1,v12
	// vsldoi v27,v0,v26,2
	_mm_store_si128((__m128i*)v27.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v26.u8), 14));
	// vslh v23,v31,v12
	// vslh v26,v3,v12
	// vslh v22,v30,v12
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// vslh v21,v29,v12
	// addi r10,r5,96
	ctx.r10.s64 = ctx.r5.s64 + 96;
	// vslh v20,v28,v12
	// addi r9,r5,144
	ctx.r9.s64 = ctx.r5.s64 + 144;
	// vslh v12,v27,v12
	// addi r8,r5,336
	ctx.r8.s64 = ctx.r5.s64 + 336;
	// vaddshs v3,v26,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v2,v25,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v1,v24,v1
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v31,v23,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v12,v12,v27
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v10,v3,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v9,v2,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v8,v1,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v30,v22,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v29,v21,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v28,v20,v28
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v6,v31,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v10,v10,v11
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v9,v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v12,v8,v11
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v7,v30,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v5,v29,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v4,v28,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v8,v6,v11
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v6,v10,v13
	// vsrah v3,v9,v13
	// vaddshs v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v10,v5,v11
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v9,v4,v11
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v6,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v0,v11
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v3,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v12,v12,v13
	// addi r11,r5,192
	r11.s64 = ctx.r5.s64 + 192;
	// vsrah v11,v8,v13
	// vsrah v0,v0,v13
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,240
	ctx.r10.s64 = ctx.r5.s64 + 240;
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v11,v10,v13
	// addi r9,r5,288
	ctx.r9.s64 = ctx.r5.s64 + 288;
	// vsrah v12,v7,v13
	// vsrah v10,v9,v13
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82703FA4"))) PPC_WEAK_FUNC(sub_82703FA4);
PPC_FUNC_IMPL(__imp__sub_82703FA4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82703FA8"))) PPC_WEAK_FUNC(sub_82703FA8);
PPC_FUNC_IMPL(__imp__sub_82703FA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// vspltish v0,1
	// cmpwi cr6,r6,8
	cr6.compare<int32_t>(ctx.r6.s32, 8, xer);
	// vspltish v13,4
	// bne cr6,0x8270407c
	if (!cr6.eq) goto loc_8270407C;
	// li r11,2
	r11.s64 = 2;
loc_82703FBC:
	// addi r10,r3,48
	ctx.r10.s64 = ctx.r3.s64 + 48;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// addi r8,r3,64
	ctx.r8.s64 = ctx.r3.s64 + 64;
	// addi r7,r3,96
	ctx.r7.s64 = ctx.r3.s64 + 96;
	// addi r6,r3,112
	ctx.r6.s64 = ctx.r3.s64 + 112;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r3,144
	ctx.r10.s64 = ctx.r3.s64 + 144;
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,160
	ctx.r9.s64 = ctx.r3.s64 + 160;
	// lvx128 v7,r0,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v8,v12,v8,2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8), 14));
	// lvx128 v10,r0,r7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v7,v11,v7,2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8), 14));
	// lvx128 v6,r0,r6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r4,144
	ctx.r8.s64 = ctx.r4.s64 + 144;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v6,v10,v6,2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8), 14));
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v12,v8,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsldoi v5,v9,v5,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v5.u8), 14));
	// vaddshs v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r10,r4,48
	ctx.r10.s64 = ctx.r4.s64 + 48;
	// vaddshs v10,v6,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// addi r9,r4,96
	ctx.r9.s64 = ctx.r4.s64 + 96;
	// vslh v12,v12,v0
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// vaddshs v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// addi r3,r3,192
	ctx.r3.s64 = ctx.r3.s64 + 192;
	// vslh v11,v11,v0
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vslh v10,v10,v0
	// vaddshs v12,v12,v1
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vslh v9,v9,v0
	// vaddshs v11,v11,v1
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsrah v12,v12,v13
	// vaddshs v9,v9,v1
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsrah v11,v11,v13
	// vsrah v10,v10,v13
	// stvx v12,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r4,192
	ctx.r4.s64 = ctx.r4.s64 + 192;
	// vsrah v9,v9,v13
	// stvx v11,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82703fbc
	if (!cr6.eq) goto loc_82703FBC;
	// blr 
	return;
loc_8270407C:
	// li r11,4
	r11.s64 = 4;
loc_82704080:
	// addi r10,r3,48
	ctx.r10.s64 = ctx.r3.s64 + 48;
	// lvx128 v8,r0,r3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,96
	ctx.r9.s64 = ctx.r3.s64 + 96;
	// addi r7,r3,64
	ctx.r7.s64 = ctx.r3.s64 + 64;
	// addi r6,r3,112
	ctx.r6.s64 = ctx.r3.s64 + 112;
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// lvx128 v7,r0,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r3,144
	ctx.r10.s64 = ctx.r3.s64 + 144;
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,160
	ctx.r9.s64 = ctx.r3.s64 + 160;
	// lvx128 v11,r0,r7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r3,128
	ctx.r7.s64 = ctx.r3.s64 + 128;
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v29,v7,v11,2
	_mm_store_si128((__m128i*)v29.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v11.u8), 14));
	// vsldoi v28,v6,v10,2
	_mm_store_si128((__m128i*)v28.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// addi r6,r3,176
	ctx.r6.s64 = ctx.r3.s64 + 176;
	// lvx128 v5,r0,r10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r4,96
	ctx.r9.s64 = ctx.r4.s64 + 96;
	// vsldoi v27,v5,v9,2
	_mm_store_si128((__m128i*)v27.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v30,v8,v12,2
	_mm_store_si128((__m128i*)v30.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 14));
	// vaddshs v7,v29,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v6,v28,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// addi r8,r3,80
	ctx.r8.s64 = ctx.r3.s64 + 80;
	// lvx128 v4,r0,r10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r4,48
	ctx.r10.s64 = ctx.r4.s64 + 48;
	// vsldoi v4,v12,v4,2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v4.u8), 14));
	// vaddshs v5,v27,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// lvx128 v2,r0,r7
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v8,v30,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v31,r0,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v2,v10,v2,2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v2.u8), 14));
	// vsldoi v31,v9,v31,2
	_mm_store_si128((__m128i*)v31.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v31.u8), 14));
	// vslh v7,v7,v0
	// vaddshs v4,v4,v12
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// lvx128 v3,r0,r8
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v6,v6,v0
	// vsldoi v3,v11,v3,2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v3.u8), 14));
	// vslh v12,v5,v0
	// addi r8,r4,144
	ctx.r8.s64 = ctx.r4.s64 + 144;
	// vaddshs v2,v2,v10
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// vaddshs v31,v31,v9
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// addi r7,r4,160
	ctx.r7.s64 = ctx.r4.s64 + 160;
	// vslh v8,v8,v0
	// addi r3,r3,192
	ctx.r3.s64 = ctx.r3.s64 + 192;
	// vaddshs v12,v12,v1
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vaddshs v10,v7,v1
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v9,v6,v1
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v3,v3,v11
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v12,v12,v13
	// vsrah v10,v10,v13
	// vsrah v9,v9,v13
	// vaddshs v11,v8,v1
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// stvx v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v12,v3,v0
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v10,v4,v0
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v9,v2,v0
	// vsrah v11,v11,v13
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// vaddshs v12,v12,v1
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// addi r9,r4,64
	ctx.r9.s64 = ctx.r4.s64 + 64;
	// vaddshs v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// addi r8,r4,112
	ctx.r8.s64 = ctx.r4.s64 + 112;
	// vaddshs v9,v9,v1
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// stvx v11,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v11,v31,v0
	// vsrah v12,v12,v13
	// addi r4,r4,192
	ctx.r4.s64 = ctx.r4.s64 + 192;
	// vsrah v10,v10,v13
	// vsrah v9,v9,v13
	// vaddshs v11,v11,v1
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v11,v11,v13
	// stvx v11,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82704080
	if (!cr6.eq) goto loc_82704080;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_827041D0"))) PPC_WEAK_FUNC(sub_827041D0);
PPC_FUNC_IMPL(__imp__sub_827041D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// addi r11,r3,48
	r11.s64 = ctx.r3.s64 + 48;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r3,96
	ctx.r10.s64 = ctx.r3.s64 + 96;
	// vspltish v0,1
	// addi r9,r3,144
	ctx.r9.s64 = ctx.r3.s64 + 144;
	// vspltish v13,4
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// addi r7,r3,64
	ctx.r7.s64 = ctx.r3.s64 + 64;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r3,112
	r11.s64 = ctx.r3.s64 + 112;
	// lvx128 v10,r0,r10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r3,192
	ctx.r10.s64 = ctx.r3.s64 + 192;
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,240
	ctx.r9.s64 = ctx.r3.s64 + 240;
	// lvx128 v4,r0,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r3,288
	ctx.r8.s64 = ctx.r3.s64 + 288;
	// lvx128 v3,r0,r7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r3,336
	ctx.r7.s64 = ctx.r3.s64 + 336;
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r3,160
	r11.s64 = ctx.r3.s64 + 160;
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r3,208
	ctx.r10.s64 = ctx.r3.s64 + 208;
	// lvx128 v7,r0,r9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,352
	ctx.r9.s64 = ctx.r3.s64 + 352;
	// vsldoi v4,v12,v4,2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v4.u8), 14));
	// lvx128 v6,r0,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v3,v11,v3,2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v3.u8), 14));
	// lvx128 v31,r0,r11
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r3,256
	r11.s64 = ctx.r3.s64 + 256;
	// lvx128 v30,r0,r10
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r3,304
	ctx.r10.s64 = ctx.r3.s64 + 304;
	// lvx128 v27,r0,r9
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v2,v10,v2,2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v2.u8), 14));
	// vsldoi v31,v9,v31,2
	_mm_store_si128((__m128i*)v31.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v31.u8), 14));
	// vslh v26,v4,v0
	// vsldoi v30,v8,v30,2
	_mm_store_si128((__m128i*)v30.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v30.u8), 14));
	// vslh v25,v3,v0
	// lvx128 v29,r0,r11
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v27,v5,v27,2
	_mm_store_si128((__m128i*)v27.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v27.u8), 14));
	// lvx128 v28,r0,r10
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v29,v7,v29,2
	_mm_store_si128((__m128i*)v29.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v29.u8), 14));
	// vsldoi v28,v6,v28,2
	_mm_store_si128((__m128i*)v28.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v28.u8), 14));
	// vslh v24,v2,v0
	// vslh v23,v31,v0
	// addi r11,r4,48
	r11.s64 = ctx.r4.s64 + 48;
	// vslh v22,v30,v0
	// addi r10,r4,96
	ctx.r10.s64 = ctx.r4.s64 + 96;
	// vslh v21,v29,v0
	// addi r9,r4,144
	ctx.r9.s64 = ctx.r4.s64 + 144;
	// vslh v20,v28,v0
	// addi r8,r4,192
	ctx.r8.s64 = ctx.r4.s64 + 192;
	// vslh v0,v27,v0
	// addi r7,r4,240
	ctx.r7.s64 = ctx.r4.s64 + 240;
	// vaddshs v12,v26,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// addi r6,r4,288
	ctx.r6.s64 = ctx.r4.s64 + 288;
	// vaddshs v11,v25,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r5,r4,336
	ctx.r5.s64 = ctx.r4.s64 + 336;
	// vaddshs v10,v24,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v9,v23,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v8,v22,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v7,v21,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v6,v20,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v0,v0,v5
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v12,v12,v4
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v11,v11,v3
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v10,v10,v2
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v9,v9,v31
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v8,v8,v30
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v7,v7,v29
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v6,v6,v28
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v0,v0,v27
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vaddshs v12,v12,v1
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v11,v11,v1
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v9,v9,v1
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v8,v8,v1
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v7,v7,v1
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v6,v6,v1
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v0,v0,v1
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsrah v12,v12,v13
	// vsrah v11,v11,v13
	// stvx v12,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v10,v10,v13
	// vsrah v9,v9,v13
	// vsrah v8,v8,v13
	// vsrah v7,v7,v13
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v6,v6,v13
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v0,v0,v13
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v6,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82704354"))) PPC_WEAK_FUNC(sub_82704354);
PPC_FUNC_IMPL(__imp__sub_82704354) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82704358"))) PPC_WEAK_FUNC(sub_82704358);
PPC_FUNC_IMPL(__imp__sub_82704358) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lvx128 v11,r0,r3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// add r8,r3,r4
	ctx.r8.u64 = ctx.r3.u64 + ctx.r4.u64;
	// vspltish v13,2
	// add r7,r9,r3
	ctx.r7.u64 = ctx.r9.u64 + ctx.r3.u64;
	// vspltish v12,4
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// addi r3,r8,16
	ctx.r3.s64 = ctx.r8.s64 + 16;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r11,16
	r30.s64 = r11.s64 + 16;
	// lvsl v6,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r6,r6,1040
	ctx.r6.s64 = ctx.r6.s64 + 1040;
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r31,r10,16
	r31.s64 = ctx.r10.s64 + 16;
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v5,r0,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v9,v9,v5,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r30
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r9,16
	ctx.r8.s64 = ctx.r9.s64 + 16;
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r9,r4
	r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// vperm v7,v7,v5,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v11,r0,r6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r10,r4
	ctx.r6.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v2,r0,r31
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r7,16
	ctx.r10.s64 = ctx.r7.s64 + 16;
	// vperm v8,v8,v2,v4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v1,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v6,r0,r6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-32
	r12.s64 = -32;
	// stvx128 v7,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r10
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r6,16
	ctx.r10.s64 = ctx.r6.s64 + 16;
	// lvsl v4,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v1,v3,v31,v1
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// vmrghb v3,v0,v10
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v2,r0,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// vperm v6,v6,v2,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v4,r0,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v5,v5,v4,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v2,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// lvx128 v30,r0,r9
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v4,v4,v30,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvx128 v2,r0,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v8,v0,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v6,v0,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v30,r0,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v27,v0,v1
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v2,v2,v30,v7
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsubshs v31,v9,v10
	// vmrghb v5,v0,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v30,v8,v9
	// vmrghb v4,v0,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v2,v10,v3
	// vsubshs v29,v5,v6
	// vsubshs v28,v4,v5
	// vsubshs v27,v27,v4
	// vslh v2,v2,v13
	// li r12,-32
	r12.s64 = -32;
	// lvx128 v7,r1,r12
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v0,v7,v8
	// vsubshs v1,v6,v7
	// vslh v0,v0,v13
	// addi r9,r5,144
	ctx.r9.s64 = ctx.r5.s64 + 144;
	// vslh v31,v31,v13
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// vslh v30,v30,v13
	// addi r10,r5,96
	ctx.r10.s64 = ctx.r5.s64 + 96;
	// vslh v1,v1,v13
	// addi r8,r5,192
	ctx.r8.s64 = ctx.r5.s64 + 192;
	// vslh v29,v29,v13
	// addi r7,r5,240
	ctx.r7.s64 = ctx.r5.s64 + 240;
	// vaddshs v0,v0,v11
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r6,r5,288
	ctx.r6.s64 = ctx.r5.s64 + 288;
	// vslh v28,v28,v13
	// addi r4,r5,336
	ctx.r4.s64 = ctx.r5.s64 + 336;
	// vslh v13,v27,v13
	// vaddshs v2,v2,v11
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v31,v31,v11
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v30,v30,v11
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v29,v29,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v1,v1,v11
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v0,v0,v12
	// vaddshs v28,v28,v11
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v11,v2,v12
	// vsrah v2,v31,v12
	// vsrah v31,v30,v12
	// vsrah v30,v29,v12
	// vaddshs v0,v0,v8
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsrah v1,v1,v12
	// vsrah v29,v28,v12
	// vsrah v13,v13,v12
	// vaddshs v12,v11,v3
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v11,v2,v10
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v10,v31,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v9,v1,v7
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v8,v30,v6
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// stvx v12,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v29,v5
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v13,v4
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82704570"))) PPC_WEAK_FUNC(sub_82704570);
PPC_FUNC_IMPL(__imp__sub_82704570) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// addi r11,r6,1040
	r11.s64 = ctx.r6.s64 + 1040;
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
	// rlwinm r6,r10,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x827033b8
	sub_827033B8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82704584"))) PPC_WEAK_FUNC(sub_82704584);
PPC_FUNC_IMPL(__imp__sub_82704584) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82704588"))) PPC_WEAK_FUNC(sub_82704588);
PPC_FUNC_IMPL(__imp__sub_82704588) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lvx128 v10,r0,r3
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r7,r9,r3
	ctx.r7.u64 = ctx.r9.u64 + ctx.r3.u64;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// vspltish v12,1
	// addi r6,r6,1040
	ctx.r6.s64 = ctx.r6.s64 + 1040;
	// vspltish v0,2
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r8,r3,r4
	ctx.r8.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r10,16
	r31.s64 = ctx.r10.s64 + 16;
	// lvx128 v11,r0,r6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r8,16
	ctx.r3.s64 = ctx.r8.s64 + 16;
	// vperm v2,v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r6,r10,r4
	ctx.r6.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r11,16
	r30.s64 = r11.s64 + 16;
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r31
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r6,16
	ctx.r10.s64 = ctx.r6.s64 + 16;
	// lvx128 v5,r0,r3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v9,v9,v4,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v6,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r9,16
	ctx.r8.s64 = ctx.r9.s64 + 16;
	// vperm v10,v10,v5,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r9,r4
	r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// lvx128 v7,r0,r30
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v9,v13,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vperm v7,v8,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v4,r0,r10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v5,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vperm v8,v8,v4,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v7,v13,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvsl v3,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// vperm v6,v6,v5,v3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// vmrghb v8,v13,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v29,v7,v12
	// vsrah v11,v11,v0
	// lvx128 v3,r0,r9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vperm v5,v5,v3,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v4,r0,r10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r7,16
	r11.s64 = ctx.r7.s64 + 16;
	// lvsl v3,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v30,v8,v12
	// vperm v4,v4,v1,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// lvx128 v3,r0,r7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v1,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v5,v13,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v28,v6,v12
	// lvx128 v31,r0,r11
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v30,v8
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vperm v3,v3,v31,v1
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vslh v1,v10,v12
	// vmrghb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v31,v9,v12
	// vslh v27,v5,v12
	// vmrghb v3,v13,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v1,v1,v10
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vslh v26,v4,v12
	// vaddshs v31,v31,v9
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vslh v12,v3,v12
	// vmrghb v2,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddshs v13,v12,v3
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v29,v29,v7
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// vaddshs v28,v28,v6
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// addi r10,r5,96
	ctx.r10.s64 = ctx.r5.s64 + 96;
	// vaddshs v27,v27,v5
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// addi r9,r5,144
	ctx.r9.s64 = ctx.r5.s64 + 144;
	// vaddshs v12,v1,v2
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// addi r8,r5,192
	ctx.r8.s64 = ctx.r5.s64 + 192;
	// vaddshs v26,v26,v4
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// addi r7,r5,240
	ctx.r7.s64 = ctx.r5.s64 + 240;
	// vaddshs v10,v31,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// addi r6,r5,288
	ctx.r6.s64 = ctx.r5.s64 + 288;
	// vaddshs v9,v30,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// addi r4,r5,336
	ctx.r4.s64 = ctx.r5.s64 + 336;
	// vaddshs v8,v29,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v7,v28,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v6,v27,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v5,v26,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v13,v13,v4
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v10,v10,v11
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v9,v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v8,v8,v11
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v12,v12,v0
	// vaddshs v6,v6,v11
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v5,v5,v11
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v11,v10,v0
	// stvx v12,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v10,v9,v0
	// vsrah v9,v8,v0
	// vsrah v8,v7,v0
	// vsrah v7,v6,v0
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v12,v5,v0
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v0,v13,v0
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82704794"))) PPC_WEAK_FUNC(sub_82704794);
PPC_FUNC_IMPL(__imp__sub_82704794) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82704798"))) PPC_WEAK_FUNC(sub_82704798);
PPC_FUNC_IMPL(__imp__sub_82704798) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// addi r10,r6,1040
	ctx.r10.s64 = ctx.r6.s64 + 1040;
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// rlwinm r7,r11,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lvx128 v1,r0,r10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// b 0x827038c0
	sub_827038C0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_827047B0"))) PPC_WEAK_FUNC(sub_827047B0);
PPC_FUNC_IMPL(__imp__sub_827047B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-832(r1)
	ea = -832 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r3,r4
	ctx.r9.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// addi r31,r9,16
	r31.s64 = ctx.r9.s64 + 16;
	// vspltish v0,1
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r11,16
	r30.s64 = r11.s64 + 16;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v9,r0,r31
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// vperm v11,v11,v9,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r8,16
	r11.s64 = ctx.r8.s64 + 16;
	// lvx128 v9,r0,r30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// vperm v9,v10,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v10,r0,r10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v8,v10,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v13,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r1,16
	r11.s64 = ctx.r1.s64 + 16;
	// vperm v7,v12,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// cmpwi cr6,r7,8
	cr6.compare<int32_t>(ctx.r7.s32, 8, xer);
	// vmrghb v12,v13,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v13,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v9,v10,v0
	// vmrghb v6,v13,v8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v7,v13,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v8,v12,v0
	// vadduhm v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v5,v11,v0
	// vslh v4,v7,v0
	// vadduhm v9,v8,v12
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v8,v10,v12
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v5,v5,v11
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v12,v4,v7
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v9,v9,v11
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,64
	r11.s64 = ctx.r1.s64 + 64;
	// vadduhm v10,v5,v7
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v11,v12,v6
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x8270497c
	if (!cr6.eq) goto loc_8270497C;
	// add r11,r10,r4
	r11.u64 = ctx.r10.u64 + ctx.r4.u64;
	// vslh v12,v6,v0
	// rlwinm r10,r4,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// addi r31,r9,16
	r31.s64 = ctx.r9.s64 + 16;
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r10,r4
	r11.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v3,r0,r8
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v7,v3,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// lvx128 v3,r0,r31
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v5,v5,v3,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vadduhm v3,v12,v6
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v6,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// vmrghb v4,v13,v5
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v6,v12,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvsl v5,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v12,v13,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v2,r0,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// vperm v5,v7,v2,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v6,v12,v0
	// vadduhm v3,v3,v12
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghb v13,v13,v5
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v5,v7,v0
	// vadduhm v12,v6,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// stvx v3,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,256
	r11.s64 = ctx.r1.s64 + 256;
	// vadduhm v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v7,v5,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v12,v13,v0
	// addi r11,r1,304
	r11.s64 = ctx.r1.s64 + 304;
	// vadduhm v7,v7,v13
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vadduhm v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,352
	r11.s64 = ctx.r1.s64 + 352;
	// vadduhm v13,v13,v4
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x82704980
	goto loc_82704980;
loc_8270497C:
	// blt cr6,0x82704a14
	if (cr6.lt) goto loc_82704A14;
loc_82704980:
	// rlwinm r31,r4,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r3,8
	ctx.r9.s64 = ctx.r3.s64 + 8;
	// addi r10,r1,32
	ctx.r10.s64 = ctx.r1.s64 + 32;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82704a14
	if (!cr6.gt) goto loc_82704A14;
	// addi r8,r7,-1
	ctx.r8.s64 = ctx.r7.s64 + -1;
	// add r11,r31,r9
	r11.u64 = r31.u64 + ctx.r9.u64;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// subf r27,r31,r4
	r27.s64 = ctx.r4.s64 - r31.s64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
loc_827049A8:
	// lbz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbzx r7,r27,r11
	ctx.r7.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// add r9,r31,r9
	ctx.r9.u64 = r31.u64 + ctx.r9.u64;
	// rotlwi r28,r4,1
	r28.u64 = __builtin_rotateleft32(ctx.r4.u32, 1);
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// add r7,r4,r28
	ctx.r7.u64 = ctx.r4.u64 + r28.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// sth r7,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r7.u16);
	// rlwinm r7,r3,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r3,r7
	ctx.r7.u64 = ctx.r3.u64 + ctx.r7.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// sth r7,48(r10)
	PPC_STORE_U16(ctx.r10.u32 + 48, ctx.r7.u16);
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// bne cr6,0x827049a8
	if (!cr6.eq) goto loc_827049A8;
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,64
	r11.s64 = ctx.r1.s64 + 64;
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,16
	r11.s64 = ctx.r1.s64 + 16;
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82704A14:
	// addi r11,r6,1040
	r11.s64 = ctx.r6.s64 + 1040;
	// vslh v6,v9,v0
	// addi r7,r1,224
	ctx.r7.s64 = ctx.r1.s64 + 224;
	// vslh v5,v10,v0
	// vslh v4,v11,v0
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// vspltish v12,4
	// addi r10,r5,96
	ctx.r10.s64 = ctx.r5.s64 + 96;
	// addi r8,r5,192
	ctx.r8.s64 = ctx.r5.s64 + 192;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,32
	r11.s64 = ctx.r1.s64 + 32;
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vsldoi v27,v8,v7,2
	_mm_store_si128((__m128i*)v27.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8), 14));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// vsldoi v26,v9,v7,2
	_mm_store_si128((__m128i*)v26.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8), 14));
	// vslh v7,v8,v0
	// vaddshs v9,v6,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v6,v4,v11
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,256
	r11.s64 = ctx.r1.s64 + 256;
	// vaddshs v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v3,v28,v0
	// vaddshs v7,v5,v10
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v9,v9,v26
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v26.s16)));
	// lvx128 v29,r0,r11
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,304
	r11.s64 = ctx.r1.s64 + 304;
	// vslh v2,v29,v0
	// vaddshs v8,v8,v27
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v27.s16)));
	// lvx128 v27,r0,r7
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,272
	ctx.r7.s64 = ctx.r1.s64 + 272;
	// vaddshs v5,v3,v28
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v9,v9,v13
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v30,r0,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,352
	r11.s64 = ctx.r1.s64 + 352;
	// vslh v1,v30,v0
	// vaddshs v4,v2,v29
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vaddshs v8,v8,v13
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vsrah v9,v9,v12
	// lvx128 v31,r0,r11
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// vaddshs v3,v1,v30
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v30.s16)));
	// lvx128 v1,r0,r9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v0,v31,v0
	// addi r9,r5,144
	ctx.r9.s64 = ctx.r5.s64 + 144;
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// vsldoi v10,v10,v2,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v2.u8), 14));
	// vaddshs v0,v0,v31
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsldoi v2,v11,v1,2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v1.u8), 14));
	// vsldoi v1,v28,v27,2
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), 14));
	// lvx128 v28,r0,r7
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,320
	ctx.r7.s64 = ctx.r1.s64 + 320;
	// vaddshs v11,v7,v10
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v10,v6,v2
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vsldoi v6,v29,v28,2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v28.u8), 14));
	// vaddshs v7,v5,v1
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// addi r11,r5,240
	r11.s64 = ctx.r5.s64 + 240;
	// vsrah v2,v8,v12
	// lvx128 v5,r0,r7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,368
	ctx.r7.s64 = ctx.r1.s64 + 368;
	// vaddshs v11,v11,v13
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vsldoi v5,v30,v5,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v5.u8), 14));
	// vaddshs v6,v4,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v10,v10,v13
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v2,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v7,v13
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v4,r0,r7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v11,v11,v12
	// vsldoi v4,v31,v4,2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v4.u8), 14));
	// vaddshs v6,v6,v13
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vsrah v10,v10,v12
	// vsrah v7,v7,v12
	// stvx v11,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v8,v3,v5
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v0,v0,v4
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// stvx v10,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v11,v6,v12
	// stvx v7,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// addi r10,r5,288
	ctx.r10.s64 = ctx.r5.s64 + 288;
	// vaddshs v10,v8,v13
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// addi r9,r5,336
	ctx.r9.s64 = ctx.r5.s64 + 336;
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v0,v0,v12
	// vsrah v13,v10,v12
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,832
	ctx.r1.s64 = ctx.r1.s64 + 832;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82704B84"))) PPC_WEAK_FUNC(sub_82704B84);
PPC_FUNC_IMPL(__imp__sub_82704B84) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82704B88"))) PPC_WEAK_FUNC(sub_82704B88);
PPC_FUNC_IMPL(__imp__sub_82704B88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-928(r1)
	ea = -928 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// vspltish v13,1
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// rlwinm r25,r11,3,0,28
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r25,4
	cr6.compare<int32_t>(r25.s32, 4, xer);
	// beq cr6,0x82705084
	if (cr6.eq) goto loc_82705084;
	// cmpwi cr6,r25,8
	cr6.compare<int32_t>(r25.s32, 8, xer);
	// beq cr6,0x82704ea4
	if (cr6.eq) goto loc_82704EA4;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x82704e38
	if (!cr6.gt) goto loc_82704E38;
	// addi r11,r25,-1
	r11.s64 = r25.s64 + -1;
	// rlwinm r28,r4,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r11,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// rlwinm r27,r4,2,0,29
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r26,r4,3,0,28
	r26.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// addi r29,r9,1
	r29.s64 = ctx.r9.s64 + 1;
loc_82704BDC:
	// add r8,r27,r10
	ctx.r8.u64 = r27.u64 + ctx.r10.u64;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r10,16
	r30.s64 = ctx.r10.s64 + 16;
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r9,r28,r10
	ctx.r9.u64 = r28.u64 + ctx.r10.u64;
	// add r31,r10,r4
	r31.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r7,r26,r10
	ctx.r7.u64 = r26.u64 + ctx.r10.u64;
	// add r10,r8,r4
	ctx.r10.u64 = ctx.r8.u64 + ctx.r4.u64;
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r30
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r30,r9,r4
	r30.u64 = ctx.r9.u64 + ctx.r4.u64;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r31
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r24,r31,16
	r24.s64 = r31.s64 + 16;
	// lvsl v5,r0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r23,r9,16
	r23.s64 = ctx.r9.s64 + 16;
	// lvsl v3,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v29,r0,r10
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r10,16
	r31.s64 = ctx.r10.s64 + 16;
	// lvsl v11,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// lvsl v1,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r22,r8,16
	r22.s64 = ctx.r8.s64 + 16;
	// addi r8,r30,16
	ctx.r8.s64 = r30.s64 + 16;
	// lvx128 v6,r0,r24
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r23
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v24,v0,v12
	_mm_store_si128((__m128i*)v24.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v31,r0,r30
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r9,r4
	ctx.r10.u64 = ctx.r9.u64 + ctx.r4.u64;
	// lvx128 v2,r0,r22
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v10,v6,v5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvx128 v30,r0,r8
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v9,v4,v3
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// lvsl v7,r0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v9,v8,v2,v1
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vperm v8,v31,v30,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v28,r0,r31
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r10
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r10,16
	r31.s64 = ctx.r10.s64 + 16;
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r10,r1,16
	ctx.r10.s64 = ctx.r1.s64 + 16;
	// addi r8,r9,16
	ctx.r8.s64 = ctx.r9.s64 + 16;
	// lvx128 v2,r0,r9
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v1,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r30,r7,16
	r30.s64 = ctx.r7.s64 + 16;
	// vmrghb v6,v0,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v26,r0,r7
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v27,r0,r31
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v5,v0,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v7,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,32
	ctx.r10.s64 = ctx.r1.s64 + 32;
	// lvsl v7,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v4,v0,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v30,r0,r8
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v3,v0,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v2,v2,v30,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvx128 v25,r0,r30
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,48
	ctx.r10.s64 = ctx.r1.s64 + 48;
	// lvx128 v7,r0,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v29,v28,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v29,v0,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v30,v0,v7
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r10,r1,16
	ctx.r10.s64 = ctx.r1.s64 + 16;
	// lvx128 v1,r0,r10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,32
	ctx.r10.s64 = ctx.r1.s64 + 32;
	// vperm v31,v31,v27,v1
	_mm_store_si128((__m128i*)v31.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvx128 v1,r0,r10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v28,v0,v31
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v1,v26,v25,v1
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v26,v6,v24
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v25,v5,v6
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrglb v24,v0,v12
	_mm_store_si128((__m128i*)v24.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vmrglb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v4,v3,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v27,v0,v1
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v10,v0,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r10,r11,-96
	ctx.r10.s64 = r11.s64 + -96;
	// vmrglb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v3,v30,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrglb v7,v0,v2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r11,-48
	ctx.r9.s64 = r11.s64 + -48;
	// vmrglb v6,v0,v31
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v31,v28,v29
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vmrglb v2,v0,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v1,v29,v30
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v26,v13
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v30,v27,v28
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v28.u16)));
	// addi r8,r11,48
	ctx.r8.s64 = r11.s64 + 48;
	// addi r31,r11,96
	r31.s64 = r11.s64 + 96;
	// vadduhm v27,v12,v24
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v24.u16)));
	// addi r30,r11,144
	r30.s64 = r11.s64 + 144;
	// vslh v28,v25,v13
	// stvx v29,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v29,v9,v10
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// addi r24,r11,192
	r24.s64 = r11.s64 + 192;
	// vadduhm v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// addi r23,r11,240
	r23.s64 = r11.s64 + 240;
	// vadduhm v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v4,v4,v13
	// stvx v28,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v3,v3,v13
	// addi r9,r11,-80
	ctx.r9.s64 = r11.s64 + -80;
	// vslh v1,v1,v13
	// addi r22,r11,208
	r22.s64 = r11.s64 + 208;
	// vslh v31,v31,v13
	// addi r21,r11,256
	r21.s64 = r11.s64 + 256;
	// vslh v30,v30,v13
	// stvx v4,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// stvx v3,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// stvx v1,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v5,v5,v13
	// stvx v31,r0,r24
	_mm_store_si128((__m128i*)(base + ((r24.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v30,r0,r23
	_mm_store_si128((__m128i*)(base + ((r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v6,v2,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// addi r8,r11,-32
	ctx.r8.s64 = r11.s64 + -32;
	// addi r31,r11,16
	r31.s64 = r11.s64 + 16;
	// vslh v10,v27,v13
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r11,64
	r30.s64 = r11.s64 + 64;
	// addi r24,r11,112
	r24.s64 = r11.s64 + 112;
	// vslh v12,v12,v13
	// addi r23,r11,160
	r23.s64 = r11.s64 + 160;
	// vslh v11,v11,v13
	// vslh v5,v29,v13
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// vslh v9,v9,v13
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// vslh v8,v8,v13
	// addi r11,r11,384
	r11.s64 = r11.s64 + 384;
	// vslh v7,v7,v13
	// stvx v10,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v6,v6,v13
	// stvx v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r24
	_mm_store_si128((__m128i*)(base + ((r24.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r23
	_mm_store_si128((__m128i*)(base + ((r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r22
	_mm_store_si128((__m128i*)(base + ((r22.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v6,r0,r21
	_mm_store_si128((__m128i*)(base + ((r21.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82704bdc
	if (!cr6.eq) goto loc_82704BDC;
loc_82704E38:
	// rlwinm r31,r4,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x82705154
	if (!cr6.gt) goto loc_82705154;
	// addi r8,r25,-1
	ctx.r8.s64 = r25.s64 + -1;
	// add r11,r31,r9
	r11.u64 = r31.u64 + ctx.r9.u64;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// subf r29,r31,r4
	r29.s64 = ctx.r4.s64 - r31.s64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
loc_82704E60:
	// lbzx r7,r29,r11
	ctx.r7.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lbz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// add r7,r4,r7
	ctx.r7.u64 = ctx.r4.u64 + ctx.r7.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r3,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r31,r9
	ctx.r9.u64 = r31.u64 + ctx.r9.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// sth r7,48(r10)
	PPC_STORE_U16(ctx.r10.u32 + 48, ctx.r7.u16);
	// sth r4,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r4.u16);
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// bne cr6,0x82704e60
	if (!cr6.eq) goto loc_82704E60;
	// b 0x82705154
	goto loc_82705154;
loc_82704EA4:
	// add r8,r3,r4
	ctx.r8.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r3,16
	ctx.r7.s64 = ctx.r3.s64 + 16;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r30,r8,16
	r30.s64 = ctx.r8.s64 + 16;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r10,16
	r29.s64 = ctx.r10.s64 + 16;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// vperm v5,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v9,r0,r30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r28,r11,16
	r28.s64 = r11.s64 + 16;
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v10,v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r7,r10,r4
	ctx.r7.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r31,r9,r3
	r31.u64 = ctx.r9.u64 + ctx.r3.u64;
	// lvx128 v9,r0,r29
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v6,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r10,r7,16
	ctx.r10.s64 = ctx.r7.s64 + 16;
	// vperm v11,v11,v9,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r9,16
	ctx.r8.s64 = ctx.r9.s64 + 16;
	// lvx128 v9,r0,r28
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r9,r4
	r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// vperm v9,v12,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r0,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v12,v0,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v7,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// vperm v8,v8,v6,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v7,r0,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// vperm v7,v9,v7,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// rlwinm r8,r4,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vmrghb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// li r7,4
	ctx.r7.s64 = 4;
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// lvx128 v4,r0,r9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v7,v4,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v6,r0,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v5,v12,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// addi r9,r3,8
	ctx.r9.s64 = ctx.r3.s64 + 8;
	// vadduhm v11,v9,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lvx128 v3,r0,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r31,16
	r11.s64 = r31.s64 + 16;
	// vperm v6,v6,v3,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v4,r0,r31
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v3,r0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v9,v10,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v4,v4,v2,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vadduhm v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v12,v12,v13
	// vslh v11,v11,v13
	// vadduhm v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghb v0,v0,v4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v9,v9,v13
	// vadduhm v0,v0,v6
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v5,v13
	// addi r11,r1,64
	r11.s64 = ctx.r1.s64 + 64;
	// vslh v10,v10,v13
	// vslh v8,v8,v13
	// subf r29,r8,r4
	r29.s64 = ctx.r4.s64 - ctx.r8.s64;
	// vslh v7,v7,v13
	// vslh v0,v0,v13
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,256
	r11.s64 = ctx.r1.s64 + 256;
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,304
	r11.s64 = ctx.r1.s64 + 304;
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,352
	r11.s64 = ctx.r1.s64 + 352;
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,400
	r11.s64 = ctx.r1.s64 + 400;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r8,r9
	r11.u64 = ctx.r8.u64 + ctx.r9.u64;
loc_82705040:
	// lbzx r4,r11,r29
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + r29.u32);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// lbz r31,0(r9)
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// add r4,r3,r4
	ctx.r4.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// rlwinm r4,r4,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r3,r31,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// sth r4,48(r10)
	PPC_STORE_U16(ctx.r10.u32 + 48, ctx.r4.u16);
	// sth r3,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r3.u16);
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// bne cr6,0x82705040
	if (!cr6.eq) goto loc_82705040;
	// b 0x82705154
	goto loc_82705154;
loc_82705084:
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r3,r4
	ctx.r10.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r4,r11,16
	ctx.r4.s64 = r11.s64 + 16;
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v0,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v12,v12,v8,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r8,16
	r11.s64 = ctx.r8.s64 + 16;
	// vperm v11,v11,v8,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// addi r10,r9,16
	ctx.r10.s64 = ctx.r9.s64 + 16;
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,64
	r11.s64 = ctx.r1.s64 + 64;
	// vadduhm v10,v12,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vperm v9,v9,v8,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vperm v8,v8,v6,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vslh v10,v10,v13
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v12,v12,v13
	// vmrghb v0,v0,v8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// vadduhm v0,v9,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vslh v12,v12,v13
	// vslh v0,v0,v13
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82705154:
	// addi r11,r6,1040
	r11.s64 = ctx.r6.s64 + 1040;
	// vspltish v12,4
	// addi r7,r1,272
	ctx.r7.s64 = ctx.r1.s64 + 272;
	// addi r10,r1,256
	ctx.r10.s64 = ctx.r1.s64 + 256;
	// addi r9,r1,224
	ctx.r9.s64 = ctx.r1.s64 + 224;
	// addi r8,r5,192
	ctx.r8.s64 = ctx.r5.s64 + 192;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,304
	r11.s64 = ctx.r1.s64 + 304;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,96
	ctx.r10.s64 = ctx.r5.s64 + 96;
	// vslh v3,v11,v13
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,352
	r11.s64 = ctx.r1.s64 + 352;
	// vslh v2,v10,v13
	// vaddshs v3,v3,v11
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,400
	r11.s64 = ctx.r1.s64 + 400;
	// vaddshs v2,v2,v10
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vslh v1,v9,v13
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vaddshs v1,v1,v9
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,64
	r11.s64 = ctx.r1.s64 + 64;
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// vsldoi v27,v28,v7,2
	_mm_store_si128((__m128i*)v27.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v7.u8), 14));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// lvx128 v29,r0,r11
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// vsldoi v26,v29,v7,2
	_mm_store_si128((__m128i*)v26.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v7.u8), 14));
	// vslh v7,v28,v13
	// vslh v6,v29,v13
	// lvx128 v30,r0,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// vaddshs v7,v7,v28
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)v28.s16)));
	// lvx128 v28,r0,r9
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v5,v30,v13
	// addi r9,r5,144
	ctx.r9.s64 = ctx.r5.s64 + 144;
	// vaddshs v6,v6,v29
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v29.s16)));
	// lvx128 v31,r0,r11
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// vaddshs v7,v7,v27
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)v27.s16)));
	// lvx128 v27,r0,r7
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v4,v31,v13
	// addi r7,r1,320
	ctx.r7.s64 = ctx.r1.s64 + 320;
	// vaddshs v5,v5,v30
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vslh v13,v8,v13
	// lvx128 v29,r0,r11
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v7,v0
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v4,v4,v31
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vsldoi v30,v30,v29,2
	_mm_store_si128((__m128i*)v30.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8), 14));
	// vsldoi v31,v31,v28,2
	_mm_store_si128((__m128i*)v31.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v28.u8), 14));
	// lvx128 v28,r0,r7
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,368
	ctx.r7.s64 = ctx.r1.s64 + 368;
	// vsldoi v29,v11,v27,2
	_mm_store_si128((__m128i*)v29.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v27.u8), 14));
	// vsldoi v10,v10,v28,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v28.u8), 14));
	// vaddshs v13,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v11,v5,v30
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v30.s16)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// vaddshs v5,v4,v31
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v4,v3,v29
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v29.s16)));
	// lvx128 v3,r0,r7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,416
	ctx.r7.s64 = ctx.r1.s64 + 416;
	// vaddshs v10,v2,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vsldoi v9,v9,v3,2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v3.u8), 14));
	// vsrah v7,v7,v12
	// vaddshs v6,v6,v26
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vaddshs v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// lvx128 v2,r0,r7
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v9,v1,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vsldoi v8,v8,v2,2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v2.u8), 14));
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v5,v0
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vsrah v11,v11,v12
	// vaddshs v13,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v8,v6,v0
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v6,v4,v0
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vsrah v7,v7,v12
	// vsrah v8,v8,v12
	// vaddshs v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v11,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,288
	ctx.r10.s64 = ctx.r5.s64 + 288;
	// stvx v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,240
	r11.s64 = ctx.r5.s64 + 240;
	// addi r9,r5,336
	ctx.r9.s64 = ctx.r5.s64 + 336;
	// vsrah v6,v6,v12
	// vsrah v11,v10,v12
	// vaddshs v10,v9,v0
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v6,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v13,v10,v12
	// vsrah v0,v0,v12
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,928
	ctx.r1.s64 = ctx.r1.s64 + 928;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_827052E4"))) PPC_WEAK_FUNC(sub_827052E4);
PPC_FUNC_IMPL(__imp__sub_827052E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827052E8"))) PPC_WEAK_FUNC(sub_827052E8);
PPC_FUNC_IMPL(__imp__sub_827052E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-880(r1)
	ea = -880 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// rlwinm r7,r11,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// bl 0x82703690
	sub_82703690(ctx, base);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vspltish v0,1
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// vspltish v12,4
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r30,1040
	r11.s64 = r30.s64 + 1040;
	// vslh v3,v11,v0
	// lvx128 v10,r0,r10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,176
	ctx.r10.s64 = ctx.r1.s64 + 176;
	// vslh v2,v10,v0
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,320
	r11.s64 = ctx.r1.s64 + 320;
	// vaddshs v3,v3,v11
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// vaddshs v2,v2,v10
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vslh v1,v9,v0
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,368
	r11.s64 = ctx.r1.s64 + 368;
	// vslh v29,v6,v0
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// vaddshs v1,v1,v9
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vslh v31,v8,v0
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,416
	r11.s64 = ctx.r1.s64 + 416;
	// vslh v28,v5,v0
	// lvx128 v7,r0,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v29,v29,v6
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vslh v30,v7,v0
	// vaddshs v31,v31,v8
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// vslh v0,v4,v0
	// vaddshs v30,v30,v7
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v28,v28,v5
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// lvx128 v27,r0,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// vsldoi v11,v11,v27,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v27.u8), 14));
	// vaddshs v0,v0,v4
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// lvx128 v27,r0,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,192
	r11.s64 = ctx.r1.s64 + 192;
	// vaddshs v11,v3,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsldoi v10,v10,v27,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v27.u8), 14));
	// lvx128 v26,r0,r11
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,240
	r11.s64 = ctx.r1.s64 + 240;
	// vsldoi v9,v9,v26,2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v26.u8), 14));
	// vaddshs v10,v2,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v11,v11,v13
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v3,r0,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,288
	r11.s64 = ctx.r1.s64 + 288;
	// vsldoi v8,v8,v3,2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v3.u8), 14));
	// vaddshs v9,v1,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v10,v10,v13
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v27,r0,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// vsldoi v7,v7,v27,2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v27.u8), 14));
	// vaddshs v8,v31,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v9,v9,v13
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v26,r0,r11
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,384
	r11.s64 = ctx.r1.s64 + 384;
	// vsldoi v6,v6,v26,2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v26.u8), 14));
	// vaddshs v7,v30,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v8,v8,v13
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v3,r0,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,432
	r11.s64 = ctx.r1.s64 + 432;
	// vsldoi v5,v5,v3,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v3.u8), 14));
	// vaddshs v6,v29,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v7,v7,v13
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v27,r0,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v5,v28,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsldoi v4,v4,v27,2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v27.u8), 14));
	// vaddshs v6,v6,v13
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v0,v0,v4
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v5,v5,v13
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// addi r11,r31,48
	r11.s64 = r31.s64 + 48;
	// vaddshs v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// addi r10,r31,96
	ctx.r10.s64 = r31.s64 + 96;
	// vsrah v13,v11,v12
	// addi r9,r31,144
	ctx.r9.s64 = r31.s64 + 144;
	// vsrah v11,v10,v12
	// addi r8,r31,192
	ctx.r8.s64 = r31.s64 + 192;
	// vsrah v10,v9,v12
	// addi r7,r31,240
	ctx.r7.s64 = r31.s64 + 240;
	// vsrah v9,v8,v12
	// addi r6,r31,288
	ctx.r6.s64 = r31.s64 + 288;
	// vsrah v8,v7,v12
	// addi r5,r31,336
	ctx.r5.s64 = r31.s64 + 336;
	// vsrah v7,v6,v12
	// stvx v13,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v6,v5,v12
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v0,v0,v12
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v6,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,880
	ctx.r1.s64 = ctx.r1.s64 + 880;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_827054BC"))) PPC_WEAK_FUNC(sub_827054BC);
PPC_FUNC_IMPL(__imp__sub_827054BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827054C0"))) PPC_WEAK_FUNC(sub_827054C0);
PPC_FUNC_IMPL(__imp__sub_827054C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// addi r10,r6,1040
	ctx.r10.s64 = ctx.r6.s64 + 1040;
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// rlwinm r7,r11,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lvx128 v1,r0,r10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// b 0x82703ab8
	sub_82703AB8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_827054D8"))) PPC_WEAK_FUNC(sub_827054D8);
PPC_FUNC_IMPL(__imp__sub_827054D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-912(r1)
	ea = -912 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r31,r3,16
	r31.s64 = ctx.r3.s64 + 16;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// add r8,r3,r9
	ctx.r8.u64 = ctx.r3.u64 + ctx.r9.u64;
	// vspltish v13,1
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v11,r0,r31
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r8,16
	r31.s64 = ctx.r8.s64 + 16;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r30,r11,16
	r30.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// lvx128 v9,r0,r31
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// vperm v11,v11,v9,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r11,r9
	ctx.r7.u64 = r11.u64 + ctx.r9.u64;
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v9,r0,r30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r5,8
	cr6.compare<int32_t>(ctx.r5.s32, 8, xer);
	// addi r11,r7,16
	r11.s64 = ctx.r7.s64 + 16;
	// vperm v10,v10,v9,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v9,v9,v8,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v7,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vperm v7,v12,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v6,v8,v13
	// vmrghb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v7,v12,v13
	// vadduhm v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v5,v11,v13
	// vslh v4,v10,v13
	// vadduhm v7,v7,v12
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v12,v8,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v6,v5,v11
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// vadduhm v8,v6,v10
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v12,v4,v10
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// vadduhm v12,v12,v9
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x827056ac
	if (!cr6.eq) goto loc_827056AC;
	// add r11,r10,r9
	r11.u64 = ctx.r10.u64 + ctx.r9.u64;
	// vslh v12,v9,v13
	// rlwinm r10,r9,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// add r8,r10,r3
	ctx.r8.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// addi r31,r8,16
	r31.s64 = ctx.r8.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r10,r9
	r11.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v8,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v8,r0,r31
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v10,v8,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vadduhm v7,v12,v9
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v6,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// vmrghb v8,v0,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v12,v10,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// vperm v9,v11,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrghb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v10,v12,v13
	// vadduhm v7,v7,v12
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghb v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v9,v11,v13
	// vadduhm v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,320
	r11.s64 = ctx.r1.s64 + 320;
	// vslh v13,v0,v13
	// vadduhm v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v9,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r11,r1,368
	r11.s64 = ctx.r1.s64 + 368;
	// vadduhm v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,416
	r11.s64 = ctx.r1.s64 + 416;
	// vadduhm v0,v0,v8
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x827056b0
	goto loc_827056B0;
loc_827056AC:
	// blt cr6,0x82705724
	if (cr6.lt) goto loc_82705724;
loc_827056B0:
	// rlwinm r30,r9,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r3,8
	ctx.r8.s64 = ctx.r3.s64 + 8;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82705724
	if (!cr6.gt) goto loc_82705724;
	// addi r7,r5,-1
	ctx.r7.s64 = ctx.r5.s64 + -1;
	// subf r26,r30,r9
	r26.s64 = ctx.r9.s64 - r30.s64;
	// rlwinm r7,r7,31,1,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// add r11,r30,r8
	r11.u64 = r30.u64 + ctx.r8.u64;
	// addi r9,r7,1
	ctx.r9.s64 = ctx.r7.s64 + 1;
loc_827056D8:
	// lbz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lbzx r7,r26,r11
	ctx.r7.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// add r8,r30,r8
	ctx.r8.u64 = r30.u64 + ctx.r8.u64;
	// rotlwi r27,r3,1
	r27.u64 = __builtin_rotateleft32(ctx.r3.u32, 1);
	// lbz r29,0(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// add r7,r3,r27
	ctx.r7.u64 = ctx.r3.u64 + r27.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 + r28.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// sth r7,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r7.u16);
	// rlwinm r7,r31,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r31,r7
	ctx.r7.u64 = r31.u64 + ctx.r7.u64;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// sth r7,48(r10)
	PPC_STORE_U16(ctx.r10.u32 + 48, ctx.r7.u16);
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// bne cr6,0x827056d8
	if (!cr6.eq) goto loc_827056D8;
loc_82705724:
	// addi r11,r6,1040
	r11.s64 = ctx.r6.s64 + 1040;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82703fa8
	sub_82703FA8(ctx, base);
	// addi r1,r1,912
	ctx.r1.s64 = ctx.r1.s64 + 912;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_82705740"))) PPC_WEAK_FUNC(sub_82705740);
PPC_FUNC_IMPL(__imp__sub_82705740) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-992(r1)
	ea = -992 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// vspltish v13,1
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// beq cr6,0x82705c44
	if (cr6.eq) goto loc_82705C44;
	// cmpwi cr6,r5,8
	cr6.compare<int32_t>(ctx.r5.s32, 8, xer);
	// beq cr6,0x82705a64
	if (cr6.eq) goto loc_82705A64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x827059f8
	if (!cr6.gt) goto loc_827059F8;
	// addi r11,r5,-1
	r11.s64 = ctx.r5.s64 + -1;
	// rlwinm r27,r10,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r11,29,3,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// rlwinm r26,r10,2,0,29
	r26.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r10,3,0,28
	r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// addi r28,r8,1
	r28.s64 = ctx.r8.s64 + 1;
loc_8270579C:
	// add r7,r26,r9
	ctx.r7.u64 = r26.u64 + ctx.r9.u64;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r9,16
	r29.s64 = ctx.r9.s64 + 16;
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r8,r27,r9
	ctx.r8.u64 = r27.u64 + ctx.r9.u64;
	// add r30,r9,r10
	r30.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r31,r25,r9
	r31.u64 = r25.u64 + ctx.r9.u64;
	// add r9,r7,r10
	ctx.r9.u64 = ctx.r7.u64 + ctx.r10.u64;
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r29
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r29,r8,r10
	r29.u64 = ctx.r8.u64 + ctx.r10.u64;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r30
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r24,r30,16
	r24.s64 = r30.s64 + 16;
	// lvsl v5,r0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r23,r8,16
	r23.s64 = ctx.r8.s64 + 16;
	// lvsl v3,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lvx128 v29,r0,r9
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r9,16
	r30.s64 = ctx.r9.s64 + 16;
	// lvsl v11,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lvsl v1,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r22,r7,16
	r22.s64 = ctx.r7.s64 + 16;
	// addi r7,r29,16
	ctx.r7.s64 = r29.s64 + 16;
	// lvx128 v6,r0,r24
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r23
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v24,v0,v12
	_mm_store_si128((__m128i*)v24.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v31,r0,r29
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r8,r10
	ctx.r9.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lvx128 v2,r0,r22
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v10,v6,v5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvx128 v30,r0,r7
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v9,v4,v3
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// lvsl v7,r0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v9,v8,v2,v1
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vperm v8,v31,v30,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v28,r0,r30
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r9
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r9,16
	r30.s64 = ctx.r9.s64 + 16;
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r7,r8,16
	ctx.r7.s64 = ctx.r8.s64 + 16;
	// lvx128 v2,r0,r8
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v1,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r29,r31,16
	r29.s64 = r31.s64 + 16;
	// vmrghb v6,v0,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v26,r0,r31
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v27,r0,r30
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v5,v0,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lvsl v7,r0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v4,v0,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v30,r0,r7
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v3,v0,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v2,v2,v30,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvx128 v25,r0,r29
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lvx128 v7,r0,r9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v29,v28,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v29,v0,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v30,v0,v7
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v1,r0,r9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// vperm v31,v31,v27,v1
	_mm_store_si128((__m128i*)v31.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvx128 v1,r0,r9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v28,v0,v31
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v1,v26,v25,v1
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v26,v6,v24
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v25,v5,v6
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrglb v24,v0,v12
	_mm_store_si128((__m128i*)v24.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vmrglb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v4,v3,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v27,v0,v1
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v10,v0,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r11,-96
	ctx.r9.s64 = r11.s64 + -96;
	// vmrglb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v3,v30,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrglb v7,v0,v2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r8,r11,-48
	ctx.r8.s64 = r11.s64 + -48;
	// vmrglb v6,v0,v31
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v31,v28,v29
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vmrglb v2,v0,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v1,v29,v30
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v26,v13
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v30,v27,v28
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v28.u16)));
	// addi r7,r11,48
	ctx.r7.s64 = r11.s64 + 48;
	// addi r30,r11,96
	r30.s64 = r11.s64 + 96;
	// vadduhm v27,v12,v24
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v24.u16)));
	// addi r29,r11,144
	r29.s64 = r11.s64 + 144;
	// vslh v28,v25,v13
	// stvx v29,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v29,v9,v10
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// addi r24,r11,192
	r24.s64 = r11.s64 + 192;
	// vadduhm v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// addi r23,r11,240
	r23.s64 = r11.s64 + 240;
	// vadduhm v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v4,v4,v13
	// stvx v28,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v3,v3,v13
	// addi r8,r11,-80
	ctx.r8.s64 = r11.s64 + -80;
	// vslh v1,v1,v13
	// addi r22,r11,208
	r22.s64 = r11.s64 + 208;
	// vslh v31,v31,v13
	// addi r21,r11,256
	r21.s64 = r11.s64 + 256;
	// vslh v30,v30,v13
	// stvx v4,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// stvx v3,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// stvx v1,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v5,v5,v13
	// stvx v31,r0,r24
	_mm_store_si128((__m128i*)(base + ((r24.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v30,r0,r23
	_mm_store_si128((__m128i*)(base + ((r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v6,v2,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// addi r7,r11,-32
	ctx.r7.s64 = r11.s64 + -32;
	// addi r30,r11,16
	r30.s64 = r11.s64 + 16;
	// vslh v10,v27,v13
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r11,64
	r29.s64 = r11.s64 + 64;
	// addi r24,r11,112
	r24.s64 = r11.s64 + 112;
	// vslh v12,v12,v13
	// addi r23,r11,160
	r23.s64 = r11.s64 + 160;
	// vslh v11,v11,v13
	// vslh v5,v29,v13
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// vslh v9,v9,v13
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// vslh v8,v8,v13
	// addi r11,r11,384
	r11.s64 = r11.s64 + 384;
	// vslh v7,v7,v13
	// stvx v10,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v6,v6,v13
	// stvx v12,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// stvx v11,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r24
	_mm_store_si128((__m128i*)(base + ((r24.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r23
	_mm_store_si128((__m128i*)(base + ((r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r22
	_mm_store_si128((__m128i*)(base + ((r22.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v6,r0,r21
	_mm_store_si128((__m128i*)(base + ((r21.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x8270579c
	if (!cr6.eq) goto loc_8270579C;
loc_827059F8:
	// rlwinm r31,r10,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82705d14
	if (!cr6.gt) goto loc_82705D14;
	// addi r7,r5,-1
	ctx.r7.s64 = ctx.r5.s64 + -1;
	// subf r28,r31,r10
	r28.s64 = ctx.r10.s64 - r31.s64;
	// rlwinm r7,r7,31,1,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// add r11,r31,r8
	r11.u64 = r31.u64 + ctx.r8.u64;
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
loc_82705A20:
	// lbzx r7,r28,r11
	ctx.r7.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lbz r30,0(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// add r7,r3,r7
	ctx.r7.u64 = ctx.r3.u64 + ctx.r7.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r3,r30,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r7,48(r9)
	PPC_STORE_U16(ctx.r9.u32 + 48, ctx.r7.u16);
	// sth r3,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r3.u16);
	// addi r9,r9,96
	ctx.r9.s64 = ctx.r9.s64 + 96;
	// bne cr6,0x82705a20
	if (!cr6.eq) goto loc_82705A20;
	// b 0x82705d14
	goto loc_82705D14;
loc_82705A64:
	// add r7,r3,r10
	ctx.r7.u64 = ctx.r3.u64 + ctx.r10.u64;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r3,16
	r30.s64 = ctx.r3.s64 + 16;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r29,r7,16
	r29.s64 = ctx.r7.s64 + 16;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lvx128 v10,r0,r7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r30
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r9,16
	r28.s64 = ctx.r9.s64 + 16;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// vperm v5,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v9,r0,r29
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r8,r10,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r27,r11,16
	r27.s64 = r11.s64 + 16;
	// lvsl v7,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v10,v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r30,r9,r10
	r30.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r31,r8,r3
	r31.u64 = ctx.r8.u64 + ctx.r3.u64;
	// lvx128 v9,r0,r28
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r10
	ctx.r8.u64 = r11.u64 + ctx.r10.u64;
	// lvsl v6,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r30,16
	ctx.r9.s64 = r30.s64 + 16;
	// vperm v11,v11,v9,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r7,r8,16
	ctx.r7.s64 = ctx.r8.s64 + 16;
	// lvx128 v9,r0,r27
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r27.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r8,r10
	r11.u64 = ctx.r8.u64 + ctx.r10.u64;
	// vperm v9,v12,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v8,r0,r30
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v12,v0,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v7,r0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// vperm v8,v8,v6,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v7,r0,r7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// vperm v7,v9,v7,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vmrghb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// lvx128 v4,r0,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v7,v4,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v5,v12,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// addi r8,r3,8
	ctx.r8.s64 = ctx.r3.s64 + 8;
	// vadduhm v11,v9,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// lvx128 v3,r0,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r31,16
	r11.s64 = r31.s64 + 16;
	// vperm v6,v6,v3,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v4,r0,r31
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v3,r0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v9,v10,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// li r3,4
	ctx.r3.s64 = 4;
	// vadduhm v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v4,v4,v2,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vadduhm v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v12,v12,v13
	// vslh v11,v11,v13
	// vadduhm v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghb v0,v0,v4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v9,v9,v13
	// vadduhm v0,v0,v6
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v5,v13
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// vslh v10,v10,v13
	// vslh v8,v8,v13
	// subf r28,r7,r10
	r28.s64 = ctx.r10.s64 - ctx.r7.s64;
	// vslh v7,v7,v13
	// vslh v0,v0,v13
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,320
	r11.s64 = ctx.r1.s64 + 320;
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,368
	r11.s64 = ctx.r1.s64 + 368;
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,416
	r11.s64 = ctx.r1.s64 + 416;
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,464
	r11.s64 = ctx.r1.s64 + 464;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r7,r8
	r11.u64 = ctx.r7.u64 + ctx.r8.u64;
loc_82705C00:
	// lbzx r10,r11,r28
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r28.u32);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// lbz r30,0(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// mr r29,r10
	r29.u64 = ctx.r10.u64;
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r30,1,0,30
	r31.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// sth r10,48(r9)
	PPC_STORE_U16(ctx.r9.u32 + 48, ctx.r10.u16);
	// sth r31,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, r31.u16);
	// addi r9,r9,96
	ctx.r9.s64 = ctx.r9.s64 + 96;
	// bne cr6,0x82705c00
	if (!cr6.eq) goto loc_82705C00;
	// b 0x82705d14
	goto loc_82705D14;
loc_82705C44:
	// addi r7,r3,16
	ctx.r7.s64 = ctx.r3.s64 + 16;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r3,r10
	ctx.r9.u64 = ctx.r3.u64 + ctx.r10.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvx128 v11,r0,r7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v0,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r8,16
	ctx.r9.s64 = ctx.r8.s64 + 16;
	// vperm v12,v12,v8,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// vperm v11,v11,v8,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v9,v9,v8,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v10,v12,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vperm v8,v8,v6,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vslh v10,v10,v13
	// vmrghb v0,v0,v8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v12,v12,v13
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// vadduhm v0,v9,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vslh v12,v12,v13
	// vslh v0,v0,v13
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82705D14:
	// addi r11,r6,1040
	r11.s64 = ctx.r6.s64 + 1040;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82703fa8
	sub_82703FA8(ctx, base);
	// addi r1,r1,992
	ctx.r1.s64 = ctx.r1.s64 + 992;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_82705D30"))) PPC_WEAK_FUNC(sub_82705D30);
PPC_FUNC_IMPL(__imp__sub_82705D30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-880(r1)
	ea = -880 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// rlwinm r31,r11,3,0,28
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// bl 0x82703690
	sub_82703690(ctx, base);
	// addi r11,r29,1040
	r11.s64 = r29.s64 + 1040;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82703fa8
	sub_82703FA8(ctx, base);
	// addi r1,r1,880
	ctx.r1.s64 = ctx.r1.s64 + 880;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82705D80"))) PPC_WEAK_FUNC(sub_82705D80);
PPC_FUNC_IMPL(__imp__sub_82705D80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// addi r10,r6,1040
	ctx.r10.s64 = ctx.r6.s64 + 1040;
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// rlwinm r7,r11,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lvx128 v1,r0,r10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// b 0x82703d90
	sub_82703D90(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82705D98"))) PPC_WEAK_FUNC(sub_82705D98);
PPC_FUNC_IMPL(__imp__sub_82705D98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-912(r1)
	ea = -912 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r31,r3,16
	r31.s64 = ctx.r3.s64 + 16;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// add r8,r3,r9
	ctx.r8.u64 = ctx.r3.u64 + ctx.r9.u64;
	// vspltish v13,1
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v11,r0,r31
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r8,16
	r31.s64 = ctx.r8.s64 + 16;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r30,r11,16
	r30.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// lvx128 v9,r0,r31
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// vperm v11,v11,v9,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r11,r9
	ctx.r7.u64 = r11.u64 + ctx.r9.u64;
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v9,r0,r30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r5,8
	cr6.compare<int32_t>(ctx.r5.s32, 8, xer);
	// addi r11,r7,16
	r11.s64 = ctx.r7.s64 + 16;
	// vperm v10,v10,v9,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v9,v9,v8,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v7,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vperm v7,v12,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v6,v8,v13
	// vmrghb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v7,v12,v13
	// vadduhm v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v5,v11,v13
	// vslh v4,v10,v13
	// vadduhm v7,v7,v12
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v12,v8,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v6,v5,v11
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// vadduhm v8,v6,v10
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v12,v4,v10
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// vadduhm v12,v12,v9
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82705f6c
	if (!cr6.eq) goto loc_82705F6C;
	// add r11,r10,r9
	r11.u64 = ctx.r10.u64 + ctx.r9.u64;
	// vslh v12,v9,v13
	// rlwinm r10,r9,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// add r8,r10,r3
	ctx.r8.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// addi r31,r8,16
	r31.s64 = ctx.r8.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r10,r9
	r11.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v8,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v8,r0,r31
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v10,v8,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vadduhm v7,v12,v9
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v6,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// vmrghb v8,v0,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v12,v10,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// vperm v9,v11,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrghb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v10,v12,v13
	// vadduhm v7,v7,v12
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vmrghb v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v9,v11,v13
	// vadduhm v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,320
	r11.s64 = ctx.r1.s64 + 320;
	// vslh v13,v0,v13
	// vadduhm v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v9,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r11,r1,368
	r11.s64 = ctx.r1.s64 + 368;
	// vadduhm v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,416
	r11.s64 = ctx.r1.s64 + 416;
	// vadduhm v0,v0,v8
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x82705f70
	goto loc_82705F70;
loc_82705F6C:
	// blt cr6,0x82705fe4
	if (cr6.lt) goto loc_82705FE4;
loc_82705F70:
	// rlwinm r30,r9,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r3,8
	ctx.r8.s64 = ctx.r3.s64 + 8;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82705fe4
	if (!cr6.gt) goto loc_82705FE4;
	// addi r7,r5,-1
	ctx.r7.s64 = ctx.r5.s64 + -1;
	// subf r26,r30,r9
	r26.s64 = ctx.r9.s64 - r30.s64;
	// rlwinm r7,r7,31,1,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// add r11,r30,r8
	r11.u64 = r30.u64 + ctx.r8.u64;
	// addi r9,r7,1
	ctx.r9.s64 = ctx.r7.s64 + 1;
loc_82705F98:
	// lbz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lbzx r7,r26,r11
	ctx.r7.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// add r8,r30,r8
	ctx.r8.u64 = r30.u64 + ctx.r8.u64;
	// rotlwi r27,r3,1
	r27.u64 = __builtin_rotateleft32(ctx.r3.u32, 1);
	// lbz r29,0(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// add r7,r3,r27
	ctx.r7.u64 = ctx.r3.u64 + r27.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 + r28.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// sth r7,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r7.u16);
	// rlwinm r7,r31,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r31,r7
	ctx.r7.u64 = r31.u64 + ctx.r7.u64;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// sth r7,48(r10)
	PPC_STORE_U16(ctx.r10.u32 + 48, ctx.r7.u16);
	// addi r10,r10,96
	ctx.r10.s64 = ctx.r10.s64 + 96;
	// bne cr6,0x82705f98
	if (!cr6.eq) goto loc_82705F98;
loc_82705FE4:
	// addi r11,r6,1040
	r11.s64 = ctx.r6.s64 + 1040;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x827041d0
	sub_827041D0(ctx, base);
	// addi r1,r1,912
	ctx.r1.s64 = ctx.r1.s64 + 912;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_82706000"))) PPC_WEAK_FUNC(sub_82706000);
PPC_FUNC_IMPL(__imp__sub_82706000) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-992(r1)
	ea = -992 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// vspltish v13,1
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// beq cr6,0x82706504
	if (cr6.eq) goto loc_82706504;
	// cmpwi cr6,r5,8
	cr6.compare<int32_t>(ctx.r5.s32, 8, xer);
	// beq cr6,0x82706324
	if (cr6.eq) goto loc_82706324;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x827062b8
	if (!cr6.gt) goto loc_827062B8;
	// addi r11,r5,-1
	r11.s64 = ctx.r5.s64 + -1;
	// rlwinm r27,r10,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r11,29,3,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// rlwinm r26,r10,2,0,29
	r26.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r10,3,0,28
	r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// addi r28,r8,1
	r28.s64 = ctx.r8.s64 + 1;
loc_8270605C:
	// add r7,r26,r9
	ctx.r7.u64 = r26.u64 + ctx.r9.u64;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r9,16
	r29.s64 = ctx.r9.s64 + 16;
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r8,r27,r9
	ctx.r8.u64 = r27.u64 + ctx.r9.u64;
	// add r30,r9,r10
	r30.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r31,r25,r9
	r31.u64 = r25.u64 + ctx.r9.u64;
	// add r9,r7,r10
	ctx.r9.u64 = ctx.r7.u64 + ctx.r10.u64;
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r29
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r29,r8,r10
	r29.u64 = ctx.r8.u64 + ctx.r10.u64;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r30
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r24,r30,16
	r24.s64 = r30.s64 + 16;
	// lvsl v5,r0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r23,r8,16
	r23.s64 = ctx.r8.s64 + 16;
	// lvsl v3,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lvx128 v29,r0,r9
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r9,16
	r30.s64 = ctx.r9.s64 + 16;
	// lvsl v11,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lvsl v1,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r22,r7,16
	r22.s64 = ctx.r7.s64 + 16;
	// addi r7,r29,16
	ctx.r7.s64 = r29.s64 + 16;
	// lvx128 v6,r0,r24
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r23
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v24,v0,v12
	_mm_store_si128((__m128i*)v24.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v31,r0,r29
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r8,r10
	ctx.r9.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lvx128 v2,r0,r22
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v10,v6,v5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvx128 v30,r0,r7
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v9,v4,v3
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// lvsl v7,r0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v9,v8,v2,v1
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vperm v8,v31,v30,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v28,r0,r30
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r9
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r9,16
	r30.s64 = ctx.r9.s64 + 16;
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r7,r8,16
	ctx.r7.s64 = ctx.r8.s64 + 16;
	// lvx128 v2,r0,r8
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v1,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r29,r31,16
	r29.s64 = r31.s64 + 16;
	// vmrghb v6,v0,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v26,r0,r31
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v27,r0,r30
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v5,v0,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lvsl v7,r0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v4,v0,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v30,r0,r7
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v3,v0,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v2,v2,v30,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvx128 v25,r0,r29
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lvx128 v7,r0,r9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v29,v28,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v29,v0,v2
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v30,v0,v7
	_mm_store_si128((__m128i*)v30.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v1,r0,r9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// vperm v31,v31,v27,v1
	_mm_store_si128((__m128i*)v31.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvx128 v1,r0,r9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v28,v0,v31
	_mm_store_si128((__m128i*)v28.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v1,v26,v25,v1
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v26,v6,v24
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v25,v5,v6
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrglb v24,v0,v12
	_mm_store_si128((__m128i*)v24.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vmrglb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v4,v3,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v27,v0,v1
	_mm_store_si128((__m128i*)v27.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v10,v0,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r11,-96
	ctx.r9.s64 = r11.s64 + -96;
	// vmrglb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v3,v30,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrglb v7,v0,v2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r8,r11,-48
	ctx.r8.s64 = r11.s64 + -48;
	// vmrglb v6,v0,v31
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v31,v28,v29
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vmrglb v2,v0,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v1,v29,v30
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v26,v13
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v30,v27,v28
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v28.u16)));
	// addi r7,r11,48
	ctx.r7.s64 = r11.s64 + 48;
	// addi r30,r11,96
	r30.s64 = r11.s64 + 96;
	// vadduhm v27,v12,v24
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v24.u16)));
	// addi r29,r11,144
	r29.s64 = r11.s64 + 144;
	// vslh v28,v25,v13
	// stvx v29,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v29,v9,v10
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// addi r24,r11,192
	r24.s64 = r11.s64 + 192;
	// vadduhm v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// addi r23,r11,240
	r23.s64 = r11.s64 + 240;
	// vadduhm v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v4,v4,v13
	// stvx v28,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v3,v3,v13
	// addi r8,r11,-80
	ctx.r8.s64 = r11.s64 + -80;
	// vslh v1,v1,v13
	// addi r22,r11,208
	r22.s64 = r11.s64 + 208;
	// vslh v31,v31,v13
	// addi r21,r11,256
	r21.s64 = r11.s64 + 256;
	// vslh v30,v30,v13
	// stvx v4,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// stvx v3,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// stvx v1,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v5,v5,v13
	// stvx v31,r0,r24
	_mm_store_si128((__m128i*)(base + ((r24.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v30,r0,r23
	_mm_store_si128((__m128i*)(base + ((r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v6,v2,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// addi r7,r11,-32
	ctx.r7.s64 = r11.s64 + -32;
	// addi r30,r11,16
	r30.s64 = r11.s64 + 16;
	// vslh v10,v27,v13
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r11,64
	r29.s64 = r11.s64 + 64;
	// addi r24,r11,112
	r24.s64 = r11.s64 + 112;
	// vslh v12,v12,v13
	// addi r23,r11,160
	r23.s64 = r11.s64 + 160;
	// vslh v11,v11,v13
	// vslh v5,v29,v13
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// vslh v9,v9,v13
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// vslh v8,v8,v13
	// addi r11,r11,384
	r11.s64 = r11.s64 + 384;
	// vslh v7,v7,v13
	// stvx v10,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v6,v6,v13
	// stvx v12,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// stvx v11,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r24
	_mm_store_si128((__m128i*)(base + ((r24.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r23
	_mm_store_si128((__m128i*)(base + ((r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r22
	_mm_store_si128((__m128i*)(base + ((r22.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v6,r0,r21
	_mm_store_si128((__m128i*)(base + ((r21.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x8270605c
	if (!cr6.eq) goto loc_8270605C;
loc_827062B8:
	// rlwinm r31,r10,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x827065d4
	if (!cr6.gt) goto loc_827065D4;
	// addi r7,r5,-1
	ctx.r7.s64 = ctx.r5.s64 + -1;
	// subf r28,r31,r10
	r28.s64 = ctx.r10.s64 - r31.s64;
	// rlwinm r7,r7,31,1,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// add r11,r31,r8
	r11.u64 = r31.u64 + ctx.r8.u64;
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
loc_827062E0:
	// lbzx r7,r28,r11
	ctx.r7.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lbz r30,0(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// add r7,r3,r7
	ctx.r7.u64 = ctx.r3.u64 + ctx.r7.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r3,r30,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r7,48(r9)
	PPC_STORE_U16(ctx.r9.u32 + 48, ctx.r7.u16);
	// sth r3,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r3.u16);
	// addi r9,r9,96
	ctx.r9.s64 = ctx.r9.s64 + 96;
	// bne cr6,0x827062e0
	if (!cr6.eq) goto loc_827062E0;
	// b 0x827065d4
	goto loc_827065D4;
loc_82706324:
	// add r7,r3,r10
	ctx.r7.u64 = ctx.r3.u64 + ctx.r10.u64;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r3,16
	r30.s64 = ctx.r3.s64 + 16;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r29,r7,16
	r29.s64 = ctx.r7.s64 + 16;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lvx128 v10,r0,r7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r30
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r9,16
	r28.s64 = ctx.r9.s64 + 16;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// vperm v5,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v9,r0,r29
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r8,r10,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r27,r11,16
	r27.s64 = r11.s64 + 16;
	// lvsl v7,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v10,v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r30,r9,r10
	r30.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r31,r8,r3
	r31.u64 = ctx.r8.u64 + ctx.r3.u64;
	// lvx128 v9,r0,r28
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r10
	ctx.r8.u64 = r11.u64 + ctx.r10.u64;
	// lvsl v6,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r30,16
	ctx.r9.s64 = r30.s64 + 16;
	// vperm v11,v11,v9,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r7,r8,16
	ctx.r7.s64 = ctx.r8.s64 + 16;
	// lvx128 v9,r0,r27
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r27.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r8,r10
	r11.u64 = ctx.r8.u64 + ctx.r10.u64;
	// vperm v9,v12,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v8,r0,r30
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v12,v0,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvsl v7,r0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// vperm v8,v8,v6,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v7,r0,r7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// vperm v7,v9,v7,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vmrghb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// lvx128 v4,r0,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v7,v4,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v5,v12,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// addi r8,r3,8
	ctx.r8.s64 = ctx.r3.s64 + 8;
	// vadduhm v11,v9,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// lvx128 v3,r0,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r31,16
	r11.s64 = r31.s64 + 16;
	// vperm v6,v6,v3,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvx128 v4,r0,r31
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v3,r0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v9,v10,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// li r3,4
	ctx.r3.s64 = 4;
	// vadduhm v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v4,v4,v2,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vadduhm v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v12,v12,v13
	// vslh v11,v11,v13
	// vadduhm v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghb v0,v0,v4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v9,v9,v13
	// vadduhm v0,v0,v6
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v5,v13
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// vslh v10,v10,v13
	// vslh v8,v8,v13
	// subf r28,r7,r10
	r28.s64 = ctx.r10.s64 - ctx.r7.s64;
	// vslh v7,v7,v13
	// vslh v0,v0,v13
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,320
	r11.s64 = ctx.r1.s64 + 320;
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,368
	r11.s64 = ctx.r1.s64 + 368;
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,416
	r11.s64 = ctx.r1.s64 + 416;
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,464
	r11.s64 = ctx.r1.s64 + 464;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r7,r8
	r11.u64 = ctx.r7.u64 + ctx.r8.u64;
loc_827064C0:
	// lbzx r10,r11,r28
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r28.u32);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// lbz r30,0(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// mr r29,r10
	r29.u64 = ctx.r10.u64;
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r30,1,0,30
	r31.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// sth r10,48(r9)
	PPC_STORE_U16(ctx.r9.u32 + 48, ctx.r10.u16);
	// sth r31,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, r31.u16);
	// addi r9,r9,96
	ctx.r9.s64 = ctx.r9.s64 + 96;
	// bne cr6,0x827064c0
	if (!cr6.eq) goto loc_827064C0;
	// b 0x827065d4
	goto loc_827065D4;
loc_82706504:
	// addi r7,r3,16
	ctx.r7.s64 = ctx.r3.s64 + 16;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r3,r10
	ctx.r9.u64 = ctx.r3.u64 + ctx.r10.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvx128 v11,r0,r7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v0,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r8,16
	ctx.r9.s64 = ctx.r8.s64 + 16;
	// vperm v12,v12,v8,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// vperm v11,v11,v8,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v9,v9,v8,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v10,v12,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vperm v8,v8,v6,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vslh v10,v10,v13
	// vmrghb v0,v0,v8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v12,v12,v13
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r11,r1,224
	r11.s64 = ctx.r1.s64 + 224;
	// vadduhm v0,v9,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vslh v12,v12,v13
	// vslh v0,v0,v13
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_827065D4:
	// addi r11,r6,1040
	r11.s64 = ctx.r6.s64 + 1040;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x827041d0
	sub_827041D0(ctx, base);
	// addi r1,r1,992
	ctx.r1.s64 = ctx.r1.s64 + 992;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_827065F0"))) PPC_WEAK_FUNC(sub_827065F0);
PPC_FUNC_IMPL(__imp__sub_827065F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-880(r1)
	ea = -880 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// rlwinm r31,r11,3,0,28
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// bl 0x82703690
	sub_82703690(ctx, base);
	// addi r11,r29,1040
	r11.s64 = r29.s64 + 1040;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x827041d0
	sub_827041D0(ctx, base);
	// addi r1,r1,880
	ctx.r1.s64 = ctx.r1.s64 + 880;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82706640"))) PPC_WEAK_FUNC(sub_82706640);
PPC_FUNC_IMPL(__imp__sub_82706640) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// li r6,48
	ctx.r6.s64 = 48;
	// lvx128 v1,r0,r4
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r7,96
	ctx.r7.s64 = 96;
	// vpkshus v24,v1,v1
	_mm_store_si128((__m128i*)v24.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// li r8,144
	ctx.r8.s64 = 144;
	// li r9,192
	ctx.r9.s64 = 192;
	// li r10,240
	ctx.r10.s64 = 240;
	// li r11,288
	r11.s64 = 288;
	// lvx128 v2,r4,r6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,336
	r12.s64 = 336;
	// lvx128 v3,r4,r7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r4,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v25,v2,v2
	_mm_store_si128((__m128i*)v25.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// lvx128 v5,r4,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v26,v3,v3
	_mm_store_si128((__m128i*)v26.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// lvx128 v6,r4,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r6,r5,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v7,r4,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v27,v4,v4
	_mm_store_si128((__m128i*)v27.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// lvx128 v8,r4,r12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r3,4
	ctx.r4.s64 = ctx.r3.s64 + 4;
	// stvewx v24,r0,r3
	ea = (ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v24.u32[3 - ((ea & 0xF) >> 2)]);
	// add r7,r5,r6
	ctx.r7.u64 = ctx.r5.u64 + ctx.r6.u64;
	// vpkshus v28,v5,v5
	_mm_store_si128((__m128i*)v28.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// vpkshus v29,v6,v6
	_mm_store_si128((__m128i*)v29.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// add r9,r5,r8
	ctx.r9.u64 = ctx.r5.u64 + ctx.r8.u64;
	// vpkshus v30,v7,v7
	_mm_store_si128((__m128i*)v30.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// stvewx v24,r0,r4
	ea = (ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v24.u32[3 - ((ea & 0xF) >> 2)]);
	// add r10,r6,r8
	ctx.r10.u64 = ctx.r6.u64 + ctx.r8.u64;
	// stvewx v25,r3,r5
	ea = (ctx.r3.u32 + ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, v25.u32[3 - ((ea & 0xF) >> 2)]);
	// vpkshus v31,v8,v8
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvewx v25,r4,r5
	ea = (ctx.r4.u32 + ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, v25.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r7,r8
	r11.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stvewx v26,r3,r6
	ea = (ctx.r3.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v26,r4,r6
	ea = (ctx.r4.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v27,r3,r7
	ea = (ctx.r3.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v27,r4,r7
	ea = (ctx.r4.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v28,r3,r8
	ea = (ctx.r3.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v28,r4,r8
	ea = (ctx.r4.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v29,r3,r9
	ea = (ctx.r3.u32 + ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v29,r4,r9
	ea = (ctx.r4.u32 + ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v30,r3,r10
	ea = (ctx.r3.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v30,r4,r10
	ea = (ctx.r4.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v31,r3,r11
	ea = (ctx.r3.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v31.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v31,r4,r11
	ea = (ctx.r4.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v31.u32[3 - ((ea & 0xF) >> 2)]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_827066FC"))) PPC_WEAK_FUNC(sub_827066FC);
PPC_FUNC_IMPL(__imp__sub_827066FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82706700"))) PPC_WEAK_FUNC(sub_82706700);
PPC_FUNC_IMPL(__imp__sub_82706700) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// li r7,48
	ctx.r7.s64 = 48;
	// lvx128 v1,r0,r4
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,96
	ctx.r8.s64 = 96;
	// lvx128 v16,r0,r5
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,144
	ctx.r9.s64 = 144;
	// vaddshs v24,v1,v16
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v16.s16)));
	// li r10,192
	ctx.r10.s64 = 192;
	// rlwinm r12,r6,1,0,30
	r12.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v2,r4,r7
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v3,r4,r8
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v24,v24,v24
	_mm_store_si128((__m128i*)v24.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v24.s16)));
	// lvx128 v4,r4,r9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r4,r10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r4,r4,r10
	ctx.r4.u64 = ctx.r4.u64 + ctx.r10.u64;
	// li r10,64
	ctx.r10.s64 = 64;
	// lvx128 v6,r4,r7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r7,16
	ctx.r7.s64 = 16;
	// lvx128 v7,r4,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,32
	ctx.r8.s64 = 32;
	// lvx128 v8,r4,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,48
	ctx.r9.s64 = 48;
	// lvx128 v20,r5,r10
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r3,4
	ctx.r4.s64 = ctx.r3.s64 + 4;
	// vaddshs v28,v5,v20
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v20.s16)));
	// lvx128 v17,r5,r7
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v18,r5,r8
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v25,v2,v17
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)v17.s16)));
	// lvx128 v19,r5,r9
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// vaddshs v26,v3,v18
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)v18.s16)));
	// vpkshus v28,v28,v28
	_mm_store_si128((__m128i*)v28.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vaddshs v27,v4,v19
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vpkshus v25,v25,v25
	_mm_store_si128((__m128i*)v25.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v25.s16)));
	// lvx128 v21,r5,r7
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v26,v26,v26
	_mm_store_si128((__m128i*)v26.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// lvx128 v22,r5,r8
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v29,v6,v21
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)v21.s16)));
	// vpkshus v27,v27,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// lvx128 v23,r5,r9
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v30,v7,v22
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)v22.s16)));
	// stvewx v24,r0,r3
	ea = (ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, v24.u32[3 - ((ea & 0xF) >> 2)]);
	// add r7,r6,r12
	ctx.r7.u64 = ctx.r6.u64 + r12.u64;
	// stvewx v24,r0,r4
	ea = (ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, v24.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r8,r6,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// vaddshs v31,v8,v23
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v23.s16)));
	// stvewx v25,r3,r6
	ea = (ctx.r3.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, v25.u32[3 - ((ea & 0xF) >> 2)]);
	// vpkshus v29,v29,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// stvewx v25,r4,r6
	ea = (ctx.r4.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, v25.u32[3 - ((ea & 0xF) >> 2)]);
	// add r9,r6,r8
	ctx.r9.u64 = ctx.r6.u64 + ctx.r8.u64;
	// stvewx v26,r3,r12
	ea = (ctx.r3.u32 + r12.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// vpkshus v30,v30,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// stvewx v26,r4,r12
	ea = (ctx.r4.u32 + r12.u32) & ~0x3;
	PPC_STORE_U32(ea, v26.u32[3 - ((ea & 0xF) >> 2)]);
	// add r10,r12,r8
	ctx.r10.u64 = r12.u64 + ctx.r8.u64;
	// stvewx v27,r3,r7
	ea = (ctx.r3.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// vpkshus v31,v31,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// stvewx v27,r4,r7
	ea = (ctx.r4.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, v27.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r7,r8
	r11.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stvewx v28,r3,r8
	ea = (ctx.r3.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v28,r4,r8
	ea = (ctx.r4.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, v28.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v29,r3,r9
	ea = (ctx.r3.u32 + ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v29,r4,r9
	ea = (ctx.r4.u32 + ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, v29.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v30,r3,r10
	ea = (ctx.r3.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v30,r4,r10
	ea = (ctx.r4.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, v30.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v31,r3,r11
	ea = (ctx.r3.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v31.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v31,r4,r11
	ea = (ctx.r4.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, v31.u32[3 - ((ea & 0xF) >> 2)]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82706808"))) PPC_WEAK_FUNC(sub_82706808);
PPC_FUNC_IMPL(__imp__sub_82706808) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	uint32_t ea{};
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// bne cr6,0x82706ad4
	if (!cr6.eq) goto loc_82706AD4;
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v13,v13,v12,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lwz r10,3356(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 3356);
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v12,v11,v10,v6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vperm v13,v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r10,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, ctx.r10.u32);
	// li r10,4
	ctx.r10.s64 = 4;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// vperm v12,v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r7,36(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// lvsl v6,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v13,v13,v12,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stw r9,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r9.u32);
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// vperm v12,v11,v10,v6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vperm v13,v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v12,v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,4
	r11.s64 = 4;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r6,36(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v6,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v13,v13,v12,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stw r9,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r9.u32);
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// vperm v12,v11,v10,v6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vperm v13,v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// vperm v12,v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r7,36(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v13,v13,v12,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stw r9,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r9.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// vperm v13,v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v0,v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// li r11,4
	r11.s64 = 4;
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v13,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r3,0
	ctx.r3.s64 = 0;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// blr 
	return;
loc_82706AD4:
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r3,r4
	ctx.r9.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvsl v13,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// add r10,r11,r3
	ctx.r10.u64 = r11.u64 + ctx.r3.u64;
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v0,v0,v12,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v9,r0,r6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// lvsl v5,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvsl v11,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r8,r11,r5
	ctx.r8.u64 = r11.u64 + ctx.r5.u64;
	// vperm v13,v10,v9,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v13,r4,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lvx128 v11,r0,r6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v0,v0,v13,v5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// lvsl v6,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v12,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v0,v0,v13,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v12,r0,r6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// vperm v12,v11,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v13,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// stvx128 v12,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// lvx128 v12,r0,r7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v0,v0,v12,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v9,r0,r6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// addi r5,r10,16
	ctx.r5.s64 = ctx.r10.s64 + 16;
	// lvsl v11,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// vperm v13,v10,v9,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvsl v5,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v13,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v11,r0,r7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r5
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// vperm v0,v0,v13,v5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvsl v6,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r3,0
	ctx.r3.s64 = 0;
	// stvx128 v12,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v0,v0,v13,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v12,r0,r6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// vperm v12,v11,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// stvx128 v12,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v0,v0,v13,v7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v11,r0,r6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// addi r5,r10,16
	ctx.r5.s64 = ctx.r10.s64 + 16;
	// lvsl v6,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// vperm v13,v12,v11,v6
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvsl v5,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v13,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v0,v13,v0,v5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvx128 v12,r0,r7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v13,v13,v12,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v13,r4,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82706CCC"))) PPC_WEAK_FUNC(sub_82706CCC);
PPC_FUNC_IMPL(__imp__sub_82706CCC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82706CD0"))) PPC_WEAK_FUNC(sub_82706CD0);
PPC_FUNC_IMPL(__imp__sub_82706CD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// cntlzw r11,r9
	r11.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// addi r9,r1,-96
	ctx.r9.s64 = ctx.r1.s64 + -96;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// vspltish v5,1
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// vspltish v4,5
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// vspltish v0,2
	// addi r8,r1,-32
	ctx.r8.s64 = ctx.r1.s64 + -32;
	// vspltish v10,4
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,1
	ctx.r9.s64 = 1;
	// addi r5,r1,-80
	ctx.r5.s64 = ctx.r1.s64 + -80;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v4,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-64
	ctx.r5.s64 = ctx.r1.s64 + -64;
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-48
	ctx.r5.s64 = ctx.r1.s64 + -48;
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// slw r8,r9,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// stw r8,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, ctx.r8.u32);
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82706ea8
	if (!cr6.eq) goto loc_82706EA8;
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v3,r0,r7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// stw r10,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, ctx.r10.u32);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// sth r7,-112(r1)
	PPC_STORE_U16(ctx.r1.u32 + -112, ctx.r7.u16);
	// lvx128 v29,r0,r9
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v9,v9,v29,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvsl v6,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v8,v8,v3,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrghb v11,v13,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v9,v13,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v12,v13,v8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82707000
	if (!cr6.gt) goto loc_82707000;
	// b 0x82706de0
	goto loc_82706DE0;
loc_82706DB0:
	// addi r11,r1,-96
	r11.s64 = ctx.r1.s64 + -96;
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-80
	r11.s64 = ctx.r1.s64 + -80;
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-64
	r11.s64 = ctx.r1.s64 + -64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
loc_82706DE0:
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v3,v12,v0
	// vslh v31,v12,v4
	// vslh v30,v12,v10
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// vslh v9,v9,v0
	// vadduhm v4,v3,v12
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v10,v11,v10
	// vadduhm v3,v30,v31
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v31.u16)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vsubshs v31,v13,v9
	// lvx128 v7,r0,r9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v9,v12,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// vslh v5,v11,v5
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// vor v12,v11,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vperm v11,v8,v7,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vadduhm v8,v4,v3
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v10,v5,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghb v11,v13,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v13,v8,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v0,v11,v0
	// vadduhm v13,v13,v2
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vsubshs v0,v11,v0
	// vadduhm v0,v0,v31
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsrah v0,v0,v1
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,4
	r11.s64 = 4;
	// stvewx v0,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lhz r11,-112(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + -112);
	// lwz r9,-108(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r9,-100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// stw r10,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, ctx.r10.u32);
	// sth r11,-112(r1)
	PPC_STORE_U16(ctx.r1.u32 + -112, r11.u16);
	// blt cr6,0x82706db0
	if (cr6.lt) goto loc_82706DB0;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_82706EA8:
	// lvx128 v30,r0,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v29,r0,r7
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v8,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvsl v6,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v12,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v9,v13,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vmrghb v3,v13,v12
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v31,v13,v12
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v11,v30,v29,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v12,v13,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v11,v13,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82707000
	if (!cr6.gt) goto loc_82707000;
	// li r9,0
	ctx.r9.s64 = 0;
loc_82706F1C:
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v27,v12,v10
	// vslh v29,v12,v0
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vslh v28,v12,v4
	// vslh v26,v9,v5
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// vslh v3,v3,v0
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v29,v29,v12
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v28,v27,v28
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v28.u16)));
	// lvx128 v30,r0,r7
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v27,v9,v10
	// vperm v6,v7,v30,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsubshs v3,v13,v3
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// vadduhm v30,v29,v28
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v27,v26,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v28,v8,v5
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v26,v11,v0
	// vadduhm v30,v30,v27
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v29,v7,v0
	// vslh v27,v31,v0
	// vadduhm v26,v26,v11
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v30,v30,v2
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vsubshs v29,v7,v29
	// vsubshs v27,v13,v27
	// vadduhm v3,v29,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v29,v11,v10
	// vadduhm v30,v30,v3
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v11,v4
	// vsrah v30,v30,v1
	// vadduhm v29,v29,v3
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v8,v10
	// vadduhm v28,v28,v3
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v6,v0
	// vsubshs v31,v6,v3
	// vor v3,v12,v12
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vor v12,v9,v9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vadduhm v7,v26,v29
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v27,v31,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v31,v11,v11
	_mm_store_si128((__m128i*)v31.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v8,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v6,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v6,v7,v28
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v6,v6,v2
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v7,v6,v27
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v7,v7,v1
	// vpkshus v7,v30,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)v30.s16)));
	// stvx v7,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// blt cr6,0x82706f1c
	if (cr6.lt) goto loc_82706F1C;
loc_82707000:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82707008"))) PPC_WEAK_FUNC(sub_82707008);
PPC_FUNC_IMPL(__imp__sub_82707008) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// cntlzw r11,r9
	r11.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// addi r9,r1,-48
	ctx.r9.s64 = ctx.r1.s64 + -48;
	// vspltish v11,3
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// addi r5,r1,-32
	ctx.r5.s64 = ctx.r1.s64 + -32;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,1
	ctx.r9.s64 = 1;
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// stvx v11,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// slw r8,r9,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// stw r8,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, ctx.r8.u32);
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82707204
	if (!cr6.eq) goto loc_82707204;
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v6,r0,r7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v12,v13,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// stw r10,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, ctx.r10.u32);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// sth r7,-80(r1)
	PPC_STORE_U16(ctx.r1.u32 + -80, ctx.r7.u16);
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v10,v10,v8,v3
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// lvsl v8,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v13,v0,v10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v9,v6,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghb v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// ble cr6,0x8270730c
	if (!cr6.gt) goto loc_8270730C;
	// b 0x827070dc
	goto loc_827070DC;
loc_827070C4:
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
loc_827070DC:
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vadduhm v12,v10,v13
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v5,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v6,v9,v6,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// li r11,4
	r11.s64 = 4;
	// lvx128 v4,r0,r7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v7,v4,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vslh v4,v12,v11
	// vadduhm v12,v12,v4
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghb v9,v0,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v7,v12,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmrghb v12,v0,v6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v6,v10,v9
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v8,v8,v12
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vsubshs v6,v0,v6
	// vadduhm v10,v13,v12
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vsubshs v0,v0,v8
	// vor v8,v13,v13
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vslh v11,v10,v11
	// vor v13,v9,v9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vadduhm v0,v7,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vor v10,v12,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vsrah v0,v0,v1
	// vadduhm v11,v11,v2
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vpkshus v12,v0,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vadduhm v0,v11,v6
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vsrah v0,v0,v1
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-64
	ctx.r9.s64 = ctx.r1.s64 + -64;
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// addi r9,r1,-64
	ctx.r9.s64 = ctx.r1.s64 + -64;
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v12,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r9,r1,-64
	ctx.r9.s64 = ctx.r1.s64 + -64;
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v0,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r9,r1,-64
	ctx.r9.s64 = ctx.r1.s64 + -64;
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v0,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lhz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + -80);
	// lwz r11,44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r9,r10,2
	ctx.r9.s64 = ctx.r10.s64 + 2;
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// extsh r11,r9
	r11.s64 = ctx.r9.s16;
	// lwz r9,-68(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// stw r10,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, ctx.r10.u32);
	// sth r11,-80(r1)
	PPC_STORE_U16(ctx.r1.u32 + -80, r11.u16);
	// blt cr6,0x827070c4
	if (cr6.lt) goto loc_827070C4;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_82707204:
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v3,r0,r7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v9,v10,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvsl v4,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v13,v13,v12,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vmrghb v6,v0,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v5,v0,v13
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v12,v8,v3,v4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v13,v0,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// ble cr6,0x8270730c
	if (!cr6.gt) goto loc_8270730C;
	// li r9,0
	ctx.r9.s64 = 0;
loc_82707278:
	// vadduhm v7,v12,v9
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v29,v5,v5
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// vadduhm v8,v13,v10
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vor v30,v6,v6
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// vslh v5,v7,v11
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// vslh v6,v8,v11
	// lvsl v3,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v31,r0,r7
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v7,v5
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vor v5,v12,v12
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vor v12,v9,v9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vperm v9,v4,v31,v3
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vadduhm v8,v8,v6
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vor v6,v13,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vor v13,v10,v10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v7,v7,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v8,v8,v2
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v4,v30,v10
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v3,v29,v9
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubshs v4,v0,v4
	// vsubshs v3,v0,v3
	// vadduhm v8,v8,v4
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v7,v7,v3
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vsrah v8,v8,v1
	// vsrah v7,v7,v1
	// vpkshus v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// blt cr6,0x82707278
	if (cr6.lt) goto loc_82707278;
loc_8270730C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82707314"))) PPC_WEAK_FUNC(sub_82707314);
PPC_FUNC_IMPL(__imp__sub_82707314) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82707318"))) PPC_WEAK_FUNC(sub_82707318);
PPC_FUNC_IMPL(__imp__sub_82707318) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// cntlzw r11,r9
	r11.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// addi r9,r1,-96
	ctx.r9.s64 = ctx.r1.s64 + -96;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// vspltish v3,1
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// vspltish v0,2
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// vspltish v31,5
	// addi r8,r1,-32
	ctx.r8.s64 = ctx.r1.s64 + -32;
	// vspltish v10,4
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,1
	ctx.r9.s64 = 1;
	// addi r5,r1,-80
	ctx.r5.s64 = ctx.r1.s64 + -80;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// stvx v31,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v3,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-64
	ctx.r5.s64 = ctx.r1.s64 + -64;
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-48
	ctx.r5.s64 = ctx.r1.s64 + -48;
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// slw r8,r9,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// stw r8,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, ctx.r8.u32);
	// bne cr6,0x827074f0
	if (!cr6.eq) goto loc_827074F0;
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r10,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, ctx.r10.u32);
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lvx128 v5,r0,r7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v12,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v29,r0,r9
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v9,v9,v29,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvsl v6,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v8,v8,v5,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// sth r7,-112(r1)
	PPC_STORE_U16(ctx.r1.u32 + -112, ctx.r7.u16);
	// vmrghb v12,v13,v9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v11,v13,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82707650
	if (!cr6.gt) goto loc_82707650;
	// b 0x82707428
	goto loc_82707428;
loc_827073F8:
	// addi r11,r1,-96
	r11.s64 = ctx.r1.s64 + -96;
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-80
	r11.s64 = ctx.r1.s64 + -80;
	// lvx128 v3,r0,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-64
	r11.s64 = ctx.r1.s64 + -64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// lvx128 v31,r0,r11
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
loc_82707428:
	// vslh v4,v12,v31
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v31,v12,v10
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// vslh v5,v12,v0
	// vslh v10,v11,v10
	// vslh v3,v11,v3
	// vadduhm v4,v31,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v31,v9,v0
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v7,r0,r9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v5,v5,v12
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v10,v3,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// vsubshs v31,v9,v31
	// vor v9,v11,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v12,v12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vperm v12,v8,v7,v6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vadduhm v8,v5,v4
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrghb v12,v13,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v0,v12,v0
	// vadduhm v10,v10,v2
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vsubshs v0,v13,v0
	// vadduhm v0,v31,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v0,v10,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsrah v0,v0,v1
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,4
	r11.s64 = 4;
	// stvewx v0,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lhz r11,-112(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + -112);
	// lwz r9,-108(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r9,-100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// stw r10,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, ctx.r10.u32);
	// sth r11,-112(r1)
	PPC_STORE_U16(ctx.r1.u32 + -112, r11.u16);
	// blt cr6,0x827073f8
	if (cr6.lt) goto loc_827073F8;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_827074F0:
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r3
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lvx128 v30,r0,r7
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v4,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v11,v12,v11,v4
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvsl v4,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v9,v9,v8,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v12,v13,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v11,v13,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vmrghb v7,v13,v9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v8,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// vperm v8,v5,v30,v4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v9,v13,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82707650
	if (!cr6.gt) goto loc_82707650;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8270756C:
	// vslh v27,v12,v10
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v28,v12,v31
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// vslh v26,v9,v3
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// vslh v25,v7,v0
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// vslh v29,v12,v0
	// vadduhm v28,v27,v28
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v28.u16)));
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v27,v9,v10
	// lvsl v4,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v30,r0,r7
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// vadduhm v29,v29,v12
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v27,v26,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsubshs v26,v7,v25
	// vslh v7,v11,v0
	// vadduhm v25,v7,v11
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vperm v7,v5,v30,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrghb v5,v13,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v4,v13,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v7,v29,v28
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v29,v11,v10
	// vslh v28,v8,v3
	// vslh v30,v5,v0
	// vadduhm v7,v7,v27
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v27,v4,v0
	// vsubshs v30,v13,v30
	// vadduhm v7,v7,v2
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vsubshs v27,v13,v27
	// vadduhm v30,v26,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vadduhm v30,v7,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v7,v11,v31
	// vsrah v30,v30,v1
	// vadduhm v29,v29,v7
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v7,v8,v10
	// vadduhm v28,v28,v7
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v7,v6,v0
	// vsubshs v6,v6,v7
	// vor v7,v9,v9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v12,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vor v12,v5,v5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vadduhm v5,v25,v29
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v27,v6,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v6,v8,v8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v11,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v4,v4
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vadduhm v4,v5,v28
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v4,v4,v2
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v5,v4,v27
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v5,v5,v1
	// vpkshus v5,v30,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)v30.s16)));
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// blt cr6,0x8270756c
	if (cr6.lt) goto loc_8270756C;
loc_82707650:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82707658"))) PPC_WEAK_FUNC(sub_82707658);
PPC_FUNC_IMPL(__imp__sub_82707658) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// vspltish v0,-5
	// li r11,2
	r11.s64 = 2;
	// vspltish v1,6
	// addi r10,r6,1056
	ctx.r10.s64 = ctx.r6.s64 + 1056;
	// li r9,0
	ctx.r9.s64 = 0;
	// vsrh v0,v0,v0
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// subf r3,r4,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v0,v13
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// slw r7,r11,r8
	ctx.r7.u64 = ctx.r8.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r8.u8 & 0x3F));
	// bl 0x82706cd0
	sub_82706CD0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_827076AC"))) PPC_WEAK_FUNC(sub_827076AC);
PPC_FUNC_IMPL(__imp__sub_827076AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827076B0"))) PPC_WEAK_FUNC(sub_827076B0);
PPC_FUNC_IMPL(__imp__sub_827076B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// vspltish v0,-5
	// li r11,2
	r11.s64 = 2;
	// vspltish v1,6
	// addi r10,r6,1056
	ctx.r10.s64 = ctx.r6.s64 + 1056;
	// li r9,0
	ctx.r9.s64 = 0;
	// vsrh v0,v0,v0
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// subf r3,r4,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v0,v13
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// slw r7,r11,r8
	ctx.r7.u64 = ctx.r8.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r8.u8 & 0x3F));
	// bl 0x82707318
	sub_82707318(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82707704"))) PPC_WEAK_FUNC(sub_82707704);
PPC_FUNC_IMPL(__imp__sub_82707704) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82707708"))) PPC_WEAK_FUNC(sub_82707708);
PPC_FUNC_IMPL(__imp__sub_82707708) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// vspltish v0,7
	// li r11,2
	r11.s64 = 2;
	// vspltish v1,4
	// addi r10,r6,1056
	ctx.r10.s64 = ctx.r6.s64 + 1056;
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// subf r3,r4,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v0,v13
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// slw r7,r11,r8
	ctx.r7.u64 = ctx.r8.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r8.u8 & 0x3F));
	// bl 0x82707008
	sub_82707008(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82707758"))) PPC_WEAK_FUNC(sub_82707758);
PPC_FUNC_IMPL(__imp__sub_82707758) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vspltish v0,2
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// vspltish v13,8
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// vspltish v12,6
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// lwz r30,1100(r6)
	r30.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// vslh v0,v13,v0
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r11,r6,1056
	r11.s64 = ctx.r6.s64 + 1056;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// vsubshs v0,v0,v12
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82700240
	sub_82700240(ctx, base);
	// clrlwi r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	// vspltish v13,-1
	// li r9,1
	ctx.r9.s64 = 1;
	// vspltisb v5,0
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_set1_epi8(char(0x0)));
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// vspltish v4,1
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// vspltish v12,4
	// mr r11,r28
	r11.u64 = r28.u64;
	// vspltish v3,5
	// vspltish v8,0
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// vslh v31,v13,v11
	// slw r9,r9,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// bne cr6,0x827078b4
	if (!cr6.eq) goto loc_827078B4;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x827079b0
	if (!cr6.gt) goto loc_827079B0;
loc_82707818:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// vsldoi v11,v13,v10,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v9,v13,v10,4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 12));
	// vsldoi v10,v13,v10,6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 10));
	// vslh v7,v11,v0
	// vslh v6,v11,v3
	// vslh v2,v11,v12
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v7,v2,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v9,v12
	// vslh v9,v9,v4
	// vslh v2,v13,v0
	// vadduhm v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v10,v0
	// vadduhm v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubshs v13,v10,v6
	// vsubshs v10,v5,v2
	// vadduhm v13,v13,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vadduhm v13,v11,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// vsrah v13,v13,v11
	// vor v8,v8,v13
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvewx v13,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v13,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// bne cr6,0x82707818
	if (!cr6.eq) goto loc_82707818;
	// b 0x827079b0
	goto loc_827079B0;
loc_827078B4:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x827079b0
	if (!cr6.gt) goto loc_827079B0;
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
loc_827078C0:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v10,v11,v13,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 14));
	// vsldoi v7,v11,v13,4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 12));
	// vsldoi v6,v11,v13,6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 10));
	// vslh v11,v11,v0
	// vslh v2,v10,v0
	// vslh v30,v10,v12
	// vslh v1,v10,v3
	// vsubshs v11,v5,v11
	// vadduhm v2,v2,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v7,v12
	// vslh v7,v7,v4
	// vadduhm v1,v30,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v7,v7,v10
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v6,v0
	// vadduhm v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// lvx128 v1,r0,r8
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vsubshs v10,v6,v10
	// vadduhm v7,v2,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v6,v10,v11
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vsldoi v11,v13,v9,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vadduhm v2,v7,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsldoi v10,v13,v9,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v9,v13,v9,6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vslh v7,v11,v0
	// vslh v30,v11,v3
	// vslh v29,v11,v12
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v7,v29,v30
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v30,v10,v12
	// vslh v10,v10,v4
	// vslh v29,v13,v0
	// vadduhm v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v10,v10,v30
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v30,v9,v0
	// vadduhm v10,v11,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v13,v9,v30
	// vsubshs v9,v5,v29
	// vadduhm v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v30,v13,v9
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v13,v2,v6
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvx128 v6,r0,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v11,v10,v30
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vsrah v13,v13,v6
	// vsrah v11,v11,v6
	// vor v9,v8,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vor v8,v9,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// bne cr6,0x827078c0
	if (!cr6.eq) goto loc_827078C0;
loc_827079B0:
	// vand v0,v8,v31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vcmpgtuh. v0,v0,v5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_827079CC"))) PPC_WEAK_FUNC(sub_827079CC);
PPC_FUNC_IMPL(__imp__sub_827079CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827079D0"))) PPC_WEAK_FUNC(sub_827079D0);
PPC_FUNC_IMPL(__imp__sub_827079D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vspltish v0,2
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// vspltish v13,8
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// vspltish v12,6
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// lwz r30,1100(r6)
	r30.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// vslh v0,v13,v0
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r11,r6,1056
	r11.s64 = ctx.r6.s64 + 1056;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// vsubshs v0,v0,v12
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82700240
	sub_82700240(ctx, base);
	// clrlwi r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	// vspltish v0,-1
	// li r9,1
	ctx.r9.s64 = 1;
	// vspltisb v6,0
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_set1_epi8(char(0x0)));
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// vspltish v5,1
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// vspltish v11,4
	// mr r11,r28
	r11.u64 = r28.u64;
	// vspltish v4,5
	// vspltish v8,0
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// vslh v31,v0,v12
	// slw r9,r9,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// bne cr6,0x82707b2c
	if (!cr6.eq) goto loc_82707B2C;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82707c28
	if (!cr6.gt) goto loc_82707C28;
loc_82707A90:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// vsldoi v12,v0,v10,4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 12));
	// vsldoi v9,v0,v10,2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v7,v0,v10,6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 10));
	// vslh v10,v12,v13
	// vslh v3,v12,v4
	// vslh v2,v12,v11
	// vslh v7,v7,v13
	// vadduhm v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v10,v2,v3
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v9,v11
	// vslh v9,v9,v5
	// vsubshs v7,v6,v7
	// vadduhm v9,v9,v3
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v0,v13
	// vsubshs v0,v0,v3
	// vadduhm v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v0,v12,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vadduhm v12,v0,v9
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v0,v12,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// vsrah v0,v0,v12
	// vor v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// bne cr6,0x82707a90
	if (!cr6.eq) goto loc_82707A90;
	// b 0x82707c28
	goto loc_82707C28;
loc_82707B2C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82707c28
	if (!cr6.gt) goto loc_82707C28;
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
loc_82707B38:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v10,v12,v0,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vsldoi v7,v12,v0,2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v2,v12,v0,6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vslh v3,v10,v13
	// vslh v30,v10,v11
	// vslh v1,v10,v4
	// vadduhm v3,v3,v10
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v7,v11
	// vslh v7,v7,v5
	// vadduhm v1,v30,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v30,v12,v13
	// vadduhm v7,v7,v10
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v2,v13
	// vsubshs v12,v12,v30
	// vadduhm v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// lvx128 v1,r0,r8
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vsubshs v10,v6,v10
	// vadduhm v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v2,v12,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsldoi v12,v0,v9,4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v10,v0,v9,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vadduhm v3,v7,v1
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsldoi v9,v0,v9,6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vslh v7,v12,v13
	// vslh v30,v12,v4
	// vslh v29,v12,v11
	// vslh v9,v9,v13
	// vadduhm v12,v7,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v7,v29,v30
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v30,v10,v11
	// vslh v10,v10,v5
	// vsubshs v9,v6,v9
	// vadduhm v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v10,v10,v30
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v30,v0,v13
	// vadduhm v10,v12,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v0,v0,v30
	// vadduhm v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v30,v0,v9
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v0,v3,v2
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// lvx128 v3,r0,r8
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v10,v30
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vsrah v0,v0,v3
	// vsrah v12,v12,v3
	// vor v9,v8,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v8,v9,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// bne cr6,0x82707b38
	if (!cr6.eq) goto loc_82707B38;
loc_82707C28:
	// vand v0,v8,v31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vcmpgtuh. v0,v0,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82707C44"))) PPC_WEAK_FUNC(sub_82707C44);
PPC_FUNC_IMPL(__imp__sub_82707C44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82707C48"))) PPC_WEAK_FUNC(sub_82707C48);
PPC_FUNC_IMPL(__imp__sub_82707C48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v2,5
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// addi r9,r6,1056
	ctx.r9.s64 = ctx.r6.s64 + 1056;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// vspltish v31,1
	// vspltish v0,2
	// clrlwi r11,r7,31
	r11.u64 = ctx.r7.u32 & 0x1;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// vspltish v12,4
	// addi r10,r1,-192
	ctx.r10.s64 = ctx.r1.s64 + -192;
	// vor v30,v2,v2
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r31,1
	r31.s64 = 1;
	// vspltish v10,15
	// addi r9,r1,-176
	ctx.r9.s64 = ctx.r1.s64 + -176;
	// vspltish v20,7
	// vrlh v9,v12,v12
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// vaddshs v29,v10,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lwz r3,1100(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// vsubshs v19,v9,v11
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-80
	ctx.r10.s64 = ctx.r1.s64 + -80;
	// stvx v20,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r3,-236(r1)
	PPC_STORE_U32(ctx.r1.u32 + -236, ctx.r3.u32);
	// stvx v31,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-224
	ctx.r10.s64 = ctx.r1.s64 + -224;
	// slw r9,r31,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r8.u8 & 0x3F));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-112
	ctx.r10.s64 = ctx.r1.s64 + -112;
	// stw r9,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, ctx.r9.u32);
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// stvx v30,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-208
	ctx.r10.s64 = ctx.r1.s64 + -208;
	// stvx v29,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-256
	ctx.r10.s64 = ctx.r1.s64 + -256;
	// stvx v19,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82707e7c
	if (!cr6.eq) goto loc_82707E7C;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r31,r1,-256
	r31.s64 = ctx.r1.s64 + -256;
	// add r11,r8,r4
	r11.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v6,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vmrghb v5,v13,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v4,v13,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vperm v10,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v3,r0,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v28,r0,r8
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v8,v3,v28,v1
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrghb v9,v13,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x827080b0
	if (!cr6.gt) goto loc_827080B0;
	// li r8,0
	ctx.r8.s64 = 0;
loc_82707D90:
	// vslh v3,v11,v12
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v6,v11,v30
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vslh v1,v9,v31
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// vslh v7,v11,v0
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v5,v5,v0
	// vadduhm v6,v3,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v3,v9,v12
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvx128 v27,r0,r6
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v5,v13,v5
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// vadduhm v3,v1,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v1,v10,v0
	// vadduhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v26,v1,v10
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v6,v28,v27,v1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v3,v7,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v28,v8,v31
	// vslh v27,v4,v0
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v3,v3,v29
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v27,v13,v27
	// vslh v1,v7,v0
	// vsubshs v1,v7,v1
	// vadduhm v5,v1,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v1,v10,v12
	// vadduhm v3,v3,v5
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v10,v30
	// vsrah v3,v3,v2
	// vadduhm v1,v1,v5
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v8,v12
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v28,v28,v5
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v6,v0
	// vsubshs v4,v6,v5
	// vor v5,v11,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v9,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vadduhm v7,v26,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v27,v4,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v4,v10,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v8,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v6,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v6,v7,v28
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v6,v6,v29
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v7,v6,v27
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v7,v7,v2
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82707d90
	if (cr6.lt) goto loc_82707D90;
	// b 0x827080b0
	goto loc_827080B0;
loc_82707E7C:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,16
	ctx.r8.s64 = 16;
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r28,r1,-160
	r28.s64 = ctx.r1.s64 + -160;
	// li r31,16
	r31.s64 = 16;
	// lvrx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r30,16
	r30.s64 = 16;
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v10,v9,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// li r29,32
	r29.s64 = 32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,-128
	r28.s64 = ctx.r1.s64 + -128;
	// vmrghb v4,v13,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// vmrglb v28,v13,v11
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v11,v4,v4
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// lvrx v8,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v9,v8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v6,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v5,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v5,v6,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vor v9,v28,v28
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v28.u8));
	// stvx v7,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-272
	ctx.r10.s64 = ctx.r1.s64 + -272;
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v3,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v4,v8,v6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvrx v1,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v8,v13,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrglb v6,v13,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v7,v13,v5
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v5,v13,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v3,v13,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x827080b0
	if (!cr6.gt) goto loc_827080B0;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_82707F4C:
	// vor v27,v11,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v11,v8,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// vor v22,v9,v9
	_mm_store_si128((__m128i*)v22.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// addi r31,r11,32
	r31.s64 = r11.s64 + 32;
	// vor v9,v6,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v4,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v8,v11,v0
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v23,v11,v12
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v24,v11,v30
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// lvx128 v26,r0,r31
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v27,v27,v0
	// vadduhm v25,v8,v11
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r31,r10,-16
	r31.s64 = ctx.r10.s64 + -16;
	// vor v8,v5,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vadduhm v24,v23,v24
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vor v23,v10,v10
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v7,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v3,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vslh v5,v8,v12
	// vslh v3,v8,v31
	// vadduhm v25,v25,v24
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsubshs v27,v13,v27
	// vslh v24,v9,v12
	// vadduhm v21,v3,v5
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// lvx128 v5,r0,r6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v4,v28,v5,v1
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvsl v3,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v5,v5,v26,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vslh v26,v9,v30
	// vslh v28,v9,v0
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// vadduhm v25,v25,v21
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vmrghb v3,v13,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v22,v22,v0
	// vmrghb v1,v13,v5
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v26,v24,v26
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v28,v28,v9
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v24,v6,v31
	// vor v5,v3,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vor v3,v1,v1
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vslh v21,v4,v0
	// vadduhm v28,v28,v26
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v1,v5,v0
	// vsubshs v1,v5,v1
	// vadduhm v1,v1,v27
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v27,v6,v12
	// vor v26,v1,v1
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vadduhm v27,v24,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsubshs v24,v4,v21
	// vsubshs v21,v13,v22
	// vslh v22,v10,v0
	// vadduhm v1,v28,v27
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v27,v25,v29
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v28,v24,v21
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vslh v25,v7,v31
	// vadduhm v1,v1,v29
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v27,v27,v26
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v26,v10,v12
	// vslh v24,v23,v0
	// vadduhm v1,v1,v28
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v28,v10,v30
	// vsrah v27,v27,v2
	// vsubshs v24,v13,v24
	// vsrah v1,v1,v2
	// vadduhm v28,v26,v28
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v26,v7,v12
	// stvx v27,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v22,v22,v10
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stvx v1,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v26,v25,v26
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v25,v3,v0
	// vsubshs v25,v3,v25
	// vadduhm v25,v25,v24
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v1,v22,v28
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v28,v1,v26
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vadduhm v28,v28,v29
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v1,v28,v25
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vsrah v1,v1,v2
	// stvx v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x82707f4c
	if (cr6.lt) goto loc_82707F4C;
loc_827080B0:
	// vspltish v11,8
	// vor v4,v31,v31
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)v31.u8));
	// vspltish v10,-1
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// vspltish v8,0
	// vor v5,v13,v13
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// vor v3,v2,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// vslh v31,v10,v11
	// bne cr6,0x82708170
	if (!cr6.eq) goto loc_82708170;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8270825c
	if (!cr6.gt) goto loc_8270825C;
loc_827080E4:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// vsldoi v11,v13,v10,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v9,v13,v10,4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 12));
	// vsldoi v10,v13,v10,6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 10));
	// vslh v7,v11,v0
	// vslh v6,v11,v3
	// vslh v2,v11,v12
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v7,v2,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v9,v12
	// vslh v9,v9,v4
	// vslh v2,v13,v0
	// vadduhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v10,v0
	// vsubshs v13,v10,v6
	// vsubshs v10,v5,v2
	// vadduhm v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v13,v11,v7
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v11,v13,v9
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v11,v11,v19
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v13,v11,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsrah v13,v13,v20
	// vor v8,v8,v13
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvewx v13,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v13,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x827080e4
	if (!cr6.eq) goto loc_827080E4;
	// b 0x8270825c
	goto loc_8270825C;
loc_82708170:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8270825c
	if (!cr6.gt) goto loc_8270825C;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_8270817C:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v10,v11,v13,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 14));
	// vsldoi v7,v11,v13,4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 12));
	// vsldoi v6,v11,v13,6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 10));
	// vslh v11,v11,v0
	// vslh v2,v10,v0
	// vslh v30,v10,v12
	// vslh v1,v10,v3
	// vsubshs v11,v5,v11
	// vadduhm v2,v2,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v7,v12
	// vslh v7,v7,v4
	// vadduhm v1,v30,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v7,v7,v10
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v6,v0
	// vadduhm v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsubshs v10,v6,v10
	// vadduhm v7,v2,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v6,v10,v11
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vsldoi v11,v13,v9,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vadduhm v2,v7,v19
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vsldoi v10,v13,v9,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v9,v13,v9,6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vslh v7,v11,v0
	// vslh v1,v11,v3
	// vslh v30,v11,v12
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v7,v30,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v10,v12
	// vslh v10,v10,v4
	// vslh v30,v13,v0
	// vadduhm v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v9,v0
	// vadduhm v10,v11,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v13,v9,v1
	// vsubshs v9,v5,v30
	// vadduhm v10,v10,v19
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v1,v13,v9
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v13,v2,v6
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v11,v10,v1
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsrah v13,v13,v20
	// vsrah v11,v11,v20
	// vor v9,v8,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vor v8,v9,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x8270817c
	if (!cr6.eq) goto loc_8270817C;
loc_8270825C:
	// vand v0,v8,v31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vcmpgtuh. v0,v0,v5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82708274"))) PPC_WEAK_FUNC(sub_82708274);
PPC_FUNC_IMPL(__imp__sub_82708274) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82708278"))) PPC_WEAK_FUNC(sub_82708278);
PPC_FUNC_IMPL(__imp__sub_82708278) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v5,5
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// addi r9,r6,1056
	ctx.r9.s64 = ctx.r6.s64 + 1056;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// vspltish v4,1
	// vspltish v0,2
	// clrlwi r11,r7,31
	r11.u64 = ctx.r7.u32 & 0x1;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// vspltish v12,4
	// addi r10,r1,-176
	ctx.r10.s64 = ctx.r1.s64 + -176;
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r31,1
	r31.s64 = 1;
	// vor v3,v5,v5
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// addi r9,r1,-256
	ctx.r9.s64 = ctx.r1.s64 + -256;
	// vspltish v10,15
	// vspltish v20,7
	// vrlh v9,v12,v12
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-192
	ctx.r10.s64 = ctx.r1.s64 + -192;
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lwz r3,1100(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// vaddshs v2,v10,v11
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// vsubshs v19,v9,v11
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// stvx v20,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r3,-220(r1)
	PPC_STORE_U32(ctx.r1.u32 + -220, ctx.r3.u32);
	// stvx v4,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-208
	ctx.r10.s64 = ctx.r1.s64 + -208;
	// slw r9,r31,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r8.u8 & 0x3F));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-112
	ctx.r10.s64 = ctx.r1.s64 + -112;
	// stw r9,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, ctx.r9.u32);
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-80
	ctx.r10.s64 = ctx.r1.s64 + -80;
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-240
	ctx.r10.s64 = ctx.r1.s64 + -240;
	// stvx v19,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x827084c4
	if (!cr6.eq) goto loc_827084C4;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r31,r1,-240
	r31.s64 = ctx.r1.s64 + -240;
	// add r11,r8,r4
	r11.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// lvx128 v6,r0,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v1,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvsl v24,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)v24.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v31,r0,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-240
	ctx.r6.s64 = ctx.r1.s64 + -240;
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-240
	ctx.r6.s64 = ctx.r1.s64 + -240;
	// vperm v10,v6,v31,v1
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v29,r0,r8
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-256
	ctx.r8.s64 = ctx.r1.s64 + -256;
	// lvx128 v30,r0,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v24,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-256
	ctx.r8.s64 = ctx.r1.s64 + -256;
	// lvx128 v7,r0,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v6,v30,v29,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82708700
	if (!cr6.gt) goto loc_82708700;
	// li r8,0
	ctx.r8.s64 = 0;
loc_827083D0:
	// vor v1,v9,v9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v11,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vor v11,v7,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v28,v1,v0
	// vslh v29,v9,v4
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vslh v30,v11,v12
	// vslh v31,v11,v3
	// lvx128 v27,r0,r6
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v7,v11,v0
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// vadduhm v31,v30,v31
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vslh v30,v9,v12
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v30,v29,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vsubshs v29,v1,v28
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v7,v31
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vor v1,v8,v8
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v10,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v6,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v31,v7,v30
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vperm v7,v28,v27,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vslh v27,v8,v4
	// vmrghb v6,v13,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v28,v31,v2
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmrglb v30,v13,v7
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v6,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v30,v30
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v30.u8));
	// vslh v30,v7,v0
	// vslh v26,v6,v0
	// vsubshs v30,v13,v30
	// vadduhm v31,v29,v30
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v10,v12
	// vslh v30,v10,v3
	// vadduhm v28,v28,v31
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vslh v31,v10,v0
	// vadduhm v30,v29,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v8,v12
	// vadduhm v31,v31,v10
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v29,v27,v29
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v27,v1,v0
	// vsubshs v1,v1,v27
	// vsubshs v27,v13,v26
	// vadduhm v27,v1,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v1,v28,v5
	// stvx v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v1,v31,v30
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v31,v1,v29
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v31,v31,v2
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v1,v31,v27
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v1,v1,v5
	// stvx v1,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x827083d0
	if (cr6.lt) goto loc_827083D0;
	// b 0x82708700
	goto loc_82708700;
loc_827084C4:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,16
	ctx.r8.s64 = 16;
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r28,r1,-160
	r28.s64 = ctx.r1.s64 + -160;
	// li r31,16
	r31.s64 = 16;
	// lvrx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r30,16
	r30.s64 = 16;
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// li r29,32
	r29.s64 = 32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,-128
	r28.s64 = ctx.r1.s64 + -128;
	// vmrghb v31,v13,v11
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// vmrglb v28,v13,v11
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v9,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvrx v8,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// lvlx v10,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v10,v10,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v7,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v1,v7,v6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vor v8,v31,v31
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)v31.u8));
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-272
	ctx.r10.s64 = ctx.r1.s64 + -272;
	// vor v6,v28,v28
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v28.u8));
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// stvx v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v30,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v31,v11,v7
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v29,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	_mm_store_si128((__m128i*)v29.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v7,v13,v9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v29,v30,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_or_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v9,v13,v10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v10,v13,v1
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v1,v13,v31
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v30,v13,v31
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v31,v13,v29
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82708700
	if (!cr6.gt) goto loc_82708700;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_82708594:
	// vor v29,v8,v8
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v8,v11,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vor v11,v1,v1
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// addi r31,r11,32
	r31.s64 = r11.s64 + 32;
	// vslh v21,v29,v0
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v22,v8,v4
	// lvx128 v26,r0,r11
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v28,v11,v0
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v27,v11,v3
	// vslh v23,v11,v12
	// lvx128 v25,r0,r31
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v29,v29,v21
	// addi r31,r10,-16
	r31.s64 = ctx.r10.s64 + -16;
	// vadduhm v24,v28,v11
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vor v28,v7,v7
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v10,v10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v31,v31
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)v31.u8));
	// vslh v31,v8,v12
	// vadduhm v23,v23,v27
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v27,v6,v6
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v9,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v30,v30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v30.u8));
	// lvsl v30,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v22,v22,v31
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v31.u16)));
	// lvx128 v31,r0,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vadduhm v24,v24,v23
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v23,v9,v12
	// vadduhm v24,v24,v22
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v22.u16)));
	// stvx v30,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vperm v30,v26,v31,v1
	_mm_store_si128((__m128i*)v30.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vslh v22,v6,v4
	// vslh v26,v9,v0
	// lvx128 v1,r0,r6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// vperm v1,v31,v25,v1
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vslh v25,v9,v3
	// vadduhm v26,v26,v9
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrghb v31,v13,v1
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v25,v23,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vmrghb v1,v13,v30
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v30,v13,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v26,v26,v25
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v21,v1,v0
	// vsubshs v23,v13,v21
	// vslh v21,v27,v0
	// vadduhm v29,v29,v23
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v23,v6,v12
	// vsubshs v27,v27,v21
	// vslh v21,v10,v0
	// vor v25,v29,v29
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)v29.u8));
	// vadduhm v23,v22,v23
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v22,v30,v0
	// vadduhm v21,v21,v10
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v22,v13,v22
	// vadduhm v29,v27,v22
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vadduhm v27,v26,v23
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v26,v24,v2
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v24,v7,v4
	// vslh v23,v31,v0
	// vadduhm v27,v27,v2
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v26,v26,v25
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v25,v10,v12
	// vadduhm v29,v27,v29
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v27,v10,v3
	// vsrah v29,v29,v5
	// vadduhm v27,v25,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v25,v7,v12
	// vadduhm v25,v24,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v24,v28,v0
	// vsubshs v28,v28,v24
	// vsubshs v24,v13,v23
	// vadduhm v24,v28,v24
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v28,v26,v5
	// stvx v28,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v29,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v29,v21,v27
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)v27.u16)));
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vadduhm v28,v29,v25
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v28,v28,v2
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v29,v28,v24
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v29,v29,v5
	// stvx v29,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x82708594
	if (cr6.lt) goto loc_82708594;
loc_82708700:
	// vspltish v11,8
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// vspltish v10,-1
	// vor v2,v13,v13
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vspltish v8,0
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// vslh v31,v10,v11
	// bne cr6,0x827087b8
	if (!cr6.eq) goto loc_827087B8;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x827088a4
	if (!cr6.gt) goto loc_827088A4;
loc_8270872C:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// vsldoi v11,v13,v10,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v9,v13,v10,4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 12));
	// vsldoi v10,v13,v10,6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 10));
	// vslh v7,v11,v0
	// vslh v6,v11,v5
	// vslh v3,v11,v12
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v7,v3,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v9,v12
	// vslh v9,v9,v4
	// vslh v3,v13,v0
	// vadduhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v10,v0
	// vsubshs v13,v10,v6
	// vsubshs v10,v2,v3
	// vadduhm v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v13,v11,v7
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v11,v13,v9
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v11,v11,v19
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v13,v11,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsrah v13,v13,v20
	// vor v8,v8,v13
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvewx v13,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v13,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x8270872c
	if (!cr6.eq) goto loc_8270872C;
	// b 0x827088a4
	goto loc_827088A4;
loc_827087B8:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x827088a4
	if (!cr6.gt) goto loc_827088A4;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_827087C4:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v10,v11,v13,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 14));
	// vsldoi v7,v11,v13,4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 12));
	// vsldoi v6,v11,v13,6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 10));
	// vslh v11,v11,v0
	// vslh v3,v10,v0
	// vslh v30,v10,v12
	// vslh v1,v10,v5
	// vsubshs v11,v2,v11
	// vadduhm v3,v3,v10
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v7,v12
	// vslh v7,v7,v4
	// vadduhm v1,v30,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v7,v7,v10
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v6,v0
	// vadduhm v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsubshs v10,v6,v10
	// vadduhm v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v6,v10,v11
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vsldoi v11,v13,v9,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vadduhm v3,v7,v19
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vsldoi v10,v13,v9,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v9,v13,v9,6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vslh v7,v11,v0
	// vslh v1,v11,v5
	// vslh v30,v11,v12
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v7,v30,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v10,v12
	// vslh v10,v10,v4
	// vslh v30,v13,v0
	// vadduhm v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v9,v0
	// vadduhm v10,v11,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v13,v9,v1
	// vsubshs v9,v2,v30
	// vadduhm v10,v10,v19
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v1,v13,v9
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v13,v3,v6
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v11,v10,v1
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsrah v13,v13,v20
	// vsrah v11,v11,v20
	// vor v9,v8,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vor v8,v9,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x827087c4
	if (!cr6.eq) goto loc_827087C4;
loc_827088A4:
	// vand v0,v8,v31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vcmpgtuh. v0,v0,v2
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_827088BC"))) PPC_WEAK_FUNC(sub_827088BC);
PPC_FUNC_IMPL(__imp__sub_827088BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827088C0"))) PPC_WEAK_FUNC(sub_827088C0);
PPC_FUNC_IMPL(__imp__sub_827088C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v2,5
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// addi r9,r6,1056
	ctx.r9.s64 = ctx.r6.s64 + 1056;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// vspltish v31,1
	// vspltish v0,2
	// clrlwi r11,r7,31
	r11.u64 = ctx.r7.u32 & 0x1;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// vspltish v12,4
	// addi r10,r1,-192
	ctx.r10.s64 = ctx.r1.s64 + -192;
	// vor v30,v2,v2
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r31,1
	r31.s64 = 1;
	// vspltish v10,15
	// addi r9,r1,-176
	ctx.r9.s64 = ctx.r1.s64 + -176;
	// vspltish v20,7
	// vrlh v9,v12,v12
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// vaddshs v29,v10,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lwz r3,1100(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// vsubshs v19,v9,v11
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-80
	ctx.r10.s64 = ctx.r1.s64 + -80;
	// stvx v20,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r3,-236(r1)
	PPC_STORE_U32(ctx.r1.u32 + -236, ctx.r3.u32);
	// stvx v31,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-224
	ctx.r10.s64 = ctx.r1.s64 + -224;
	// slw r9,r31,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r8.u8 & 0x3F));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-112
	ctx.r10.s64 = ctx.r1.s64 + -112;
	// stw r9,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, ctx.r9.u32);
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// stvx v30,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-208
	ctx.r10.s64 = ctx.r1.s64 + -208;
	// stvx v29,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-256
	ctx.r10.s64 = ctx.r1.s64 + -256;
	// stvx v19,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82708af4
	if (!cr6.eq) goto loc_82708AF4;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r31,r1,-256
	r31.s64 = ctx.r1.s64 + -256;
	// add r11,r8,r4
	r11.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v6,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vmrghb v5,v13,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v4,v13,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vperm v10,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v3,r0,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v28,r0,r8
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v8,v3,v28,v1
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrghb v9,v13,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82708d28
	if (!cr6.gt) goto loc_82708D28;
	// li r8,0
	ctx.r8.s64 = 0;
loc_82708A08:
	// vslh v3,v11,v12
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v6,v11,v30
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vslh v1,v9,v31
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// vslh v7,v11,v0
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v5,v5,v0
	// vadduhm v6,v3,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v3,v9,v12
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvx128 v27,r0,r6
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v5,v13,v5
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// vadduhm v3,v1,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v1,v10,v0
	// vadduhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v26,v1,v10
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v6,v28,v27,v1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v3,v7,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v28,v8,v31
	// vslh v27,v4,v0
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v3,v3,v29
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v27,v13,v27
	// vslh v1,v7,v0
	// vsubshs v1,v7,v1
	// vadduhm v5,v1,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v1,v10,v12
	// vadduhm v3,v3,v5
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v10,v30
	// vsrah v3,v3,v2
	// vadduhm v1,v1,v5
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v8,v12
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v28,v28,v5
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v6,v0
	// vsubshs v4,v6,v5
	// vor v5,v11,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v9,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vadduhm v7,v26,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v27,v4,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v4,v10,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v8,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v6,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v6,v7,v28
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v6,v6,v29
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v7,v6,v27
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v7,v7,v2
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82708a08
	if (cr6.lt) goto loc_82708A08;
	// b 0x82708d28
	goto loc_82708D28;
loc_82708AF4:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,16
	ctx.r8.s64 = 16;
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r28,r1,-160
	r28.s64 = ctx.r1.s64 + -160;
	// li r31,16
	r31.s64 = 16;
	// lvrx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r30,16
	r30.s64 = 16;
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v10,v9,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// li r29,32
	r29.s64 = 32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,-128
	r28.s64 = ctx.r1.s64 + -128;
	// vmrghb v4,v13,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// vmrglb v28,v13,v11
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v11,v4,v4
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// lvrx v8,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v9,v8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v6,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v5,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v5,v6,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vor v9,v28,v28
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v28.u8));
	// stvx v7,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-272
	ctx.r10.s64 = ctx.r1.s64 + -272;
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v3,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v4,v8,v6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvrx v1,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v8,v13,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrglb v6,v13,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v7,v13,v5
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v5,v13,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v3,v13,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82708d28
	if (!cr6.gt) goto loc_82708D28;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_82708BC4:
	// vor v27,v11,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v11,v8,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// vor v22,v9,v9
	_mm_store_si128((__m128i*)v22.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// addi r31,r11,32
	r31.s64 = r11.s64 + 32;
	// vor v9,v6,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v4,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v8,v11,v0
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v23,v11,v12
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v24,v11,v30
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// lvx128 v26,r0,r31
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v27,v27,v0
	// vadduhm v25,v8,v11
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r31,r10,-16
	r31.s64 = ctx.r10.s64 + -16;
	// vor v8,v5,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vadduhm v24,v23,v24
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vor v23,v10,v10
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v7,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v3,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vslh v5,v8,v12
	// vslh v3,v8,v31
	// vadduhm v25,v25,v24
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsubshs v27,v13,v27
	// vslh v24,v9,v12
	// vadduhm v21,v3,v5
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// lvx128 v5,r0,r6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v4,v28,v5,v1
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvsl v3,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v5,v5,v26,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vslh v26,v9,v30
	// vslh v28,v9,v0
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// vadduhm v25,v25,v21
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vmrghb v3,v13,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v22,v22,v0
	// vmrghb v1,v13,v5
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v26,v24,v26
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v28,v28,v9
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v24,v6,v31
	// vor v5,v3,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vor v3,v1,v1
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vslh v21,v4,v0
	// vadduhm v28,v28,v26
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v1,v5,v0
	// vsubshs v1,v5,v1
	// vadduhm v1,v1,v27
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v27,v6,v12
	// vor v26,v1,v1
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vadduhm v27,v24,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsubshs v24,v4,v21
	// vsubshs v21,v13,v22
	// vslh v22,v10,v0
	// vadduhm v1,v28,v27
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v27,v25,v29
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v28,v24,v21
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vslh v25,v7,v31
	// vadduhm v1,v1,v29
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v27,v27,v26
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v26,v10,v12
	// vslh v24,v23,v0
	// vadduhm v1,v1,v28
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v28,v10,v30
	// vsrah v27,v27,v2
	// vsubshs v24,v13,v24
	// vsrah v1,v1,v2
	// vadduhm v28,v26,v28
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v26,v7,v12
	// stvx v27,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v22,v22,v10
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stvx v1,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v26,v25,v26
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v25,v3,v0
	// vsubshs v25,v3,v25
	// vadduhm v25,v25,v24
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v1,v22,v28
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v28,v1,v26
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vadduhm v28,v28,v29
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v1,v28,v25
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vsrah v1,v1,v2
	// stvx v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x82708bc4
	if (cr6.lt) goto loc_82708BC4;
loc_82708D28:
	// vspltish v11,8
	// vor v5,v31,v31
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)v31.u8));
	// vspltish v10,-1
	// vor v6,v13,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// vspltish v8,0
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// vor v13,v0,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// vor v4,v2,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// vslh v31,v10,v11
	// vor v11,v12,v12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// bne cr6,0x82708df0
	if (!cr6.eq) goto loc_82708DF0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82708edc
	if (!cr6.gt) goto loc_82708EDC;
loc_82708D64:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// vsldoi v12,v0,v10,4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 12));
	// vsldoi v9,v0,v10,2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v7,v0,v10,6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 10));
	// vslh v10,v12,v13
	// vslh v3,v12,v4
	// vslh v2,v12,v11
	// vslh v7,v7,v13
	// vadduhm v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v10,v2,v3
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v9,v11
	// vslh v9,v9,v5
	// vsubshs v7,v6,v7
	// vadduhm v9,v9,v3
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v0,v13
	// vsubshs v0,v0,v3
	// vadduhm v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v0,v12,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v12,v0,v9
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v12,v12,v19
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v0,v12,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsrah v0,v0,v20
	// vor v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x82708d64
	if (!cr6.eq) goto loc_82708D64;
	// b 0x82708edc
	goto loc_82708EDC;
loc_82708DF0:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82708edc
	if (!cr6.gt) goto loc_82708EDC;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_82708DFC:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v10,v12,v0,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vsldoi v7,v12,v0,2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v2,v12,v0,6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vslh v3,v10,v13
	// vslh v30,v10,v11
	// vslh v1,v10,v4
	// vadduhm v3,v3,v10
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v7,v11
	// vslh v7,v7,v5
	// vadduhm v1,v30,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v30,v12,v13
	// vadduhm v7,v7,v10
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v2,v13
	// vsubshs v12,v12,v30
	// vadduhm v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsubshs v10,v6,v10
	// vadduhm v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v2,v12,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsldoi v12,v0,v9,4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v10,v0,v9,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vadduhm v3,v7,v19
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vsldoi v9,v0,v9,6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vslh v7,v12,v13
	// vslh v1,v12,v4
	// vslh v30,v12,v11
	// vslh v9,v9,v13
	// vadduhm v12,v7,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v7,v30,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v10,v11
	// vslh v10,v10,v5
	// vsubshs v9,v6,v9
	// vadduhm v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v0,v13
	// vadduhm v10,v12,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v0,v0,v1
	// vadduhm v10,v10,v19
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v1,v0,v9
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v0,v3,v2
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v12,v10,v1
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsrah v0,v0,v20
	// vsrah v12,v12,v20
	// vor v9,v8,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v8,v9,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x82708dfc
	if (!cr6.eq) goto loc_82708DFC;
loc_82708EDC:
	// vand v0,v8,v31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vcmpgtuh. v0,v0,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82708EF4"))) PPC_WEAK_FUNC(sub_82708EF4);
PPC_FUNC_IMPL(__imp__sub_82708EF4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82708EF8"))) PPC_WEAK_FUNC(sub_82708EF8);
PPC_FUNC_IMPL(__imp__sub_82708EF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v5,5
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// addi r9,r6,1056
	ctx.r9.s64 = ctx.r6.s64 + 1056;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// vspltish v4,1
	// vspltish v0,2
	// clrlwi r11,r7,31
	r11.u64 = ctx.r7.u32 & 0x1;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// vspltish v12,4
	// addi r10,r1,-176
	ctx.r10.s64 = ctx.r1.s64 + -176;
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r31,1
	r31.s64 = 1;
	// vor v3,v5,v5
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// addi r9,r1,-256
	ctx.r9.s64 = ctx.r1.s64 + -256;
	// vspltish v10,15
	// vspltish v20,7
	// vrlh v9,v12,v12
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-192
	ctx.r10.s64 = ctx.r1.s64 + -192;
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lwz r3,1100(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// vaddshs v2,v10,v11
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// vsubshs v19,v9,v11
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// stvx v20,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r3,-220(r1)
	PPC_STORE_U32(ctx.r1.u32 + -220, ctx.r3.u32);
	// stvx v4,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-208
	ctx.r10.s64 = ctx.r1.s64 + -208;
	// slw r9,r31,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r8.u8 & 0x3F));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-112
	ctx.r10.s64 = ctx.r1.s64 + -112;
	// stw r9,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, ctx.r9.u32);
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-80
	ctx.r10.s64 = ctx.r1.s64 + -80;
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-240
	ctx.r10.s64 = ctx.r1.s64 + -240;
	// stvx v19,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82709144
	if (!cr6.eq) goto loc_82709144;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r31,r1,-240
	r31.s64 = ctx.r1.s64 + -240;
	// add r11,r8,r4
	r11.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// lvx128 v6,r0,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v1,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvsl v24,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)v24.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v31,r0,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-240
	ctx.r6.s64 = ctx.r1.s64 + -240;
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-240
	ctx.r6.s64 = ctx.r1.s64 + -240;
	// vperm v10,v6,v31,v1
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v29,r0,r8
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-256
	ctx.r8.s64 = ctx.r1.s64 + -256;
	// lvx128 v30,r0,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v24,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-256
	ctx.r8.s64 = ctx.r1.s64 + -256;
	// lvx128 v7,r0,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v6,v30,v29,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82709380
	if (!cr6.gt) goto loc_82709380;
	// li r8,0
	ctx.r8.s64 = 0;
loc_82709050:
	// vor v1,v9,v9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v11,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vor v11,v7,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v28,v1,v0
	// vslh v29,v9,v4
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vslh v30,v11,v12
	// vslh v31,v11,v3
	// lvx128 v27,r0,r6
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v7,v11,v0
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// vadduhm v31,v30,v31
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vslh v30,v9,v12
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v30,v29,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vsubshs v29,v1,v28
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v7,v31
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vor v1,v8,v8
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v10,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v6,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v31,v7,v30
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vperm v7,v28,v27,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vslh v27,v8,v4
	// vmrghb v6,v13,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v28,v31,v2
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmrglb v30,v13,v7
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v6,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v30,v30
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v30.u8));
	// vslh v30,v7,v0
	// vslh v26,v6,v0
	// vsubshs v30,v13,v30
	// vadduhm v31,v29,v30
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v10,v12
	// vslh v30,v10,v3
	// vadduhm v28,v28,v31
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vslh v31,v10,v0
	// vadduhm v30,v29,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v8,v12
	// vadduhm v31,v31,v10
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v29,v27,v29
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v27,v1,v0
	// vsubshs v1,v1,v27
	// vsubshs v27,v13,v26
	// vadduhm v27,v1,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v1,v28,v5
	// stvx v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v1,v31,v30
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v31,v1,v29
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v31,v31,v2
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v1,v31,v27
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v1,v1,v5
	// stvx v1,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82709050
	if (cr6.lt) goto loc_82709050;
	// b 0x82709380
	goto loc_82709380;
loc_82709144:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,16
	ctx.r8.s64 = 16;
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r28,r1,-160
	r28.s64 = ctx.r1.s64 + -160;
	// li r31,16
	r31.s64 = 16;
	// lvrx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r30,16
	r30.s64 = 16;
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// li r29,32
	r29.s64 = 32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,-128
	r28.s64 = ctx.r1.s64 + -128;
	// vmrghb v31,v13,v11
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// vmrglb v28,v13,v11
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v9,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvrx v8,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// lvlx v10,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v10,v10,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v7,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v1,v7,v6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vor v8,v31,v31
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)v31.u8));
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-272
	ctx.r10.s64 = ctx.r1.s64 + -272;
	// vor v6,v28,v28
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v28.u8));
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// stvx v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v30,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v31,v11,v7
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v29,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	_mm_store_si128((__m128i*)v29.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v7,v13,v9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v29,v30,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_or_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v9,v13,v10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v10,v13,v1
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v1,v13,v31
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v30,v13,v31
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v31,v13,v29
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x82709380
	if (!cr6.gt) goto loc_82709380;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_82709214:
	// vor v29,v8,v8
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v8,v11,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vor v11,v1,v1
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// addi r31,r11,32
	r31.s64 = r11.s64 + 32;
	// vslh v21,v29,v0
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v22,v8,v4
	// lvx128 v26,r0,r11
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v28,v11,v0
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v27,v11,v3
	// vslh v23,v11,v12
	// lvx128 v25,r0,r31
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v29,v29,v21
	// addi r31,r10,-16
	r31.s64 = ctx.r10.s64 + -16;
	// vadduhm v24,v28,v11
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vor v28,v7,v7
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v10,v10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v31,v31
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)v31.u8));
	// vslh v31,v8,v12
	// vadduhm v23,v23,v27
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v27,v6,v6
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v9,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v30,v30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v30.u8));
	// lvsl v30,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v22,v22,v31
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v31.u16)));
	// lvx128 v31,r0,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vadduhm v24,v24,v23
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v23,v9,v12
	// vadduhm v24,v24,v22
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v22.u16)));
	// stvx v30,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vperm v30,v26,v31,v1
	_mm_store_si128((__m128i*)v30.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vslh v22,v6,v4
	// vslh v26,v9,v0
	// lvx128 v1,r0,r6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// vperm v1,v31,v25,v1
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vslh v25,v9,v3
	// vadduhm v26,v26,v9
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrghb v31,v13,v1
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v25,v23,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vmrghb v1,v13,v30
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v30,v13,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v26,v26,v25
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v21,v1,v0
	// vsubshs v23,v13,v21
	// vslh v21,v27,v0
	// vadduhm v29,v29,v23
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v23,v6,v12
	// vsubshs v27,v27,v21
	// vslh v21,v10,v0
	// vor v25,v29,v29
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)v29.u8));
	// vadduhm v23,v22,v23
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v22,v30,v0
	// vadduhm v21,v21,v10
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v22,v13,v22
	// vadduhm v29,v27,v22
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vadduhm v27,v26,v23
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v26,v24,v2
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v24,v7,v4
	// vslh v23,v31,v0
	// vadduhm v27,v27,v2
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v26,v26,v25
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v25,v10,v12
	// vadduhm v29,v27,v29
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v27,v10,v3
	// vsrah v29,v29,v5
	// vadduhm v27,v25,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v25,v7,v12
	// vadduhm v25,v24,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v24,v28,v0
	// vsubshs v28,v28,v24
	// vsubshs v24,v13,v23
	// vadduhm v24,v28,v24
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v28,v26,v5
	// stvx v28,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v29,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v29,v21,v27
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)v27.u16)));
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vadduhm v28,v29,v25
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v28,v28,v2
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v29,v28,v24
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v29,v29,v5
	// stvx v29,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x82709214
	if (cr6.lt) goto loc_82709214;
loc_82709380:
	// vspltish v11,8
	// vor v6,v13,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vspltish v10,-1
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// vspltish v8,0
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// vor v13,v0,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// vslh v31,v10,v11
	// vor v11,v12,v12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// bne cr6,0x82709440
	if (!cr6.eq) goto loc_82709440;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8270952c
	if (!cr6.gt) goto loc_8270952C;
loc_827093B4:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// vsldoi v12,v0,v10,4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 12));
	// vsldoi v9,v0,v10,2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v7,v0,v10,6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 10));
	// vslh v10,v12,v13
	// vslh v3,v12,v5
	// vslh v2,v12,v11
	// vslh v7,v7,v13
	// vadduhm v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v10,v2,v3
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v9,v11
	// vslh v9,v9,v4
	// vsubshs v7,v6,v7
	// vadduhm v9,v9,v3
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v0,v13
	// vsubshs v0,v0,v3
	// vadduhm v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v0,v12,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v12,v0,v9
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v12,v12,v19
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v0,v12,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsrah v0,v0,v20
	// vor v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x827093b4
	if (!cr6.eq) goto loc_827093B4;
	// b 0x8270952c
	goto loc_8270952C;
loc_82709440:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8270952c
	if (!cr6.gt) goto loc_8270952C;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_8270944C:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v10,v12,v0,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vsldoi v7,v12,v0,2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v2,v12,v0,6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vslh v3,v10,v13
	// vslh v30,v10,v11
	// vslh v1,v10,v5
	// vadduhm v3,v3,v10
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v7,v11
	// vslh v7,v7,v4
	// vadduhm v1,v30,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v30,v12,v13
	// vadduhm v7,v7,v10
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v2,v13
	// vsubshs v12,v12,v30
	// vadduhm v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsubshs v10,v6,v10
	// vadduhm v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v2,v12,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsldoi v12,v0,v9,4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v10,v0,v9,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vadduhm v3,v7,v19
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vsldoi v9,v0,v9,6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vslh v7,v12,v13
	// vslh v1,v12,v5
	// vslh v30,v12,v11
	// vslh v9,v9,v13
	// vadduhm v12,v7,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v7,v30,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v10,v11
	// vslh v10,v10,v4
	// vsubshs v9,v6,v9
	// vadduhm v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v0,v13
	// vadduhm v10,v12,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v0,v0,v1
	// vadduhm v10,v10,v19
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v1,v0,v9
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v0,v3,v2
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v12,v10,v1
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsrah v0,v0,v20
	// vsrah v12,v12,v20
	// vor v9,v8,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v8,v9,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x8270944c
	if (!cr6.eq) goto loc_8270944C;
loc_8270952C:
	// vand v0,v8,v31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vcmpgtuh. v0,v0,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82709544"))) PPC_WEAK_FUNC(sub_82709544);
PPC_FUNC_IMPL(__imp__sub_82709544) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82709548"))) PPC_WEAK_FUNC(sub_82709548);
PPC_FUNC_IMPL(__imp__sub_82709548) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r10,r6,1056
	ctx.r10.s64 = ctx.r6.s64 + 1056;
	// vspltish v1,3
	// vspltish v22,4
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// lwz r30,1100(r6)
	r30.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// vspltish v24,7
	// rlwinm r11,r31,1,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlh v13,v22,v22
	// vaddshs v2,v1,v0
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r6,r11,3
	ctx.r6.s64 = r11.s64 + 3;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// vsubshs v23,v13,v0
	// bl 0x82700aa0
	sub_82700AA0(ctx, base);
	// clrlwi r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	// li r9,1
	ctx.r9.s64 = 1;
	// vspltish v13,8
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// vspltish v11,-1
	// vspltisb v5,0
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_set1_epi8(char(0x0)));
	// mr r11,r29
	r11.u64 = r29.u64;
	// vspltish v4,1
	// vor v12,v22,v22
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)v22.u8));
	// vspltish v0,2
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// vslh v31,v11,v13
	// vspltish v3,5
	// vspltish v8,0
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// slw r9,r9,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// bne cr6,0x82709674
	if (!cr6.eq) goto loc_82709674;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82709760
	if (!cr6.gt) goto loc_82709760;
loc_827095E8:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// vsldoi v11,v13,v10,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v9,v13,v10,4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 12));
	// vsldoi v10,v13,v10,6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 10));
	// vslh v7,v11,v0
	// vslh v6,v11,v3
	// vslh v2,v11,v12
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v7,v2,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v9,v12
	// vslh v9,v9,v4
	// vslh v2,v13,v0
	// vadduhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v10,v0
	// vsubshs v13,v10,v6
	// vsubshs v10,v5,v2
	// vadduhm v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v13,v11,v7
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v11,v13,v9
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v11,v11,v23
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v13,v11,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsrah v13,v13,v24
	// vor v8,v8,v13
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvewx v13,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v13,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x827095e8
	if (!cr6.eq) goto loc_827095E8;
	// b 0x82709760
	goto loc_82709760;
loc_82709674:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82709760
	if (!cr6.gt) goto loc_82709760;
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
loc_82709680:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v10,v11,v13,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 14));
	// vsldoi v7,v11,v13,4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 12));
	// vsldoi v6,v11,v13,6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 10));
	// vslh v11,v11,v0
	// vslh v2,v10,v0
	// vslh v30,v10,v12
	// vslh v1,v10,v3
	// vsubshs v11,v5,v11
	// vadduhm v2,v2,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v7,v12
	// vslh v7,v7,v4
	// vadduhm v1,v30,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v7,v7,v10
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v6,v0
	// vadduhm v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsubshs v10,v6,v10
	// vadduhm v7,v2,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v6,v10,v11
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vsldoi v11,v13,v9,2
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vadduhm v2,v7,v23
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vsldoi v10,v13,v9,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v9,v13,v9,6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vslh v7,v11,v0
	// vslh v1,v11,v3
	// vslh v30,v11,v12
	// vadduhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v7,v30,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v10,v12
	// vslh v10,v10,v4
	// vslh v30,v13,v0
	// vadduhm v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v9,v0
	// vadduhm v10,v11,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v13,v9,v1
	// vsubshs v9,v5,v30
	// vadduhm v10,v10,v23
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v1,v13,v9
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v13,v2,v6
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v11,v10,v1
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsrah v13,v13,v24
	// vsrah v11,v11,v24
	// vor v9,v8,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vpkshus v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vor v8,v9,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x82709680
	if (!cr6.eq) goto loc_82709680;
loc_82709760:
	// vand v0,v8,v31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vcmpgtuh. v0,v0,v5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8270977C"))) PPC_WEAK_FUNC(sub_8270977C);
PPC_FUNC_IMPL(__imp__sub_8270977C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82709780"))) PPC_WEAK_FUNC(sub_82709780);
PPC_FUNC_IMPL(__imp__sub_82709780) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r10,r6,1056
	ctx.r10.s64 = ctx.r6.s64 + 1056;
	// vspltish v1,3
	// vspltish v22,4
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// lwz r30,1100(r6)
	r30.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// vspltish v24,7
	// rlwinm r11,r31,1,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlh v13,v22,v22
	// vaddshs v2,v1,v0
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r6,r11,3
	ctx.r6.s64 = r11.s64 + 3;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// vsubshs v23,v13,v0
	// bl 0x82700aa0
	sub_82700AA0(ctx, base);
	// clrlwi r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	// li r9,1
	ctx.r9.s64 = 1;
	// vspltish v0,8
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// vspltish v12,-1
	// vspltisb v6,0
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_set1_epi8(char(0x0)));
	// mr r11,r29
	r11.u64 = r29.u64;
	// vspltish v5,1
	// vor v11,v22,v22
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v22.u8));
	// vspltish v13,2
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// vslh v31,v12,v0
	// vspltish v4,5
	// vspltish v8,0
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// slw r9,r9,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// bne cr6,0x827098ac
	if (!cr6.eq) goto loc_827098AC;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82709998
	if (!cr6.gt) goto loc_82709998;
loc_82709820:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// vsldoi v12,v0,v10,4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 12));
	// vsldoi v9,v0,v10,2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 14));
	// vsldoi v7,v0,v10,6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 10));
	// vslh v10,v12,v13
	// vslh v3,v12,v4
	// vslh v2,v12,v11
	// vslh v7,v7,v13
	// vadduhm v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v10,v2,v3
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v9,v11
	// vslh v9,v9,v5
	// vsubshs v7,v6,v7
	// vadduhm v9,v9,v3
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v3,v0,v13
	// vsubshs v0,v0,v3
	// vadduhm v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v0,v12,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v12,v0,v9
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v12,v12,v23
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v0,v12,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsrah v0,v0,v24
	// vor v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x82709820
	if (!cr6.eq) goto loc_82709820;
	// b 0x82709998
	goto loc_82709998;
loc_827098AC:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82709998
	if (!cr6.gt) goto loc_82709998;
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
loc_827098B8:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v10,v12,v0,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vsldoi v7,v12,v0,2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v2,v12,v0,6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vslh v3,v10,v13
	// vslh v30,v10,v11
	// vslh v1,v10,v4
	// vadduhm v3,v3,v10
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v7,v11
	// vslh v7,v7,v5
	// vadduhm v1,v30,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v30,v12,v13
	// vadduhm v7,v7,v10
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v10,v2,v13
	// vsubshs v12,v12,v30
	// vadduhm v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsubshs v10,v6,v10
	// vadduhm v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v2,v12,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsldoi v12,v0,v9,4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v10,v0,v9,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vadduhm v3,v7,v23
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vsldoi v9,v0,v9,6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vslh v7,v12,v13
	// vslh v1,v12,v4
	// vslh v30,v12,v11
	// vslh v9,v9,v13
	// vadduhm v12,v7,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v7,v30,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v10,v11
	// vslh v10,v10,v5
	// vsubshs v9,v6,v9
	// vadduhm v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v10,v10,v1
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vslh v1,v0,v13
	// vadduhm v10,v12,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v0,v0,v1
	// vadduhm v10,v10,v23
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v1,v0,v9
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v0,v3,v2
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v12,v10,v1
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vsrah v0,v0,v24
	// vsrah v12,v12,v24
	// vor v9,v8,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v8,v9,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x827098b8
	if (!cr6.eq) goto loc_827098B8;
loc_82709998:
	// vand v0,v8,v31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vcmpgtuh. v0,v0,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_827099B4"))) PPC_WEAK_FUNC(sub_827099B4);
PPC_FUNC_IMPL(__imp__sub_827099B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_827099B8"))) PPC_WEAK_FUNC(sub_827099B8);
PPC_FUNC_IMPL(__imp__sub_827099B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vspltish v0,8
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// vspltish v13,4
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// lwz r30,1100(r6)
	r30.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r6,1056
	r11.s64 = ctx.r6.s64 + 1056;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// vsubshs v0,v0,v13
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82700240
	sub_82700240(ctx, base);
	// clrlwi r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	// vspltish v0,-1
	// li r9,1
	ctx.r9.s64 = 1;
	// vspltisb v9,0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_set1_epi8(char(0x0)));
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// vspltish v8,3
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// vspltish v11,0
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v3,v0,v13
	// slw r9,r9,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// bne cr6,0x82709ad0
	if (!cr6.eq) goto loc_82709AD0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82709b80
	if (!cr6.gt) goto loc_82709B80;
loc_82709A58:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r7,4
	ctx.r7.s64 = 4;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// vsldoi v12,v0,v13,2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 14));
	// vsldoi v10,v0,v13,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 12));
	// vsldoi v7,v0,v13,6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 10));
	// vadduhm v13,v12,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v12,v13,v8
	// vsubshs v0,v9,v0
	// vadduhm v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vadduhm v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v0,v0,v13
	// vpkshus v13,v0,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v0,v13,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r11,r7
	ea = (r11.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// bne cr6,0x82709a58
	if (!cr6.eq) goto loc_82709A58;
	// b 0x82709b80
	goto loc_82709B80;
loc_82709AD0:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82709b80
	if (!cr6.gt) goto loc_82709B80;
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
loc_82709ADC:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v7,v13,v0,4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vsldoi v10,v13,v0,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v5,v0,v12,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 14));
	// vsldoi v4,v0,v12,4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 12));
	// vsldoi v12,v0,v12,6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 10));
	// vadduhm v10,v10,v7
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsldoi v6,v13,v0,6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vadduhm v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vslh v7,v10,v8
	// vadduhm v13,v13,v6
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v6,v5,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vsubshs v12,v9,v12
	// vadduhm v0,v10,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// lvx128 v7,r0,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v13,v9,v13
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vadduhm v10,v0,v7
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vor v0,v6,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v13,v10,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vslh v10,v0,v8
	// vadduhm v0,v0,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vsrah v0,v13,v10
	// vsrah v13,v12,v10
	// vor v12,v11,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v11,v12,v13
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// bne cr6,0x82709adc
	if (!cr6.eq) goto loc_82709ADC;
loc_82709B80:
	// vand v0,v11,v3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vcmpgtuh. v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82709B9C"))) PPC_WEAK_FUNC(sub_82709B9C);
PPC_FUNC_IMPL(__imp__sub_82709B9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82709BA0"))) PPC_WEAK_FUNC(sub_82709BA0);
PPC_FUNC_IMPL(__imp__sub_82709BA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v30,3
	// clrlwi r11,r7,31
	r11.u64 = ctx.r7.u32 & 0x1;
	// vspltish v20,7
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// vspltish v2,1
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// vspltish v12,4
	// addi r10,r1,-224
	ctx.r10.s64 = ctx.r1.s64 + -224;
	// vspltish v0,2
	// addi r9,r6,1056
	ctx.r9.s64 = ctx.r6.s64 + 1056;
	// vspltish v31,5
	// addi r11,r1,-176
	r11.s64 = ctx.r1.s64 + -176;
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// li r31,1
	r31.s64 = 1;
	// vrlh v10,v12,v12
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stvx v30,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-112
	ctx.r10.s64 = ctx.r1.s64 + -112;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-80
	ctx.r9.s64 = ctx.r1.s64 + -80;
	// stvx v20,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-192
	r11.s64 = ctx.r1.s64 + -192;
	// vaddshs v29,v30,v11
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// vsubshs v19,v10,v11
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// stvx v31,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v2,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-64
	r11.s64 = ctx.r1.s64 + -64;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-208
	ctx.r10.s64 = ctx.r1.s64 + -208;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lwz r3,1100(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v29,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-256
	ctx.r10.s64 = ctx.r1.s64 + -256;
	// slw r9,r31,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r8.u8 & 0x3F));
	// stw r3,-236(r1)
	PPC_STORE_U32(ctx.r1.u32 + -236, ctx.r3.u32);
	// stvx v19,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r9,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, ctx.r9.u32);
	// bne cr6,0x82709dd0
	if (!cr6.eq) goto loc_82709DD0;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r31,r1,-256
	r31.s64 = ctx.r1.s64 + -256;
	// add r11,r8,r4
	r11.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v6,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vmrghb v5,v13,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v4,v13,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vperm v10,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v3,r0,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v28,r0,r8
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v8,v3,v28,v1
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrghb v9,v13,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x8270a004
	if (!cr6.gt) goto loc_8270A004;
	// li r8,0
	ctx.r8.s64 = 0;
loc_82709CE4:
	// vslh v3,v11,v12
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vslh v6,v11,v31
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vslh v1,v9,v2
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// vslh v7,v11,v0
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v5,v5,v0
	// vadduhm v6,v3,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v3,v9,v12
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// lvx128 v27,r0,r6
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v5,v13,v5
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// vadduhm v3,v1,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v1,v10,v0
	// vadduhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v26,v1,v10
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v6,v28,v27,v1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v3,v7,v3
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vslh v28,v8,v2
	// vslh v27,v4,v0
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v3,v3,v29
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vsubshs v27,v13,v27
	// vslh v1,v7,v0
	// vsubshs v1,v7,v1
	// vadduhm v5,v1,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v1,v10,v12
	// vadduhm v3,v3,v5
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v10,v31
	// vsrah v3,v3,v30
	// vadduhm v1,v1,v5
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v8,v12
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v28,v28,v5
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v6,v0
	// vsubshs v4,v6,v5
	// vor v5,v11,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v9,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vadduhm v7,v26,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v27,v4,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v4,v10,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v8,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v6,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v6,v7,v28
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v6,v6,v29
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v7,v6,v27
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v7,v7,v30
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82709ce4
	if (cr6.lt) goto loc_82709CE4;
	// b 0x8270a004
	goto loc_8270A004;
loc_82709DD0:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,16
	ctx.r8.s64 = 16;
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r28,r1,-160
	r28.s64 = ctx.r1.s64 + -160;
	// li r31,16
	r31.s64 = 16;
	// lvrx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r30,16
	r30.s64 = 16;
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v10,v9,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// li r29,32
	r29.s64 = 32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,-128
	r28.s64 = ctx.r1.s64 + -128;
	// vmrghb v4,v13,v11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// vmrglb v28,v13,v11
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v11,v4,v4
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// lvrx v8,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v9,v8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v6,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v5,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v5,v6,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vor v9,v28,v28
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v28.u8));
	// stvx v7,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-272
	ctx.r10.s64 = ctx.r1.s64 + -272;
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// stvx v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v3,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v4,v8,v6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvrx v1,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v8,v13,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrglb v6,v13,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v7,v13,v5
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v5,v13,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v3,v13,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x8270a004
	if (!cr6.gt) goto loc_8270A004;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_82709EA0:
	// vor v27,v11,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v11,v8,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// vor v22,v9,v9
	_mm_store_si128((__m128i*)v22.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// addi r31,r11,32
	r31.s64 = r11.s64 + 32;
	// vor v9,v6,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v4,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v8,v11,v0
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v23,v11,v12
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v24,v11,v31
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// lvx128 v26,r0,r31
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v27,v27,v0
	// vadduhm v25,v8,v11
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// addi r31,r10,-16
	r31.s64 = ctx.r10.s64 + -16;
	// vor v8,v5,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vadduhm v24,v23,v24
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vor v23,v10,v10
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v7,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v3,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vslh v5,v8,v12
	// vslh v3,v8,v2
	// vadduhm v25,v25,v24
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsubshs v27,v13,v27
	// vslh v24,v9,v12
	// vadduhm v21,v3,v5
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// lvx128 v5,r0,r6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v4,v28,v5,v1
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// lvsl v3,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vperm v5,v5,v26,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vslh v26,v9,v31
	// vslh v28,v9,v0
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// vadduhm v25,v25,v21
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vmrghb v3,v13,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vslh v22,v22,v0
	// vmrghb v1,v13,v5
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v26,v24,v26
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vmrglb v4,v13,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v28,v28,v9
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v24,v6,v2
	// vor v5,v3,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vor v3,v1,v1
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vslh v21,v4,v0
	// vadduhm v28,v28,v26
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v1,v5,v0
	// vsubshs v1,v5,v1
	// vadduhm v1,v1,v27
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v27,v6,v12
	// vor v26,v1,v1
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vadduhm v27,v24,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsubshs v24,v4,v21
	// vsubshs v21,v13,v22
	// vslh v22,v10,v0
	// vadduhm v1,v28,v27
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v27,v25,v29
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v28,v24,v21
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vslh v25,v7,v2
	// vadduhm v1,v1,v29
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v27,v27,v26
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v26,v10,v12
	// vslh v24,v23,v0
	// vadduhm v1,v1,v28
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v28,v10,v31
	// vsrah v27,v27,v30
	// vsubshs v24,v13,v24
	// vsrah v1,v1,v30
	// vadduhm v28,v26,v28
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v26,v7,v12
	// stvx v27,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v22,v22,v10
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stvx v1,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v26,v25,v26
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vslh v25,v3,v0
	// vsubshs v25,v3,v25
	// vadduhm v25,v25,v24
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v1,v22,v28
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v28,v1,v26
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vadduhm v28,v28,v29
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v1,v28,v25
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vsrah v1,v1,v30
	// stvx v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x82709ea0
	if (cr6.lt) goto loc_82709EA0;
loc_8270A004:
	// vspltish v0,8
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// vspltish v12,-1
	// vor v9,v13,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vspltish v11,0
	// vor v8,v30,v30
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)v30.u8));
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// vslh v3,v12,v0
	// bne cr6,0x8270a09c
	if (!cr6.eq) goto loc_8270A09C;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8270a13c
	if (!cr6.gt) goto loc_8270A13C;
loc_8270A034:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r7,4
	ctx.r7.s64 = 4;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v12,v0,v13,2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 14));
	// vsldoi v10,v0,v13,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 12));
	// vsldoi v7,v0,v13,6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 10));
	// vadduhm v13,v12,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v12,v13,v8
	// vsubshs v0,v9,v0
	// vadduhm v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v13,v13,v19
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsrah v0,v0,v20
	// vpkshus v13,v0,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v0,v13,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r11,r7
	ea = (r11.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x8270a034
	if (!cr6.eq) goto loc_8270A034;
	// b 0x8270a13c
	goto loc_8270A13C;
loc_8270A09C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8270a13c
	if (!cr6.gt) goto loc_8270A13C;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_8270A0A8:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v7,v13,v0,4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vsldoi v10,v13,v0,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v5,v0,v12,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 14));
	// vsldoi v4,v0,v12,4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 12));
	// vsldoi v12,v0,v12,6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 10));
	// vadduhm v10,v10,v7
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsldoi v6,v13,v0,6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vadduhm v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vslh v7,v10,v8
	// vadduhm v13,v13,v6
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v6,v5,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vsubshs v12,v9,v12
	// vadduhm v0,v10,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubshs v13,v9,v13
	// vadduhm v10,v0,v19
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vor v0,v6,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v13,v10,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vslh v10,v0,v8
	// vadduhm v0,v0,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v0,v0,v19
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vsrah v0,v13,v20
	// vsrah v13,v12,v20
	// vor v12,v11,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v11,v12,v13
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x8270a0a8
	if (!cr6.eq) goto loc_8270A0A8;
loc_8270A13C:
	// vand v0,v11,v3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vcmpgtuh. v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8270A154"))) PPC_WEAK_FUNC(sub_8270A154);
PPC_FUNC_IMPL(__imp__sub_8270A154) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8270A158"))) PPC_WEAK_FUNC(sub_8270A158);
PPC_FUNC_IMPL(__imp__sub_8270A158) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v3,3
	// clrlwi r11,r7,31
	r11.u64 = ctx.r7.u32 & 0x1;
	// vspltish v20,7
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// vspltisb v13,0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x0)));
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// vspltish v5,1
	// addi r10,r1,-208
	ctx.r10.s64 = ctx.r1.s64 + -208;
	// vspltish v12,4
	// addi r9,r6,1056
	ctx.r9.s64 = ctx.r6.s64 + 1056;
	// vspltish v0,2
	// addi r11,r1,-256
	r11.s64 = ctx.r1.s64 + -256;
	// vspltish v4,5
	// li r31,1
	r31.s64 = 1;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// vrlh v10,v12,v12
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// stvx v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-112
	ctx.r10.s64 = ctx.r1.s64 + -112;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-64
	ctx.r9.s64 = ctx.r1.s64 + -64;
	// stvx v20,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-176
	r11.s64 = ctx.r1.s64 + -176;
	// vaddshs v2,v3,v11
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubshs v19,v10,v11
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// stvx v4,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-192
	r11.s64 = ctx.r1.s64 + -192;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-80
	ctx.r10.s64 = ctx.r1.s64 + -80;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lwz r3,1100(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-240
	ctx.r10.s64 = ctx.r1.s64 + -240;
	// slw r9,r31,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (r31.u32 << (ctx.r8.u8 & 0x3F));
	// stw r3,-220(r1)
	PPC_STORE_U32(ctx.r1.u32 + -220, ctx.r3.u32);
	// stvx v19,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r9,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, ctx.r9.u32);
	// bne cr6,0x8270a3a0
	if (!cr6.eq) goto loc_8270A3A0;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r31,r1,-240
	r31.s64 = ctx.r1.s64 + -240;
	// add r11,r8,r4
	r11.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// lvx128 v6,r0,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v1,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lvsl v24,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)v24.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v31,r0,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-240
	ctx.r6.s64 = ctx.r1.s64 + -240;
	// vmrghb v9,v13,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v8,v13,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-240
	ctx.r6.s64 = ctx.r1.s64 + -240;
	// vperm v10,v6,v31,v1
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// stvx v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvx128 v29,r0,r8
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-256
	ctx.r8.s64 = ctx.r1.s64 + -256;
	// lvx128 v30,r0,r11
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v10,v13,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v24,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-256
	ctx.r8.s64 = ctx.r1.s64 + -256;
	// lvx128 v7,r0,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v6,v30,v29,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v7,v13,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v6,v13,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x8270a5dc
	if (!cr6.gt) goto loc_8270A5DC;
	// li r8,0
	ctx.r8.s64 = 0;
loc_8270A2AC:
	// vor v1,v9,v9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v11,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vor v11,v7,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v28,v1,v0
	// vslh v29,v9,v5
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vslh v30,v11,v12
	// vslh v31,v11,v4
	// lvx128 v27,r0,r6
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v7,v11,v0
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// vadduhm v31,v30,v31
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vslh v30,v9,v12
	// vadduhm v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v30,v29,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vsubshs v29,v1,v28
	// lvx128 v28,r0,r11
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v7,v31
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vor v1,v8,v8
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v10,v10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v6,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v31,v7,v30
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vperm v7,v28,v27,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vslh v27,v8,v5
	// vmrghb v6,v13,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v28,v31,v2
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vmrglb v30,v13,v7
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v7,v6,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v30,v30
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v30.u8));
	// vslh v30,v7,v0
	// vslh v26,v6,v0
	// vsubshs v30,v13,v30
	// vadduhm v31,v29,v30
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v10,v12
	// vslh v30,v10,v4
	// vadduhm v28,v28,v31
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vslh v31,v10,v0
	// vadduhm v30,v29,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v29,v8,v12
	// vadduhm v31,v31,v10
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v29,v27,v29
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v27,v1,v0
	// vsubshs v1,v1,v27
	// vsubshs v27,v13,v26
	// vadduhm v27,v1,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v1,v28,v3
	// stvx v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v1,v31,v30
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// vadduhm v31,v1,v29
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vadduhm v31,v31,v2
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v1,v31,v27
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vsrah v1,v1,v3
	// stvx v1,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x8270a2ac
	if (cr6.lt) goto loc_8270A2AC;
	// b 0x8270a5dc
	goto loc_8270A5DC;
loc_8270A3A0:
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,16
	ctx.r8.s64 = 16;
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r28,r1,-160
	r28.s64 = ctx.r1.s64 + -160;
	// li r31,16
	r31.s64 = 16;
	// lvrx v10,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r30,16
	r30.s64 = 16;
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// li r29,32
	r29.s64 = 32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stvx v11,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,-128
	r28.s64 = ctx.r1.s64 + -128;
	// vmrghb v31,v13,v11
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// vmrglb v28,v13,v11
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v9,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvrx v8,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// lvlx v10,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v10,v10,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v7,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v1,v7,v6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vor v8,v31,v31
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)v31.u8));
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-272
	ctx.r10.s64 = ctx.r1.s64 + -272;
	// vor v6,v28,v28
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v28.u8));
	// stw r11,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r11.u32);
	// stvx v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v30,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v31,v11,v7
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v29,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	_mm_store_si128((__m128i*)v29.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v7,v13,v9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vor v29,v30,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_or_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vmrghb v11,v13,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v9,v13,v10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v10,v13,v1
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v1,v13,v31
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v30,v13,v31
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v31,v13,v29
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// ble cr6,0x8270a5dc
	if (!cr6.gt) goto loc_8270A5DC;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_8270A470:
	// vor v29,v8,v8
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v8,v11,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vor v11,v1,v1
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// addi r31,r11,32
	r31.s64 = r11.s64 + 32;
	// vslh v21,v29,v0
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// vslh v22,v8,v5
	// lvx128 v26,r0,r11
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v28,v11,v0
	// lvsl v1,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslh v27,v11,v4
	// vslh v23,v11,v12
	// lvx128 v25,r0,r31
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubshs v29,v29,v21
	// addi r31,r10,-16
	r31.s64 = ctx.r10.s64 + -16;
	// vadduhm v24,v28,v11
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vor v28,v7,v7
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v10,v10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v31,v31
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)v31.u8));
	// vslh v31,v8,v12
	// vadduhm v23,v23,v27
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vor v27,v6,v6
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v9,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v30,v30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v30.u8));
	// lvsl v30,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vadduhm v22,v22,v31
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v31.u16)));
	// lvx128 v31,r0,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vadduhm v24,v24,v23
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v23,v9,v12
	// vadduhm v24,v24,v22
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v22.u16)));
	// stvx v30,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-256
	ctx.r6.s64 = ctx.r1.s64 + -256;
	// vperm v30,v26,v31,v1
	_mm_store_si128((__m128i*)v30.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vslh v22,v6,v5
	// vslh v26,v9,v0
	// lvx128 v1,r0,r6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r10,-32
	ctx.r6.s64 = ctx.r10.s64 + -32;
	// vperm v1,v31,v25,v1
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vslh v25,v9,v4
	// vadduhm v26,v26,v9
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrghb v31,v13,v1
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v25,v23,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vmrghb v1,v13,v30
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v30,v13,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v26,v26,v25
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v21,v1,v0
	// vsubshs v23,v13,v21
	// vslh v21,v27,v0
	// vadduhm v29,v29,v23
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v23,v6,v12
	// vsubshs v27,v27,v21
	// vslh v21,v10,v0
	// vor v25,v29,v29
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)v29.u8));
	// vadduhm v23,v22,v23
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vslh v22,v30,v0
	// vadduhm v21,v21,v10
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubshs v22,v13,v22
	// vadduhm v29,v27,v22
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vadduhm v27,v26,v23
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v26,v24,v2
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v24,v7,v5
	// vslh v23,v31,v0
	// vadduhm v27,v27,v2
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v26,v26,v25
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v25,v10,v12
	// vadduhm v29,v27,v29
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v27,v10,v4
	// vsrah v29,v29,v3
	// vadduhm v27,v25,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v25,v7,v12
	// vadduhm v25,v24,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v24,v28,v0
	// vsubshs v28,v28,v24
	// vsubshs v24,v13,v23
	// vadduhm v24,v28,v24
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v28,v26,v3
	// stvx v28,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v29,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v29,v21,v27
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)v27.u16)));
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// vadduhm v28,v29,v25
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v28,v28,v2
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v29,v28,v24
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsrah v29,v29,v3
	// stvx v29,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// blt cr6,0x8270a470
	if (cr6.lt) goto loc_8270A470;
loc_8270A5DC:
	// vspltish v0,8
	// vor v8,v3,v3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vspltish v12,-1
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// vspltish v11,0
	// vor v9,v13,v13
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// vslh v3,v12,v0
	// bne cr6,0x8270a674
	if (!cr6.eq) goto loc_8270A674;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8270a714
	if (!cr6.gt) goto loc_8270A714;
loc_8270A60C:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r7,4
	ctx.r7.s64 = 4;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v12,v0,v13,2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 14));
	// vsldoi v10,v0,v13,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 12));
	// vsldoi v7,v0,v13,6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 10));
	// vadduhm v13,v12,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v12,v13,v8
	// vsubshs v0,v9,v0
	// vadduhm v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v13,v13,v19
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsrah v0,v0,v20
	// vpkshus v13,v0,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v0,v13,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r11,r7
	ea = (r11.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x8270a60c
	if (!cr6.eq) goto loc_8270A60C;
	// b 0x8270a714
	goto loc_8270A714;
loc_8270A674:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8270a714
	if (!cr6.gt) goto loc_8270A714;
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
loc_8270A680:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v7,v13,v0,4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vsldoi v10,v13,v0,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v5,v0,v12,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 14));
	// vsldoi v4,v0,v12,4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 12));
	// vsldoi v12,v0,v12,6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 10));
	// vadduhm v10,v10,v7
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsldoi v6,v13,v0,6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vadduhm v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vslh v7,v10,v8
	// vadduhm v13,v13,v6
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v6,v5,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vsubshs v12,v9,v12
	// vadduhm v0,v10,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubshs v13,v9,v13
	// vadduhm v10,v0,v19
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vor v0,v6,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v13,v10,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vslh v10,v0,v8
	// vadduhm v0,v0,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v0,v0,v19
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)v19.u16)));
	// vadduhm v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vsrah v0,v13,v20
	// vsrah v13,v12,v20
	// vor v12,v11,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v11,v12,v13
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x8270a680
	if (!cr6.eq) goto loc_8270A680;
loc_8270A714:
	// vand v0,v11,v3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vcmpgtuh. v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8270A72C"))) PPC_WEAK_FUNC(sub_8270A72C);
PPC_FUNC_IMPL(__imp__sub_8270A72C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8270A730"))) PPC_WEAK_FUNC(sub_8270A730);
PPC_FUNC_IMPL(__imp__sub_8270A730) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v23{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vspltish v0,4
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// addi r10,r6,1056
	ctx.r10.s64 = ctx.r6.s64 + 1056;
	// lwz r30,1100(r6)
	r30.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1100);
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// vspltish v1,1
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// vspltish v24,7
	// rlwinm r11,r31,1,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// vrlh v0,v0,v0
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lvx128 v2,r0,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r6,r11,3
	ctx.r6.s64 = r11.s64 + 3;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// vsubshs v23,v0,v2
	// bl 0x82700aa0
	sub_82700AA0(ctx, base);
	// clrlwi r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	// li r9,1
	ctx.r9.s64 = 1;
	// vspltish v0,8
	// addi r8,r11,3
	ctx.r8.s64 = r11.s64 + 3;
	// vspltish v13,-1
	// vspltisb v9,0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_set1_epi8(char(0x0)));
	// mr r11,r29
	r11.u64 = r29.u64;
	// vspltish v8,3
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// vspltish v11,0
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// vslh v3,v13,v0
	// slw r9,r9,r8
	ctx.r9.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// bne cr6,0x8270a828
	if (!cr6.eq) goto loc_8270A828;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8270a8c8
	if (!cr6.gt) goto loc_8270A8C8;
loc_8270A7C0:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r7,4
	ctx.r7.s64 = 4;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v12,v0,v13,2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 14));
	// vsldoi v10,v0,v13,4
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 12));
	// vsldoi v7,v0,v13,6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 10));
	// vadduhm v13,v12,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v12,v13,v8
	// vsubshs v0,v9,v0
	// vadduhm v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v13,v13,v23
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsrah v0,v0,v24
	// vpkshus v13,v0,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v0,v13,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r11,r7
	ea = (r11.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x8270a7c0
	if (!cr6.eq) goto loc_8270A7C0;
	// b 0x8270a8c8
	goto loc_8270A8C8;
loc_8270A828:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8270a8c8
	if (!cr6.gt) goto loc_8270A8C8;
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
loc_8270A834:
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v7,v13,v0,4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 12));
	// vsldoi v10,v13,v0,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v5,v0,v12,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 14));
	// vsldoi v4,v0,v12,4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 12));
	// vsldoi v12,v0,v12,6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 10));
	// vadduhm v10,v10,v7
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsldoi v6,v13,v0,6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 10));
	// vadduhm v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vslh v7,v10,v8
	// vadduhm v13,v13,v6
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v6,v5,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vsubshs v12,v9,v12
	// vadduhm v0,v10,v7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubshs v13,v9,v13
	// vadduhm v10,v0,v23
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vor v0,v6,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v13,v10,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vslh v10,v0,v8
	// vadduhm v0,v0,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v0,v0,v23
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vsrah v0,v13,v24
	// vsrah v13,v12,v24
	// vor v12,v11,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vpkshus v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v11,v12,v13
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x8270a834
	if (!cr6.eq) goto loc_8270A834;
loc_8270A8C8:
	// vand v0,v11,v3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vcmpgtuh. v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_cmpgt_epu16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// mfocrf r11,2
	r11.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8270A8E4"))) PPC_WEAK_FUNC(sub_8270A8E4);
PPC_FUNC_IMPL(__imp__sub_8270A8E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8270A8E8"))) PPC_WEAK_FUNC(sub_8270A8E8);
PPC_FUNC_IMPL(__imp__sub_8270A8E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcf4
	// addi r31,r3,16
	r31.s64 = ctx.r3.s64 + 16;
	// lvx128 v11,r0,r3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r3,r4
	ctx.r9.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lvsl v7,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// vspltish v13,2
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// vspltish v12,4
	// lvx128 v10,r0,r31
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r9,16
	r31.s64 = ctx.r9.s64 + 16;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vperm v11,v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// addi r28,r11,16
	r28.s64 = r11.s64 + 16;
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// add r30,r11,r4
	r30.u64 = r11.u64 + ctx.r4.u64;
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r31
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r7,8
	cr6.compare<int32_t>(ctx.r7.s32, 8, xer);
	// vperm v10,v10,v8,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v6,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// lvx128 v8,r0,r28
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r30,16
	r11.s64 = r30.s64 + 16;
	// vperm v9,v9,v8,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-80
	ctx.r7.s64 = ctx.r1.s64 + -80;
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// rlwinm r29,r6,1,0,30
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// vperm v8,v8,v6,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v6,r0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v7,v0,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v11,r0,r30
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r5,r6
	r11.u64 = ctx.r5.u64 + ctx.r6.u64;
	// vperm v6,v11,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// add r31,r29,r5
	r31.u64 = r29.u64 + ctx.r5.u64;
	// vmrghb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// add r9,r31,r6
	ctx.r9.u64 = r31.u64 + ctx.r6.u64;
	// vmrghb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v6,v11,v7
	// vsubshs v5,v10,v11
	// vsubshs v4,v9,v10
	// vslh v6,v6,v13
	// vslh v5,v5,v13
	// vsubshs v3,v8,v9
	// vslh v4,v4,v13
	// vaddshs v6,v6,v1
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v5,v5,v1
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vslh v3,v3,v13
	// vaddshs v4,v4,v1
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsrah v6,v6,v12
	// vsrah v5,v5,v12
	// vaddshs v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsrah v4,v4,v12
	// stvx v6,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-64
	ctx.r7.s64 = ctx.r1.s64 + -64;
	// vaddshs v11,v5,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsrah v3,v3,v12
	// vaddshs v10,v4,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// stvx v4,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// addi r7,r1,-80
	ctx.r7.s64 = ctx.r1.s64 + -80;
	// stvx v11,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v11,v3,v9
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// addi r30,r1,-64
	r30.s64 = ctx.r1.s64 + -64;
	// vpkshus v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// stvx v11,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r7,-80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lwz r30,-72(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// stw r7,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r7.u32);
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// lwz r7,-64(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// lwz r30,-56(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// stw r7,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r7.u32);
	// stw r30,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r30.u32);
	// bne cr6,0x8270aa5c
	if (!cr6.eq) goto loc_8270AA5C;
	// lwz r7,-76(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r30,-68(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// lwz r28,-60(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// lwz r27,-52(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// stw r7,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r7.u32);
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// stw r28,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r28.u32);
	// stw r27,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r27.u32);
loc_8270AA5C:
	// cmpwi cr6,r8,8
	cr6.compare<int32_t>(ctx.r8.s32, 8, xer);
	// bne cr6,0x8270aba4
	if (!cr6.eq) goto loc_8270ABA4;
	// add r11,r10,r4
	r11.u64 = ctx.r10.u64 + ctx.r4.u64;
	// rlwinm r10,r4,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r11,r10,r4
	r11.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v11,v9,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// lvx128 v9,r0,r7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v10,v10,v9,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-64
	ctx.r9.s64 = ctx.r1.s64 + -64;
	// vperm v9,v9,v6,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvsl v7,r0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vmrghb v5,v0,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r0,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r29,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// vperm v7,v10,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v9,v11,v8
	// add r11,r10,r5
	r11.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// vmrghb v0,v0,v7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v9,v9,v13
	// add r8,r10,r6
	ctx.r8.u64 = ctx.r10.u64 + ctx.r6.u64;
	// vsubshs v7,v10,v11
	// vsubshs v6,v0,v10
	// vaddshs v9,v9,v1
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsubshs v5,v5,v0
	// vslh v7,v7,v13
	// vslh v6,v6,v13
	// vsrah v9,v9,v12
	// vslh v13,v5,v13
	// vaddshs v7,v7,v1
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v6,v6,v1
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-80
	ctx.r9.s64 = ctx.r1.s64 + -80;
	// vaddshs v13,v13,v1
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vsrah v7,v7,v12
	// vsrah v6,v6,v12
	// vsrah v13,v13,v12
	// vaddshs v12,v9,v8
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v6,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v10,v6,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// add r9,r11,r6
	ctx.r9.u64 = r11.u64 + ctx.r6.u64;
	// vaddshs v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vpkshus v13,v12,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vpkshus v0,v10,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// addi r7,r1,-64
	ctx.r7.s64 = ctx.r1.s64 + -64;
	// stvx v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-80
	ctx.r7.s64 = ctx.r1.s64 + -80;
	// stvx v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r7,-64(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// lwz r6,-60(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// lwz r5,-56(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// lwz r4,-52(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// lwz r3,-80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lwz r31,-76(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r7,-72(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// stw r5,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r5.u32);
	// stw r4,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r4.u32);
	// stw r3,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r3.u32);
	// stw r31,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r31.u32);
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// stw r11,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r11.u32);
loc_8270ABA4:
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8270ABA8"))) PPC_WEAK_FUNC(sub_8270ABA8);
PPC_FUNC_IMPL(__imp__sub_8270ABA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// vspltish v13,2
	// addi r11,r1,-128
	r11.s64 = ctx.r1.s64 + -128;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// rlwinm r30,r6,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v12,1
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// cmpwi cr6,r8,8
	cr6.compare<int32_t>(ctx.r8.s32, 8, xer);
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// vsrah v11,v1,v13
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-160
	r11.s64 = ctx.r1.s64 + -160;
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// stw r30,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, r30.u32);
	// stw r9,-220(r1)
	PPC_STORE_U32(ctx.r1.u32 + -220, ctx.r9.u32);
	// stw r10,-216(r1)
	PPC_STORE_U32(ctx.r1.u32 + -216, ctx.r10.u32);
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-208
	r11.s64 = ctx.r1.s64 + -208;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r30,r5
	r11.u64 = r30.u64 + ctx.r5.u64;
	// stw r11,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r11.u32);
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r4,r11
	ctx.r7.u64 = ctx.r4.u64 + r11.u64;
	// stw r7,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, ctx.r7.u32);
	// bgt cr6,0x8270ae80
	if (cr6.gt) goto loc_8270AE80;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r11,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r11.u32);
	// ble cr6,0x8270b058
	if (!cr6.gt) goto loc_8270B058;
	// b 0x8270ac5c
	goto loc_8270AC5C;
loc_8270AC38:
	// addi r11,r1,-160
	r11.s64 = ctx.r1.s64 + -160;
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,-220(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -220);
	// lwz r7,-332(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-208
	r11.s64 = ctx.r1.s64 + -208;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-128
	r11.s64 = ctx.r1.s64 + -128;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8270AC5C:
	// addi r11,r1,-304
	r11.s64 = ctx.r1.s64 + -304;
	// lvlx v10,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-240
	r11.s64 = ctx.r1.s64 + -240;
	// lvlx v9,r3,r4
	temp.u32 = ctx.r3.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-256
	r11.s64 = ctx.r1.s64 + -256;
	// lvlx v8,r3,r9
	temp.u32 = ctx.r3.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-288
	r11.s64 = ctx.r1.s64 + -288;
	// lvlx v7,r3,r7
	temp.u32 = ctx.r3.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-272
	r11.s64 = ctx.r1.s64 + -272;
	// lvlx v6,r3,r10
	temp.u32 = ctx.r3.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r3,16
	r11.s64 = ctx.r3.s64 + 16;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lvrx v5,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v10,v10,v5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvrx v4,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvrx v3,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvrx v2,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v8,v8,v3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// lvrx v1,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,-304
	r11.s64 = ctx.r1.s64 + -304;
	// vmrghb v5,v0,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v7,v7,v2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v6,v6,v1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v7,v5,v10
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vmrghb v0,v0,v6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-240
	r11.s64 = ctx.r1.s64 + -240;
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-256
	r11.s64 = ctx.r1.s64 + -256;
	// vaddshs v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-288
	r11.s64 = ctx.r1.s64 + -288;
	// vslh v10,v10,v12
	// vaddshs v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-272
	r11.s64 = ctx.r1.s64 + -272;
	// vaddshs v10,v10,v11
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vslh v9,v9,v12
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// vaddshs v0,v8,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// vsrah v10,v10,v13
	// addi r11,r1,-176
	r11.s64 = ctx.r1.s64 + -176;
	// vaddshs v9,v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vslh v8,v7,v12
	// vslh v0,v0,v12
	// vsrah v9,v9,v13
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-320
	r11.s64 = ctx.r1.s64 + -320;
	// vaddshs v12,v8,v11
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v0,v0,v11
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-144
	r11.s64 = ctx.r1.s64 + -144;
	// vsrah v12,v12,v13
	// vsrah v13,v0,v13
	// vpkshus v0,v12,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-192
	r11.s64 = ctx.r1.s64 + -192;
	// vpkshus v13,v9,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-320
	r11.s64 = ctx.r1.s64 + -320;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// stvewx v13,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// bne cr6,0x8270adf8
	if (!cr6.eq) goto loc_8270ADF8;
	// vspltw v12,v0,1
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xAA));
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// vspltw v10,v0,2
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x55));
	// vspltw v0,v0,3
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x0));
	// stvewx v12,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stvewx v10,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r11,r1,-320
	r11.s64 = ctx.r1.s64 + -320;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,-336(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// vspltw v12,v0,1
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xAA));
	// vspltw v10,v0,2
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x55));
	// vspltw v0,v0,3
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x0));
	// stvewx v13,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-336(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stvewx v12,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// stvewx v10,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// b 0x8270ae30
	goto loc_8270AE30;
loc_8270ADF8:
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// vspltw v0,v0,2
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x55));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r11,r1,-320
	r11.s64 = ctx.r1.s64 + -320;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,-336(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// vspltw v0,v0,2
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x55));
	// stvewx v13,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-336(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// stvewx v0,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
loc_8270AE30:
	// lwz r10,-224(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -224);
	// lwz r7,20(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,-328(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
	// addi r8,r10,4
	ctx.r8.s64 = ctx.r10.s64 + 4;
	// lwz r10,-216(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -216);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r3,r10,r7
	ctx.r3.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lwz r7,60(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// stw r8,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, ctx.r8.u32);
	// lwz r8,36(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// add r5,r9,r8
	ctx.r5.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r11,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x8270ac38
	if (cr6.lt) goto loc_8270AC38;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_8270AE80:
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r31,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r31.u32);
	// ble cr6,0x8270b058
	if (!cr6.gt) goto loc_8270B058;
loc_8270AE90:
	// addi r11,r1,-304
	r11.s64 = ctx.r1.s64 + -304;
	// lvlx v10,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-240
	r11.s64 = ctx.r1.s64 + -240;
	// lvlx v9,r3,r4
	temp.u32 = ctx.r3.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-256
	r11.s64 = ctx.r1.s64 + -256;
	// lvlx v8,r3,r9
	temp.u32 = ctx.r3.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-288
	r11.s64 = ctx.r1.s64 + -288;
	// lvlx v7,r3,r7
	temp.u32 = ctx.r3.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-272
	r11.s64 = ctx.r1.s64 + -272;
	// lvlx v6,r3,r10
	temp.u32 = ctx.r3.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r3,16
	r11.s64 = ctx.r3.s64 + 16;
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// lvrx v5,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v10,v10,v5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// lvrx v2,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v7,v7,v2
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvrx v4,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvrx v3,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvrx v1,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,-80
	r11.s64 = ctx.r1.s64 + -80;
	// vmrghb v2,v0,v10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v8,v8,v3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vmrghb v3,v0,v7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v6,v6,v1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vmrghb v5,v0,v9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v31,v0,v10
	_mm_store_si128((__m128i*)v31.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v4,v0,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v1,v0,v6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v9,v2,v5
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// stvx v2,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// vaddshs v2,v31,v10
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// vaddshs v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vslh v2,v2,v12
	// stvx v4,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-64
	r11.s64 = ctx.r1.s64 + -64;
	// vslh v5,v5,v12
	// vaddshs v2,v2,v11
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v3,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-112
	r11.s64 = ctx.r1.s64 + -112;
	// vaddshs v5,v5,v11
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v2,v2,v13
	// stvx v1,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-304
	r11.s64 = ctx.r1.s64 + -304;
	// vsrah v5,v5,v13
	// stvx v31,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v31,v9,v12
	// vmrglb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r1,-240
	r11.s64 = ctx.r1.s64 + -240;
	// vaddshs v8,v10,v9
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-256
	r11.s64 = ctx.r1.s64 + -256;
	// vmrglb v10,v0,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v7,v0,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v6,v31,v11
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vslh v8,v8,v12
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-288
	r11.s64 = ctx.r1.s64 + -288;
	// vaddshs v9,v9,v10
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v8,v8,v11
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v6,v6,v13
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-272
	r11.s64 = ctx.r1.s64 + -272;
	// vsrah v8,v8,v13
	// vaddshs v10,v10,v7
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vpkshus v6,v6,v2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vslh v9,v9,v12
	// vpkshus v8,v5,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v5,v4,v3
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v4,v3,v1
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vslh v10,v10,v12
	// vaddshs v9,v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r11,r1,-320
	r11.s64 = ctx.r1.s64 + -320;
	// vslh v7,v5,v12
	// stvx v6,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v10,v10,v11
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx128 v8,r5,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v8,v4,v12
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// vsrah v9,v9,v13
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vaddshs v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// cmpw cr6,r31,r8
	cr6.compare<int32_t>(r31.s32, ctx.r8.s32, xer);
	// vsrah v10,v10,v13
	// vaddshs v8,v8,v11
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-144
	r11.s64 = ctx.r1.s64 + -144;
	// vsrah v7,v7,v13
	// stw r31,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r31.u32);
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// vsrah v8,v8,v13
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// add r11,r30,r5
	r11.u64 = r30.u64 + ctx.r5.u64;
	// addi r5,r1,-192
	ctx.r5.s64 = ctx.r1.s64 + -192;
	// vpkshus v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v9,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-176
	ctx.r5.s64 = ctx.r1.s64 + -176;
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v10,r11,r6
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r5,r30,r11
	ctx.r5.u64 = r30.u64 + r11.u64;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x8270ae90
	if (cr6.lt) goto loc_8270AE90;
loc_8270B058:
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8270B064"))) PPC_WEAK_FUNC(sub_8270B064);
PPC_FUNC_IMPL(__imp__sub_8270B064) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

